filename: find_search-for-duplicate-file-names_multif_20160419.txt
http://askubuntu.com/questions/48524/search-for-duplicate-file-names-within-folder-hierarchy

Search for duplicate file names within folder hierarchy?

   I have a folder called img, this folder has many levels of sub-folders, all of which containing
   images. I am going to import them into an image server.

   Normally images (or any files) can have the same name as long as they are in a different directory
   path or have a different extension. However, the image server I am importing them into requires all
   the image names to be unique (even if the extensions are different).

   For example the images background.png and background.gif would not be allowed because even though
   they have different extensions they still have the same file name. Even if they are in separate
   sub-folders, they still need to be unique.

   So I am wondering if I can do a recursive search in the img folder to find a list of files that have
   the same name (excluding extension).

   Is there a command that can do this?

***
   This is much tougher than I thought initially. I want a one liner but most everything forces you into
   an inellegant solution of creating something exec calls. Tried for an hour. I can get it to work on
   Linx but not my AIX or HP boxes. – [54]ojblass Jun 13 '11 at 21:58

   alright it performs like hell but it works – [55]ojblass Jun 13 '11 at 22:20

***
   FSlint
   The FSlint package for Ubuntu emphasizes the graphical interface, but as is explained in the
   FSlint FAQ a command-line interface is available via the programs in /usr/share/fslint/fslint/.
   Use the --help option for documentation, e.g.:
	$ /usr/share/fslint/fslint/fslint --help
	File system lint.
	A collection of utilities to find lint on a filesystem.
	To get more info on each utility run 'util --help'.

	findup -- find DUPlicate files
	findnl -- find Name Lint (problems with filenames)
	findu8 -- find filenames with invalid utf8 encoding
	findbl -- find Bad Links (various problems with symlinks)
	findsn -- find Same Name (problems with clashing names)
	finded -- find Empty Directories
	findid -- find files with dead user IDs
	findns -- find Non Stripped executables
	findrs -- find Redundant Whitespace in files
	findtf -- find Temporary Files
	findul -- find possibly Unused Libraries
	zipdir -- Reclaim wasted space in ext2 directory entries
	$ /usr/share/fslint/fslint/findsn --help
	find (files) with duplicate or conflicting names.
	Usage: findsn [-A -c -C] [[-r] [-f] paths(s) ...]

	If no arguments are supplied the $PATH is searched for any redundant
	or conflicting files.

	-A reports all aliases (soft and hard links) to files.
	If no path(s) specified then the $PATH is searched.

	If only path(s) specified then they are checked for duplicate named
	files. You can qualify this with -C to ignore case in this search.
	Qualifying with -c is more restictive as only files (or directories)
	in the same directory whose names differ only in case are reported.
	I.E. -c will flag files & directories that will conflict if transfered
	to a case insensitive file system. Note if -c or -C specified and
	no path(s) specifed the current directory is assumed.

   Example usage:
	$ /usr/share/fslint/fslint/findsn /usr/share/icons/ > icons-with-duplicate-names.txt
	$ head icons-with-duplicate-names.txt
	-rw-r--r-- 1 root root    683 2011-04-15 10:31 Humanity-Dark/AUTHORS
	-rw-r--r-- 1 root root    683 2011-04-15 10:31 Humanity/AUTHORS
	-rw-r--r-- 1 root root  17992 2011-04-15 10:31 Humanity-Dark/COPYING
	-rw-r--r-- 1 root root  17992 2011-04-15 10:31 Humanity/COPYING
	-rw-r--r-- 1 root root   4776 2011-03-29 08:57 Faenza/apps/16/DC++.xpm
	-rw-r--r-- 1 root root   3816 2011-03-29 08:57 Faenza/apps/22/DC++.xpm
	-rw-r--r-- 1 root root   4008 2011-03-29 08:57 Faenza/apps/24/DC++.xpm
	-rw-r--r-- 1 root root   4456 2011-03-29 08:57 Faenza/apps/32/DC++.xpm
	-rw-r--r-- 1 root root   7336 2011-03-29 08:57 Faenza/apps/48/DC++.xpm
	-rw-r--r-- 1 root root    918 2011-03-29 09:03 Faenza/apps/16/Thunar.png

***
	find . -exec basename {} \; | sed 's/\(.*\)\..*/\1/' | sort | uniq -c | grep -v "^[ \t]*1 "

   As the comment states this will find folders as well.

   Here is the command to restrict it to files:
   	find . -type f -exec basename {} \; | sed 's/\(.*\)\..*/\1/' | sort | uniq -c | grep -v "^[ \t]*1 "

***   
   I'm assuming you only need to see these "duplicates", then handle them manually. If so, this bash4
   code should do what you want I think:
	declare -A array=() dupes=()
	while IFS= read -r -d '' file; do
	    base=${file##*/} base=${base%.*}
	    if [[ ${array[$base]} ]]; then
		dupes[$base]+=" $file"
	    else
		array[$base]=$file
	    fi
	done < <(find /the/dir -type f -print0)

	for key in "${!dupes[@]}"; do
	    echo "$key: ${array[$key]}${dupes[$key]}"
	done

   See http://mywiki.wooledge.org/BashGuide/Arrays#Associative_Arrays and/or the bash manual for
   help on the associative array syntax.

***   
Save this to a file named duplicates.py
	#/usr/bin/env python

	# Syntax: duplicates.py DIRECTORY

	import os, sys

	top = sys.argv[1]
	d = {}

	for root, dirs, files in os.walk(top, topdown=False):
	    for name in files:
		fn = os.path.join(root, name)
		basename, extension = os.path.splitext(name)

		basename = basename.lower() # ignore case

		if d.has_key(basename):
		    print d[basename]
		    print fn
		else:
		    d[basename] = fn

   Then make the file executable:
	chmod +x duplicates.py

   Run in e.g. like this:
	./duplicates.py ~/images

   It should output pairs of files that have the same basename(1). Written in python, you should be able
   to modify it.

***   
   This is bname:
	#!/bin/bash
	#
	#  find for jpg/png/gif more files of same basename
	#
	# echo "processing ($1) $2"
	bname=$(basename "$1" .$2)
	find -name "$bname.jpg" -or -name "$bname.png"

	   Make it executable:
	chmod a+x bname

   Invoke it:
	for ext in jpg png jpeg gif tiff; do find -name "*.$ext" -exec ./bname "{}" $ext ";"  ; done

   Pro:
     * It's straightforward and simple, therefore extensible.
     * Handles blanks, tabs, linebreaks and pagefeeds in filenames, afaik. (Assuming no such thing in
       the extension-name).

   Con:
     * It finds always the file itself, and if it finds a.gif for a.jpg, it will find a.jpg for a.gif
       too. So for 10 files of same basename, it finds 100 matches in the end.

       
       
---
http://unix.stackexchange.com/questions/71176/find-duplicate-files

... (middle part) ...
You can also do this manually if you don't want to or cannot install third party tools. The way most such
programs work is by calculating file checksums. Files with the same md5sum almost certainly contain exactly
the same data. So, you could do something like this:
	find / -type f -exec md5sum {} \; > md5sums
	gawk '{print $1}' md5sums | sort | uniq -d > dupes
	while read d; do echo "---"; grep $d md5sums | cut -d ' ' -f 2-; done < dupes
	
Sample output (the file names in this example are the same, but it will also work when they are different):
	$ while read d; do echo "---"; grep $d md5sums | cut -d ' ' -f 2-; done < dupes 
	---
	 /usr/src/linux-headers-3.2.0-3-common/include/linux/if_bonding.h
	 /usr/src/linux-headers-3.2.0-4-common/include/linux/if_bonding.h
	---
	 /usr/src/linux-headers-3.2.0-3-common/include/linux/route.h
	 /usr/src/linux-headers-3.2.0-4-common/include/linux/route.h
	---
	 /usr/src/linux-headers-3.2.0-3-common/include/drm/Kbuild
	 /usr/src/linux-headers-3.2.0-4-common/include/drm/Kbuild
	---
	
This will be much slower than the dedicated tools already mentioned, but it will work.

*** 	
It would be much, much faster to find any files with the same size as another file using st_size, eliminating
any that only have one file of this size, and then calculating md5sums only between files with the same st_size.

***	 	
@ChrisDown yeah, just wanted to keep it simple. What you suggest will greatly speed things up of course.
That's why I have the disclaimer about it being slow at the end of my answer. – terdon♦ Apr 4 '13 at 16:37

***
Short answer: yes.

Longer version: have a look at the wikipedia fdupes entry, it sports quite nice list of ready made solutions.
Of course you can write your own, it's not that difficult - hashing programs like diff, sha*sum, find, sort
and uniq should do the job. You can even put it on one line, and it will still be understandable.

***
If you believe a hash function (here MD5) is collision free on your domain:
	find $target -type f -exec md5sum '{}' + | sort | uniq --all-repeated --check-chars=32 \
	| cut --characters=35-
	 
Wanna identical file names grouped? Write a simple script not_uniq.sh to format output:
	#!/bin/bash

	last_checksum=0
	while read line; do
	    checksum=${line:0:32}
	    filename=${line:34}
	    if [ $checksum == $last_checksum ]; then
		if [ ${last_filename:-0} != '0' ]; then
		    echo $last_filename
		    unset last_filename
		fi
		echo $filename
	    else
		if [ ${last_filename:-0} == '0' ]; then
		    echo "======="
		fi
		last_filename=$filename
	    fi

	    last_checksum=$checksum
	done
	
Then change find command to use your script:
	chmod +x not_uniq.sh
	find $target -type f -exec md5sum '{}' + | sort | not_uniq.sh

This is basic idea. Probably you should change find if your file names containing some characters. (e.g space)
... (middle part) ...

---
http://www.commandlinefu.com/commands/view/3555/find-duplicate-files-based-on-size-first-then-md5-hash

Find Duplicate Files (based on size first, then MD5 hash)
Terminal - Find Duplicate Files (based on size first, then MD5 hash)
	find -not -empty -type f -printf "%s\n" | sort -rn | uniq -d | xargs -I{} -n1 find -type f -size {}c \
	-print0 | xargs -0 md5sum | sort | uniq -w32 --all-repeated=separate
	
This dup finder saves time by comparing size first, then md5sum, it doesn't delete anything, just lists them.

***
Terminal - Alternatives:
	find -type f -exec md5sum '{}' ';' | sort | uniq --all-repeated=separate -w 33 | cut -c 35-

Calculates md5 sum of files. sort (required for uniq to work). uniq based on only the hash. use cut ro remove
the hash from the result.

***
	fdupes -r .

If you have the fdupes command, you'll save a lot of typing. It can do recursive searches (-r,-R) and it
allows you to interactively select which of the duplicate files found you wish to keep or delete.

***
	find -type d -name ".svn" -prune -o -not -empty -type f -printf "%s\n" | sort -rn | uniq -d \
	| xargs -I{} -n1 find -type d -name ".svn" -prune -o -type f -size {}c -print0 | xargs -0 md5sum \
	| sort | uniq -w32 --all-repeated=separate
	
	Show sample output:
	This is sample output - yours may be different.
	[...]
	f2e6bb247f110dcab63b4d38ff7b2dee  ./themes/darkblue_orange/img/b_relations.png
	f2e6bb247f110dcab63b4d38ff7b2dee  ./themes/original/img/b_relations.png

	f5309bd2a2fc5e512a0cc38ac6f10c09  ./themes/darkblue_orange/img/b_deltbl.png
	f5309bd2a2fc5e512a0cc38ac6f10c09  ./themes/original/img/b_deltbl.png

	f60bfbb7ce218a55650c1abbbbee06ae  ./themes/darkblue_orange/img/s_lang.png
	f60bfbb7ce218a55650c1abbbbee06ae  ./themes/original/img/s_lang.png

	f63a5ad833147eeb94adb4496ddbec41  ./themes/darkblue_orange/img/s_theme.png
	f63a5ad833147eeb94adb4496ddbec41  ./themes/original/img/s_theme.png

	f6ae61146ce3de8fa11b9e84e086bd04  ./themes/darkblue_orange/img/bd_drop.png
	f6ae61146ce3de8fa11b9e84e086bd04  ./themes/original/img/bd_drop.png

	f95d66c11bfed9198d13a278269c32b2  ./themes/darkblue_orange/img/s_loggoff.png
	f95d66c11bfed9198d13a278269c32b2  ./themes/original/img/s_loggoff.png
	[...]
	
Improvement of the command "Find Duplicate Files (based on size first, then MD5 hash)" when searching for
duplicate files in a directory containing a subversion working copy. This way the (multiple dupicates) in the
meta-information directories are ignored.
Can easily be adopted for other VCS as well. For CVS i.e. change ".svn" into ".csv":
	find -type d -name ".csv" -prune -o -not -empty -type f -printf "%s\n" | sort -rn | uniq -d \
	| xargs -I{} -n1 find -type d -name ".csv" -prune -o -type f -size {}c -print0 | xargs -0 md5sum \
	| sort | uniq -w32 --all-repeated=separate
	
***
	find -not -empty -type f -printf "%-30s'\t\"%h/%f\"\n" | sort -rn -t$'\t' | uniq -w30 -D \
	| cut -f 2 -d $'\t' | xargs md5sum | sort | uniq -w32 --all-repeated=separate

Finds duplicates based on MD5 sum. Compares only files with the same size. Performance improvements on:
	find -not -empty -type f -printf "%s\n" | sort -rn | uniq -d | xargs -I{} -n1 find -type f -size {}c \
	-print0 | xargs -0 md5sum | sort | uniq -w32 --all-repeated=separate
	
The new version takes around 3 seconds where the old version took around 17 minutes. The bottle neck in the
old command was the second find. It searches for the files with the specified file size. The new version keeps
the file path and size from the beginning.

***
	find . -type f -exec md5 '{}' ';' | sort | uniq -f 3 -d | sed -e "s/.*(\(.*\)).*/\1/"

Find Duplicate Files (based on MD5 hash) -- For Mac OS X
This works on Mac OS X using the `md5` command instead of `md5sum`, which works similarly, but has a different
output format. Note that this only prints the name of the duplicates, not the original file. This is handy
because you can add `| xargs rm` to the end of the command to delete all the duplicates while leaving the
original.



---
