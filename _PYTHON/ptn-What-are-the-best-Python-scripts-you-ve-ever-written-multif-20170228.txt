filename: What_are_the_best_Python_scripts_you-ve_ever_written-multif_20170228.txt
https://www.quora.com/What-are-the-best-Python-scripts-youve-ever-written

   What are the best Python scripts you've ever written?


***
   Everybody knows how hard it is to get IPL tickets.
   CocaCola had an online voting contest, where you upload your picture and other Facebook users have to
   vote on the picture, at the end of the day the user with most votes will get 2 IPL tickets. I used
   the following script, and won 2 tickets (twice). FYI, MI v/s CSK and MI v/s RCB ;)
<code>
import pickle
import time
import urllib2
import urllib
import random
import thread
from p_friends import data as friends

url = "https://www.afancan.com/thefanstation/happinesswall/api/castVotefb.php"
TIME_LOWER_LIMIT = 2
TIME_UPPER_LIMIT = 10


def go_fish(friends):
    for friend in friends:
        req = urllib2.Request(url)
        #req = urllib2.Request("Page on Localhost")
        req.add_header('Accept', '*/*')
        # req.add_header('Accept-Encoding', 'gzip, deflate')
        req.add_header('Content-Type','application/x-www-form-urlencoded; charset=UTF-8')
        req.add_header('Cookie', 'a very long cookie')
        req.add_header('Host', 'Page on Afancan')
        req.add_header('Referer','http://www.afancan.com/thefanstation/happinesswall/')
        req.add_header('User-Agent', 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:12.0) Gecko/20100101 Firefox/
12.0')
        req.add_header('X-Requested-With', 'XMLHttpRequest')
        s = urllib2.urlopen(req, urllib.urlencode({'m':'1116591190', 'uid':friend['id']}))
        print "%s(%s) voted with response as %s" % (friend['name'], friend['id'], s.read())
        # time.sleep(random.randint(TIME_LOWER_LIMIT, TIME_UPPER_LIMIT))

try:
    thread.start_new_thread(go_fish, (friends[ : len(friends)/5],))
    thread.start_new_thread(go_fish, (friends[len(friends)/5 : 2*len(friends)/5],))
    thread.start_new_thread(go_fish, (friends[2*len(friends)/5 : 3*len(friends)/5],))
    thread.start_new_thread(go_fish, (friends[3*len(friends)/5 : 4*len(friends)/5],))
    thread.start_new_thread(go_fish, (friends[4*len(friends)/5 : ],))
except:
    print "Unable to start thread"

while 1:
    pass
</code>


   Crack another online contest
   This one was just for the fun of it, to see if its possible to do it, didn't run the script for too
   long, and of course didn't win :)
<code>
import urllib2
import thread
from p_friends import data as friends

url = "https://faceoffmen.com/ajax/vote"
TIME_LOWER_LIMIT = 2
TIME_UPPER_LIMIT = 10


def go_fish(friends):
    for friend in friends:
        req = urllib2.Request(url)
        #req = urllib2.Request("Page on Localhost")
        req.add_header('Accept', 'application/json, text/javascript, */*; q=0.01')
        # req.add_header('Accept-Encoding', 'gzip, deflate')
        req.add_header('Content-Type', 'application/x-www-form-urlencoded; charset=UTF-8')
        req.add_header('Cookie', 'my cookie')
        req.add_header('Host', 'Page on Faceoffmen')
        req.add_header('Referer', 'Page on Faceoffmen')
        req.add_header('User-Agent', 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:12.0) Gecko/20100101 Firefox/
12.0')
        req.add_header('X-Requested-With', 'XMLHttpRequest')
        s = urllib2.urlopen(req, "facebook_id=10000000000000000000&prev_facebook_id=100000184461066&next_faceb
ook_friend=0")
        print "%s(%s) voted with response as %s" % (friend['name'], friend['id'], s.read())
        # time.sleep(random.randint(TIME_LOWER_LIMIT, TIME_UPPER_LIMIT))

try:
    thread.start_new_thread(go_fish, (friends[:len(friends)/5],))
    thread.start_new_thread(go_fish, (friends[len(friends)/5:2*len(friends)/5],))
    thread.start_new_thread(go_fish, (friends[2*len(friends)/5:3*len(friends)/5],))
    thread.start_new_thread(go_fish, (friends[3*len(friends)/5:4*len(friends)/5],))
    thread.start_new_thread(go_fish, (friends[4*len(friends)/5:],))
except:
    print "Unable to start thread"

while 1:
    pass
</code>


   then I never tried breaking into these contests, I only cracked them to see if they could be.
   Check Mumbai University results
   Check if the results of my final year exam were out or not, if the results were out, the script would
   start the VLC player on my machine and start playing music. Used the same script to check result for
   HSC examinations.
<code>
import urllib2
import time
import tweepy
import os

consumer_key = ''
consumer_secret = ''
access_token = ''
access_secret = ''

tweet_string = 'twitter set @dhruvbaldawa Result for %s %s declared.'
url = 'http://results.mu.ac.in/choose_nob.php?exam_id=%s&exam_year=2012&exam_month=MAY'

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_secret)

t = tweepy.API(auth)

id_start = 2765

id_ = id_start
while True:
    url_socket = urllib2.urlopen(url % id_)
    temp = url_socket.read().lower()
    if temp.find('no such exam!!') < 0:
        if temp.find('computer') > 0:
            print(tweet_string % (id_, True))
            # os.system(tweet_string % (id_, True))
            os.system("vlc ~/01.mp3")
        else:
            print(tweet_string % (id_, False))
            # os.system(tweet_string % (id_, False))
        id_ += 1
    else:
        time.sleep(300)
</code>

   Send free SMS through Way2SMS/Site2SMS
   The code is here: https://github.com/dhruvbaldawa/...
   
   My solutions to Project Euler problems
   https://github.com/dhruvbaldawa/...
   Android push notifications server
      In roughly 20 lines of code.
   https://github.com/dhruvbaldawa/...

***
Download all songs of a TV show from youtube as mp3. Can also download individual songs.

   It all started with How I met your mother. As is well known, their music choice is excellent. So I
   wanted to download all the songs that appeared in HIMYM. So I wrote the following code. For actual
   download, it queues the url in IDM. You have to start those manually. Use cases are at the end of the
   code. We'll get to the code later, first an explanation.
    1. The site [31]TuneFind lists music of Movies and TV shows, I extract the season list, episode list
       per season and songs list per episode from there.
    2. Now that I have the name of the song and the artist, I get the first search result on youtube
       with these keywords.
    3. Now that I have a youtube video id for a song, I send a request to [32]YouTube to MP3 Converter
       to convert it to mp3 and get the url of the converted mp3.
    4. Download the mp3 in the correct heirarchical location.

   Finally, all the mp3s are downloaded and saved in respective folders for each episode for each
   season:
   [ image ]
<code>
import os
import urllib
import re
import cPickle
import HTMLParser
import requests
import time

show_url = "http://www.tunefind.com/show/%s"
season_url = "http://www.tunefind.com/show/%s/season-%d"
episode_url = "http://www.tunefind.com/show/%s/season-%d/%s"


def get_youtube_mp3_url(url):
    for i in xrange(2):
        statusurl = None
        r = requests.post("http://www.listentoyoutube.com/cc/conversioncloud.php", data={"mediaurl": url, \
	"client_urlmap": "none"})
        try:
            statusurl = eval(r.text)['statusurl'].replace('\\/', '/') + "&json"
            break
        except:
            print eval(r.text)['error']
            time.sleep(1)
    while True:
        if not statusurl:
            raise Exception("")
        try:
            resp = eval(requests.get(statusurl).text)
            if 'downloadurl' in resp:
                downloadurl = resp['downloadurl'].replace('\\/', '/')
                break
            time.sleep(1)
        except Exception:
            pass
    return downloadurl


def urlopen(url, tries=10):
    exc = "Couldn't open url %s" % url
    for i in xrange(tries):
        try:
            stream = urllib.urlopen(url)
            return str(stream.read())
        except Exception as e:
            exc = e
    raise Exception(exc)


def download_song(song, location):
    print song
    song_name = song[1] + " - " + song[0] + ".mp3"
    if not os.path.exists(location):
        os.makedirs(location)
    if not os.path.exists(os.path.join(location, song_name)):
        try:
            r = requests.get("YouTube", params={"search_query": "%s %s" % (song[1], song[0])}).text
            top_vid_id = re.findall(r'data-context-item-id="(.*?)"', r)[0]
            mp3_url = get_youtube_mp3_url("YouTube" + top_vid_id)
            cmd = 'idman /d %s /p "%s" /f "%s" /a' % (mp3_url, os.path.join(os.getcwd(), location), song_name)
            os.system(cmd)
        except Exception:
            raise


def get_episode_music(show, season, episode):
    episode_num, episode_name = episode
    episode_dir = os.path.join(show, str(season), episode_name.replace(':', ''))
    episode_data_path = os.path.join(show, str(season), episode_name.replace(':', ''), "data")
    if not os.path.exists(episode_dir):
        os.mkdir(episode_dir)
    if os.path.exists(episode_data_path):
        with open(episode_data_path) as episode_data_file:
            episode_data = cPickle.load(episode_data_file)
    else:
        episode_data = {}
        pg = urlopen(episode_url % (show, season, episode_num))
        h = HTMLParser.HTMLParser()
        try:
            episode_data['songs'] = list((h.unescape(b), h.unescape(c), a) for (a, b, c, d) in \
	    re.findall(r'<a .*?name="song-\d+" href="(/song/\d+/\d+.*?)".*?><i.*?></i>(.*?)</a>\W*by \
	    (.*?)\W*<div.*?>\W*<div.*?>(.*?)</div>\W*</div>', pg))
        except:
            episode_data['songs'] = list((b, c, a) for (a, b, c, d) in re.findall(r'<a .*?name="song-\d+" \
	    href="(/song/\d+/\d+.*?)".*?><i.*?></i>(.*?)</a>\W*by (.*?)\W*<div.*?>\W*<div.*?>(.*?)</div>\W*</div>', pg))
        with open(episode_data_path, 'wb') as episode_data_file:
            cPickle.dump(episode_data, episode_data_file)

    for song in episode_data['songs']:
        download_song(song, episode_dir)


def get_season_music(show, season):
    season_dir = os.path.join(show, str(season))
    season_data_path = os.path.join(show, str(season), 'data')
    if not os.path.exists(season_dir):
        os.mkdir(season_dir)
    if os.path.exists(season_data_path):
        with open(season_data_path) as season_data_file:
            season_data = cPickle.load(season_data_file)
    else:
        season_data = {}
        h = HTMLParser.HTMLParser()
        pg = urlopen(season_url % (show, season))
        season_data['episodes'] = list((a, h.unescape(b)) for (a, b) in re.findall(r'<a href=".*?" \
	name="episode(.*?)">\W*(.*?)\W*</a>', pg))
        with open(season_data_path, 'wb') as season_data_file:
            cPickle.dump(season_data, season_data_file)
    # print season_data
    for episode in season_data['episodes']:
        get_episode_music(show, season, episode)


def get_show_music(show):
    if not os.path.exists(show):
        os.mkdir(show)
    if os.path.exists(os.path.join(show, 'data')):
        with open(os.path.join(show, 'data')) as show_data_file:
            show_data = cPickle.load(show_data_file)
    else:
        show_data = {}
        slug = show.lower().replace(' ', '-')
        pg = urlopen(show_url % show)
        season_finder_pattern = r'/show/' + slug + r'/season-\d+'
        print season_finder_pattern
        season_links = list(set(re.findall(season_finder_pattern, pg)))
        season_links.sort()
        seasons = list(int(sl[sl.find('season-') + len('season-'):]) for sl in season_links)
        seasons.sort()
        show_data['seasons'] = seasons
        with open(os.path.join(show, 'data'), 'wb') as show_data_file:
            cPickle.dump(show_data, show_data_file)
    # print show_data
    for season in show_data['seasons']:
        get_season_music(show, season)

    os.system("idman /s")

if __name__ == '__main__':
    # download_song(["Humme hai hero", "A R Rahman"], "Misc")
    get_show_music("How I Met Your Mother")
    get_show_music("Castle")
</code>

***
   College Mates Resume:
   It was my final year of B.Tech, when I wrote my very first Python script to download my all batch
   mates resumes from the college placement website just to get everyone's mobile number and address.
   Though there was authentication to prevent unauthorized access to the website, but they were using a
   session_id which was just a hash without any salt, so it wasn't so hard to make HTTP GET requests
   with dynamically generated cookie header based on hash of everyone's registration number which was a
   basic sequence. And since that day Python has become a part of my life :)
   Reply To Birthday wishes:
   It was just the beginning of love for Python. In 2012, it was my 22nd birthday. I was at home which
   is in a village of UP (India), where one doesn't find electricity for more than 8 hours in a day.
   When I opened Facebook the next day to my birthday, I saw so many number of wishes on Facebook. I had
   just one hour left to reply to everyone before electricity went. Again my best friend Python helped
   me to make it happen. I wrote a script to say thanks and like the posts.
   [ image ]
   Bus travelers all over India on day:
   It was a week before my birthday in 2013, when I was again excited to do something like last time
   with Python. Now I work in a bus ticket booking company. I can have data of bus bookings all over
   India. I thought of mapping all those on an India map. The very first problem was that I didn't have
   latitude and longitude information of the cities, just their names.Then I came to know that the
   Google reverse geocoding API provides a way map city name to latitude and longitude, but there was
   restriction of making a maximum number of 2500 requests per day. So I had to run the script 4 times
   to collect data of around 8000 cities. Then using the website [37]Indiemapper, I created a map of
   India having raw data of cities and generated an SVG image and embedded it into an HTML file with
   some JavaScript code to render city pairs (source, destination) for which bookings are made. After
   rendering for one day (5th July) of bookings, this is how it looks like:
   [ image ]
   A College Entrance Exam Result:
   This time it was for someone special, which happened a week ago. I wanted to give her the result of
   the college entrance exam for which she applied few days back. I neither know her rollno nor did I
   want to ask her. I know it seems silly. I went to the college website and tried to find rollno
   pattern. For that I had to fill a fake form applying for commerce which generated a new rollno for
   me. Now I know the rollno pattern which was like "C-700". I needed to know the possible range of
   RollNos and backend database table details. I tried a few SQL injections and came to know few things.
   It was a SQL Server server and fields name roll_no, result, quota and class. The unfortunate thing
   was there was no field for student name, so couldn't find her name in it :( Anyway, I wrote a Python
   script downloading all the results by making an HTTP post for each roll_no ranging from C-109 to
   C-708.
   By:
   A Python Lover
   B'day Wish Replier Code
#@author: Heera(heera.jaiswal810@gmail.com)
# note: it assumes all posts on ur date of birth are wishes
# steps to use:
# 1. Before running it, u need to take a valid access_token and set date of birth
# 2. goto Facebook
# 3. click on "Get Acccess token"
# 4. mark 'read_stream', 'publish_stream' permissions and click on get access token
# 5. now copy the access token and put in the variable named 'acces_token'
# 6. put ur date of birth in varible dob in format 'YYYY-MM-DD'
# 7. now run the script
# 8. happy b'day and enjoy :)
<code>
import httplib, urllib
from bs4 import BeautifulSoup
import os
import json
import time
import calendar

#put access_token here
access_token='ABACEdEose0cBABnHFHoDhGozFNQ76ntIWwUESZCtnrfg7lIFmMr5qPbH82EHozYXNgX6ZBrFbZB65NalOVHldbeNlsswuCV\
GjWdn8oVZBCs9ZCXDb5ppioUfsreiHRZCC16XOYAEZCTPC8tI1blkbuyvU6uZC0DxgvJvLEFr5b108V4PZAH0eabyfDWtxg339nMjSan87X2OI\
ahR1A0AXRmZA06rod0ubdr2JvvodaDDvULgZDZF'
#date of birth here, example: '2013-04-24'
dob='2013-04-24'
conn = httplib.HTTPSConnection("Page on Facebook")
print 'requesting...'
#conn.request("GET",path,urllib.urlencode(data),{})
has_more=False
def convert_to_local(s):
    t=time.strptime(s[:19],"%Y-%m-%dT%H:%M:%S")
    t=time.localtime(calendar.timegm(t))
    t=time.strftime("%Y-%m-%d",t)
    return t

def getRandomThnx(msg):
    #TODO :P
    return 'thanks :)'
def process_posts(url):
    conn = httplib.HTTPSConnection("Page on Facebook")
    conn.request("GET",url)
    res = conn.getresponse()
    conn.getresponse
    data=res.read()
    res_obj=json.loads(data)
    posts=res_obj["data"]
    processed=0
    for post in posts:
        if not "message" in post:
            continue
        msg=post["message"]
        post_date=convert_to_local(post["created_time"])
        if dob == post_date:
            if "from" in post and "message" in post:
                user= post["from"]["name"]


                path='/'+post['id']+'/comments'
                param_data={  'format':'json',
                        'message':getRandomThnx(msg),
                        'access_token':access_token
                      }
                conn = httplib.HTTPSConnection("Page on Facebook")
                if post["comments"]["count"]==0:
                    print 'responding to :'+user+'->'+msg
                    conn.request("POST",path,urllib.urlencode(param_data),{})
                    res = conn.getresponse()
                path='/'+post['id']+'/likes'
                param_data={  'format':'json',
                        'access_token':access_token
                      }

                conn = httplib.HTTPSConnection("Page on Facebook")
                processed+=1

    if "paging" in res_obj:
        return processed+process_posts(res_obj["paging"]["next"][len("Page on page on Facebook"):])
    else:
        print "Finished"
        return processed
url='/me/feed?access_token='+access_token
print 'total='+str(process_posts(url))
print 'Thanx to all wisher :)'
</code>

***
   Everybody wants a cool wallpaper on their desktop, but its tough to find one. It takes a lot of
   googling. I found the bing website backgrounds so cool which they update everyday on their
   website([42]Bing). These are free to use as wallpapers.
   So I wrote a pyhton script to automatically download bing’s image for the day & set as wallpaper on
   my ubuntu desktop. Now everyday when I switch on the laptop I find a cool wallpaper on my screen,
   along with a notification containing the wallpaper description.
   Here are the some recent wallpapers:
   Bing wallpaper on 16-june-2016...
   [ 3 images ... ]

   Note: Script should be added in startup apps to launch automatically after starting the machine. Also
   you can run the script manually from terminal.
   Platform: Linux
   Github repo: rjstyles/BingWallpaper
   —————————————————————————————————————
   2. BounceGame :
   A simple GUI application written in python using tkinter module
   [ image ]
   ​
   Github Repo: [44]rjstyles/Bounce-Game
   —————————————————————————————————————
   3. StackOverflow-Crawler:
   It is a web crawler which crawls the stackoverflow website ([45]http://stackoverflow.com/) and finds
   the most popular technologies at current point of time by getting the tags info of newest questions
   asked on the website.
   Github: rjstyles/StackOverflow-Crawler

***
   UPDATE: I got a job because of these scripts!
   Read: Manoj Memana Jayakumar's answer to Has anyone gotten a job through Quora? Or somehow made
   lots of money through Quora?
   1. One-click subtitle downloader for any movie/tv series episode:
   We have all had those days where we open up subscene or opensubtitles, search for the movie/TV series
   name, select the correct ripper, download the subtitle, unzip it, cut and paste it in the movie's
   folder and rename it to match the name of our movie file.
   Too tedious right? Well, I wrote a script to download the correct subtitle for a movie/tv series
   episode and save it, right next to your movie file. All with just one click.
   Don't understand what I'm saying?
   See for yourself:
   That's right. Just click once, and abrakadabra!
   The most matching English subtitle for your movie or tv series episode is downloaded to the very same
   folder, renamed to the very same name as that of your movie file.
   All of this in under 4 seconds!
   So all you have to do now is open the movie file, crunch the popcorn and enjoy the show. :)
   Source on GitHub: subtitle-downloader
   Update: Now you can download subtitles for multiple files at once. Press Ctrl and select the files
   you need to find subtitles for, and then execute the script.
     ________________________________________________________________________________________________

   2. IMDb Lookup/ Spreadsheet Generator
   I am a movie buff, and I love watching movies. I have a huge collection of movies with me, and I get
   confused as to which one to watch.
   So what do I do to avoid this confusion and to select one to watch tonight?
   That's right. IMDb.
   I open up http://imdb.com, type in the name of the movie, see the rating, read the review and
   find out if it is worth watching.
   But hey, I have so many movies with me! Who wants to type in all these names in the search box ? I
   don't want to, especially because I believe "If something is repetitive, it can be automated"
   So I wrote a Python script which makes use of the unofficial IMDb API to fetch the data.
   I select a movie folder/file. Right click it. Click on Send To. Click on IMDB.cmd ( This, BTW, calls
   the Python script which I have written.)
   And Voila!
   My browser opens up with the exact IMDb page of the movie!
   All this, just with the click of a button.
   If you did not understand how cool this is and how much time it can save you, watch the video here:
   From now on you don't have to open up your browser, wait for IMDb to load and type in the movie name.
   The script does all this for you!
   As always, the source is on GitHub: [54]imdb with the instructions on how to use it.
   Of course, there is a certain percentage of error, because of the fact that the script has to clean
   up all the junk values in the file/folder name like "DVDRip, YIFY, BRrip", etc., but the script works
   well with almost all the movies that I have it tested on.
   UPDATE (04-01-2014):
   Many people asked me whether I could write a script so as to find details of all movies within a
   folder, as finding details of one movie at a time is cumbersome.
   I have now updated the script to send a folder to the script, analyze all folders within the folder,
   fetch details of all movies from IMDb and then open a spreadsheet, sorted by decreasing order of IMDb
   rating.
   It also consists details like the IMDb URL, year, plot, genre, awards, actors and everything else you
   could possibly find in IMBb.
   Here is how the spreadsheet generated by the script looks after the script is executed.
   [ image ]
   Your very own personal IMDb database! What more can a movie buff ask for? ;)
   Source on GitHub: imdb
     ________________________________________________________________________________________________

   3. theoatmeal.com COMICS DOWNLOADER
   [ image ]
   I'm a huge fan of Matthew Inman's comic. They are insanely funny and thought-provoking at the same
   time.
   However, I was tired of clicking next and reading every comic.
   Also, it was difficult to download them even manually because each comic is divided to many pictures.
   So, I wrote a Python script to download all the comics from the site.
   The script makes use of BeautifulSoup ([56]http://www.crummy.com/software/B... ) to parse the HTML
   data, so make sure you have it installed before trying to run the script.
   The source code of oatmeal downloader is available on GitHub: [57]theoatmeal.com-downloader
   This is how the folder looked after download :D
   [ image ]
     ________________________________________________________________________________________________

   4. someecards.com DOWNLOADER
   After successfully downloading comics from [58]http://www.theoatmeal.com, I wondered if I could do
   the same thing and download stuff from another favourite site of mine - the hilarious, one and only
   http://www.someecards.com .
   [ image ]
   The problem with someecards was that the pictures are purely randomly named and there is no specific
   order in which they are arranged and there are tons of photos in each category. And there are 52 such
   categories.
   I understood that it would be best if my script is multithreaded since there is so much data to be
   parsed and downloaded, and hence I allocated a thread each to each page in each category.
   The script downloads all of those hilarious ecards from each single category of the website. Every
   single one of them (!) into a separate folder.
   Now I have my own private collection of the most amazingly funny ecards on the planet.
   This is how the folder looked after the download was complete.
   [ image ]
   That's right. 52 categories, 5036 ecards in my very own private collection.
   The source code is available here: [60]someecards.com-downloader
   EDIT: Lot of people asked me whether I could provide all the files that I have downloaded. I could
   not upload it to a file hosting service owing to my very unreliable network, but I have uploaded a
   torrent of the same and you can download it from here: [61]somecards.com Site Rip torrent
   Seed and spread the love! :)
   Do comment or message me if you have trouble running any of these scripts. I am always happy to help
   :)
   Python FTW \m/


---
https://github.com/manojmj92/subtitle-downloader

manojmj92/]subtitle-downloader
README.md

Subtitle Downloader

   Python script to download english subtitles of any movie/tv series episode.

   Get the latest version of Python at [65]http://www.python.org/getit/.

Usage:

   If you have python in your path, simply drag and drop files and folders you wish to get subtitles
   from on subtitle-downloader.py

Windows:

     * Install Python Follow the steps at
       [66]http://stackoverflow.com/questions/3701646/how-to-add-to-the-pythonpath-in-windows-7 to set
       the python path.
     * Place subtitle-downloader.py file in C:\
     * Place the Subtitle.cmd in sendto folder in windows (can be accessed by typing shell:sendto in
       address bar)
     * Right click the movie file (not the movie folder). You can also select multiple files. and click
       sendto -> Subtitle.cmd
     * If you want to download subtitles for all movies in a folder, you can use a folder as parameter
       to subtitle-downloader.py.

Mac:

     * Install python and make sure it is in your path
     * Open Automator and create a new document of type "Service"
     * Add an action called "Run Shell Script"
     * On the top, under 'Service receives selected' drop-down, select 'files or folder' and in the next
       drop-down, for 'in' select 'Finder'
     * On the right top of the Run Shell Script action, make sure you selected "as arguments" for "Pass
       Input"
     * Copy the contents of subtitle.sh in the action
     * Edit the path to "Subtitle_downloader.py" to wherever you have downloaded
     * Press Command + S to save it and give some name like "Download Subtitle"
     * Now, Right click the movie file (not the movie folder). You can also select multiple files. Click
       Services -> Download Subtitle

Linux with Nautilus file manager(Tested on Debian Based with nautilus as file manager):

     * Install python
     * Go to ~/.local/share/nautilus/scripts/ (Ubuntu 13.04 or above) OR ~/.gnome2/nautilus-scripts/
       (Ubuntu 12.10 and below) folder and add Subtitles_in_english.sh in the folder.
     * The path for Subtitle_downloader.py is hardcoded to Desktop ...U can change it accordingly.
     * Now Right Click on the movie file (not the movie folder). You can also select multiple files.
       Click Services -> Subtitles_in_english.

   Voila. the .srt subtitle file will be created right next to your movie file.

   Enjoy the show!

   More details can be found here : qr.ae/GxOcx A how to video can be found here:
   http://www.youtube.com/watch?v=Q5YWEqgw9X8

Linux with NEMO file manager(Tested on Debian Based with NEMO as file manager):

     * Install python
     * Go to ~/.gnome2/nemo-scripts folder and add Subtitles_In_English_for_nemo.sh in the folder.
     * The path for Subtitle_downloader.py is hardcoded to Desktop ...You can change it accordingly.
     * Open terminal and execute following command :

chmod +x ~/.gnome2/nemo-scripts/Subtitles_In_English_for_nemo.sh

     * Now Right Click on the movie file (not the movie folder). You can also select multiple files.
       Click Services -> Subtitles_In_English_for_nemo.

   Voila. the .srt subtitle file will be created right next to your movie file.


---
https://github.com/manojmj92/subtitle-downloader/blob/master/Subtitles_In_English.sh

<code>
   IFS_BAK=$IFS
   IFS="
   "
   for line in $NAUTILUS_SCRIPT_SELECTED_FILE_PATHS; do
   full_path="/home/"$USER"/Desktop/"subtitle-downloader.py
   python $full_path $line
   notify-send $line
   done
   IFS=$IFS_BAK
   ##to do::#give a check here to see if the files coming are .mp4........videoformat files only
   otherwise exit.....
   #to do::also we can give languagae as second parameter
   #path to save in ~/.gnome2/nautilus-scripts
</code>


---
https://github.com/manojmj92/subtitle-downloader/blob/master/Subtitles_In_English_for_nemo.sh

<code>
   IFS_BAK=$IFS
   IFS="
   "
   for line in $NEMO_SCRIPT_SELECTED_FILE_PATHS; do
   full_path="/home/"$USER"/Desktop/"subtitle-downloader.py
   python $full_path $line
   notify-send $line
   done
   IFS=$IFS_BAK
   ##to do::#give a check here to see if the files coming are .mp4........videoformat files only
   otherwise exit.....
   #to do::also we can give languagae as second parameter
   #path to save in ~/.gnome2/nautilus-scripts
</code>



---
https://github.com/manojmj92/subtitle-downloader/blob/master/subtitle-downloader.py

<code>
   #!/usr/bin/env python
   #-------------------------------------------------------------------------------
   # Name : subtitle downloader
   # Purpose : One step subtitle download
   #
   # Authors : manoj m j, arun shivaram p, Valentin Vetter, niroyb
   # Edited by : Valentin Vetter
   # Created :
   # Copyright : (c) www.manojmj.com
   # Licence : GPL v3
   #-------------------------------------------------------------------------------
   # TODO: use another DB if subs are not found on subDB
   import hashlib
   import os
   import shutil
   import sys
   import logging
   import requests,time,re,zipfile
   from bs4 import BeautifulSoup
   PY_VERSION = sys.version_info[0]
   if PY_VERSION == 2:
   import urllib2
   if PY_VERSION == 3:
   import urllib.request
   def get_hash(file_path):
   read_size = 64 * 1024
   with open(file_path, 'rb') as f:
   data = f.read(read_size)
   f.seek(-read_size, os.SEEK_END)
   data += f.read(read_size)
   return hashlib.md5(data).hexdigest()
   def sub_downloader(file_path):
   # Put the code in a try catch block in order to continue for other video files, if it fails during execution
   try:
   # Skip this file if it is not a video
   root, extension = os.path.splitext(file_path)
   if extension not in [".avi", ".mp4", ".mkv", ".mpg", ".mpeg", ".mov", ".rm", ".vob", ".wmv", ".flv", ".3gp",".3g2"]:
   return
   if not os.path.exists(root + ".srt"):
   headers = {'User-Agent': 'SubDB/1.0 (subtitle-downloader/1.0;
   http://github.com/manojmj92/subtitle-downloader)'}
   url = "http://api.thesubdb.com/?action=download&hash=" + get_hash(file_path) + "&language=en"
   if PY_VERSION == 3:
   req = urllib.request.Request(url, None, headers)
   response = urllib.request.urlopen(req).read()
   if PY_VERSION == 2:
   req = urllib2.Request(url, '', headers)
   response = urllib2.urlopen(req).read()
   with open(root + ".srt", "wb") as subtitle:
   subtitle.write(response)
   logging.info("Subtitle successfully downloaded for " + file_path)
   except:
   #download subs from subscene if not found in subdb
   sub_downloader2(file_path)
   def sub_downloader2(file_path):
   try:
   root, extension = os.path.splitext(file_path)
   if extension not in [".avi", ".mp4", ".mkv", ".mpg", ".mpeg", ".mov", ".rm", ".vob", ".wmv", ".flv", ".3gp",".3g2"]:
   return
   if os.path.exists(root + ".srt"):
   return
   j=-1
   root2=root
   for i in range(0,len(root)):
   if(root[i]=="\\" or root[i] =="/"):
   j=i
   break
   root=root2[j+1:]
   root2=root2[:j+1]
   r=requests.get("http://subscene.com/subtitles/release?q="+root);
   soup=BeautifulSoup(r.content,"lxml")
   atags=soup.find_all("a")
   href=""
   for i in range(0,len(atags)):
   spans=atags[i].find_all("span")
   if(len(spans)==2 and spans[0].get_text().strip()=="English"):
   href=atags[i].get("href").strip()
   if(len(href)>0):
   r=requests.get("http://subscene.com"+href);
   soup=BeautifulSoup(r.content,"lxml")
   lin=soup.find_all('a',attrs={'id':'downloadButton'})[0].get("href")
   r=requests.get("http://subscene.com"+lin);
   soup=BeautifulSoup(r.content,"lxml")
   subfile=open(root2+".zip", 'wb')
   for chunk in r.iter_content(100000):
   subfile.write(chunk)
   subfile.close()
   time.sleep(1)
   zip=zipfile.ZipFile(root2+".zip")
   zip.extractall(root2)
   zip.close()
   os.unlink(root2+".zip")
   shutil.move(root2+zip.namelist()[0], os.path.join(root2, root + ".srt"))
   except:
   #Ignore exception and continue
   print("Error in fetching subtitle for " + file_path)
   print("Error", sys.exc_info())
   logging.error("Error in fetching subtitle for " + file_path + str(sys.exc_info()))
   def main():
   root, _ = os.path.splitext(sys.argv[0])
   logging.basicConfig(filename=root + '.log', level=logging.INFO)
   logging.info("Started with params " + str(sys.argv))
   if len(sys.argv) == 1:
   print("This program requires at least one parameter")
   sys.exit(1)
   for path in sys.argv:
   if os.path.isdir(path):
   # Iterate the root directory recursively using os.walk and for each video file present get the subtitle
   for dir_path, _, file_names in os.walk(path):
   for filename in file_names:
   file_path = os.path.join(dir_path, filename)
   sub_downloader(file_path)
   else:
   sub_downloader(path)
   if __name__ == '__main__':
   main()
</code>


---
https://github.com/manojmj92/subtitle-downloader/blob/master/subtitle.sh

<code>
   for i in "$@"
   do
   python path/to/the/repo/subtitle-downloader.py "$i"
   done
</code>
