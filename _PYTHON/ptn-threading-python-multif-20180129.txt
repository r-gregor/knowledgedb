filename: threading-python-multif_20180129.txt
http://www.linuxjournal.com/content/threading-python


Threading in Python
Wed, 01/24/2018

   Threads can provide concurrency, even if they're not truly parallel.

   In my [***] last article, I took a short tour through the ways you can add concurrency to your programs.
   In this article, I focus on one of those forms that has a reputation for being particularly
   frustrating for many developers: threading. I explore the ways you can use threads in Python and the
   limitations the language puts upon you when doing so.

   The basic idea behind threading is a simple one: just as the computer can run more than one process
   at a time, so too can your process run more than one thread at a time. When you want your program to
   do something in the background, you can launch a new thread. The main thread continues to run in the
   foreground, allowing the program to do two (or more) things at once.

   What's the difference between launching a new process and a new thread? A new process is completely
   independent of your existing process, giving you more stability (in that the processes cannot affect
   or corrupt one another) but also less flexibility (in that data cannot easily flow from one thread to
   another). Because multiple threads within a process share data, they can work with one another more
   closely and easily.

   For example, let's say you want to retrieve all of the data from a variety of websites. My preferred
   Python package for retrieving data from the web is the "requests" package, available from PyPI. Thus,
   I can use a for loop, as follows:

length = {}

for one_url in urls:
    response = requests.get(one_url)
    length[one_url] = len(response.content)

for key, value in length.items():
    print("{0:30}: {1:8,}".format(key, value))


   How does this program work? It goes through a list of URLs (as strings), one by one, calculating the
   length of the content and then storing that content inside a dictionary called length. The keys in
   length are URLs, and the values are the lengths of the requested URL content.

   So far, so good; I've turned this into a complete program (retrieve1.py), which is shown in Listing
   1. I put nine URLs into a text file called urls.txt (Listing 2), and then timed how long retrieving
   each of them took. On my computer, the total time was about 15 seconds, although there was clearly
   some variation in the timing.

   Listing 1. retrieve1.py

#!/usr/bin/env python3

import requests
import time

urls = [one_line.strip()
        for one_line in open('urls.txt')]

length = {}

start_time = time.time()

for one_url in urls:
    response = requests.get(one_url)
    length[one_url] = len(response.content)

for key, value in length.items():
    print("{0:30}: {1:8,}".format(key, value))

end_time = time.time()

total_time = end_time - start_time

print("\nTotal time: {0:.3} seconds".format(total_time))


   Listing 2. urls.txt

http://lerner.co.il
http://LinuxJournal.com
http://en.wikipedia.org
http://news.ycombinator.com
http://NYTimes.com
http://Facebook.com
http://WashingtonPost.com
http://Haaretz.co.il
http://thetech.com


Improving the Timing with Threads
   How can I improve the timing? Well, Python provides threading. Many people think of Python's threads
   as fatally flawed, because only one thread actually can execute at a time, thanks to the GIL (global
   interpreter lock). This is true if you're running a program that is performing serious calculations,
   and in which you really want the system to be using multiple CPUs in parallel.

   However, I have a different sort of use case here. I'm interested in retrieving data from different
   websites. Python knows that I/O can take a long time, and so whenever a Python thread engages in I/O
   (that is, the screen, disk or network), it gives up control and hands use of the GIL over to a
   different thread.

   In the case of my "retrieve" program, this is perfect. I can spawn a separate thread to retrieve each
   of the URLs in the array. I then can wait for the URLs to be retrieved in parallel, checking in with
   each of the threads one at a time. In this way, I probably can save time.

   Let's start with the core of my rewritten program. I'll want to implement the retrieval as a
   function, and then invoke that function along with one argument—the URL I want to retrieve. I then
   can invoke that function by creating a new instance of threading.Thread, telling the new instance not
   only which function I want to run in a new thread, but also which argument(s) I want to pass. This is
   how that code will look:

for one_url in urls:
    t = threading.Thread(target=get_length, args=(one_url,))
    t.start()

   But wait. How will the get_length function communicate the content length to the rest of the program?
   In a threaded program, you really must not have individual threads modify built-in data structures,
   such as a list. This is because such data structures aren't thread-safe, and doing something such as
   an "append" from one thread might cause all sorts of problems.

   However, you can use a "queue" data structure, which is thread-safe, and thus guarantees a form of
   communication. The function can put its results on the queue, and then, when all of the threads have
   completed their run, you can read those results from the queue.

   Here, then, is how the function might look:

from queue import Queue

queue = Queue()

def get_length(one_url):
    response = requests.get(one_url)
    queue.put((one_url, len(response.content)))

   As you can see, the function retrieves the content of one_url and then places the URL itself, as well
   as the length of the content, in a tuple. That tuple is then placed in the queue.

   It's a nice little program. The main thread spawns a new thread, each of which runs get_length. In
   get_length, the information gets stuck on the queue.

   The thing is, now it needs to retrieve things from the queue. But if you do this just after launching
   the threads, you run the risk of reading from the queue before the threads have completed. So, you
   need to "join" the threads, which means to wait until they have finished. Once the threads have all
   been joined, you can read all of their information from the queue.

   There are a few different ways to join the threads. An easy one is to create a list where you will
   store the threads and then append each new thread object to that list as you create it:

threads = [ ]

for one_url in urls:
    t = threading.Thread(target=get_length, args=(one_url,))
    threads.append(t)
    t.start()

   You then can iterate over each of the thread objects, joining them:

for one_thread in threads:
    one_thread.join()

   Note that when you call one_thread.join() in this way, the call blocks. Perhaps that's not the most
   efficient way to do things, but in my experiments, it still took about one second—15 times faster—to
   retrieve all of the URLs.

   In other words, Python threads are routinely seen as terrible and useless. But in this case, you can
   see that they allowed me to parallelize the program without too much trouble, having different
   sections execute concurrently.

   Listing 3. retrieve2.py

#!/usr/bin/env python3

import requests
import time
import threading
from queue import Queue

urls = [one_line.strip()
        for one_line in open('urls.txt')]

length = {}
queue = Queue()
start_time = time.time()
threads = [ ]

def get_length(one_url):
    response = requests.get(one_url)
    queue.put((one_url, len(response.content)))

# Launch our function in a thread
print("Launching")
for one_url in urls:
    t = threading.Thread(target=get_length, args=(one_url,))
    threads.append(t)
    t.start()

# Joining all
print("Joining")
for one_thread in threads:
    one_thread.join()

# Retrieving + printing
print("Retrieving + printing")
while not queue.empty():
    one_url, length = queue.get()
    print("{0:30}: {1:8,}".format(one_url, length))

end_time = time.time()

total_time = end_time - start_time

print("\nTotal time: {0:.3} seconds".format(total_time))

Considerations
   The good news is that this demonstrates how using threads can be effective when you're doing
   numerous, time-intensive I/O actions. This is especially good news if you're writing a server in
   Python that uses threads; you can open up a new thread for each incoming request and/or allocate each
   new request to an existing, pre-created thread. Again, if the threads don't really need to execute in
   a truly parallel fashion, you're fine.

   But, what if your system receives a very large number of requests? In such a case, your threads might
   not be able to keep up. This is particularly true if the code being executed in each thread is
   CPU-intensive.

   In such a case, you don't want to use threads. A popular option—indeed, the popular option—is to use
   processes. In my next article, I plan to look at how such processes can work and interact.
   [8]View the discussion thread.


---
[***] http://www.linuxjournal.com/content/thinking-concurrently
Thinking Concurrently: How Modern Network Applications Handle Multiple Connections
Fri, 01/12/2018

   Reuven explores different types of multiprocessing and looks at the advantages and disadvantages of
   each.

   When I first started consulting, and my clients were small organizations just getting started on the
   web, they inevitably would ask me what kind of high-powered server they would need. My clients were
   all convinced that they were going to be incredibly popular and important, and that they would have
   lots of visitors coming to their websites—and it was important that their sites would be able to
   stand up under this load.

   I would remind them that each day has 86,400 seconds. This means that if one new person visits their
   site each second, the server will need to handle 86,400 requests per day—a trivial number, for most
   modern computers, especially if you're just serving up static files.

   I would then ask, do they really expect to get more than 86,000 visitors per day? The client would
   almost inevitably answer, somewhat sheepishly, "No, definitely not."

   Now, I knew that my clients didn't need to worry about the size or speed of their servers; I really
   did have their best interests at heart, and I was trying to convince them, in a somewhat dramatic
   way, that they didn't need to spend money on a new server. But I did take certain liberties with the
   truth when I presented those numbers—for example:
     * There's a difference between 86,400 visitors in one day, spread out evenly across the entire day,
       and a spike during lunch hour, when many people do their shopping and leisure reading.
     * Web pages that contain CSS, JavaScript and images—which is all of them, in the modern era—require
       more than one HTTP request for each page load. Even if you have 10,000 visitors, you might well
       have more than 100,000 HTTP requests to your server.
     * When a simple web site becomes a web application, you need to start worrying about the speed of
       back-end databases and third-party services, as well as the time it takes to compute certain
       things.

   So, what do you in such cases? If you can handle precisely one request per second, what happens if
   more than one person visits your site at the same time? You could make one of them wait until the
   other is finished and then service the next one, but if you have 10 or 15 simultaneous requests, that
   tactic eventually will backfire on you.

   In most modern systems, the solution has been to take advantage of multiprocessing: have the computer
   do more than one thing at a time. If a computer can do two things each second, and if your visitors
   are spread out precisely over the course of a day, then you can handle 172,800 visitors. And if you
   can do three things at a time, you suddenly can handle 259,200 visitors—and so forth.

   How can a computer do more than one thing at a time? With a single CPU, each process gets a "time
   slice", meaning one fraction of the time that the CPU is working. If you have ten processes, and each
   gets an equal time slice, then each will run once per second for 0.1 seconds. As you increase the
   number of processes, the time allocated to each process goes down.

   Modern computers come with multiple CPUs (aka "cores"), which means that they actually can do things
   in parallel, rather than simply give each process a time slice on the system's single processor. In
   theory, a dual-core system with ten processes will run each process once per second for 0.2 seconds,
   divided across the processors.

   Scaling is never perfectly linear, so you can't really predict things in that way, but it's not a bad
   way to think about this.

   Processes, as described here, are a great way for the computer to do more than one thing at a time.
   And yet, many applications have other ways of dealing with concurrency. Two of the most popular
   alternatives to processes are threads and the reactor pattern, especially popular and well known in
   node.js and the nginx HTTP server.

   So in this article, I explore the different types of multiprocessing that exist, looking at the
   advantages and disadvantages of each one. Even if you're not interested in switching, it's useful to
   know what is out there.

Processes
   The idea behind a process is fairly simple. A running program consists of not only executing code,
   but also data and some context. Because the code, data and context all exist in memory, the operating
   system can switch from one process to another very quickly. This combination of code + data + context
   is known as a "process", and it's the basis for how Linux systems work.

   When you start your Linux box, it has a single process. That process then "forks" itself, such that
   two identical processes are running. The second ("child") process reads new code, data and context
   ("exec"), and thus starts running a new process. This continues throughout the time that a system is
   running. When you execute a new program on the command line with & at the end of the line, you're
   forking the shell process and then exec'ing your desired program in its place.

   The Apache httpd server, which is extremely popular and standard on many Linux systems, works by
   default on a process model. You might think that when a new request comes in, Apache will start up a
   new process to handle it. But starting it up takes some time, and no one wants to wait for it to
   happen. The solution is, thus, to "prefork" a bunch of servers. This way, when a new request arrives,
   Apache can hand off that request to a process. When Apache sees that you're running low on processes,
   it will add a bunch to the pool, ensuring that there are always enough spare servers. If you reach
   the limit, things start to cause problems for users,

   In many cases, the process model is great. Linux is great at launching processes; it's a fairly
   low-cost operation, and one that a typical system does hundreds or even thousands of times every
   hour. Moreover, the kernel developers have learned through the years how to do things intelligently,
   such that a forked process uses (and writes to) its own memory only when it needs to; until that
   time, it continues to use memory from its parent process.

   Moreover, processes are extremely stable and secure. Memory owned by one process is typically not
   visible to other processes, let alone writable by other processes. And if a process goes down, it
   shouldn't take the entire system down with it.

   So, what's not to like? Processes are great, no?

   Yes, but they also require a fair amount of overhead. If all you're doing is serving up some files,
   or doing a tiny amount of processing, using a full process for that might seem excessive.

   Moreover, if you're doing a number of related tasks that are using the same memory, the fact that
   every process keeps data separate might make things safe, but also more of a memory hog.

Threads
   People coming from a Windows or Java background often scoff at the UNIX tradition of using processes.
   They say that processes are too heavy for most things, and that you would be better off using threads
   instead. A thread is similar to a process, except that it exists inside a process.

   Just as a computer splits its time across different processes, giving each one a time slice, a
   process splits its time across different threads, giving each one a time slice.

   Because threads exist within an existing process, their startup time is much faster. And because
   threads share memory with other threads in the process, they consume less memory and are more
   efficient.

   The fact that threads share memory can lead to all sorts of problematic situations. How do you ensure
   that two different threads aren't modifying the same data at the same time? How do you ensure that
   your threads execute in the appropriate order—or, how do you make sure that the order isn't actually
   that important? There are all sorts of issues associated with threading, and people who work with
   threads know them all too well. The benefits of threads are obvious, but ensuring that they work, and
   work correctly, can be quite frustrating. Indeed, people who grew up using processes find threads to
   be fraught with danger and complexity, and they do whatever they can to avoid them.

   As a general rule, people with a Microsoft technology background use threads all of the time,
   starting up a new process only when necessary. By contrast, starting a new process in the Microsoft
   world takes a long time. People with a UNIX background, meanwhile, think that starting a new process
   is the easiest and safest thing to do, and they tend to shy away from threads.

   Which is the right answer? It all depends. You probably can squeeze more performance out of your
   computer if you use threads, but at the same time, you can be sure that the code is written in a way
   that takes advantage of the threads, and it doesn't fall into any of the common traps.

   What if you want to use threads, rather than processes, with your Apache server? Years ago, the
   Apache httpd developers realized that it was foolish for them to push a single model upon their
   users. For some, and especially for people using Windows, threads were preferable. For many, the
   traditional processes were preferable. And in some cases, it wasn't obvious which would be better
   without first doing some benchmarking.

   The solution was something known as an MPM (multi-processing module). You can choose the traditional
   "pre-fork" MPM, typically used on UNIX. You can use the "worker" MPM, which is a combination of
   processes and threads. If you're on Windows, there's a special "mpm_winnt" MPM, which uses a single
   process and many threads.

   The "worker" MPM is perhaps the most interesting of the bunch, in that it allows you to control the
   maximum number of processes (with the MaxClients directive), but also a number of threads per process
   (with the ThreadsPerChild directive). You then can experiment with the optimal configuration for your
   server, deciding which mixture of processes and threads is going to give you the best performance.

An Old-New Idea
   During the last few years, a number of network applications have re-discovered a way of writing code
   that seems counter to all of these ideas. Instead of having multiple processes, or multiple threads,
   just have a single process, without any threads. That process then can handle all of the incoming
   network traffic.

   At first blush, that sounds a bit crazy. Why keep everything together in a single process?

   But, then consider the fact that even on a highly optimized Linux system, there is still some
   overhead to the "context switch", moving from one process to another. This overhead is repeated at a
   smaller level within a process, when you switch from thread to thread.

   If you handle all of the incoming network requests within a single process, you avoid all of those
   context switches. You can do that by having an event loop, and then by hanging functions on that
   event loop. If your event loop contains functions A, B and C, the system gives A a chance to run,
   then B, then C and then A again. With each opportunity provided by the event loop, you find that A, B
   and C each progress forward a bit each time.

   This actually works quite well, and it has been demonstrated to scale better than processes and
   threads. What's the problem then?

   First of all, the code needs to be written in such a way that it can be divided into functions and
   put into an event loop. Thinking this way, and writing this style of code, is different from what
   people typically are used to in the procedural and object-oriented worlds. In many ways, it's like
   creating a callback function, in that you don't know quite when it'll run.

   Among other things, you need to be super careful when working with I/O in this kind of function.
   That's because disks and networks are extremely slow, and if function B is reading from the disk,
   then it's waiting idly while neither A nor C has a chance to run. Working with I/O thus requires
   special handling.

   This is part of a bigger issue, namely that modern operating systems use "pre-emptive multitasking",
   telling each process when its time slice has expired. The reactor pattern uses "cooperative
   multitasking", in that a rogue function can hog the CPU simply by failing to abide by the rules.

   This paradigm is known as the "reactor pattern" and underlies the nginx HTTP server, node.js, Twisted
   Python and the new asyncio libraries in Python. It has proven itself, but it does require that
   developers think in new and different ways.

   Not to be outdone by nginx, Apache now has an "event" MPM, which handles things using this method. So
   if you're a fan of Apache and want to try out the reactor pattern without switching to nginx, you can
   do so. If you're simply using the server and connecting to an external application, rather than
   writing code that will be embedded within Apache, the MPM will affect performance, but not how you
   write your application.

Conclusion
   So, where does this leave you? First of all, it means you have a number of options. It also means
   that when you start to worry about performance—and only then—you can start to run experiments to
   compare the different paradigms and how they work.

   But it also means that in today's highly networked world, you might want to consider one or more of
   these options right away. At the very least, you should be familiar with them and how they work, and
   the trade-offs associated with them. In particular, while the reactor pattern can be hard to
   understand, such understanding will make it easier to design architectures that will scale—especially
   when you truly need it.
