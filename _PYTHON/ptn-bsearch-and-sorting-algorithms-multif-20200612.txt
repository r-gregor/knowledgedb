filename: ptn_bsearch-and-sorting-algorithms-multif_20200612.txt
https://stackabuse.com/binary-search-in-python/

Binary Search in Python

Introduction
   In this article, we'll be diving into the idea behind and Python implementation of Binary Search.

   Binary Search is an efficient search algorithm that works on sorted arrays. It's often used as one of
   the first examples of algorithms that run in logarithmic time (O(logn)) because of its intuitive
   behavior, and is a fundamental algorithm in Computer Science.

Binary Search - Example
   Binary Search works on a divide-and-conquer approach and relies on the fact that the array is sorted
   to eliminate half of possible candidates in each iteration. More specifically, it compares the middle
   element of the sorted array to the element it's searching for in order to decide where to continue
   the search.

   If the target element is larger than the middle element - it can't be located in the first half of
   the collection so it's discarded. The same goes the other way around.

   Note: If the array has an even number of elements, it doesn't matter which of the two "middle"
   elements we use to begin with.

   Let's look at an example quickly before we continue to explain how binary search works:

           initial array:   1 2 5 7 13 15 16 18 24 28 29
           looking for: 18
           start at the middle: 1 2 5 7 13 [15] 16 18 24 28 29
           Is 18 larger/smaller than or equal to 15?
           Larger, so 18 mast be in rhe "right" half of the array.
           We can continue our serach in that half: 16 18 24 28 29

   As we can see, we know for sure that, since the array is sorted, x is not in the first half of the
   original array.

   When we know in which half of the original array x is, we can repeat this exact process with that
   half and split it into halves again, discarding the half that surely doesn't contain x:

           Current array: 16 18 24 28 29
           Is 18 larger/smaller than or equal to 24?
           Smaller, so 18 mast be in rhe "left" half of the array.
           
           Current array: 16 18
           Which one we choose depends on how the code was written (since there's even number of elements),
           but let's asume we chose 16.
           Notice that the left half doesn't exist, if we had to go left, we would conclude that the
           element we are looking for wasn't in the array.
           
           Current array: 18
           Is 18 larger/smaller than or equal to 18?
           Equal, we have found 18.

   We repeat this process until we end up with a subarray that contains only one element. We check
   whether that element is x. If it is - we found x, if it isn't - x doesn't exist in the array at all.

   If you take a closer look at this, you can notice that in the worst-case scenario (x not existing in
   the array), we need to check a much smaller number of elements than we'd need to in an unsorted array
   - which would require something more along the lines of Linear Search, which is insanely inefficient.

   To be more precise, the number of elements we need to check in the worst case is log[2]N where N is
   the number of elements in the array.

   This makes a bigger impact the bigger the array is:

     If our array had 10 elements, we would need to check only 3 elements to either find x or conclude
     it's not there. That's 33.3%.

     However, if our array had 10,000,000 elements we would only need to check 24 elements. That's
     0.0002%.

Binary Search Implementation
   Binary Search is a naturally recursive algorithm, since the same process is repeated on smaller and
   smaller arrays until an array of size 1 has been found. However, there is of course an iterative
   implementation as well, and we will be showing both approaches.

Recursive
   Let's start off with the recursive implementation as it's more natural:
def binary_search_recursive(array, element, start, end):
    if start > end:
        return -1

    mid = (start + end) // 2
    if element == array[mid]:
        return mid

    if element < array[mid]:
        return binary_search_recursive(array, element, start, mid-1)
    else:
        return binary_search_recursive(array, element, mid+1, end)

   Let's take a closer look at this code. We exit the recursion if the start element is higher than the
   end element:
if start > end:
        return -1

   This is because this situation occurs only when the element doesn't exist in the array. What happens
   is that we end up with only one element in the current sub-array, and that element doesn't match the
   one we're looking for.

   At this point, start is equal to end. However, since element isn't equal to array[mid], we "split"
   the array again in such a way that we either decrease end by 1, or increase start by one, and the
   recursion exists on that condition.

   We could have done this using a different approach:
if len(array) == 1:
    if element == array[mid]:
        return mid
    else:
        return -1

   The rest of the code does the "check middle element, continue search in the appropriate half of the
   array" logic. We find the index of the middle element and check whether the element we're searching
   for matches it:
mid = (start + end) // 2
if elem == array[mid]:
    return mid

   If it doesn't, we check whether the element is smaller or larger than the middle element:
if element < array[mid]:
    # Continue the search in the left half
    return binary_search_recursive(array, element, start, mid-1)
else:
    # Continue the search in the right half
    return binary_search_recursive(array, element, mid+1, end)

   Let's go ahead and run this algorithm, with a slight modification so that it prints out which
   subarray it's working on currently:
element = 18
array = [1, 2, 5, 7, 13, 15, 16, 18, 24, 28, 29]

print("Searching for {}".format(element))
print("Index of {}: {}".format(element, binary_search_recursive(array, element, 0, len(array))))

   Running this code will result in:
Searching for 18
Subarray in step 0:[1, 2, 5, 7, 13, 15, 16, 18, 24, 28, 29]
Subarray in step 1:[16, 18, 24, 28, 29]
Subarray in step 2:[16, 18]
Subarray in step 3:[18]
Index of 18: 7

   It's clear to see how it halves the search space in each iteration, getting closer and closer to the
   element we're looking for. If we tried searching for an element that doesn't exist in the array, the
   output would be:
Searching for 20
Subarray in step 0: [4, 14, 16, 17, 19, 21, 24, 28, 30, 35, 36, 38, 39, 40, 41, 43]
Subarray in step 1: [4, 14, 16, 17, 19, 21, 24, 28]
Subarray in step 2: [19, 21, 24, 28]
Subarray in step 3: [19]
Index of 20: -1

   And just for the fun of it, we can try searching some large arrays and seeing how many steps it takes
   Binary Search to figure out whether a number exists:
Searching for 421, in an array with 200 elements
Search finished in 6 steps. Index of 421: 169

Searching for 1800, in an array with 1500 elements
Search finished in 11 steps. Index of 1800: -1

Searching for 3101, in an array with 3000 elements
Search finished in 8 steps. Index of 3101: 1551

Iterative
   The iterative approach is very simple and similar to the recursive approach. Here, we just perform
   the checks in a while loop:
def binary_search_iterative(array, element):
    mid = 0
    start = 0
    end = len(array)
    step = 0

    while (start <= end):
        print("Subarray in step {}: {}".format(step, str(array[start:end+1])))
        step = step+1
        mid = (start + end) // 2

        if element == array[mid]:
            return mid

        if element < array[mid]:
            end = mid - 1
        else:
            start = mid + 1
    return -1

   Let's populate an array and search for an element within it:
array = [1, 2, 5, 7, 13, 15, 16, 18, 24, 28, 29]

print("Searching for {} in {}".format(element, array))
print("Index of {}: {}".format(element, binary_search_iterative(array, element)))

   Running this code gives us the output of:
Searching for 18 in [1, 2, 5, 7, 13, 15, 16, 18, 24, 28, 29]
Subarray in step 0: [1, 2, 5, 7, 13, 15, 16, 18, 24, 28, 29]
Subarray in step 1: [16, 18, 24, 28, 29]
Subarray in step 2: [16, 18]
Subarray in step 3: [18]
Index of 18: 7

Conclusion
   Binary Search is an incredible algorithm to use on large, sorted arrays, or whenever we plan to
   search for elements repeatedly in a single array.

   The cost of sorting the array once and then using Binary Search to find elements in it multiple times
   is far better than using Linear Search on an unsorted array just so we could avoid the cost of
   sorting it.

   If we're sorting the array and searching for an element just once, it's more efficient to just do a
   Linear Search on the unsorted array.


---
https://stackabuse.com/sorting-algorithms-in-python/

Sorting Algorithms in Python

Introduction
   Sometimes, data we store or retrieve in an application can have little or no order. We may have to
   rearrange the data to correctly process it or efficiently use it. Over the years, computer scientists
   have created many sorting algorithms to organize data.

   In this article we'll have a look at popular sorting algorithms, understand how they work and code
   them in Python. We'll also compare how quickly they sort items in a list.

   For simplicity, algorithm implementations would be sorting lists of numbers in ascending order. Of
   course, you're free to adapt them to your needs

   If you'd like to learn about a specific algorithm, you can jump to it here:
     * Bubble Sort
     * Selection Sort
     * Insertion Sort
     * Merge Sort
     * Heap Sort
     * Quick Sort
     * Sorting in Python

Bubble Sort
   This simple sorting algorithm iterates over a list, comparing elements in pairs and swapping them
   until the larger elements "bubble up" to the end of the list, and the smaller elements stay at the
   "bottom".

Explanation
   We begin by comparing the first two elements of the list. If the first element is larger than the
   second element, we swap them. If they are already in order we leave them as is. We then move to the
   next pair of elements, compare their values and swap as necessary. This process continues to the last
   pair of items in the list.

   Upon reaching the end of the list, it repeats this process for every item. Though, this is highly
   inefficient. What if only a single swap needs to be made in the array? Why would we still iterate
   though it n^2 times, even though it's already sorted?

   Obviously, to optimize the algorithm, we need to stop it when it's finished sorting, otherwise it'll
   reevaluate an already sorted array many times.

   How would we know that we're finished sorting? If the items were in order then we would not have to
   swap any. So, whenever we swap values we set a flag to True to repeat sorting process. If no swaps
   occurred, the flag would remain False and the algorithm will stop.

Implementation
   With the optimization, we can implement Bubble Sort in Python as follows:
def bubble_sort(nums):
    # We set swapped to True so the loop looks runs at least once
    swapped = True
    while swapped:
        swapped = False
        for i in range(len(nums) - 1):
            if nums[i] > nums[i + 1]:
                # Swap the elements
                nums[i], nums[i + 1] = nums[i + 1], nums[i]
                # Set the flag to True so we'll loop again
                swapped = True


# Verify it works
random_list_of_nums = [5, 2, 1, 8, 4]
bubble_sort(random_list_of_nums)
print(random_list_of_nums)

   The algorithm runs in a while loop, only breaking when no items are swapped. We set swapped to True
   in the beginning to ensure that the algorithm runs at least once.

Time Complexity
   In the worst case scenario (when the list is in reverse order), this algorithm would have to swap
   every single item of the array. Our swapped flag would be set to True on every iteration.

   Therefore, if we have n elements in our list, we would have n iterations per item - thus Bubble
   Sort's time complexity is O(n^2).

Selection Sort
   This algorithm segments the list into two parts: sorted and unsorted. We continuously remove the
   smallest element of the unsorted segment of the list and append it to the sorted segment.

Explanation
   In practice, we don't need to create a new list for the sorted elements, what we do is treat the
   leftmost part of the list as the sorted segment. We then search the entire list for the smallest
   element, and swap it with the first element.

   Now we know that the first element of the list is sorted, we get the smallest element of the
   remaining items and swap it with the second element. This reiterates until the last item of the list
   is the remaining element to be examined.

Implementation
def selection_sort(nums):
    # This value of i corresponds to how many values were sorted
    for i in range(len(nums)):
        # We assume that the first item of the unsorted segment is the smallest
        lowest_value_index = i
        # This loop iterates over the unsorted items
        for j in range(i + 1, len(nums)):
            if nums[j] < nums[lowest_value_index]:
                lowest_value_index = j
        # Swap values of the lowest unsorted element with the first unsorted
        # element
        nums[i], nums[lowest_value_index] = nums[lowest_value_index], nums[i]


# Verify it works
random_list_of_nums = [12, 8, 3, 20, 11]
selection_sort(random_list_of_nums)
print(random_list_of_nums)

   We see that as i increases, we need to need to check less items.

Time Complexity
   We can easily get the time complexity by examining the for loops in the Selection Sort algorithm. For
   a list with n elements, the outer loop iterates n times.

   The inner loop iterate n-1 when i is equal to 1, and then n-2 as i is equal to 2 and so forth.

   The amount of comparisons are (n - 1) + (n - 2) + ... + 1, which gives Selection Sort a time
   complexity of O(n^2).

Insertion Sort
   Like Selection Sort, this algorithm segments the list into sorted and unsorted parts. It iterates
   over the unsorted segment, and inserts the element being viewed into the correct position of the
   sorted list.

Explanation
   We assume that the first element of the list is sorted. We then go to the next element, let's call it
   x. If x is larger than the first element we leave as is. If x is smaller, we copy the value of the
   first element to the second position and then set the first element to x.

   As we go to the other elements of the unsorted segment, we continuously move larger elements in the
   sorted segment up the list until we encounter an element smaller than x or reach the end of the
   sorted segment, and then place x in it's correct position.

Implementation
def insertion_sort(nums):
    # Start on the second element as we assume the first element is sorted
    for i in range(1, len(nums)):
        item_to_insert = nums[i]
        # And keep a reference of the index of the previous element
        j = i - 1
        # Move all items of the sorted segment forward if they are larger than
        # the item to insert
        while j >= 0 and nums[j] > item_to_insert:
            nums[j + 1] = nums[j]
            j -= 1
        # Insert the item
        nums[j + 1] = item_to_insert


# Verify it works
random_list_of_nums = [9, 1, 15, 28, 6]
insertion_sort(random_list_of_nums)
print(random_list_of_nums)

Time Complexity
   In the worst case scenario, an array would be sorted in reverse order. The outer for loop in
   Insertion Sort function always iterates n-1 times.

   In the worst case scenario, the inner for loop would swap once, then swap two and so forth. The
   amount of swaps would then be 1 + 2 + ... + (n - 3) + (n - 2) + (n - 1) which gives Insertion Sort a
   time complexity of O(n^2).

Heap Sort
   This popular sorting algorithm, like the Insertion and Selection sorts, segments the list into sorted
   and unsorted parts. It converts the unsorted segment of the list to a Heap data structure, so that we
   can efficiently determine the largest element.

Explanation
   We begin by transforming the list into a Max Heap - a Binary Tree where the biggest element is the
   root node. We then place that item to the end of the list. We then rebuild our Max Heap which now has
   one less value, placing the new largest value before the last item of the list.

   We iterate this process of building the heap until all nodes are removed.

Implementation
   We'll create an helper function heapify to implement this algorithm:
def heapify(nums, heap_size, root_index):
    # Assume the index of the largest element is the root index
    largest = root_index
    left_child = (2 * root_index) + 1
    right_child = (2 * root_index) + 2

    # If the left child of the root is a valid index, and the element is greater
    # than the current largest element, then update the largest element
    if left_child < heap_size and nums[left_child] > nums[largest]:
        largest = left_child

    # Do the same for the right child of the root
    if right_child < heap_size and nums[right_child] > nums[largest]:
        largest = right_child

    # If the largest element is no longer the root element, swap them
    if largest != root_index:
        nums[root_index], nums[largest] = nums[largest], nums[root_index]
        # Heapify the new root element to ensure it's the largest
        heapify(nums, heap_size, largest)


def heap_sort(nums):
    n = len(nums)

    # Create a Max Heap from the list
    # The 2nd argument of range means we stop at the element before -1 i.e.
    # the first element of the list.
    # The 3rd argument of range means we iterate backwards, reducing the count
    # of i by 1
    for i in range(n, -1, -1):
        heapify(nums, n, i)

    # Move the root of the max heap to the end of
    for i in range(n - 1, 0, -1):
        nums[i], nums[0] = nums[0], nums[i]
        heapify(nums, i, 0)


# Verify it works
random_list_of_nums = [35, 12, 43, 8, 51]
heap_sort(random_list_of_nums)
print(random_list_of_nums)

Time Complexity
   Let's first look at the time complexity of the heapify function. In the worst case the largest
   element is never the root element, this causes a recursive call to heapify. While recursive calls
   might seem dauntingly expensive, remember that we're working with a binary tree.

   Visualize a binary tree with 3 elements, it has a height of 2. Now visualize a binary tree with 7
   elements, it has a height of 3. The tree grows logarithmically to n. The heapify function traverses
   that tree in O(log(n)) time.

   The heap_sort function iterates over the array n times. Therefore the overall time complexity of the
   Heap Sort algorithm is O(nlog(n)).

Merge Sort
   This divide and conquer algorithm splits a list in half, and keeps splitting the list by 2 until it
   only has singular elements.

   Adjacent elements become sorted pairs, then sorted pairs are merged and sorted with other pairs as
   well. This process continues until we get a sorted list with all the elements of the unsorted input
   list.

Explanation
   We recursively split the list in half until we have lists with size one. We then merge each half that
   was split, sorting them in the process.

   Sorting is done by comparing the smallest elements of each half. The first element of each list are
   the first to be compared. If the first half begins with a smaller value, then we add that to the
   sorted list. We then compare the second smallest value of the first half with the first smallest
   value of the second half.

   Every time we select the smaller value at the beginning of a half, we move the index of which item
   needs to be compared by one.

   If you'd like to read a detailed, dedicated article for [18]Merge Sort, we've got you covered!

Implementation
def merge(left_list, right_list):
    sorted_list = []
    left_list_index = right_list_index = 0

    # We use the list lengths often, so its handy to make variables
    left_list_length, right_list_length = len(left_list), len(right_list)

    for _ in range(left_list_length + right_list_length):
        if left_list_index < left_list_length and right_list_index < right_list_length:
            # We check which value from the start of each list is smaller
            # If the item at the beginning of the left list is smaller, add it
            # to the sorted list
            if left_list[left_list_index] <= right_list[right_list_index]:
                sorted_list.append(left_list[left_list_index])
                left_list_index += 1
            # If the item at the beginning of the right list is smaller, add it
            # to the sorted list
            else:
                sorted_list.append(right_list[right_list_index])
                right_list_index += 1

        # If we've reached the end of the of the left list, add the elements
        # from the right list
        elif left_list_index == left_list_length:
            sorted_list.append(right_list[right_list_index])
            right_list_index += 1
        # If we've reached the end of the of the right list, add the elements
        # from the left list
        elif right_list_index == right_list_length:
            sorted_list.append(left_list[left_list_index])
            left_list_index += 1

    return sorted_list


def merge_sort(nums):
    # If the list is a single element, return it
    if len(nums) <= 1:
        return nums

    # Use floor division to get midpoint, indices must be integers
    mid = len(nums) // 2

    # Sort and merge each half
    left_list = merge_sort(nums[:mid])
    right_list = merge_sort(nums[mid:])

    # Merge the sorted lists into a new one
    return merge(left_list, right_list)


# Verify it works
random_list_of_nums = [120, 45, 68, 250, 176]
random_list_of_nums = merge_sort(random_list_of_nums)
print(random_list_of_nums)

   Note that the merge_sort() function, unlike the previous sorting algorithms, returns a new list that
   is sorted, rather than sorting the existing list.

   Therefore, Merge Sort requires space to create a new list of the same size as the input list.

Time Complexity
   Let's first look at the merge function. It takes two lists, and iterates n times, where n is the size
   of their combined input.

   The merge_sort function splits its given array in 2, and recursively sorts the sub-arrays. As the
   input being recursed is half of what was given, like binary trees this makes the time it takes to
   process grow logarithmically to n.

   Therefore the overall time complexity of the Merge Sort algorithm is O(nlog(n)).

Quick Sort
   This divide and conquer algorithm is the most often used sorting algorithm covered in this article.
   When configured correctly, it's extremely efficient and does not require the extra space Merge Sort
   uses. We partition the list around a pivot element, sorting values around the pivot.

Explanation
   Quick Sort begins by partitioning the list - picking one value of the list that will be in its sorted
   place. This value is called a pivot. All elements smaller than the pivot are moved to its left. All
   larger elements are moved to its right.

   Knowing that the pivot is in it's rightful place, we recursively sort the values around the pivot
   until the entire list is sorted.

   If you'd like to read a detailed, dedicated article for [19]Quick Sort, we've got you covered!

Implementation
# There are different ways to do a Quick Sort partition, this implements the
# Hoare partition scheme. Tony Hoare also created the Quick Sort algorithm.
def partition(nums, low, high):
    # We select the middle element to be the pivot. Some implementations select
    # the first element or the last element. Sometimes the median value becomes
    # the pivot, or a random one. There are many more strategies that can be
    # chosen or created.
    pivot = nums[(low + high) // 2]
    i = low - 1
    j = high + 1
    while True:
        i += 1
        while nums[i] < pivot:
            i += 1

        j -= 1
        while nums[j] > pivot:
            j -= 1

        if i >= j:
            return j

        # If an element at i (on the left of the pivot) is larger than the
        # element at j (on right right of the pivot), then swap them
        nums[i], nums[j] = nums[j], nums[i]


def quick_sort(nums):
    # Create a helper function that will be called recursively
    def _quick_sort(items, low, high):
        if low < high:
            # This is the index after the pivot, where our lists are split
            split_index = partition(items, low, high)
            _quick_sort(items, low, split_index)
            _quick_sort(items, split_index + 1, high)

    _quick_sort(nums, 0, len(nums) - 1)


# Verify it works
random_list_of_nums = [22, 5, 1, 18, 99]
quick_sort(random_list_of_nums)
print(random_list_of_nums)

Time Complexity
   The worst case scenario is when the smallest or largest element is always selected as the pivot. This
   would create partitions of size n-1, causing recursive calls n-1 times. This leads us to a worst case
   time complexity of O(n^2).

   While this is a terrible worst case, Quick Sort is heavily used because it's average time complexity
   is much quicker. While the partition function utilizes nested while loops, it does comparisons on all
   elements of the array to make its swaps. As such, it has a time complexity of O(n).

   With a good pivot, the Quick Sort function would partition the array into halves which grows
   logarithmically with n. Therefore the average time complexity of the Quick Sort algorithm is
   O(nlog(n)).

Python's Built-in Sort Functions
   While it's beneficial to understand these sorting algorithms, in most Python projects you would
   probably use the sort functions already provided in the language.

   We can change our list to have it's contents sorted with the sort() method:
apples_eaten_a_day = [2, 1, 1, 3, 1, 2, 2]
apples_eaten_a_day.sort()
print(apples_eaten_a_day) # [1, 1, 1, 2, 2, 2, 3]

   Or we can use the sorted() function to create a new sorted list:
apples_eaten_a_day_2 = [2, 1, 1, 3, 1, 2, 2]
sorted_apples = sorted(apples_eaten_a_day_2)
print(sorted_apples) # [1, 1, 1, 2, 2, 2, 3]

   They both sort in ascending order, but you can easily sort in descending order by setting the reverse
   flag to True:
# Reverse sort the list in-place
apples_eaten_a_day.sort(reverse=True)
print(apples_eaten_a_day) # [3, 2, 2, 2, 1, 1, 1]

# Reverse sort to get a new list
sorted_apples_desc = sorted(apples_eaten_a_day_2, reverse=True)
print(sorted_apples_desc) # [3, 2, 2, 2, 1, 1, 1]

   Unlike the sorting algorithm functions we created, both these functions can sort lists of tuples and
   classes. The sorted() function can sort any iterable object and that includes - lists, strings,
   tuples, dictionaries, sets, and custom [20]iterators you can create.

   These sort functions implement the [21]Tim Sort algorithm, an algorithm inspired by Merge Sort and
   Insertion Sort.

Speed Comparisons
   To get an idea of how quickly they perform, we generate a list of 5000 numbers between 0 and 1000. We
   then time how long it takes for each algorithm to complete. This is repeated 10 times so that we can
   more reliably establish a pattern of performance.

   These were the results, the time is in seconds:
   Run  Bubble      Selection   Insertion   Heap        Merge       Quick
   1    5.53188     1.23152     1.60355     0.04006     0.02619     0.01639
   2    4.92176     1.24728     1.59103     0.03999     0.02584     0.01661
   3    4.91642     1.22440     1.59362     0.04407     0.02862     0.01646
   4    5.15470     1.25053     1.63463     0.04128     0.02882     0.01860
   5    4.95522     1.28987     1.61759     0.04515     0.03314     0.01885
   6    5.04907     1.25466     1.62515     0.04257     0.02595     0.01628
   7    5.05591     1.24911     1.61981     0.04028     0.02733     0.01760
   8    5.08799     1.25808     1.62603     0.04264     0.02633     0.01705
   9    5.03289     1.24915     1.61446     0.04302     0.03293     0.01762
   10   5.14292     1.22021     1.57273     0.03966     0.02572     0.01606
   Avg  5.08488     1.24748     1.60986     0.04187     0.02809     0.01715

   You would get different values if you set up the test yourself, but the patterns observed should be
   the same or similar. Bubble Sort is the slowest the worst performer of all the algorithms. While it
   useful as an introduction to sorting and algorithms, it's not fit for practical use.

   We also notice that Quick Sort is very fast, nearly twice as fast as Merge Sort and it wouldn't need
   as much space to run. Recall that our partition was based on the middle element of the list,
   different partitions could have different outcomes.

   As Insertion Sort performs much less comparisons than Selection Sort, the implementations are usually
   quicker but in these runs Selection Sort is slightly faster.

   Insertion Sorts does much more swaps than Selection Sort. If swapping values takes up considerably
   more time than comparing values, then this "contrary" result would be plausible.

   Be mindful of the environment when choosing your sorting algorithm, as it will affect performance.

Conclusion
   Sorting algorithms gives us many ways to order our data. We looked at 6 different algorithms - Bubble
   Sort, Selection Sort, Insertion Sort, Merge Sort, Heap Sort, Quick Sort - and their implementations
   in Python.

   The amount of comparison and swaps the algorithm performs along with the environment the code runs
   are key determinants of performance. In real Python applications, it's recommended we stick with the
   built in Python sort functions for their flexibility on the input and speed.


---
