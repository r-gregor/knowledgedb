filename: ptn_pytest_testing-python-code_20181218.txt
https://www.linuxjournal.com/content/testing-your-code-pythons-pytest

Testing Your Code with Python's pytest
November 27, 2018

   Don't test your code? pytest removes any excuses.

   Software developers don't just write software; they also use software. So, they're the first to
   recognize, and understand, that software is complex and inevitably contains bugs.

   But, just because bugs are inevitable doesn't mean that developers can or should try to prevent them.
   And, thus, during the past few decades, there's been rapid growth in software testing. Testing is no
   longer seen as an optional or "nice to have" part of software development; it's considered an
   absolute must—part of the software development process. In many cases, the people in the Python
   courses I teach at various companies aren't developers per se, but instead testers—people with the
   full-time job of writing tests to ensure that the company's software is robust.

   I must admit that even though I've been writing software for a long time, I have rarely been as good
   about testing as I'd like to be. Sure, when I'm working on a large, complex app, I'll write tests,
   but it always seemed to be a bit of a burden. I know that it's good for me, will save tons of time in
   the future, will make the software more robust and maintenance easier, but really, if I just want to
   get my program out the door, why test? And besides, the various test frameworks I've used through the
   years never struck me as very impressive or easy to use.

   So for the past few years, I've been in a bit of a holding pattern. I want to test more, but testing
   is annoying, so I don't test, which makes it seem like even more of a burden, because it's not part
   of my regular process.

   All of this has changed for me recently, thanks to my discovery (long after other people, I admit) of
   the pytest library for Python. pytest turns out to be easy to use, easy to work with and easy to
   integrate into my work. Part of the reason for this is that pytest abandons the Python idea of
   "there's only one way to do it", giving developers a great degree of flexibility and freedom in
   choosing how to write tests.

   So in this article, I provide an introduction to pytest, showing how to start integrating it into
   your development process today. I plan to expand on this in my next article and describe some more
   advanced pytest features that you might need to use.

Basic pytest
   The idea behind pytest is that if you want to test a function, you'll write a separate function to
   test it. Actually, you'll probably want to write more than one test function, but that's in addition.

   For example, let's assume you have the following function that sums numbers:

def mysum(numbers):
    output = 0
    for one_number in numbers:
        output += one_number
    return output


   How can you test this function? (And yes, I'm ignoring the "test-driven development" mode of testing,
   in which you first write the tests and then write the code. You certainly can do TDD with pytest, but
   that isn't my point right now.)

   I put this function definition in mysum.py. I next can create a file called test_mysum.py in the same
   directory. Then, when I run pytest in the current directory, it'll run all of the files starting with
   test_. How might test_mysum.py look? Let's start with something simple:

from mysum import mysum

def test_sum_integers():
    assert mysum([0,1,2,3,4]) == 10


   As you can see, my test file test_mysum.py is fairly short. But it contains an actual test, and it
   also points to how tests can and will be written.

   First, you have to import the file that you want to test. This can be a simple "import XYZ"
   statement, or you can import names selectively from the module with "from X import Y". Either way,
   you'll need to import the functions and classes you'll be testing.

   The tests themselves are written as Python functions whose names begin with test_. (Yes, this means
   that tests are written in files whose names begin with test_, and then with functions in those files
   whose names begin with test_.)

   In simple cases, these test functions take no parameters. The functions are called by pytest, and the
   key to the tests is the assert statement. Normally, the assert statement in Python evaluates an
   expression. If the expression returns True, the assertion is recorded as a success, but otherwise
   ignored.

   So in the case of these example test functions, I'm basically saying "if I call the function with one
   argument, the list [0,1,2,3,4], I'm expecting to get the integer 10 back as a result".

   How do I run my test? I go into the directory where my files are located, and I type:

pytest
   Sure enough, pytest notices that there's a file matching the "test_*" pattern, which it runs. After
   some initial boilerplate indicating my system's configuration, I get the following output:

collected 1 item

test_mysum.py .                             [100%]

================1 passed in 0.02 seconds=====================


   In other words, there was one file (test_mysum.py). It contained a single test function, represented
   by a dot (.). And, 100% of those tests ran successfully—meaning, what I asserted is indeed what was
   actually returned.

   But of course, it's not enough to test with this sort of thing. I should probably call it with an
   empty list to make sure I get a 0 value back. So, let's add another test. Now test_mysum.py looks
   like this:

from mysum import mysum

def test_sum_integers():
    assert mysum([0,1,2,3,4]) == 10

def test_sum_nothing():
    assert mysum([]) == 0


   And when I run the tests, I get:

collected 2 items

test_mysum.py ..                             [100%]

================= 2 passed in 0.10 seconds ==================


   Let's add another test to see what happens if I invoke it with some floating-point numbers:

from mysum import mysum

def test_sum_integers():
    assert mysum([0,1,2,3,4]) == 10

def test_sum_floats():
    assert mysum([0.1,1.2,2.3,3.4,4.5]) == 11.5

def test_sum_nothing():
    assert mysum([]) == 0


   And now, when I test things, I get:

collected 3 items

test_mysum.py ...                             [100%]

=================== 3 passed in 0.06 seconds ================


   Sure enough, I've done a great job of testing so far.

   I should note that while I've used only a single assert statement in each function here, you
   definitely can have more than one. I prefer to keep each test function as focused as possible, and
   thus, I use as few assert statements as I can.

Failing Tests
   What if a test fails? Let's give it a shot by deliberately introducing a test that will fail. In
   test_mysum.py, I've added:

def test_one_and_one_are_three():
    assert mysum([1,1]) == 3


   When I run the tests, I get the following output:

test_mysum.py ...F                             [100%]

========================== FAILURES ==========================
____________________ test_one_and_one_are_three ____________

    def test_one_and_one_are_three():
>       assert mysum([1,1]) == 3
E       assert 2 == 3
E        +  where 2 = mysum([1, 1])

test_mysum.py:15: AssertionError
================== 1 failed, 3 passed in 0.30 seconds ========


   First, you can see that four test ran in test_mysum.py. The first three ran successfully and were
   represented by dots. The fourth test failed though. "Failure" in this case means that the assert
   statement claimed that there would be one answer (3), but that running the function produced a
   different answer (2). pytest not only indicates that there was a failure, but it also indicates in
   which test function the error occurred and the line where it took place. This allows you to figure
   out where the problem lies.

   In the case of failure, of course, there are two possibilities: the original code is wrong, or your
   test is wrong. Don't forget that tests are code, which means that they can be prone to problems too!
   However, if you write your tests cleanly and clearly (and before or as you write the code), I've
   found that most tests will be simple and straightforward, making it less likely that the tests have
   problems and easier to identify the location of bugs.

   You even can get more detailed output from pytest with the -v option:

test_mysum.py::test_sum_integers PASSED                [ 25%]
test_mysum.py::test_sum_floats PASSED                [ 50%]
test_mysum.py::test_sum_nothing PASSED                [ 75%]
test_mysum.py::test_one_and_one_are_three FAILED       [100%]

============================= FAILURES =======================
_______________________ test_one_and_one_are_three ___________

    def test_one_and_one_are_three():
>       assert mysum([1,1]) == 3
E       assert 2 == 3
E        +  where 2 = mysum([1, 1])

test_mysum.py:15: AssertionError
==================  1 failed, 3 passed in 0.22 seconds =======


   Now you can see precisely which tests passed and failed, as well as where the failures took place.

Parametrized Tests
   The successful tests created so far (test_sum_nothing, test_sum_integers and test_sum_floats) are all
   great and useful. But if you're like me, you might be wondering why you need three separate test
   functions just to check those three similar, but not identical, invocations. The pytest people agree,
   and they suggest the use of "parametrized tests". The idea here is that you define the test a single
   time, but tell pytest which inputs and outputs to provide.

   You can do this by applying a Python decorator to the test function. The decorator will take two
   arguments: a string with comma-separated names representing the parameters you want to pass to the
   test and a list of two-element tuples describing the inputs and outputs. For example, given all of
   these tests:

def test_sum_integers():
    assert mysum([0,1,2,3,4]) == 10

def test_sum_floats():
    assert mysum([0.1,1.2,2.3,3.4,4.5]) == 11.5

def test_sum_nothing():
    assert mysum([]) == 0


   You can replace them all with a single test:

import pytest
@pytest.mark.parametrize('numbers,output', [
    ([], 0),
    ([10, 20, 30], 60),
    ([0.1, 1.2, 2.3, 3.4, 4.5], 11.5)])
def test_mysum(numbers, output):
    assert mysum(numbers) == output


   While this does the same thing as before, it definitely looks a bit more complex. Let's break it
   down:
     * First, you need to import pytest, so that you'll have access to the decorator.
     * Then you use @pytest.mark.parametrize as the decorator. Note that if you are like me and prefer
       to spell it "parameterize", you'll get an error message scolding you for misspelling it.
     * The first decorator argument is a string containing the comma-separated names of the variables
       you want to pass. You'll always need at least two: one for the input and one for the output. If
       your function takes two inputs, you'll need to define three parameters (two input and one
       output).
     * The second argument is a list of two-element tuples. Each tuple describes a test run. Each tuple
       element will be assigned to a test function parameter.
     * Finally, the test function now needs to take two parameters, with the same names as defined in
       the decorator argument.

   With this in place, you can now run your tests, and you'll get the following output:

test_mysum.py::test_mysum[numbers0-0] PASSED         [33%]
test_mysum.py::test_mysum[numbers1-60] PASSED          [66%]
test_mysum.py::test_mysum[numbers2-11.5] PASSED         [100%]

======================== 3 passed in 0.12 seconds ============


   If you're thinking, "wow, that looks a lot like the output from the three separate tests"—well,
   that's exactly right.

Summary
   There's much more to say about pytest, but what I've written here covers most of the cases you'll
   encounter in your day-to-day work. Next time, I plan to cover a few other topics, including how to
   deal with exceptions, user input and output, and checking the code coverage.

Resources
   The pytest website is at https://docs.pytest.org/en/latest.

   An excellent book on the subject is Brian Okken's Python Testing with pytest, published by Pragmatic
   Programmers. He also has many other resources, about pytest and code testing in general, here.


---
https://www.linuxjournal.com/content/testing-your-code-pythons-pytest-part-ii

Testing Your Code with Python's pytest, Part II
December 11, 2018

   Testing functions isn't hard, but how do you test user input and output?

   In my last article, I started looking at "pytest", a framework for testing Python programs that's
   really changed the way I look at testing. For the first time, I really feel like testing is something
   I can and should do on a regular basis; pytest makes things so easy and straightforward.

   One of the main topics I didn't cover in my last article is user input and output. How can you test
   programs that expect to get input from files or from the user? And, how can you test programs that
   are supposed to display something on the screen?

   So in this article, I describe how to test input and output in a variety of ways, allowing you to
   test programs that interact with the outside world. I try not only to explain what you can do, but
   also show how it fits into the larger context of testing in general and pytest in particular.

User Input
   Say you have a function that asks the user to enter an integer and then returns the value of that
   integer, doubled. You can imagine that the function would look like this:

def double():
    x = input("Enter an integer: ")
    return int(x) * 2


   How can you test that function with pytest? If the function were to take an argument, the answer
   would be easy. But in this case, the function is asking for interactive input from the user. That's a
   bit harder to deal with. After all, how can you, in your tests, pretend to ask the user for input?

   In most programming languages, user input comes from a source known as standard input (or stdin). In
   Python, sys.stdin is a read-only file object from which you can grab the user's input.

   So, if you want to test the "double" function from above, you can (should) replace sys.stdin with
   another file. There are two problems with this, however. First, you don't really want to start
   opening files on disk. And second, do you really want to replace the value of sys.stdin in your
   tests? That'll affect more than just one test.

   The solution comes in two parts. First, you can use the pytest "monkey patching" facility to assign a
   value to a system object temporarily for the duration of the test. This facility requires that you
   define your test function with a parameter named monkeypatch. The pytest system notices that you've
   defined it with that parameter, and then not only sets the monkeypatch local variable, but also sets
   it up to let you temporarily set attribute names.

   In theory, then, you could define your test like this:

def test_double(monkeypatch):
    monkeypatch.setattr('sys.stdin', open('/etc/passwd'))
    print(double())


   In other words, this tells pytest that you want to open /etc/passwd and feed its contents to pytest.
   This has numerous problems, starting with the fact that /etc/passwd contains multiple lines, and that
   each of its lines is non-numeric. The function thus chokes and exits with an error before it even
   gets to the (useless) call to print.

   But there's another problem here, one that I mentioned above. You don't really want to be opening
   files during testing, if you can avoid it. Thus, one of the great tools in my testing toolbox is
   Python's StringIO class. The beauty of StringIO is its simplicity. It implements the API of a "file"
   object, but exists only in memory and is effectively a string. If you can create a StringIO instance,
   you can pass it to the call to monkeypatch.setattr, and thus make your tests precisely the way you
   want.

   Here's how to do that:

from io import StringIO
from double import double

number_inputs = StringIO('1234\n')

def test_double(monkeypatch):
    monkeypatch.setattr('sys.stdin', number_inputs)
    assert double() == 2468


   You first create a StringIO object containing the input you want to simulate from the user. Note that
   it must contain a newline (\n) to ensure that you'll see the end of the user's input and not hang.

   You assign that to a global variable, which means you'll be able to access it from within your test
   function. You then add the assertion to your test function, saying that the result should be 2468.
   And sure enough, it works.

   I've used this technique to simulate much longer files, and I've been quite satisfied by the speed
   and flexibility. Just remember that each line in the input "file" should end with a newline
   character. I've found that creating a StringIO with a triple-quoted string, which lets me include
   newlines and write in a more natural file-like way, works well.

   You can use monkeypatch to simulate calls to a variety of other objects as well. I haven't had much
   occasion to do that, but you can imagine all sorts of network-related objects that you don't actually
   want to use when testing. By monkey-patching those objects during testing, you can pretend to connect
   to a remote server, when in fact you're just getting pre-programmed text back.

Exceptions

   What happens if you call the test_double function with a string? You probably should test that too:

str_inputs = StringIO('abcd\n')
def test_double_str(monkeypatch):
    monkeypatch.setattr('sys.stdin', str_inputs)
    assert double() == 'abcdabcd'


   It looks great, right? Actually, not so much:

E   ValueError: invalid literal for int() with base 10: 'abcd'


   The test failed, because the function exited with an exception. And that's okay; after all, the
   function should raise an exception if the user gives input that isn't numeric. But, wouldn't it be
   nice to specify and test it?

   The thing is, how can you test for an exception? You can't exactly use a usual assert statement, much
   as you might like to. After all, you somehow need to trap the exception, and you can't simply say:

assert double() == ValueError

   That's because exceptions aren't values that are returned. They are raised through a different
   mechanism.

   Fortunately, pytest offers a good solution to this, albeit with slightly different syntax than you've
   seen before. It uses a with statement, which many Python developers recognize from its common use in
   ensuring that files are flushed and closed when you write to them. The with statement opens a block,
   and if an exception occurs during that block, then the "context manager"—that is, the object that the
   with runs on—has an opportunity to handle the exception. pytest takes advantage of this with the
   pytest.raises context manager, which you can use in the following way:

def test_double_str(monkeypatch):
    with pytest.raises(ValueError):
        monkeypatch.setattr('sys.stdin', str_inputs)
        result = double()


   Notice that you don't need an assert statement here, because the pytest.raises is, effectively, the
   assert statement. And, you do have to indicate the type of error (ValueError) that you're trying to
   trap, meaning what you expect to receive.

   If you want to capture (or assert) something having to do with the exception that was raised, you can
   use the as part of the with statement. For example:

def test_double_str(monkeypatch):
    with pytest.raises(ValueError) as e:
        monkeypatch.setattr('sys.stdin', str_inputs)
        results = double()
    assert str(e.value) == "invalid literal for int()
     ↪with base 10: 'abcd'"


   Now you can be sure that not only was a ValueError exception raised, but also what message was
   raised.

Output
   I generally advise people not to use print in their functions. After all, I'd like to get some value
   back from a function; I don't really want to display something on the screen. But at some point, you
   really do actually need to display things to the user. How can you test for that?

   The pytest solution is via the capsys plugin. Similar to monkeypatch, you declare capsys as a
   parameter to your test function. You then run your function, allowing it to produce its output. Then
   you invoke the readouterr function on capsys, which returns a tuple of two strings containing the
   output to stdout and its error-message counterpart, stderr. You then can run assertions on those
   strings.

   For example, let's assume this function:

def hello(name):
    print(f"Hello, {name}!")


   You can test it in the following way:

def test_hello(capsys):
    hello('world')
    captured_stdout, captured_stderr = capsys.readouterr()
    assert captured_stdout == 'Hello, world!'


   But wait! This test actually fails:

E   AssertionError: assert 'Hello, world!\n' == 'Hello, world!'
E     - Hello, world!
E     ?              -
E     + Hello, world!


   Do you see the problem? The output, as written by print, includes a trailing newline (\n) character.
   But the test didn't check for that. Thus, you can check for the trailing newline, or you can use
   str.strip on stdout:

def test_hello(capsys):
    hello('world')
    captured_stdout, captured_stderr = capsys.readouterr()
    assert captured_stdout.strip() == 'Hello, world!'


Summary
   pytest continues to impress me as a testing framework, in no small part because it combines a lot of
   small, simple ideas in ways that feel natural to me as a developer. It has gone a long way toward
   increasing my use of tests, both in general development and in my teaching. My "Weekly Python
   Exercise" subscription service now includes tests, and I feel like it has improved a great deal as a
   result.

   In my next article, I plan to take a third and final look at pytest, exploring some of the other ways
   it can interact with (and help) write robust and useful programs.

---
https://www.linuxjournal.com/content/python-testing-pytest-fixtures-and-coverage

Python Testing with pytest: Fixtures and Coverage
January 14, 2019

   Improve your Python testing even more.

   In my last two articles, I introduced pytest, a library for testing Python code (see "Testing Your
   Code with Python's pytest" [https://www.linuxjournal.com/content/testing-your-code-pythons-pytest]Part I
   and [https://www.linuxjournal.com/content/testing-your-code-pythons-pytest-part-ii]Part II). pytest has
   become quite popular, in no small    part because it's so easy to write tests and integrate those tests into
   your software development process. I've become a big fan, mostly because after years of saying I should get
   better about testing my software, pytest finally has made it possible.

   So in this article, I review two features of pytest that I haven't had a chance to cover yet:
   fixtures and code coverage, which will (I hope) convince you that pytest is worth exploring and
   incorporating into your work.

Fixtures
   When you're writing tests, you're rarely going to write just one or two. Rather, you're going to
   write an entire "test suite", with each test aiming to check a different path through your code. In
   many cases, this means you'll have a few tests with similar characteristics, something that pytest
   handles with "parametrized tests".

   But in other cases, things are a bit more complex. You'll want to have some objects available to all
   of your tests. Those objects might contain data you want to share across tests, or they might involve
   the network or filesystem. These are often known as "fixtures" in the testing world, and they take a
   variety of different forms.

   In pytest, you define fixtures using a combination of the pytest.fixture decorator, along with a
   function definition. For example, say you have a file that returns a list of lines from a file, in
   which each line is reversed:

def reverse_lines(f):
   return [one_line.rstrip()[::-1] + '\n'
           for one_line in f]

   Note that in order to avoid the newline character from being placed at the start of the line, you
   remove it from the string before reversing and then add a '\n' in each returned string. Also note
   that although it probably would be a good idea to use a generator expression rather than a list
   comprehension, I'm trying to keep things relatively simple here.

   If you're going to test this function, you'll need to pass it a file-like object. In my [26]last
   article, I showed how you could use a StringIO object for such a thing, and that remains the case.
   But rather than defining global variables in your test file, you can create a fixture that'll provide
   your test with the appropriate object at the right time.

   Here's how that looks in pytest:

@pytest.fixture
def simple_file():
   return StringIO('\n'.join(['abc', 'def', 'ghi', 'jkl']))

   On the face of it, this looks like a simple function—one that returns the value you'll want to use
   later. And in many ways, it's similar to what you'd get if you were to define a global variable by
   the name of "simple_file".

   At the same time, fixtures are used differently from global variables. For example, let's say you
   want to include this fixture in one of your tests. You then can mention it in the test's parameter
   list. Then, inside the test, you can access the fixture by name. For example:

def test_reverse_lines(simple_file):
   assert reverse_lines(simple_file) == ['cba\n', 'fed\n',
 ↪'ihg\n', 'lkj\n']

   But it gets even better. Your fixture might act like data, in that you don't invoke it with
   parentheses. But it's actually a function under the hood, which means it executes every time you
   invoke a test using that fixture. This means that the fixture, in contrast with regular-old data, can
   make calculations and decisions.

   You also can decide how often a fixture is run. For example, as it's written now, this fixture will
   run once per test that mentions it. That's great in this case, when you want to compare with a list
   or file-like structure. But what if you want to set up an object and then use it multiple times
   without creating it again? You can do that by setting the fixture's "scope". For example, if you set
   the scope of the fixture to be "module", it'll be available throughout your tests but will execute
   only a single time. You can do this by passing the scope parameter to the @pytest.fixture decorator:

@pytest.fixture(scope='module')
def simple_file():
   return StringIO('\n'.join(['abc', 'def', 'ghi', 'jkl']))

   I should note that giving this particular fixture "module" scope is a bad idea, since the second test
   will end up having a StringIO whose location pointer (checked with file.tell) is already at the end.

   These fixtures work quite differently from the traditional setup/teardown system that many other test
   systems use. However, the pytest people definitely have convinced me that this is a better way.

   But wait—perhaps you can see where the "setup" functionality exists in these fixtures. And, where's
   the "teardown" functionality? The answer is both simple and elegant. If your fixture uses "yield"
   instead of "return", pytest understands that the post-yield code is for tearing down objects and
   connections. And yes, if your fixture has "module" scope, pytest will wait until all of the functions
   in the scope have finished executing before tearing it down.

Coverage
   This is all great, but if you've ever done any testing, you know there's always the question of how
   thoroughly you have tested your code. After all, let's say you've written five functions, and that
   you've written tests for all of them. Can you be sure you've actually tested all of the possible
   paths through those functions?

   For example, let's assume you have a very strange function, only_odd_mul, which multiplies only odd
   numbers:

def only_odd_mul(x, y):
   if x%2 and y%2:
       return x * y
   else:
       raise NoEvenNumbersHereException(f'{x} and/or {y}
 ↪not odd')


   Here's a test you can run on it:

def test_odd_numbers():
   assert only_odd_mul(3, 5) == 15

   Sure enough, the test passed. It works great! The software is terrific!

   Oh, but wait—as you've probably noticed, that wasn't a very good job of testing it. There are ways in
   which the function could give a totally different result (for example, raise an exception) that the
   test didn't check.

   Perhaps it's easy to see it in this example, but when software gets larger and more complex, it's not
   going to be so easy to eyeball it. That where you want to have "code coverage", checking that your
   tests have run all of the code.

   Now, 100% code coverage doesn't mean that your code is perfect or that it lacks bugs. But it does
   give you a greater degree of confidence in the code and the fact that it has been run at least once.

   So, how can you include code coverage with pytest? It turns out that there's a package called
   pytest-cov on PyPI that you can download and install. Once that's done, you can invoke pytest with
   the --cov option. If you don't say anything more than that, you'll get a coverage report for every
   part of the Python library that your program used, so I strongly suggest you provide an argument to
   --cov, specifying which program(s) you want to test. And, you should indicate the directory into
   which the report should be written. So in this case, you would say:

pytest --cov=mymul .

   Once you've done this, you'll need to turn the coverage report into something human-readable. I
   suggest using HTML, although other output formats are available:

coverage html
   This creates a directory called htmlcov. Open the index.html file in this directory using your
   browser, and you'll get a web-based report showing (in red) where your program still lacks coverage.
   Sure enough, in this case, it showed that the even-number path wasn't covered. Let's add a test to do
   this:

def test_even_numbers():
   with pytest.raises(NoEvenNumbersHereException):
       only_odd_mul(2,4)


   And as expected, coverage has now gone up to 100%! That's definitely something to appreciate and
   celebrate, but it doesn't mean you've reached optimal testing. You can and should cover different
   mixtures of arguments and what will happen when you pass them.

Summary
   If you haven't guessed from my three-part focus on pytest, I've been bowled over by the way this
   testing system has been designed. After years of hanging my head in shame when talking about testing,
   I've started to incorporate it into my code, including in my online "Weekly Python Exercise" course.
   If I can get into testing, so can you. And although I haven't covered everything pytest offers, you
   now should have a good sense of what it is and how to start using it.


---
