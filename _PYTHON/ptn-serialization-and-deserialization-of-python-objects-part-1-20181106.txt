filename: ptn_serialization-and-deserialization-of-python-objects_20181106.txt
https://code.tutsplus.com/tutorials/serialization-and-deserialization-of-python-objects-part-1--cms-26183

Serialization and Deserialization of Python Objects: Part 1

Python object serialization and deserialization is an important aspect of any non-trivial program. If in
Python you save something to a file, if you read a configuration file, or if you respond to an HTTP request,
you do object serialization and deserialization. 

In one sense, serialization and deserialization are the most boring things in the world. Who cares about all
the formats and protocols? You just want to persist or stream some Python objects and get them back later
intact. 

This is a very healthy way to look at the world at the conceptual level. But, at the pragmatic level, which
serialization scheme, format or protocol you choose may determine how fast your program runs, how secure it
is, how much freedom you have to maintain your state, and how well you're going to interoperate with other
systems. 

The reason there are so many options is that different circumstances call for different solutions. There is
no "one size fits all". In this two-part tutorial I'll go over the pros and cons of the most successful
serialization and deserialization schemes, show how to use them, and provide guidelines for choosing between
them when faced with a specific use case.

	Running Example
In the following sections I'll serialize and deserialize the same Python object graphs using different
serializers. To avoid repetition, I'll define these object graphs here.

Simple Object Graph

The simple object graph is a dictionary that contains a list of integers, a string, a float, a boolean, and a
None.

simple = dict(int_list=[1, 2, 3],
              text='string',
              number=3.44,
              boolean=True,
              none=None)

	Complex Object Graph
The complex object graph is also a dictionary, but it contains a datetime object and user-defined class
instance that has a self.simple attribute, which is set to the simple object graph.

from datetime import datetime

class A(object):
    def __init__(self, simple):
        self.simple = simple
		
    def __eq__(self, other):
        if not hasattr(other, 'simple'):
            return False
        return self.simple == other.simple

    def __ne__(self, other):
        if not hasattr(other, 'simple'):
            return True
        return self.simple != other.simple

complex = dict(a=A(simple), when=datetime(2016, 3, 7))

Pickle
Pickle is a staple. It is a native Python object serialization format. The pickle interface provides four
methods: dump, dumps, load, and loads. The dump() method serializes to an open file (file-like object). The
dumps() method serializes to a string. The load() method deserializes from an open file-like object. The
loads() method deserializes from a string.

Pickle supports by default a textual protocol, but has also a binary protocol, which is more efficient, but
not human-readable (helpful when debugging).

Here is how you pickle a Python object graph to a string and to a file using both protocols.

import cPickle as pickle

pickle.dumps(simple)
"(dp1\nS'text'\np2\nS'string'\np3\nsS'none'\np4\nNsS'boolean'\np5\nI01\nsS'number'\np6\nF3.4399999999999999\nsS'int_list'\np7\n(lp8\nI1\naI2\naI3\nas."

pickle.dumps(simple, protocol=pickle.HIGHEST_PROTOCOL)
'\x80\x02}q\x01(U\x04textq\x02U\x06stringq\x03U\x04noneq\x04NU\x07boolean\x88U\x06numberq\x05G@\x0b\x85\x1e\xb8Q\xeb\x85U\x08int_list]q\x06(K\x01K\x02K\x03eu.'

The binary representation may seem larger, but this is an illusion due to its presentation. When dumping to a
file, the textual protocol is 130 bytes, while the binary protocol is only 85 bytes.

pickle.dump(simple, open('simple1.pkl', 'w'))
pickle.dump(simple, open('simple2.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)

ls -la sim*.*
-rw-r--r--  1 gigi  staff  130 Mar  9 02:42 simple1.pkl
-rw-r--r--  1 gigi  staff   85 Mar  9 02:43 simple2.pkl

Unpickling from a string is as simple as:
x = pickle.loads("(dp1\nS'text'\np2\nS'string'\np3\nsS'none'\np4\nNsS'boolean'\np5\nI01\nsS'number'\np6\nF3.4399999999999999\nsS'int_list'\np7\n(lp8\nI1\naI2\naI3\nas.")
assert x == simple

x = pickle.loads('\x80\x02}q\x01(U\x04textq\x02U\x06stringq\x03U\x04noneq\x04NU\x07boolean\x88U\x06numberq\x05G@\x0b\x85\x1e\xb8Q\xeb\x85U\x08int_list]q\x06(K\x01K\x02K\x03eu.')
assert x == simple

Note that pickle can figure out the protocol automatically. There is no need to specify a protocol even for
the binary one.

Unpickling from a file is just as easy. You just need to provide an open file.
x = pickle.load(open('simple1.pkl'))
assert x == simple
x = pickle.load(open('simple2.pkl'))
assert x == simple
x = pickle.load(open('simple2.pkl', 'rb'))
assert x == simple

According to the documentation, you're supposed to open binary pickles using the 'rb' mode, but as you can
see it works either way.

Let's see how pickle deals with the complex object graph.
pickle.dumps(complex)

"(dp1\nS'a'\nccopy_reg\n_reconstructor\np2\n(c__main__\nA\np3\nc__builtin__\nobject\np4\nNtRp5\n(dp6\nS'simple'\np7\n(dp8\nS'text'\np9\nS'string'\np10\nsS'none'\np11\nNsS'boolean'\np12\nI01\nsS'number'\np13\nF3.4399999999999999\nsS'int_list'\np14\n(lp15\nI1\naI2\naI3\nassbsS'when'\np16\ncdatetime\ndatetime\np17\n(S'\\x07\\xe0\\x03\\x07\\x00\\x00\\x00\\x00\\x00\\x00'\ntRp18\ns."

pickle.dumps(complex, protocol=pickle.HIGHEST_PROTOCOL)
'\x80\x02}q\x01(U\x01ac__main__\nA\nq\x02)\x81q\x03}q\x04U\x06simpleq\x05}q\x06(U\x04textq\x07U\x06stringq\x08U\x04noneq\tNU\x07boolean\x88U\x06numberq\nG@\x0b\x85\x1e\xb8Q\xeb\x85U\x08int_list]q\x0b(K\x01K\x02K\x03eusbU\x04whenq\x0ccdatetime\ndatetime\nq\rU\n\x07\xe0\x03\x07\x00\x00\x00\x00\x00\x00\x85Rq\x0eu.'

pickle.dump(complex, open('complex1.pkl', 'w'))
pickle.dump(complex, open('complex2.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)

ls -la comp*.*
-rw-r--r--  1 gigi  staff  327 Mar  9 02:58 complex1.pkl
-rw-r--r--  1 gigi  staff  171 Mar  9 02:58 complex2.pkl

The efficiency of the binary protocol is even greater with complex object graphs.

	JSON
JSON (JavaScript Object Notation) has been part of the Python standard library since Python 2.5. I'll
consider it a native format at this point. It is a text-based format and is the unofficial king of the web as
far as object serialization goes. Its type system naturally models JavaScript, so it is pretty limited. 

Let's serialize and deserialize the simple and complex objects graphs and see what happens. The interface is
almost identical to the pickle interface. You have dump(), dumps(), load(), and loads() functions. But, there
are no protocols to select, and there are many optional arguments to control the process. Let's start simple
by dumping the simple object graph without any special arguments:

import json
print json.dumps(simple)
{"text": "string", "none": null, "boolean": true, "number": 3.44, "int_list": [1, 2, 3]}

The output looks pretty readable, but there is no indentation. For a larger object graph, this can be a
problem. Let's indent the output:

print json.dumps(simple, indent=4)
{
    "text": "string",
    "none": null,
    "boolean": true,
    "number": 3.44,
    "int_list": [
        1,
        2,
        3
    ]
}

That looks much better. Let's move on to the complex object graph.

json.dumps(complex)
---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)
<ipython-input-19-1be2d89d5d0d> in <module>()
----> 1 json.dumps(complex)

/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/__init__.pyc in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, encoding, default, sort_keys, **kw)
    241         cls is None and indent is None and separators is None and
    242         encoding == 'utf-8' and default is None and not sort_keys and not kw):
--> 243         return _default_encoder.encode(obj)
    244     if cls is None:
    245         cls = JSONEncoder

/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.pyc in encode(self, o)
    205         # exceptions aren't as detailed.  The list call should be roughly
    206         # equivalent to the PySequence_Fast that ''.join() would do.
--> 207         chunks = self.iterencode(o, _one_shot=True)
    208         if not isinstance(chunks, (list, tuple)):
    209             chunks = list(chunks)

/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.pyc in iterencode(self, o, _one_shot)
    268                 self.key_separator, self.item_separator, self.sort_keys,
    269                 self.skipkeys, _one_shot)
--> 270         return _iterencode(o, 0)
    271
    272 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,

/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.pyc in default(self, o)
    182
    183         """
--> 184         raise TypeError(repr(o) + " is not JSON serializable")
    185
    186     def encode(self, o):
TypeError: <__main__.A object at 0x10f367cd0> is not JSON serializable

Whoa! That doesn't look good at all. What happened? The error message is that the A object is not JSON
serializable. Remember that JSON has a very limited type system and it can't serialize user defined classes
automatically. The way to address it is to subclass the JSONEncoder class used by the json module and
implement the default() that is called whenever the JSON encoder runs into an object it can't serialize. 

The job of the custom encoder is to convert it to a Python object graph that the JSON encoder is able to
encode. In this case we have two objects that require special encoding: the datetime object and the A class.
The following encoder does the job. Each special object is converted to a dict where the key is the name of
the type surrounded by dunders (double underscores). This will be important for decoding. 

from datetime import datetime
import json

class CustomEncoder(json.JSONEncoder):
     def default(self, o):
         if isinstance(o, datetime):
             return {'__datetime__': o.replace(microsecond=0).isoformat()}
         return {'__{}__'.format(o.__class__.__name__): o.__dict__}

Let's try again with our custom encoder:
serialized = json.dumps(complex, indent=4, cls=CustomEncoder)
print serialized
{
    "a": {
        "__A__": {
            "simple": {
                "text": "string",
                "none": null,
                "boolean": true,
                "number": 3.44,
                "int_list": [
                    1,
                    2,
                    3
                ]
            }
        }
    },
    "when": {
        "__datetime__": "2016-03-07T00:00:00"
    }
}

This is beautiful. The complex object graph was serialized properly, and the original type information of the
components was retained via the keys: "__A__" and "__datetime__". If you use dunders for your names, then you
need to come up with a different convention to denote special types.

Let's decode the complex object graph.
> deserialized = json.loads(serialized)
> deserialized == complex
False

Hmmm, the deserialization worked (no errors), but it is different than the original complex object graph we
serialized. Something is wrong. Let's take a look at the deserialized object graph. I'll use the pprint
function of the pprint module for pretty printing.

> from pprint import pprint
> pprint(deserialized)
{u'a': {u'__A__': {u'simple': {u'boolean': True,
                               u'int_list': [1, 2, 3],
                               u'none': None,
                               u'number': 3.44,
                               u'text': u'string'}}},
 u'when': {u'__datetime__': u'2016-03-07T00:00:00'}}

Ok. The problem is that the json module doesn't know anything about the A class or even the standard datetime
object. It just deserializes everything by default to the Python object that matches its type system. In
order to get back to a rich Python object graph, you need custom decoding. 

There is no need for a custom decoder subclass. The load() and loads() functions provide the "object_hook"
parameter that lets you provide a custom function that converts dicts to objects. 

def decode_object(o):
    if '__A__' in o:
        a = A()
        a.__dict__.update(o['__A__'])
        return a
    elif '__datetime__' in o:
        return datetime.strptime(o['__datetime__'], '%Y-%m-%dT%H:%M:%S')
    return o

Let's decode using the decode_object() function as a parameter to the loads() object_hook parameter.
> deserialized = json.loads(serialized, object_hook=decode_object)
> print deserialized
{u'a': <__main__.A object at 0x10d984790>, u'when': datetime.datetime(2016, 3, 7, 0, 0)}

> deserialized == complex
True

	Conclusion
In part one of this tutorial, you've learned about the general concept of serialization and deserialization
of Python objects and explored the ins and out of serializing Python objects using Pickle and JSON. 

In part two, you'll learn about YAML, performance and security concerns, and a quick review of additional
serialization schemes.


---
https://code.tutsplus.com/tutorials/serialization-and-deserialization-of-python-objects-part-2--cms-26184

Serialization and Deserialization of Python Objects: Part 2

This is part two of a tutorial on serializing and deserializing Python objects. In part one, you learned the
basics and then dove into the ins and outs of Pickle and JSON. 

In this part you'll explore YAML (make sure to have the running example from part one), discuss performance
and security considerations, get a review of additional serialization formats, and finally learn how to
choose the right scheme.

	YAML
YAML is my favorite format. It is a human-friendly data serialization format. Unlike Pickle and JSON, it is
not part of the Python standard library, so you need to install it:
pip install yaml

The yaml module has only load() and dump() functions. By default they work with strings like loads() and
dumps(), but can take a second argument, which is an open stream and then can dump/load to/from files.

import yaml
print yaml.dump(simple)

boolean: true
int_list: [1, 2, 3]
none: null
number: 3.44
text: string

Note how readable YAML is compared to Pickle or even JSON. And now for the coolest part about YAML: it
understands Python objects! No need for custom encoders and decoders. Here is the complex serialization/
deserialization using YAML:

> serialized = yaml.dump(complex)
> print serialized

a: !!python/object:__main__.A
  simple:
    boolean: true
    int_list: [1, 2, 3]
    none: null
    number: 3.44
    text: string
when: 2016-03-07 00:00:00



> deserialized = yaml.load(serialized)
> deserialized == complex
True

As you can see, YAML has its own notation to tag Python objects. The output is still very human readable. The
datetime object doesn't require any special tagging because YAML inherently supports datetime objects. 

	Performance
Before you start thinking of performance, you need to think if performance is a concern at all. If you
serialize/deserialize a small amount of data relatively infrequently (e.g. reading a config file at the
beginning of a program) then performance is not really a concern and you can move on.

But, assuming you profiled your system and discovered that serialization and/or deserialization are causing
performance issues, here are the things to address.

The are two aspects for performance: how fast can you serialize/deserialize, and how big is the serialized
representation?

To test the performance of the various serialization formats, I'll create a largish data structure and
serialize/deserialize it using Pickle, YAML, and JSON. The big_data list contains 5,000 complex objects.

big_data = [dict(a=simple, when=datetime.now().replace(microsecond=0)) for i in range(5000)]

	Pickle
I'll use IPython here for its convenient %timeit magic function that measures execution times.

import cPickle as pickle

In [190]: %timeit serialized = pickle.dumps(big_data)
10 loops, best of 3: 51 ms per loop

In [191]: %timeit deserialized = pickle.loads(serialized)
10 loops, best of 3: 24.2 ms per loop

In [192]: deserialized == big_data
Out[192]: True

In [193]: len(serialized)
Out[193]: 747328

The default pickle takes 83.1 milliseconds to serialize and 29.2 milliseconds to deserialize, and the
serialized size is 747,328 bytes.

Let's try with the highest protocol.
In [195]: %timeit serialized = pickle.dumps(big_data, protocol=pickle.HIGHEST_PROTOCOL)
10 loops, best of 3: 21.2 ms per loop

In [196]: %timeit deserialized = pickle.loads(serialized)
10 loops, best of 3: 25.2 ms per loop

In [197]: len(serialized)
Out[197]: 394350

Interesting results. The serialization time shrank to only 21.2 milliseconds, but the deserialization time
increased a little to 25.2 milliseconds. The serialized size shrank significantly to 394,350 bytes (52%).

	JSON
In [253] %timeit serialized = json.dumps(big_data, cls=CustomEncoder)
10 loops, best of 3: 34.7 ms per loop

In [253] %timeit deserialized = json.loads(serialized, object_hook=decode_object)
10 loops, best of 3: 148 ms per loop

In [255]: len(serialized)
Out[255]: 730000

Ok. Performance seems to be a little worse than Pickle for encoding, but much, much worse for decoding: 6
times slower. What's going on? This is an artifact of the object_hook function that needs to run for every
dictionary to check if it needs to convert it to an object. Running without the object hook is much faster.

%timeit deserialized = json.loads(serialized)
10 loops, best of 3: 36.2 ms per loop

The lesson here is that when serializing and deserializing to JSON, consider very carefully any custom
encodings because they may have a major impact on the overall performance.

	YAML
In [293]: %timeit serialized = yaml.dump(big_data)
1 loops, best of 3: 1.22 s per loop

In[294]: %timeit deserialized = yaml.load(serialized)
1 loops, best of 3: 2.03 s per loop

In [295]: len(serialized)
Out[295]: 200091

Ok. YAML is really, really slow. But, note something interesting: the serialized size is just 200,091 bytes.
Much better than both Pickle and JSON. Let's look inside real quick:

In [300]: print serialized[:211]
- a: &id001
    boolean: true
    int_list: [1, 2, 3]
    none: null
    number: 3.44
    text: string
  when: 2016-03-13 00:11:44
- a: *id001
  when: 2016-03-13 00:11:44
- a: *id001
  when: 2016-03-13 00:11:44

YAML is being very clever here. It identified that all 5,000 dicts share the same value for the 'a' key, so
it stores it only once and references it using *id001 for all objects.

	Security
Security is an often a critical concern. Pickle and YAML, by virtue of constructing Python objects, are
vulnerable to code execution attacks. A cleverly formatted file can contain arbitrary code that will be
executed by Pickle or YAML. There is no need to be alarmed. This is by design and is documented in Pickle's
documentation:
    Warning: The pickle module is not intended to be secure against erroneous or maliciously constructed
    data. Never unpickle data received from an untrusted or unauthenticated source.

As well as in YAML's documentation:
    Warning: It is not safe to call yaml.load with any data received from an untrusted source! yaml.load is
    as powerful as pickle.load and so may call any Python function.

You just need to understand that you shouldn't load serialized data received from untrusted sources using
Pickle or YAML. JSON is OK, but again if you have custom encoders/decoders than you may be exposed, too.

The yaml module provides the yaml.safe_load() function that will load only simple objects, but then you lose
a lot of YAML's power and maybe opt to just use JSON.

	Other Formats
There are many other serialization formats available. Here are a few of them.

	Protobuf
Protobuf, or protocol buffers, is Google's data interchange format. It is implemented in C++ but has Python
bindings. It has a sophisticated schema and packs data efficiently. Very powerful, but not very easy to use.

	MessagePack
MessagePack is another popular serialization format. It is also binary and efficient, but unlike Protobuf it
doesn't require a schema. It has a type system that's similar to JSON, but a little richer. Keys can be any
type, and not just strings and non-UTF8 strings are supported.

	CBOR
CBOR stands for Concise Binary Object Representation. Again, it supports the JSON data model. CBOR is not as
well-known as Protobuf or MessagePack but is interesting for two reasons: 
 1. It is an official Internet standard: RFC 7049.
 2. It was designed specifically for the Internet of Things (IoT).

	How to Choose?
This is the big question. With so many options, how do you choose? Let's consider the various factors that
should be taken into account:
 1. Should the serialized format be human-readable and/or human-editable?
 2. Is serialized content going to be received from untrusted sources?
 3. Is serialization/deserialization a performance bottleneck?
 4. Does serialized data need to be exchanged with non-Python environments?

I'll make it very easy for you and cover several common scenarios and which format I recommend for each one:

	Auto-Saving Local State of a Python Program
Use pickle (cPickle) here with the HIGHEST_PROTOCOL. It's fast, efficient and can store and load most Python
objects without any special code. It can be used as a local persistent cache also.

	Configuration Files
Definitely YAML. Nothing beats its simplicity for anything humans need to read or edit. It's used
successfully by Ansible and many other projects. In some situations, you may prefer to use straight Python
modules as configuration files. This may be the right choice, but then it's not serialization, and it's
really part of the program and not a separate configuration file.

	Web APIs
JSON is the clear winner here. These days, Web APIs are consumed most often by JavaScript web applications
that speak JSON natively. Some Web APIs may return other formats (e.g. csv for dense tabular result sets),
but I would argue that you can package csv data into JSON with minimal overhead (no need to repeat each row
as an object with all the column names). 

	High-Volume / Low-Latency Large-Scale Communication
Use one of the binary protocols: Protobuf (if you need a schema), MessagePack, or CBOR. Run your own tests to
verify the performance and the representative power of each option.

	Conclusion
Serialization and deserialization of Python objects is an important aspect of distributed systems. You can't
send Python objects directly over the wire. You often need to interoperate with other systems implemented in
other languages, and sometimes you just want to store the state of your program in persistent storage. 

Python comes with several serialization schemes in its standard library, and many more are available as
third-party modules. Being aware of all the options and the pros and cons of each one will let you choose the
best method for your situation.

---
