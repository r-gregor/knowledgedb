filename: python_sorting_big_files_or_csv-multif_20171019.txt
https://stackoverflow.com/questions/7079473/sorting-large-text-data

sorting large text data

   I have a large file (100 million lines of tab separated values - about 1.5GB in size). What is the
   fastest known way to sort this based on one of the fields?

   I have tried hive. I would like to see if this can be done faster using python.

***
   Have you considered using the *nix sort program? in raw terms, it'll probably be faster than most
   Python scripts.

   Use -t $'\t' to specify that it's tab-separated, -k n to specify the field, where n is the field
   number, and -o outputfile if you want to output the result to a new file. Example:
sort -t $'\t' -k 4 -o sorted.txt input.txt

   Will sort input.txt on its 4th field, and output the result to sorted.txt

***
   the unix sort command is a very powerful tool indeed. You can control the format of the field to sort
   on (numeric, date, etc.) and the amount of memory which the program can allocate, performing a split
   + merge sort if necessary. – [43]gurney alex Aug 16 '11 at 16:16

***
   alex can you give an example? The sort program on its own is taking quite a long time ... of the
   order of 40 mins. This may have something to do with memory allocation or disk IO. I'm not sure how
   to figure out what the bottleneck is, but I'm guessing that your suggestion may be useful.

***
   one error in the solution above: to use only the 2nd field, one needs -k 2,2 ... so it is not zero
   indexed (at least not on Kubuntu 11.04's version of sort). – [45]fodon Aug 16 '11 at 17:31

***
   you want to build an in-memory index for the file:
    1. create an empty list
    2. open the file
    3. read it line by line (using f.readline(), and store in the list a tuple consisting of the value
       on which you want to sort (extracted with line.split('\t').strip()) and the offset of the line in
       the file (which you can get by calling f.tell() before calling f.readline())
    4. close the file
    5. sort the list

   Then to print the sorted file, reopen the file and for each element of your list, use f.seek(offset)
   to move the file pointer to the beginning of the line, f.readline() to read the line and print the
   line.

   Optimization: you may want to store the length of the line in the list, so that you can use
   f.read(length) in the printing phase.

   Sample code (optimized for readability, not speed):
<code>   
def build_index(filename, sort_col):
    index = []
    f = open(filename)
    while True:
        offset = f.tell()
        line = f.readline()
        if not line:
            break
        length = len(line)
        col = line.split('\t')[sort_col].strip()
        index.append((col, offset, length))
    f.close()
    index.sort()
    return index

def print_sorted(filename, col_sort):
    index = build_index(filename, col_sort)
    f = open(filename)
    for col, offset, length in index:
        f.seek(offset)
        print f.read(length).rstrip('\n')

if __name__ == '__main__':
    filename = 'somefile.txt'
    sort_col = 2
    print_sorted(filename, sort_col)
</code>

***
   Split up into files that can be sorted in memory. Sort each file in memory. Then merge the resulting
   files.

   Merge by reading a portion of each of the files to be merged. The same amount from each file leaving
   enough space in memory for the merged result. Once merged saving this. Repeating adding blocks of
   merged data onto the file.

   This minimises the file i/o and moving around the file on the disk.

***
   I would store the file in a good relational database, index it on the field your are interested in


---
https://stackoverflow.com/questions/18776370/converting-a-csv-file-into-a-list-of-tuples-with-python

Converting a csv file into a list of tuples with python

   I am to take a csv with 4 columns: brand, price, weight, and type.

   The types are orange, apple, pear, plum.

   Parameters: I need to select the most possible weight, but by selecting 1 orange, 2 pears, 3 apples,
   and 1 plum by not exceeding as $20 budget. I cannot repeat brands of the same fruit (like selecting
   the same brand of apple 3 times, etc).

   I can open and read the csv file through Python, but I'm not sure how to create a dictionary or list
   of tuples from the csv file?

   For more clarity, here's an idea of the data.
Brand, Price, Weight, Type
brand1, 6.05, 3.2, orange
brand2, 8.05, 5.2, orange
brand3, 6.54, 4.2, orange
brand1, 6.05, 3.2, pear
brand2, 7.05, 3.6, pear
brand3, 7.45, 3.9, pear
brand1, 5.45, 2.7, apple
brand2, 6.05, 3.2, apple
brand3, 6.43, 3.5, apple
brand4, 7.05, 3.9, apple
brand1, 8.05, 4.2, plum
brand2, 3.05, 2.2, plum

   Here's all I have right now:
import csv
test_file = 'testallpos.csv'
csv_file = csv.DictReader(open(test_file, 'rb'), ["brand"], ["price"], ["weight"], ["type"])

***
   All of the field names need to be in a single list, like so csv.DictReader(open(test_file, 'rb'),
   ["brand", "price", "weight", "type"]).
   
***
   Does your file have the blanks as your example does?
   
***
   You can ponder this:
   
import csv

def fitem(item):
    item=item.strip()
    try:
        item=float(item)
    except ValueError:
        pass
    return item

with open('/tmp/test.csv', 'r') as csvin:
    reader=csv.DictReader(csvin)
    data={k.strip():[fitem(v)] for k,v in reader.next().items()}
    for line in reader:
        for k,v in line.items():
            k=k.strip()
            data[k].append(fitem(v))

print data

   Prints:
{'Price': [6.05, 8.05, 6.54, 6.05, 7.05, 7.45, 5.45, 6.05, 6.43, 7.05, 8.05, 3.05],
 'Type': ['orange', 'orange', 'orange', 'pear', 'pear', 'pear', 'apple', 'apple', 'apple', 'apple', 'plum', 'p
lum'],
 'Brand': ['brand1', 'brand2', 'brand3', 'brand1', 'brand2', 'brand3', 'brand1', 'brand2', 'brand3', 'brand4',
 'brand1', 'brand2'],
 'Weight': [3.2, 5.2, 4.2, 3.2, 3.6, 3.9, 2.7, 3.2, 3.5, 3.9, 4.2, 2.2]}

   If you want the csv file literally as tuples by rows:
import csv
with open('/tmp/test.csv') as f:
    data=[tuple(line) for line in csv.reader(f)]

print data
# [('Brand', ' Price', ' Weight', ' Type'), ('brand1', ' 6.05', ' 3.2', ' orange'), ('brand2', ' 8.05', ' 5.2'
, ' orange'), ('brand3', ' 6.54', ' 4.2', ' orange'), ('brand1', ' 6.05', ' 3.2', ' pear'), ('brand2', ' 7.05'
, ' 3.6', ' pear'), ('brand3', ' 7.45', ' 3.9', ' pear'), ('brand1', ' 5.45', ' 2.7', ' apple'), ('brand2', '
6.05', ' 3.2', ' apple'), ('brand3', ' 6.43', ' 3.5', ' apple'), ('brand4', ' 7.05', ' 3.9', ' apple'), ('bran
d1', ' 8.05', ' 4.2', ' plum'), ('brand2', ' 3.05', ' 2.2', ' plum')]

***
   this helps a lot! going to play around with this. thanks!
   
***
import csv
with open("some.csv") as f:
       r = csv.reader(f)
       print filter(None,r)

   or with list comprehension
import csv
with open("some.csv") as f:
       r = csv.reader(f)
       print [row for row in r if row]

   for comparison
In [3]: N = 100000

In [4]: the_list = [randint(0,3) for _ in range(N)]

In [5]: %timeit filter(None,the_list)
1000 loops, best of 3: 1.91 ms per loop

In [6]: %timeit [i for i in the_list if i]
100 loops, best of 3: 4.01 ms per loop

   Since your actual output does not have blanks you donot need the list comprehension or the
   filter you can just say list(r)

   Final answer without blank lines
import csv
with open("some.csv") as f:
       print list(csv.reader(f))

   if you want dicts you can do
import csv
with open("some.csv") as f:
       reader = list(csv.reader(f))
       print [dict(zip(reader[0],x)) for x in reader]
       #or
       print map(lambda x:dict(zip(reader[0],x)), reader)

***
   don't do filter(bool, ...), use filter(None, ...), filter() has a special case that can avoid the
   excessive conversion to bool (since the result of calling bool is also checked for truthyness). Also,
   don't do filter(..., list(seq)), just do filter(..., seq), filter knows how to iterate over
   sequences, the intermediate list just wastes space.
  
***
   Worthless use of filter You can just do print [e for e in r] which is faster and more readable.


---
https://stackoverflow.com/questions/5643281/reading-in-a-csv-file-and-sorting-it-in-python

Reading in a CSV file AND sorting it in Python

   I am trying to read in a CSV file that looks like this:
ruby,2,100
diamond,1,400
emerald,3,250
amethyst,2,50
opal,1,300
sapphire,2,500
malachite,1,60

   Here is some code I have been experimenting with.
class jewel:
    def __init__(gem, name, carat, value):
            gem.name = name
            gem.carot = carat
            gem.value = value
    def __repr__(gem):
            return repr((gem.name, gem.carat, gem.value))

jewel_objects = [jewel('diamond', '1', 400),
                 jewel('ruby', '2', 200),
                 jewel('opal', '1', 600),
                ]

aList = [sorted(jewel_objects, key=lambda jewel: (jewel.value))]
print aList

   I would like to read in the values and assign them to name, carat, and value but I'm not sure how to
   do so. Then once I get them read in I would like to sort them by value per carat so value/carat. I
   have done quite a bit of searching and have came up blank. Thank you very much for your help in
   advance.
  
***
   What is 'gem', is that a base class?
     
***
   You need to do two things here, the first is actually loading the data into the objects. I recommend
   you look at the 'csv' module in the standard python library for this. It's very complete and will
   read each row and make it easily accessable

   CSV docs: http://docs.python.org/library/csv.html

   I would create a list of the objects, and then implement either an cmp function in your object, or
   (if you're using an older version of python) you can pass a function to sorted() that would define
   it. You can get more info about sorting in the python wiki

   Wiki docs: http://wiki.python.org/moin/HowTo/Sorting

   You would implement the cmp function like this in your class (this can be made a bit more efficent,
   but I'm being descriptive here)
def __cmp__(gem, other):
    if (gem.value / gem.carot) < (other.value / other.carot):
        return -1
    elif (gem.value / gem.carot) > (other.value / other.carot):
        return 1
    else:
        return 0
  
***
   Just for completeness, __cmp__ is depreciated and has been removed from python 3, it's best to use
   the __ne__, __eq__, __lt__, __gt__, __le__, __ge__, comparison methods. – [48]Mike Ramirez Apr 13 '11
   at 1:14
  
***
   you could use functools.total_ordering decorator. In this case you need to define only one
   comparison method the rest is generated automatically.
     
***
   Python has a csv module that should be really helpful to you.
  
***
   You can use numpy structured arrays along with the csv module and use numpy.sort() to sort the data.
   The following code should work. Suppose your csv file is named geminfo.csv
import numpy as np
import csv

fileobj = open('geminfo.csv','rb')
csvreader = csv.reader(fileobj)

# Convert data to a list of lists
importeddata = list(csvreader)

# Calculate Value/Carat and add it to the imported data
# and convert each entry to a tuple
importeddata = [tuple(entry + [float(entry[2])/entry[1]]) for entry in importeddata]

   One way to sort this data is to use numpy as shown below.
# create an empty array
data = np.zeros(len(importeddata), dtype = [('Stone Name','a20'),
                            ('Carats', 'f4'),
                            ('Value', 'f4'),
                            ('valuepercarat', 'f4')]
                        )
data[:] = importeddata[:]
datasortedbyvaluepercarat = np.sort(data, order='valuepercarat')
  
***
   I don't see what value Numpy is adding to this approach (over simmply using standard Python features
   and modules included with the standard libraries).
     
***
   He also wanted to assign the values to "name", "carat" and "value" for which he is creating the jewel
   class. With numpy structured arrays he can achieve this without creating a jewel class.
  
***
   But creating a class is a built-in feature of Python and could be the basis for implementing whatever
   else he wants to do with his Jewels; thus my question: why introduce a large, complex external
   dependency (and the additional cognitive overhead of that package and its types) when his
   requirements seem to be far simpler in scale and semantics than Numpy is suited for?  

***
import csv
import operator

class Jewel(object):
    @classmethod
    def fromSeq(cls, seq):
        return cls(*seq)

    def __init__(self, name, carat, value):
        self.name  = str(name)
        self.carat = float(carat)
        self.value = float(value)

    def __repr__(self):
        return "{0}{1}".format(self.__class__.__name__, (self.name, self.carat, self.value))

    @property
    def valuePerCarat(self):
        return self.value / self.carat

def loadJewels(fname):
    with open(fname, 'rb') as inf:
        incsv = csv.reader(inf)
        jewels = [Jewel.fromSeq(row) for row in incsv if row]
    jewels.sort(key=operator.attrgetter('valuePerCarat'))
    return jewels

def main():
    jewels = loadJewels('jewels.csv')
    for jewel in jewels:
        print("{0:35} ({1:>7.2f})".format(jewel, jewel.valuePerCarat))

if __name__=="__main__":
    main()

   produces
Jewel('amethyst', 2.0, 50.0)        (  25.00)
Jewel('ruby', 2.0, 100.0)           (  50.00)
Jewel('malachite', 1.0, 60.0)       (  60.00)
Jewel('emerald', 3.0, 250.0)        (  83.33)
Jewel('sapphire', 2.0, 500.0)       ( 250.00)
Jewel('opal', 1.0, 300.0)           ( 300.00)
Jewel('diamond', 1.0, 400.0)        ( 400.00)

  
***
   For parsing real-world CSV (comma-separated values) data you'll want to use the CSV module that's
   included with recent versions of Python.

   CSV is a set of conventions rather than standard. The sample data you show is simple and regular, but
   CSV generally has some ugly corner cases for quoting where the contents of any field might have
   embedded commas, for example.

   Here is a very crude program, based on your code, which does naïve parsing of the data (splitting by
   lines, then splitting each line on commas). It will not handle any data which doesn't split to
   precisely the correct number of fields, nor any where the numeric fields aren't correctly parsed by
   the Python int() and float() functions (object constructors). In other words this contains no error
   checking nor exception handling.

   However, I've kept it deliberately simple so it can be easily compared to your rough notes. Also note
   that I've used the normal Python conventions regarding "self" references in the class definition.
   (About the only time one would use names other than "self" for these is when doing "meta-class"
   programming ... writing classes which dynamically instantiate other classes. Any other case will
   almost certainly cause serious concerns in the minds of any experienced Python programmers looking at
   your code).
#!/usr/bin/env python
class Jewel:
    def __init__(self, name, carat, value):
        self.name = name
        self.carat = int(carat)
        self.value = float(value)
        assert self.carat != 0      # Division by zero would result from this
    def __repr__(self):
        return repr((self.name, self.carat, self.value))

if __name__ == '__main__':
    sample='''ruby,2,100
diamond,1,400
emerald,3,250
amethyst,2,50
opal,1,300
sapphire,2,500
malachite,1,60'''

    these_jewels = list()
    for each_line in sample.split('\n'):
        gem_type, carat, value = each_line.split(',')
        these_jewels.append(Jewel(gem_type, carat, value))
        # Equivalently:
        # these_jewels.append(Jewel(*each_line.split(',')))

    decorated = [(x.value/x.carat, x) for x in these_jewels]
    results = [x[1] for x in sorted(decorated)]
    print '\n'.join([str(x) for x in results])

   The parsing here is done simply using the string .split() method, and the data is extracted into
   names using Python's "tuple unpacking" syntax (this would fail if any line of input were to have the
   wrong number of fields).

   The alternative syntax to those two lines uses Python's "apply" syntax. The * prefix on the argument
   causes it to be unpacked into separate arguments which are passed to the Jewel() class instantiation.

   This code also uses the widespread (and widely recommended) DSU (decorate, sort, undecorate) pattern
   for sorting on some field of your data. I "decorate" the data by creating a series of tuples:
   (computed value, object reference), then "undecorate" the sorted data in a way which I hope is clear
   to you. (It would be immediately clear to any experienced Python programmer).

   Yes the whole DSU could be reduced to a single line; I've separated it here for legibility and
   pedagogical purposes.

   Again this sample code is purely for your edification. You should use the CSV module on any
   real-world data; and you should introduce exception handling either in the parsing or in the
   Jewel.__init__ handling (for converting the numeric data into the correct Python types. (Also note
   that you should consider using Python's Decimal module rather than float()s for representing monetary
   values ... or at least storing the values in cents or mils and using your own functions to represent
   those as dollars and cents).


---
https://stackoverflow.com/questions/44113209/sorting-large-text-file-in-python

  
Sorting large text file in python

   Sort the content of a file based on second field, e.g.

   Input file:
Jervie,12,M

Jaimy,11,F

Tony,23,M

Janey,11,F

   Output file:
Jaimy,11,F

Janey,11,F

Jervie,12,M

Tony,23,M

   We need to use external sort.
   Input file can be of size 4GB. RAM is 1GB.

   I used [https://stackoverflow.com/questions/30493806/implementing-an-external-merge-sort] this but it does
   not work as it treats all the content as int. Also I have doubt related to the buffer size in each turn of
   the external sort. How to decide on that?

   This sorts file with integers only.
file = open("i2.txt","r")
temp_files = []
e = []
while True:
    temp_file = tempfile.TemporaryFile()
    e = list(islice(file,2))
    if not e:
        break
    e.sort(key=lambda line: int(line.split()[0]))
    temp_file.writelines(e)
    temp_files.append(temp_file)
    temp_file.flush()
    temp_file.seek(0)
file.close()

with open('o.txt', 'w') as out:
    out.writelines(imap('{}\n'.format, heapq.merge(*(imap(int, f) for f in temp_files))))
out.close()

   I am able to create temporary files sorted on the second field, but how do I merge them based on
   that?
  
***
   what have you tried? Try to write to a file as you read token by token.
     
***
   I did it with the following code : Divide the big file into smaller files. Here it is assumed that
   max 4 lines can be read. So I initially divide the file into lines of 4 and sort them and write into
   temp files. Then read these files in pairs 2 lines from each file and merge them. Corner cases are
   not handled but, this should be a starter for others to think.
f = open("i1.txt", "r")
temp_files = []
e = []
while True:
    temp_file = tempfile.NamedTemporaryFile()
    e = list(islice(f, 4))
    if not e:
        temp_file.close()
        break
    # e.sort(key=lambda line:int(line.split()[1]))
    e.sort(key=lambda line: int(line.split()[1]))
    temp_file.writelines(e)
    temp_files.append(temp_file)
    temp_file.flush()
    temp_file.seek(0)
f.close()

aux = []
z = 0
while len(temp_files) != 1:
    while z < len(temp_files)-1:
        tem = tempfile.NamedTemporaryFile()
        t1 = temp_files[z]
        t2 = temp_files[z+1]
        t1.seek(0)
        t2.seek(0)
        n = 2
        e1 = None
        e2 = None
        while True:
            if not e1:
                e1 = list(islice(t1, 2))
            if not e2:
                e2 = list(islice(t2, 2))
            if not e1 and not e2:
                break
            elif e1 and not e2:
                tem.writelines(imap('{}'.format,e1))
                e1 = None
                continue
            elif not e1 and e2:
                tem.writelines(imap('{}'.format,e2))
                e2 = None
                continue
            i = 0
            j = 0
            while i<len(e1) and j<len(e2):
                l1 = e1[i]
                l2 = e2[j]
                if int(l1.split()[1]) == int(l2.split()[1]):
                    tem.writelines(imap('{}'.format,[l1,l2]))
                    i+=1
                    j+=1
                elif int(l1.split()[1]) < int(l2.split()[1]):
                    tem.writelines(imap('{}'.format,[l1]))
                    i+=1
                else:
                    tem.writelines(imap('{}'.format,[l2]))
                    j+=1
            if i>=len(e1):
                e1 = None
            else:
                e1 = e1[i:]
            if j>= len(e2):
                e2 = None
            else:
                e2 = e2[j:]
        z+=2
        aux.append(tem)
        t1.close()
        t2.close()
        tem.flush()
        tem.seek(0)
    temp_files = aux
    z = 0
    aux = []
with open("o.txt",'w') as out:
    out.writelines(imap('{}'.format,temp_files[0]))
  
***
   Try using out of the core processing with Blaze (http://blaze.readthedocs.io/en/latest/ooc.html)


---
http://code.activestate.com/recipes/415581-big-file-sorting/

big file sorting (Python recipe) by Tomasz Bieruta

   ActiveState Code (http://code.activestate.com/recipes/415581/)
   Python, 147 lines
   
<code>
import os

class FileSort(object):
    def __init__(self, inFile, outFile=None, splitSize=20):
        """ split size (in MB) """
        self._inFile = inFile
        if outFile is None:
            self._outFile = inFile
        else:
            self._outFile = outFile

        self._splitSize = splitSize * 1000000
        self.setKeyExtractMethod()


    def setKeyExtractMethod(self, keyExtractMethod=None):
        """ key extract from line for sort method:
            def f(line):
                return line[1:3], line[5:10]
        """
        if keyExtractMethod is None:
            self._getKey = lambda line: line
        else:
            self._getKey = keyExtractMethod

    def sort(self):
        files = self._splitFile()

        if files is None:
            """ file size <= self._splitSize """
            self._sortFile(self._inFile, self._outFile)
            return

        for fn in files:
            self._sortFile(fn)

        self._mergeFiles(files)
        self._deleteFiles(files)


    def _sortFile(self, fileName, outFile=None):
        lines = open(fileName).readlines()
        get_key = self._getKey
        data = [(get_key(line), line) for line in lines if line!='']
        data.sort()
        lines = [line[1] for line in data]
        if outFile is not None:
            open(outFile, 'w').write(''.join(lines))
        else:
            open(fileName, 'w').write(''.join(lines))



    def _splitFile(self):
        totalSize = os.path.getsize(self._inFile)
        if totalSize <= self._splitSize:
            # do not split file, the file isn't so big.
            return None

        fileNames = []

        fn,e = os.path.splitext(self._inFile)
        f = open(self._inFile)
        try:
            i = size = 0
            lines = []
            for line in f:
                size += len(line)
                lines.append(line)
                if size >= self._splitSize:
                    i += 1
                    tmpFile = fn + '.%03d' % i
                    fileNames.append(tmpFile)
                    open(tmpFile,'w').write(''.join(lines))
                    del lines[:]
                    size = 0


            if size > 0:
                tmpFile = fn + '.%03d' % (i+1)
                fileNames.append(tmpFile)
                open(tmpFile,'w').write(''.join(lines))

            return fileNames
        finally:
            f.close()

    def _mergeFiles(self, files):
        files = [open(f) for f in files]
        lines = []
        keys = []

        for f in files:
            l = f.readline()
            lines.append(l)
            keys.append(self._getKey(l))

        buff = []
        buffSize = self._splitSize/2
        append = buff.append
        output = open(self._outFile,'w')
        try:
            key = min(keys)
            index = keys.index(key)
            get_key = self._getKey
            while 1:
                while key == min(keys):
                    append(lines[index])
                    if len(buff) > buffSize:
                        output.write(''.join(buff))
                        del buff[:]

                    line = files[index].readline()
                    if not line:
                        files[index].close()
                        del files[index]
                        del keys[index]
                        del lines[index]
                        break
                    key = get_key(line)
                    keys[index] = key
                    lines[index] = line

                if len(files)==0:
                    break
                # key != min(keys), see for new index (file)
                key = min(keys)
                index = keys.index(key)

            if len(buff)>0:
                output.write(''.join(buff))
        finally:
            output.close()

    def _deleteFiles(self, files):
        for fn in files:
            os.remove(fn)



def sort(inFileName, outFileName=None, getKeyMethod=None):
    fs = FileSort(inFileName, outFileName)
    if getKeyMethod is not None:
        fs.setKeyExtractMethod(getKeyMethod)

    fs.sort()
    fs = None
</code>

   I didn't find such sorting algorythmus. I'm not shure about the performance, maybe somebody has
   better/faster solution.

***
   Wirth is GOD ;). Early, I read about sorting sequences placed on tapes, on "Algorithms + Data
   Structures = Programs" by Niklaus Wirth; here we have speed -optimized one-step version with more
   disk and memory requirements.
  
***
   ups ... to be clear, I wanted to say, that I didn't find this algorythmus implemented in python :-)
  
***
   Does work, but uses lots of memory. I have tested this algorithm with a 280 MB ASCII file.

   The idea is compelling: for huge files do not sort them in one step, but break the file into chunks,
   sort each chunk separately and merge the files.

   The result is correct, but the merge step uses a hugh amount of memory. Split and sort does not use a
   lot of memory, but merge does, around 700 MB. This is more than loading the whole file into memory
   and sort then.

   As an alternative I have used the following 3 liner
lines = inFile.readlines()
lines.sort()
map(outFile.write, lines)

   which has used 'only' 400MB RAM to sort a 280 MB ASCII file.

   I don't understand from the code why merge is using so much RAM. Otherwise the algorithm would be
   great.
  
***
Hi,
I found some improvements :-)
just before I save the file (in _splitFile() method) I can also sort the lines:
   ...
          fileNames.append(tmpFile)
          data = self._sortRecords(lines)  -- here
          open(tmpFile,'w').write(''.join(data))
          del lines[:]
          data = None
          size = 0

  if size > 0:
      tmpFile = fn + '.%03d' % (i+1)
      fileNames.append(tmpFile)
      data = self._sortRecords(lines)    -- and here
      open(tmpFile,'w').write(''.join(data))
      data = None
  ....

the _sortRecords() method looks now like:

    def _sortRecords(self, records):
        get_key = self._getKey
        data = [(get_key(line), line) for line in records if line!='']
        data.sort()
        return [line[1] for line in data]

in the sort() method I also commented out the code lines:

   ...
   #for fn in files:
   #    self._sortFile(fn)
   ...

And finally I change the code line

   ...
   buffSize = self._splitSize/(2*len(lines[0]))
   ...

in _mergeFiles() method - don't remember why :-(

In my tests the memory stays on same level, with the file size I can optimize memory/speed. In your case when
I tried 750 MB file I got memory problems.

regards
Tomasz Bieruta
  
***
   Thanks - this was exactly what I was after. In regards to using a lot of memory - I'm not a computer
   scientist but I assume that it quite depends on the file you are sorting, specifically - how large
   the file you are sorting is.

   For me, this sort worked on a 16Gb file using ~8Gb of RAM taking ~13 cpu hours whereas other sorts
   may be more memory efficient for smaller files but not for larger ones.


---
https://codereview.stackexchange.com/questions/88885/efficiently-filter-a-large-100gb-csv-file-v3

Efficiently filter a large (100gb+) csv file (v3)

   I'm in the process of filtering some very(!) large files (100gb+): I can't download files with a
   lower granularity.

   The problem is as follows: I need to filter large files that look like the following (3b+ rows).

TIC, Date, Time, Bid, Offer
AAPL, 20090901, 09:45, 145, 145.5
AAPL, 20090902, 09:45, 145, 145.5
AAPL, 20090903, 09:45, 145, 145.5

   I filter based on TICKER+DATE combinations found in an external file. I have, on average, ~ 1200
   dates of interest per firm for ~ 700 firms. The large file contains all dates for the firms of
   interest, for which I want to extract only a few dates of interest. The big files are split by month
   (2013-01, 2013-02 etc.).

# Example firm_dates_of_interest file.
AAPL, 20090902
AAPL, 20090903

   A few changes were made since the previous post:
     * Extract the filtering process into a generator function
     * Extract chunks from this iterator

   I'm currently at 4 minutes of processing time for 30 million rows (1% of the file); I tested a few
   files and it works properly. However, with about 3 billion rows per file, that puts it at ~8 hours
   for one 120gb file. Seeing as I have about twelve files, I'm very curious whether I can get
   significant performance improvements by doing things differently. Example files can be found [28]here
   (sourcefile, 3gb zipped) and [29]here (firmdates).

   Any tips are greatly appreciated.
   
<code>
import os
import datetime
import csv
import cProfile
import re
ROOT_DIR = "H:/ROOT_DIR/"
SOURCE_FILES = os.path.join(ROOT_DIR, '15. Speedtest')
EXPORT_DIR = ROOT_DIR
DATES_FILE = os.path.join(ROOT_DIR, "10. Dates of interest/firm_date_of_interest_withSP.csv")

# Build the original date dict
# For example:
#    d['AAPL'] is a list with ['20140901', '20140902', '20140901']
with open(DATES_FILE, "r") as csvfile:
    d = {}
    reader = csv.reader(csvfile)
    reader.next()
    for line in reader:
        firm = line[1]
        date = line[2]
        if firm in d.keys():
            d[firm].append(date)
        else:
            d[firm] = [date]


def filter_lines(filename, d):
    """ Given a dictionary with key, value a Ticker and dates_of_interest, yield
    only the filtered rows with those ticker / dates pairs. """
    with open(filename, "rb") as csvfile:
        datareader = csv.reader(csvfile)
        for row in datareader:
            try:
                if row[1] in d[row[0]]:
                    yield row
            except KeyError:
                continue
            except IndexError:
                continue


def get_chunk(iterable, chunk_size):
    """ Given an iterable and chunk_size, return chunks of chunk_size"""
    result = []
    for item in iterable:
        result.append(item)
        if len(result) == chunk_size:
            yield tuple(result)
            result = []
    if len(result) > 0:
        yield tuple(result)


def main():
    start = datetime.datetime.now()
    for root, dir, files in os.walk(SOURCE_FILES):
        for i, file in enumerate(files):
            basename = os.path.splitext(file)[0]
            source_filepath = os.path.join(root, file)

            # Annotate files with 'DONE' after succesful processing: skip those
            if re.search("DONE", basename):
                continue

            startfile = datetime.datetime.now()
            iterable = filter_lines(source_filepath, d)
            for num_saved, chunk in enumerate(get_chunk(iterable, 3000000)):
                output_filename = os.path.join(EXPORT_DIR, basename+' EXTRACT' + str(num_saved) + '.csv')
                with open(output_filename, 'wb') as csvfile:
                    writer = csv.writer(csvfile, quoting=csv.QUOTE_NONNUMERIC)
                    for line in chunk:
                        writer.writerow(line)
                file_elapsed = datetime.datetime.now() - startfile
                print("Took me %d seconds for 3000000 extracted rows.." % file_elapsed.seconds)

            new_filename = os.path.join(root, os.path.splitext(file)[0]+'- DONE.csv')
            os.rename(source_filepath, new_filename)

    elapsed = datetime.datetime.now() - start
    print("Took me %d seconds..." % elapsed.seconds)

if __name__ == "__main__":
    main()
</code>

  
***
   Does it have to be Python? You could get higher processing speeds in other languages, e.g. C++, C#,
   Java, Javascript
     
***
   I'm sorry to have to say so, but reading back over the answers to your two previous questions, it
   seems to me as if you've been given some bad advice. In particular, this answer advised you to
   switch to using the csv module. But that was a mistake, because:
    1. The csv module has to handle all the details of the CSV format, which can be quite complicated
       (quoted fields, choice of field separator etc).
    2. The csv module splits all the fields of each line, but here you are only interested in the first
       two fields.

   So there is a lot of wasted effort. To demonstrate this, I made a file with 10 million records in
   your format:
>>> with open('data.csv', 'w') as f:
...     for i in range(10**7):
...         _ = f.write('AAPL, {:08d}, 09:45, 145, 145.5\n'.format(i//1000))

   This is about a third of a gigabyte in size. To read (and discard) all the lines from this file takes
   about 7.5 seconds:
>>> from collections import deque
>>> from timeit import timeit
>>> with open('data.csv') as f:
...     timeit(lambda:deque(f, maxlen=0), number=1)
7.537129107047804

   Which is a rate of 1.3 million lines a second. (Using [43]collections.deque with maxlen=0 to consume
   an iterable with no Python interpreter overhead is a useful trick to know about.)

   Now, using the csv module, reading all the lines in the file takes about three times as long:
>>> import csv
>>> with open('data.csv') as f:
...    timeit(lambda:deque(csv.reader(f), maxlen=0), number=1)
22.454868661938235

   So switching to csv was a mistake.

   What to do instead? Well, if I understand your question correctly, the records are quite constrained
   in format: they start with a [44]stock ticker symbol, a comma, a space, a date in [45]ISO 8601
   format, and a comma. Moreover, you want to select lines based on just these two fields. So you might
   try putting the fields you are looking for into a set of strings:
keys = set(line.strip() for line in open('dates-of-interest.csv'))

   and then finding the field boundaries yourself:
def filter_lines(in_filename, out_filename, keys):
    """Read records from in_filename and write records to out_filename if
    the string consisting of the first two comma-separated fields is
    found in the set keys.

    """
    with open(in_filename) as in_f, open(out_filename, 'w') as out_f:
        for line in in_f:
            ticker_end = line.find(',')
            date_end = line.find(',', ticker_end + 1)
            if line[:date_end] in keys:
                out_f.write(line)

   In this test I filter out one in every five lines:
>>> keys = {'AAPL, {:08d}'.format(i) for i in range(0, 10000, 5)}
>>> timeit(lambda:filter_lines('data.csv', 'out.csv', keys), number=1)
14.926778070977889

   This is about 670,000 lines a second.

   Now, it might be the case that your records are even more constrained than I described above. For
   example, it might be the case that all the stock ticker symbols that you are looking for fall into a
   small range of lengths. For example maybe they are all between one and five letters long. Then the
   second comma must appear between positions 11 and 16 of the string:
01234567890123456789
A, 20150102, 09:45, 145, 145.5
AB, 20150102, 09:45, 145, 145.5
ABC, 20150102, 09:45, 145, 145.5
ABCD, 20150102, 09:45, 145, 145.5
ABCDE, 20150102, 09:45, 145, 145.5

   So we can restrict our search to these positions:
def filter_lines(in_filename, out_filename, keys):
    """Read records from in_filename and write records to out_filename if
    the string up to the first comma between positions 11 and 16 of
    line is found in the set keys.

    """
    with open(in_filename) as in_f, open(out_filename, 'w') as out_f:
        for line in in_f:
            date_end = line.find(',', 11, 16)
            if line[:date_end] in keys:
                out_f.write(line)

   and this is substantially faster, about 850,000 lines a second on my computer:
>>> timeit(lambda:filter_lines('data.csv', 'out.csv', keys), number=1)
11.833066276973113

   There are some more optimizations we can do:
    1. open the files in binary mode, to avoid dealing with character set encodings;
    2. avoid storing date_end in a local variable;
    3. avoid passing the end postion 16 to the find (this is superfluous);
    4. (suggested in comments by d33tah) run it under [46]PyPy.

   This results in:
def filter_lines(in_filename, out_filename, keys):
    """Read records from in_filename and write records to out_filename if
    the beginning of the line (taken up to the first comma at or after
    position 11) is found in keys (which must be a set of byte strings).

    """
    with open(in_filename, 'rb') as in_f, open(out_filename, 'wb') as out_f:
        for line in in_f:
            if line[:line.find(b',', 11)] in keys:
                out_f.write(line)

   Note that we now require keys to be a set of byte strings, so we better encode them:
>>>> keys = {'AAPL, {:08d}'.format(i).encode() for i in range(0, 10000, 5)}

   This processes about 1.8 million lines per second:
>>>> timeit(lambda:filter_lines('data.csv', 'out.csv', keys), number=1)
5.53329086304

   which suggests that a 100 GiB file could be filtered in about 30 minutes. Of course, this is all on
   my computer, which might be faster or slower than yours. But the general approach almost certainly
   applies.
  
***
   Receiving incorrect advice and now understanding why it was incorrect, to me, is a valuable learning
   opportunity, certainly not a waste of effort in my case (I'm still starting out)! I very much
   appreciate your answer, and I'll post the results as soon as I implement it. Thanks.
     
***
   You processing speed starts to approach levels where your hard disk performance becomes a bottleneck.
   Your output files should be on a different disk from your input files. Otherwise your hard disk will
   become really slow as the read/write head jumps around like crazy. That problem may not show for
   small files of a few hundred MB since those fit into the OS's disk cache but will be severe for large
   files.


---
https://codereview.stackexchange.com/questions/104340/splitting-large-text-file-and-sorting-by-content

Splitting large text file and sorting by content

   I have a large text file (~2GB) full of data. The data (sample below) gives an x, y, z coordinate,
   and a corresponding result on each line (there is other stuff but I don't care about it). The single
   large text file is too large to be useful, so I want to split it into several smaller files. However,
   I want each file to contain all the points on one y-plane. The first few lines of the file are below:
 mcnp   version 6     ld=05/08/13  probid =  09/09/15 23:06:39
 Detector Test
 Number of histories used for normalizing tallies =    2237295223.00


 Mesh Tally Number        14
 photon   mesh tally.

 Tally bin boundaries:

    X direction:   -600.00   -598.00   -596.00   ... 1236.00   1238.00   1240.00   1242.00   1244.00    \
    1258.00   1260.00

    Y direction:      0.00     10.00     20.00     ...    740.00    750.00    760.00    770.00    780.00    \
    790.00    800.00    810.00    820.00    830.00   840.00    850.00    860.00

    Z direction:    -60.00    -58.00    -56.00    ...  592.00    594.00    596.00    598.00    600.00
    Energy bin boundaries: 1.00E-03 1.00E+36

   Energy         X         Y         Z     Result     Rel Error     Volume    Rslt * Vol
  1.000E+36  -599.000     5.000   -59.000 0.00000E+00 0.00000E+00 4.00000E+01 0.00000E+00
  1.000E+36  -599.000     5.000   -57.000 0.00000E+00 0.00000E+00 4.00000E+01 0.00000E+00
  1.000E+36  -599.000     5.000   -55.000 0.00000E+00 0.00000E+00 4.00000E+01 0.00000E+00
... and repeat forever...

   I've truncated some of it for readability, but you get the idea. The data I want is the four last
   lines.

   The code currently does the following:
    1. Find the line data headers (Energy X Y ...)
    2. Find the y value of the first line of data
    3. Add the data to a list until we find data with a different y value
    4. Dump the list to a file named with the y value, delete the list
    5. Repeat steps 3 and 4 until the end of the file.

   Not all the data for each y plane is together, so if I encounter data at a y-value I've seen before,
   the data is appended to an existing file.

   My code is below, it functions, but I feel like I could improve efficiency somewhere (execution took
   ~30 min). As always, readability/style improvements are welcome, but performance is the primary goal.

<code>
import os

with open("meshtal", 'r') as f:
    i = 0
    coords = []
    curY = 0
    for l in f:
        #If data header already found
        if i:
            line = l.split()
            #If this is the first line of data
            if i == 1:
                curY = line[2]
                coords.extend([(line[1],line[2],line[3],line[4])])
                i += 1
            else:
                #If data has the same y value as previous
                if curY == line[2]:
                    coords.extend([(line[1],line[2],line[3],line[4])])
                    i += 1
                #New y value, dump existing data to file
                else:
                    fname = "Y={}.txt".format(curY)
                    #if y value has already been encountered, append existing file
                    if os.path.exists(fname):
                        with open("Y={}.txt".format(curY), 'a') as out:
                            for coord in coords:
                                out.write("{:10}{:10}{:10}{:10}\n".format(*coord))
                    #New y value, create new file
                    else:
                        with open("Y={}.txt".format(curY), 'w') as out:
                            out.write("X         Y         Z         Result     \n")
                            for coord in coords:
                                out.write("{:10}{:10}{:10}{:10}\n".format(*coord))
                    i = 1
                    coords = []
                    curY = line[2]
                    coords.extend([(line[1],line[2],line[3],line[4])])
                    i += 1
        #If no data header has been found
        else:
            #If current line is data header, raise flag
            if l.lstrip().startswith("Energy         X         Y         Z     Result     Rel Error     Volume
    Rslt * Vol"):
                i += 1
                print "found start"
</code>

   [27]python [28]performance [29]python-2.7
   [30]share|[31]improve this question
   asked Sep 10 '15 at 14:55
   [32]wnnmaw
   1557
   add a comment |

2 Answers 2

   [33]active [34]oldest [35]votes
   up vote 2 down vote accepted

***
   You have a lot of nesting going on here. That's generally harder to read and parse, especially when
   you could actually make liberal use of continue instead. continue will skip to the next iteration of
   the loop, ignoring all remaining code. So you could move your check for the header file to the top
   and avoid indentation:
for l in f:
    #If data header not found
    if not i:
        if l.lstrip().startswith("Energy         X         Y         Z     Result     Rel Error     VolumeRslt * Vol"):
            i += 1
            print "found start"
        continue

   Also i is a terrible variable here. i is initially being used to indicate that a line has been found,
   then seems to become an index value. Instead I would initialise i as your index once this line is
   found, but use a named boolean like found_header instead. Something that's clear could remove the
   need for comments since if found_header is self explanatory. Likewise, I think you should use line
   instead of l. You do use line to replace l later. l in particular can look like a one or an upper
   case letter i, so it's not clear.

   Also there's nothing wrong with doing line = line.split() since you don't need the original value of
   line after this part.

   I'd move i+=1 out of the if else, since it happens in both cases anyway. You can do it at the start
   of the loop anyway if you just initialise i as 0. Once again, I'd use continue to save a level of
   nesting, like so:
#If this is the first line of data
i += 1
if i == 1:
    curY = line[2]
    coords.extend([(line[1],line[2],line[3],line[4])])
    continue

#If data has the same y value as previous
if curY == line[2]:
    coords.extend([(line[1],line[2],line[3],line[4])])
    continue

#New y value, dump existing data to file
fname = "Y={}.txt".format(curY)

   Also append mode will still create a new empty file if none exists, so you don't need to check for
   it. Just always open with 'a' and then write your data. You can just check if the file exists
   beforehand and store the result as a boolean.
#if y value has already been encountered, append existing file
new_file = os.path.exists(fname)
with open("Y={}.txt".format(curY), 'a') as out:
    if new_file:
        out.write("X         Y         Z         Result     \n")
    for coord in coords:
        out.write("{:10}{:10}{:10}{:10}\n".format(*coord))

   So here's how I'd put together the whole thing:
import os

header = "Energy         X         Y         Z     Result     Rel Error     Volume    Rslt * Vol"

with open("meshtal", 'r') as f:
    header_found = False
    i = 0
    coords = []
    curY = 0

    for line in f:
        if not header_found:
            if line.lstrip().startswith(header):
                print "found start"
                header_found = True
            continue

        line = line.split()
        i += 1

        #If this is the first line of data
        if i == 1:
            curY = line[2]
            coords.extend([(line[1],line[2],line[3],line[4])])
            continue

        #If data has the same y value as previous
        if curY == line[2]:
            coords.extend([(line[1],line[2],line[3],line[4])])
            continue

        #New y value, dump existing data to file
        filename = "Y={}.txt".format(curY)
        new_file = os.path.exists(fname)
        with open("Y={}.txt".format(curY), 'a') as out:
            if new_file:
                out.write("X         Y         Z         Result     \n")
            for coord in coords:
                out.write("{:10}{:10}{:10}{:10}\n".format(*coord))

        i = 1
        coords = []
        curY = line[2]
        coords.extend([(line[1],line[2],line[3],line[4])])


***
   It seems like, since the data is particularly un-ordered, that I end up switching between y-values a
   lot. Is it worth leaving each output file open? (open it on the first instance of a y-value, then
   close them all at the end)
   
***
   Will you have a lot of them to open and write a lot of data? I'd have to check how large those
   objects would be if you were to do that. I'm actually not sure. 

***
   There are 90 files, and I'm opening and closing each one several times a minute, and the file size
   goes up between 40 - 70 KB each time.

***
   @wnnmaw From what I can see that would save a lot of time, yes. I think it'd be best to store as a
   dictionary of curY: open(file). However you can't use with in that case, so you should make sure to
   handle file closing properly. Do you want me to add a section to my answer about this?

***
   Storing the files in a dict is a good idea, I can implement that. Thanks 


---
http://neopythonic.blogspot.si/2008/10/sorting-million-32-bit-integers-in-2mb.html

Sorting a million 32-bit integers in 2MB of RAM using Python

   Someone jokingly asked me how I would sort a million 32-bit integers in 2 megabytes of RAM, using
   Python. Taking up the challenge, I leared something about buffered I/O.

   Obviously this is a joke question -- the data alone would take up 4 megabytes, assuming binary
   encoding! But there's a possible interpretation: given a file containing a million 32-bit integers,
   how would you sort them with minimal memory usage? This would have to be some kind of merge sort,
   where small chunks of the data are sorted in memory and written to a temporary file, and then the
   temporary files are merged into the eventual output area.

   Here is my solution. I'll annotate it in a minute.

   NOTE: All my examples use Python 3.0. The main difference in this case is the use of file.buffer to
   access the binary stream underlying the text stream file.
<code>
#!/usr/bin/env python3.0
import sys, array, tempfile, heapq
assert array.array('i').itemsize == 4
def intsfromfile(f):
  while True:
     a = array.array('i')
     a.fromstring(f.read(4000))
     if not a:
         break
     for x in a:
         yield x
iters = []
while True:
  a = array.array('i')
  a.fromstring(sys.stdin.buffer.read(40000))
  if not a:
      break
  f = tempfile.TemporaryFile()
  array.array('i', sorted(a)).tofile(f)
  f.seek(0)
  iters.append(intsfromfile(f))
a = array.array('i')
for x in heapq.merge(*iters):
  a.append(x)
  if len(a) >= 1000:
      a.tofile(sys.stdout.buffer)
      del a[:]
if a:
  a.tofile(sys.stdout.buffer)
</code>

   On my Google desktop (a 3 year old PC running a Googlified Linux, rating about 34000 Python 3.0
   pystones) this took about 5.4 seconds to run, with an input file containing exactly 1,000,000 32-bit
   random integers. That's not so bad, given that a straightforward in-memory sort of the same input
   took about 2 seconds:
#!/usr/bin/env python3.0
import sys, array
a = array.array('i', sys.stdin.buffer.read())
a = list(a)
a.sort()
a = array.array('i', a)
a.tofile(sys.stdout.buffer)

   Back to the merge-sort solution. The first three lines are obvious:
#!/usr/bin/env python3.0
import sys, array, tempfile, heapq
assert array.array('i').itemsize == 4

   The first line says we're using Python 3.0. The second line imports the modules we're going to need.
   The third line here makes it break on those 64-bit systems where the 'i' typecode doesn't represent a
   32-bit int; I am making no attempts to write this code portably.
   Then we define a little helper that is a generator which reads integers from a file and yields them
   one at a time:
def intsfromfile(f):
  while True:
      a = array.array('i')
      a.fromstring(f.read(4000))
      if not a:
          break
      for x in a:
          yield x

   This is where the performance tuning of the algorithm takes place: it reads up to 1000 integers at a
   time, and yields them one by one. I had originally written this without buffering -- it would just
   read 4 bytes from the file, convert them to an integer, and yield the result. But that ran about 4
   times as slow! Note that we can't use a.fromfile(f, 1000) because the fromfile() method complains
   bitterly when there aren't enough values in the file, and I want the code to adapt automatically to
   however many integers are on the file. (It turns out we write about 10,000 integers to a typical temp
   file.)

   Next we have the input loop. This repeatedly reads a chunk of 10,000 integers from the input file,
   sorts them in memory, and writes them to a temporary file. We then add an iterator over that
   temporary file, using the above intsfromfile() function, to a list of iterators that we'll use in the
   subsequent merge phase.
iters = []
while True:
  a = array.array('i')
  a.fromstring(sys.stdin.buffer.read(40000))
  if not a:
      break
  f = tempfile.TemporaryFile()
  array.array('i', sorted(a)).tofile(f)
  f.seek(0)
  iters.append(intsfromfile(f))

   Note that for an input containing a million values, this creates 100 temporary files each containing
   10,000 values.

   Finally we merge all these files (each of which is sorted) together. The heapq module has a really
   nice function for this purpose: heapq.merge(iter1, iter2, ...) returns an iterator that yields the
   input values in order, assuming each input itself yields its values in order (as is the case here).
a = array.array('i')
for x in heapq.merge(*iters):
  a.append(x)
  if len(a) >= 1000:
      a.tofile(sys.stdout.buffer)
      del a[:]
if a:
  a.tofile(sys.stdout.buffer)

   This is another place where buffered I/O turned out to be essential: Writing each individual value to
   a file as soon as it is available slows the algorithm down about twofold. Thus, by simply adding
   input and output buffering, we gained a tenfold speed-up!
   And that's the main moral of the story.

   Another lesson is praise for the heapq module, which contains the iterator merge functionality needed
   in the output phase. Also, let's not forget the utility of the array module for managing binary data.

   And finally, let this example remind you that Python 3.0 is notso different from Python 2.5!

***
          You could use an embeddable pastebin (such as gist) and get syntax highlighting for free:
          http://gist.github.com/18896 . Beautiful python code deserves to be shown nicely :)


***
   Guido van Rossum said...
          PS. Python 2.6 does have heapq.merge()...

***
          Guido, my post wasn't meant as a flame if that is what you are referring to. I was just
          messing around. Actually, I've been playing with python since 1.5(.4 I think... but I can't
          remember) and I had never seen PEP 8 until maybe a month ago, so it was fresh in my mind as I
          read your code.
          I was poking fun, when, in reality I have a habit of just putting all my imports on one line
          also.

***
   [55]Guido van Rossum said...
          @olivier: that's a cool way of doing it, but not for Python, where an int takes 12 bytes and
          the list takes another 4 bytes per value. Given some other overhead you'd end up having to
          make a lot of passes over the full input (the same number as would be feasible with my version
          -- I used 100 but it could likely be less), which would probably be slower. It's interesting
          that a heap features in both algorithms. I'm not convinced that it's the fastest way to sort
          though, despite the O(N log N) complexity. Python's built-in sort is wicked.

***
   Joel Hockey said...
          I had a go at implementing this using a different approach which runs much faster on my
          computer. I read the file multiple times and select values within a given range that will
          (hopefully) fit in the 1Mb memory limit. This is then sorted and appended to the output. I
          choose the ranges by partitioning the possible 32-bit values (-2^31, 2^31-1) into 8 sections.
          The timing numbers I get are:
          in-memory sort: 2.09s
          guido merge sort: 10.90s
          my select range sort: 2.92s
          #!/usr/bin/env python3.0
          import array
          outf = open('joel-sorted.dat', 'wb')
          INT_MIN, INT_MAX = -(2**31), 2**31-1
          size = int(2**32 / 8)
          cutoffs = range(INT_MIN, INT_MAX, size)
          # read file through and only collect values in our range
          for i in range(len(cutoffs)):
          inf = open('nums.dat', 'rb')
            min, max = cutoffs[i], cutoffs[i]+size
            a = array.array('i')
            while True:
              temp = array.array('i', inf.read(500000))
              if not temp:
                break
              for j in temp:
                if j >= min and j < max:
                  a.append(j)
            # sort and append to output
            array.array('i', sorted(a)).tofile(outf)

***
          How about using Numpy to do read them in with frombuffer()? Something like in the
          multiprocess test here.

***
          Yes it is possible for this approach to exceed the 2M memory limit if the numbers
          are skewed. I should have mentioned that it makes the assumption that the numbers are
          uniformly random.
          If the 1M 32-bit ints are sufficiently uniform though, then this approach uses about 1Mb of
          memory. The range of possible values for a 32-bit signed int is [-2147483648, 2147483648)
          (lower number is included, upper number is excluded). I break this range up into 8 smaller
          ranges - [-2147483648, -1610612736), [-1610612736, -1073741824), ... [1610612736, 2147483648).
          Out of 1 million uniformly random 32-bit numbers, you would expect 125,000 to fall into each
          range. In fact, as long as there are not more than 250,000 numbers in each sub-range, the
          approach will not exceed the memory limit.
          I chose 8 sub-ranges to be conservative. You could choose 3 or 4, or even possibly 2, but then
          you would need to sort the numbers in-place and be a lot trickier which is likely to make the
          code slower. The performance for 8 was not much worse than 3, and not much different to a full
          in-memory sort. My calculation for the memory used is:
          * 500,000 bytes holding input numbers which can be reclaimed before sub-range sorting.
          * 500,000 (approx) bytes holding numbers in given sub-range (125,000 32-bit ints)
          * 500,000 (approx) bytes to hold sorted numbers in sub-range.
          Total mem: approx 1Mb
          Like I said, this does assume that the numbers are uniformly random. I did consider doing a
          first pass of the numbers to work out the optimal bounds for the sub-ranges, but this crude
          approach was enough to satisfy my curiosity.
          Hopefully Jon Bentley will show up soon and wow us all with some way to do it that is way
          faster and uses less memory.

***
          For what it's worth, NumPy and memory-mapped arrays can solve this problem quite simply and
          quickly (how much memory is used depends on the OS):
          import numpy as np
          np.memmap("file_with_integers", dtype=int).sort(kind='merge')

***
          private void SortFile(string unsortedFileName, string sortedFileName)
          {
          //Read the file into an array of ints
          byte[] file = System.IO.File.ReadAllBytes(unsortedFileName);
          int[] numbers = new int[file.Length / 4];
          for (int i = 0; i < numbers.Length; i++)
          {
          numbers[i] = BitConverter.ToInt32(file, i);
          }
          //Sort the array
          Array.Sort(numbers);
          //Write the results back to file
          byte[] tempArray = new byte[4];
          for(int j = 0; j < numbers.Length; j++)
          {
          tempArray = BitConverter.GetBytes(numbers[j]);
          file[j] = tempArray[0];
          file[j+1] = tempArray[1];
          file[j+2] = tempArray[2];
          file[j+3] = tempArray[3];
          }
          System.IO.File.WriteAllBytes(sortedFileName,file);
          }

***
          @Linn: By reading all 1000000 4byte integers in at once, you're using atleast 4000000 bytes of
          memory. But you're only given 2MB. Plus, it looks like you're using another 4000000 bytes in
          your "file" array.

***
          The question is not a joke! But since it seems like the kind of thing Microsoft would use in a
          job- interview puzzle (pointless obsessive byte-squeezing optimization!), I'll be a little
          cryptic:
          With a main working array that's only 2,000,000 bytes (Python or any language will have some
          fixed overhead, I think it's fair not to count that), you can write the program so that the
          only I/O it does is to read the unsorted input file in one pass, then write the sorted output
          file in one pass. No temporary files, seeks, additional buffers, or virtual arrays are needed.
          2e6 bytes is a small percentage more than is needed. Given a fixed percentage slack, the run
          time scales like n log n.
          In Python you can avoid the memory overhead that Guido mentions, by using the array module.

***
   Guido van Rossum said...
          Joel: the magic is all in heapq.merge(). It does the "merge" phase of a merge-sort. Think of
          what happens at the very beginning. There are 1000 files each with 1000 sorted numbers. We
          want the smallest number of all, which must be the first number in one of the files (because
          they are sorted). It finds and yields this value, and advances that file's iterator. Now we
          want the next smallest number. Again, this must be at the head of one of the file iterators
          (but not necessarily the same one). And so on, until all files have been exhausted.


---
https://realpython.com/blog/python/working-with-large-excel-files-in-pandas/

Working With Large Excel Files in Pandas

   Today we are going to learn how to work with large files in [18]Pandas, focusing on reading and
   analyzing an Excel file and then working with a subset of the original data.
   flask-angular-auth

     This tutorial utilizes Python (tested with 64-bit versions of v2.7.9 and v3.4.3), [19]Pandas
     (v0.16.1), and [20]XlsxWriter (v0.7.3). We recommend using the [21]Anaconda distribution to
     quickly get started, as it comes pre-installed with all the needed libraries.

   This is a collaboration piece between Shantnu Tiwari, founder of [22]Python For Engineers, and the
   fine folks at Real Python.

Reading the File
   The first file we’ll work with is a compilation of all the car accidents in England from 1979-2004,
   to extract all accidents that happened in London in the year 2000.

Excel
   Start by downloading the source ZIP file from [23]data.gov.uk, and extract the contents. Then try to
   open Accidents7904.csv in Excel. Be careful. If you don’t have enough memory, this could very well
   crash your computer.

   What happens?
   You should see a “File Not Loaded Completely” error since Excel can only [24]handle one million rows
   at a time.

     We tested this in [25]LibreOffice as well and received a similar error – “The data could not be
     loaded completely because the maximum number of rows per sheet was exceeded.”

   To solve this, we can open the file in Pandas. Before we start, the source code is on [26]Github.

Pandas
   Within a new project directory, activate a virtualenv, and then install Pandas:
$ pip install pandas==0.16.1

   Now let’s build the script. Create a file called pandas_accidents.py and the add the following code:
import pandas as pd


# Read the file
data = pd.read_csv("Accidents7904.csv", low_memory=False)
# Output the number of rows
print("Total rows: {0}".format(len(data)))
# See which headers are available
print(list(data))

   Here, we imported Pandas, read in the file – which could take some time, depending on how much memory
   your system has – and outputted the total number of rows the file has as well as the available
   headers (e.g., column titles).

   When ran, you should see:
Total rows: 6224198
['\xef\xbb\xbfAccident_Index', 'Location_Easting_OSGR', 'Location_Northing_OSGR',
 'Longitude', 'Latitude', 'Police_Force', 'Accident_Severity', 'Number_of_Vehicles',
 'Number_of_Casualties', 'Date', 'Day_of_Week', 'Time', 'Local_Authority_(District)',
 'Local_Authority_(Highway)', '1st_Road_Class', '1st_Road_Number', 'Road_Type',
 'Speed_limit', 'Junction_Detail', 'Junction_Control', '2nd_Road_Class',
 '2nd_Road_Number', 'Pedestrian_Crossing-Human_Control',
 'Pedestrian_Crossing-Physical_Facilities', 'Light_Conditions', 'Weather_Conditions',
 'Road_Surface_Conditions', 'Special_Conditions_at_Site', 'Carriageway_Hazards',
 'Urban_or_Rural_Area', 'Did_Police_Officer_Attend_Scene_of_Accident',
 'LSOA_of_Accident_Location']

   So, there are over six millions rows! No wonder Excel choked. Turn your attention to the list of
   headers, the first one in particular:
'\xef\xbb\xbfAccident_Index',

   This should read Accident_Index. What’s with the extra \xef\xbb\xbf at the beginning? Well, the \x
   actually means that the value is [27]hexadecimal, which is a [28]Byte Order Mark, indicating that the
   text is Unicode.

   Why does it matter to us?
   You cannot assume the files you read are clean. They might contain extra symbols like this that can
   throw your scripts off.

   This file is good, in that it is otherwise clean – but many files have missing data, data in internal
   inconsistent format, etc.. So any time you have a file to analyze, the first thing you must do is
   clean it. How much cleaning? Enough to allow you to do some analysis. Follow the [29]KISS principle.

   What sort of cleanup might you require?
     * Fix date/time. The same file might have dates in different formats, like the American (mm-dd-yy)
       or European (dd-mm-yy) formats. These need to be brought into a common format.
     * Remove any empty values. The file might have blank columns and/or rows, and this will come up as
       NaN (Not a number) in Pandas. Pandas provides a simple way to remove these: the dropna()
       function. We saw an example of this in the [30]last blog post.
     * Remove any garbage values that have made their way into the data. These are values which do not
       make sense (like the byte order mark we saw earlier). Sometimes, it might be possible to work
       around them. For example, there could be a dataset where the age was entered as a floating point
       number (by mistake). The int() function then could be used to make sure all ages are in integer
       format.

Analyzing
   For those of you who know SQL, you can use the SELECT, WHERE, AND/OR statements with different
   keywords to refine your search. We can do the same in Pandas, and in a way that is [31]more
   programmer friendly.

   To start off, let’s find all the accidents that happened on a Sunday. Looking at the headers above,
   there is a Day_of_Weeks field, which we will use.

   In the ZIP file you downloaded, there’s a file called Road-Accident-Safety-Data-Guide-1979-2004.xls,
   which contains extra info on the codes used. If you open it up, you will see that Sunday has the code
   1.
print("\nAccidents")
print("-----------")

# Accidents which happened on a Sunday
accidents_sunday = data[data.Day_of_Week == 1]
print("Accidents which happened on a Sunday: {0}".format(
    len(accidents_sunday)))

   That’s how simple it is.

   Here, we targeted the Day_of_Weeks field and returned a DataFrame with the condition we checked for –
   day of week == 1.

   When ran you should see:
Accidents
-----------
Accidents which happened on a Sunday: 693847

   As you can see, there were 693,847 accidents that happened on a Sunday.

   Let’s make our query more complicated: Find out all accidents that happened on a Sunday and involved
   more than twenty cars:
# Accidents which happened on a Sunday, > 20 cars
accidents_sunday_twenty_cars = data[
    (data.Day_of_Week == 1) & (data.Number_of_Vehicles > 20)]
print("Accidents which happened on a Sunday involving > 20 cars: {0}".format(
    len(accidents_sunday_twenty_cars)))

   Run the script. Now we have 10 accidents:
Accidents
-----------
Accidents which happened on a Sunday: 693847
Accidents which happened on a Sunday involving > 20 cars: 10

   Let’s add another condition – weather.

   Open the Road-Accident-Safety-Data-Guide-1979-2004.xls, and go to the Weather sheet. You’ll see that
   the code 2 means, “Raining with no heavy winds”.

   Add that to our query:
# Accidents which happened on a Sunday, > 20 cars, in the rain
accidents_sunday_twenty_cars_rain = data[
    (data.Day_of_Week == 1) & (data.Number_of_Vehicles > 20) &
    (data.Weather_Conditions == 2)]
print("Accidents which happened on a Sunday involving > 20 cars in the rain: {0}".format(
    len(accidents_sunday_twenty_cars_rain)))

   So there were four accidents that happened on a Sunday, involving more than twenty cars, while it was
   raining:
Accidents
-----------
Accidents which happened on a Sunday: 693847
Accidents which happened on a Sunday involving > 20 cars: 10
Accidents which happened on a Sunday involving > 20 cars in the rain: 4

   We could continue making this more and more complicated, as needed. For now, we’ll stop since our
   main interest is to look at accidents in London.

   If you look at Road-Accident-Safety-Data-Guide-1979-2004.xls again, there is a sheet called Police
   Force. The code for 1 says, “Metropolitan Police”. This is what is more commonly known as Scotland
   Yard, and is the police force responsible for most (though not all) of London. For our case, this is
   good enough, and we can extract this info like so:
# Accidents in London on a Sunday
london_data = data[data['Police_Force'] == 1 & (data.Day_of_Week == 1)]
print("\nAccidents in London from 1979-2004 on a Sunday: {0}".format(
    len(london_data)))

   Run the script. This created a new DataFrame with the accidents handled by the “Metropolitan Police”
   from 1979 to 2004 on a Sunday:
Accidents
-----------
Accidents which happened on a Sunday: 693847
Accidents which happened on a Sunday involving > 20 cars: 10
Accidents which happened on a Sunday involving > 20 cars in the rain: 4

Accidents in London from 1979-2004 on a Sunday: 114624

   What if you wanted to create a new DataFrame that only contains accidents in the year 2000?

   The first thing we need to do is convert the date format to one which Python can understand using the
   pd.to_datetime() [32]function. This takes a date in any format and converts it to a format that we
   can understand (yyyy-mm-dd). Then we can create another DataFrame that only contains accidents for
   2000:
# Convert date to Pandas date/time
london_data_2000 = london_data[
    (pd.to_datetime(london_data['Date'], coerce=True) >
        pd.to_datetime('2000-01-01', coerce=True)) &
    (pd.to_datetime(london_data['Date'], coerce=True) <
        pd.to_datetime('2000-12-31', coerce=True))
]
print("Accidents in London in the year 2000 on a Sunday: {0}".format(
    len(london_data_2000)))

   When ran, you should see:
Accidents which happened on a Sunday: 693847
Accidents which happened on a Sunday involving > 20 cars: 10
Accidents which happened on a Sunday involving > 20 cars in the rain: 4

Accidents in London from 1979-2004 on a Sunday: 114624
Accidents in London in the year 2000 on a Sunday: 3889

   So, this is a bit confusing at first. Normally, to filter an array you would just use a for loop with
   a conditional:
for data in array:
    if data > X and data < X:
        # do something

   However, you really shouldn’t define your own loop since many high-performance libraries, like
   Pandas, have helper functions in place. In this case, the above code loops over all the elements and
   filters out data outside the set dates, and then returns the data points that do fall within the
   dates.

   Nice!

Converting

   Chances are that, while using Pandas, everyone else in your organization is stuck with Excel. Want to
   share the DataFrame with those using Excel?

   First, we need to do some cleanup. Remember the byte order mark we saw earlier? That causes problems
   when writing this data to an Excel file – Pandas throws a UnicodeDecodeError. Why? Because the rest
   of the text is decoded as ASCII, but the hexadecimal values can’t be represented in ASCII.

   We could write everything as Unicode, but remember this byte order mark is an unnecessary (to us)
   extra we don’t want or need. So we will get rid of it by renaming the column header:
london_data_2000.rename(
    columns={'\xef\xbb\xbfAccident_Index': 'Accident_Index'}, inplace=True)

   This is the way to rename a column in Pandas; a bit complicated, to be honest. inplace = True is
   needed because we want to modify the existing structure, and not create a copy, which is what Pandas
   does by default.

   Now we can save the data to Excel:
# Save to Excel
writer = pd.ExcelWriter(
    'London_Sundays_2000.xlsx', engine='xlsxwriter')
london_data_2000.to_excel(writer, 'Sheet1')
writer.save()

   Make sure to install [33]XlsxWriter before running:
pip install XlsxWriter==0.7.3

   If all went well, this should have created a file called London_Sundays_2000.xlsx, and then saved our
   data to Sheet1. Open this file up in Excel or LibreOffice, and confirm that the data is correct.

Conclusion
   So, what did we accomplish? Well, we took a very large file that Excel could not open and utilized
   Pandas to-
    1. Open the file.
    2. Perform SQL-like queries against the data.
    3. Create a new XLSX file with a subset of the original data.

   Keep in mind that even though this file is nearly 800MB, in the age of big data, it’s still quite
   small. What if you wanted to open a 4GB file? Even if you have 8GB or more of RAM, that might still
   not be possible since much of your RAM is reserved for the OS and other system processes. In fact, my
   laptop froze a few times when first reading in the 800MB file. If I opened a 4GB file, it would have
   a heart attack.

   So how do we proceed?

   The trick is not to open the whole file in one go. That’s what we’ll look at in the next blog post.
   Until then, analyze your own data. Leave questions/comments below. Grab the code from the [34]repo.

   Posted by Real Python Jun 28^th, 2015


---
http://pythondata.wpengine.com/working-large-csv-files-python/

Working with large CSV files in Python

large csv files in pythonI’m currently working on a project that has multiple very large CSV files
(6 gigabytes+). Normally when working with CSV data, I read the data in using pandas and then start munging
and analyzing the data. With files this large, reading the data into pandas directly can be difficult
(or impossible) due to memory constrictions, especially if you’re working on a prosumer computer. In this post,
I describe a method that will help you when working with large CSV files in python.

While it would be pretty straightforward to load the data from these CSV files into a database, there might be
times when you don’t have access to a database server and/or you don’t want to go through the hassle of setting
up a server.  If you are going to be working on a data set long-term, you absolutely should load that data into
a database of some type (mySQL, postgreSQL, etc) but if you just need to do some quick checks / tests / analysis
of the data, below is one way to get a look at the data in these large files with python, pandas and sqllite.

To get started, you’ll need to import pandas and sqlalchemy. The commands below will do that.

import pandas as pd
from sqlalchemy import create_engine

Next, set up a variable that points to your csv file.  This isn’t necessary but it does help in re-usability.
file = '/path/to/csv/file'

With these three lines of code, we are ready to start analyzing our data. Let’s take a look at the ‘head’ of
the csv file to see what the contents might look like.

print pd.read_csv(file, nrows=5)
This command uses pandas’ “read_csv” command to read in only 5 rows (nrows=5) and then print those rows to the
screen. This lets you understand the structure of the csv file and make sure the data is formatted in a way
that makes sense for your work.

Before we can actually work with the data, we need to do something with it so we can begin to filter it to
work with subsets of the data. This is usually what I would use pandas’ dataframe for but with large data
files, we need to store the data somewhere else. In this case, we’ll set up a local sqllite database, read the
csv file in chunks and then write those chunks to sqllite.

To do this, we’ll first need to create the sqllite database using the following command.
csv_database = create_engine('sqlite:///csv_database.db')

Next, we need to iterate through the CSV file in chunks and store the data into sqllite.
chunksize = 100000
i = 0
j = 1
for df in pd.read_csv(file, chunksize=chunksize, iterator=True):
      df = df.rename(columns={c: c.replace(' ', '') for c in df.columns}) 
      df.index += j
      i+=1
      df.to_sql('table', csv_database, if_exists='append')
      j = df.index[-1] + 1

With this code, we are setting the chunksize at 100,000 to keep the size of the chunks managable, initializing
a couple of iterators (i=0, j=0) and then running through a for loop.  The for loop reads a chunk of data from
the CSV file, removes spaces from any of column names, then stores the chunk into the sqllite database
(df.to_sql(…)).

This might take a while if your CSV file is sufficiently large, but the time spent waiting is worth it because
you can now use pandas ‘sql’ tools to pull data from the database without worrying about memory constraints.

To access the data now, you can run commands like the following:
df = pd.read_sql_query('SELECT * FROM table', csv_database)

Of course, using ‘select *…’ will load all data into memory, which is the problem we are trying to get away
from so you should throw from filters into your select statements to filter the data. For example:


df = pd.read_sql_query('SELECT COl1, COL2 FROM table where COL1 = SOMEVALUE', csv_database)


---
http://pythondata.com/dask-large-csv-python/

Dask – A better way to work with large CSV files in Python

Posted on November 24, 2016

Dask dataframeIn a recent post titled Working with Large CSV files in Python, I shared an approach I use when
I have very large CSV files (and other file types) that are too large to load into memory. While the approach
I previously highlighted works well, it can be tedious to first load data into sqllite (or any other database)
and then access that database to analyze data.   I just found a better approach using Dask.

While looking around the web to learn about some parallel processing capabilities, I ran across a python module
named Dask, which describes itself as:

…is a flexible parallel computing library for analytic computing.

When I saw that, I was intrigued. There’s a lot that can be done with that statement  and I’ve got plans to
introduce Dask into my various tool sets for data analytics.

While reading the docs, I ran across the ‘dataframe‘ concept and immediately new I’d found a new tool for
working with large CSV files.  With Dask’s dataframe concept,  you can do out-of-core analysis (e.g., analyze
data in the CSV without loading the entire CSV file into memory). Other than out-of-core manipulation, dask’s
dataframe uses the pandas API, which makes things extremely easy for those of us who use and love pandas.

With Dask and its dataframe construct, you set up the dataframe must like you would in pandas but rather than
loading the data into pandas, this appraoch keeps the dataframe as a sort of ‘pointer’ to the data file and
doesn’t load anything until you specifically tell it to do so.

One note (that I always have to share):  If you are planning on working with your data set over time, its
probably best to get the data into a database of some type.

An example using Dask and the Dataframe
First, let’s get everything installed. The documentation claims that you just need to install dask, but I had
to install ‘toolz’ and ‘cloudpickle’ to get dask’s dataframe to import.  To install dask and its requirements,
open a terminal and type (you need pip for this):
pip install dask[complete] toolz cloudpickle

NOTE: I mistakenly had “pip install dask” listed initially. This only installs the base dask system and not
the dataframe (and other dependancies). Thanks to Kevin for pointing this out.

Now, let’s write some code to load csv data and and start analyzing it. For this example, I’m using the 311
Service Requests dataset from NYC’s Open Data portal.   You can download the dataset here: 311 Service
Requests – 7Gb+ CSV

Set up your dataframe so you can analyze the 311_Service_Requests.csv file. This file is assumed to be stored
in the directory that you are working in.

import dask.dataframe as dd

filename = '311_Service_Requests.csv'
df = dd.read_csv(filename, dtype='str')

Unlike pandas, the data isn’t read into memory…we’ve just set up the dataframe to be ready to do some compute
functions on the data in the csv file using familiar functions from pandas. Note: I used “dtype=’str'” in the
read_csv to get around some strange formatting issues in this particular file.

Let’s take a look at the first few rows of the file using pandas’ head() call.  When you run this, the first X
rows (however many rows you are looking at with head(X)) and then displays those rows.
df.head(2)


Note: a small subset of the columns are shown below for simplicity

Unique Key	Created Date	Closed Date	Agency
25513481	05/09/2013 12:00:00 AM	05/14/2013 12:00:00 AM	HPD	
25513482	05/09/2013 12:00:00 AM	05/13/2013 12:00:00 AM	HPD	
25513483	05/09/2013 12:00:00 AM	05/22/2013 12:00:00 AM	HPD	
25513484	05/09/2013 12:00:00 AM	05/12/2013 12:00:00 AM	HPD	
25513485	05/09/2013 12:00:00 AM	05/11/2013 12:00:00 AM	HPD	

┌──────────┬──────────────────────┬──────────────────────┬──────┬┐
│Unique Key│     Created Date     │     Closed Date      │Agency││
├──────────┼──────────────────────┼──────────────────────┼──────┼┤
│ 25513481 │05/09/2013 12:00:00 AM│05/14/2013 12:00:00 AM│HPD   ││
├──────────┼──────────────────────┼──────────────────────┼──────┼┤
│ 25513482 │05/09/2013 12:00:00 AM│05/13/2013 12:00:00 AM│HPD   ││
├──────────┼──────────────────────┼──────────────────────┼──────┼┤
│ 25513483 │05/09/2013 12:00:00 AM│05/22/2013 12:00:00 AM│HPD   ││
├──────────┼──────────────────────┼──────────────────────┼──────┼┤
│ 25513484 │05/09/2013 12:00:00 AM│05/12/2013 12:00:00 AM│HPD   ││
├──────────┼──────────────────────┼──────────────────────┼──────┼┤
│ 25513485 │05/09/2013 12:00:00 AM│05/11/2013 12:00:00 AM│HPD   ││
└──────────┴──────────────────────┴──────────────────────┴──────┴┘

We see that there’s some spaces in the column names. Let’s remove those spaces to make things easier to work
with.
df = df.rename(columns={c: c.replace(' ', '') for c in df.columns})

The cool thing about dask is that you can do things like renaming columns without loading all the data into
memory.

There’s a column in this data called ‘Descriptor’ that has the problem types, and “radiator” is one of those
problem types. Let’s take a look at how many service requests were because of some problem with a radiator. 
To do this, you can filter the dataframe using standard pandas filtering (see below) to create a new dataframe.
# create a new dataframe with only 'RADIATOR' service calls
radiator_df=df[df.Descriptor=='RADIATOR']

Let’s see how many rows we have using the ‘count’ command

radiator_df.Descriptor.count()

You’ll notice that when you run the above command, you don’t actually get count returned. You get a descriptor
back similar  like “dd.Scalar<series-…, dtype=int64>”

To actually compute the count, you have to call “compute” to get dask to run through the dataframe and count
the number of records.
radiator_df.compute()

When you run this command, you should get something like the following
[52077 rows x 52 columns]

The above are just some samples for using dask’s dataframe construct.  Remember, we built a new dataframe using
pandas’ filters without loading the entire original data set into memory.  They may not seem like much, but
when working with a 7Gb+ file, you can save a great deal of time and effort using dask when compared to using
the approach I previously mentioned.

Dask seems to have a ton of other great features that I’ll be diving into at some point in the near future,
but for now, the dataframe construct has been an awesome find.


---
https://www.quora.com/What-is-the-best-way-to-sort-a-csv-file-in-python-with-multiple-columns-on-date

   What is the best way to sort a csv file in python with multiple columns on date?

***   
   There are two native Python libraries that will help you out here: csv and datetime.
   [code python]
   import csv
   with open('sample.csv') as f:
       for line in csv.reader(f):
           # Each line is a list of values, do what you need to with it
   import datetime
   datetime.datetime.strptime("21/11/06 16:30", "%d/%m/%y %H:%M")
   # creates a "datetime" object
   [/code]
   
   Combine these with Python's built in "sorted" function, and you should be good to go.
   2. Built-in Functions
   8.1. datetime - Basic date and time types - Python v2.7.6 documentation
   13.1. csv - CSV File Reading and Writing - Python v2.7.6 documentation

***
   "Note that sorted() is applied to each line and not to the column entries. You would need to import
   the operator module and use operator.itemgetter(). Since the entries in a CSV file are strings, the
   sorting will place a nmber like "90" as 'higher' than a nuber like "864". This should not affect your
   dates, but would cause a problem with values. This would have to be handled seperately. I have not
   yet examined sorted() to determine if this can be avoided without conversion.
import csv
import operator
ifile =open('myfile.csv', 'rb')
infile = csv.reader(ifile)
# The first entry is the header line
infields = infile.next()
statindex = infields.index('Desired Header')
# create the sorted list
sortedlist = sorted(infile, key=operator.itemgetter(statindex), reverse=True)
ifile.close
# open the output file - it can be the same as the input file
ofile = open('myoutput.csv, 'wb')
# write the header
outfile.writerow(infields)
# write the sorted list
for row in sortedlist:
  outfile.writerow(row)
# processing finished, close the output file
ofile.close()

   UPDATE: If any of the rows in the entire file contain non-numeric data, then the sorted MUST sort by
   a text method. This includes if the header line is not row 1 (for example follows a title line). or
   if there is a blank entry in the column. If all entries in that column are numeric then it can be
   sorted as such. For example:
try:
  sortedlist = sorted(infile, key=lambda d: float(d[statistic]), reverse=True)
except ValueError:
  sortedlist = sorted(infile, key=lambda d: d[statistic], reverse=True)


---
http://metadatascience.com/2014/02/27/random-sampling-from-very-large-files/

Random Sampling From Very Large Files
Feb 27th, 2014

Random Sampling
   Random sampling from a set of entities means any entity has the same chance of selection as any
   other such entities. Suppose we want to randomly select $k$ lines from a large text file containing
   hundreds of millions of lines. We desire that the probability of being selected be the same for every
   line in the file.

Algorithm 1

   The first approach which comes in mind is to
     * Count the number of lines in the file,
     * Create a sorted random set of $k$ integers between 1 and number of lines in the file,
     * Iterate over the random integers and read the file line by line. Pick the line if the line number
       matches one of the the random integers.

   This algorithm in Python is shown below.
   Random Sampling 1 (random_sampler1.py)
<code>
import random

def random_sampler(filename, k):
        sample = []
        with open(filename, 'rb') as f:
                linecount = sum(1 for line in f)
                f.seek(0)

                random_linenos = sorted(random.sample(xrange(linecount), k), reverse = True)
                lineno = random_linenos.pop()
                for n, line in enumerate(f):
                        if n == lineno:
                                sample.append(line.rstrip())
                                if len(random_linenos) > 0:
                                        lineno = random_linenos.pop()
                                else:
                                        break
        return sample
</code>

Algorithm 2: Reservoir sampling
   As you see, in previous algorithm, we scan the file two times. First time for counting the number of
   lines in the file, and second time to select random lines. There are some algorithms which even work
   without knowing in advance the total number of items. One classical algorithm form Alan Waterman
   called [11]Reservoir sampling is exposed in the second volume of Donald Knuth’s “[12]The Art of
   Computer Programming”.

   Suppose we want to select $k$ items from a set of items. We start by filling the “reservoir” with the
   first $k$ items, and then for each $i^{th}$ item remaining in the set, we generate a random number
   $r$ between $1$ and $i$. If $r$ is less than $k$, we replace the $r^{th}$ item of the reservoir with
   the $i^{th}$ item of the set. We continue processing items until we reach the end of the set.
   Random Sampling 2 (random_sampler2.py) [13]download
<code>
import random

def random_sampler(filename, k):
        sample = []
        with open(filename) as f:
                for n, line in enumerate(f):
                        if n < k:
                                sample.append(line.rstrip())
                        else:
                                r = random.randint(0, n)
                                if r < k:
                                        sample[r] = line.rstrip()
        return sample
</code>

   It is easy to prove by induction that this approach works and each line has the same probability of
   being selected as the other lines:

   Suppose we need to collect a random sample of $k$ items from a list of items coming as an online
   stream. We desire that after seeing $n$ item, each item in the sample set had $\frac{k}{n}$ chance to
   be there.

   For example, suppose $k=10$. According to the algorithm, the first $10$ items go directly to the
   reservoir. So for them, the probability of being selected is $\frac{10}{10} = 1 \checkmark$.

   Now, suppose the $11^{th}$ item comes. The desired probability is now $\frac{k}{n} = \frac{10}{11}$.
   We have:
     * According to the reservoir sampling algorithm above, the probability of $11^{th}$ item to being
       selected is $\frac{10}{11} \checkmark$.
     * For the items already in the reservoir, the chance of being in the sample set and also remaining
       in the sample set after seeing the $11^{th}$ item, is their previous probability to be there,
       multiple the probability of not being replace by the $11^{th}$. So we have:
       Pr = Probability that a selected item remains in the reservoir
       = Previous probability to be there * Probability of not being replaced
       = Previous probability to be there * ( 1 - Probability of being replaced by $11^{th}$ item)
       The chance that an item in the reservoir being replaced with $11^{th}$ item is the probability of
       $11^{th}$ item to be selected, which is $\frac{10}{11}$, multiple the probability of being the
       replacement candidate between 10 items, which is $\frac{1}{10}$. So we have: .

   Likewise, for the $12^{th}$ item we have:
     * Probability of $12^{th}$ item to being selected is $\frac{10}{12} \checkmark$.
     * For the items already in the reservoir:

   And this can be extended for the $n^{th}$ item. Although reservoir sampling is an interesting
   approach but it is too slow for our problem here.

Algorithm 3

   There is another interesting approach when the lines have approximately the same length (for example,
   we deal with a huge list of email addresses). In this case, there is a correlation between line
   numbers and the file size. So, we can use the algorithm below:
   Random Sampling 3 (random_sampler3.py) [14]download
<code>
import random

def random_sampler(filename, k):
        sample = []
        with open(filename, 'rb') as f:
                f.seek(0, 2)
                filesize = f.tell()

                random_set = sorted(random.sample(xrange(filesize), k))

                for i in xrange(k):
                        f.seek(random_set[i])
                        # Skip current line (because we might be in the middle of a line)
                        f.readline()
                        # Append the next line to the sample set
                        sample.append(f.readline().rstrip())

        return sample
</code>

   Basically, we get the file size. Create a sorted random set of k random positions in the file
   (between 1 and the file size). For each random position, we seek that position, skip a line, and put
   the next line to the sample set.

Benchmark
   The table below shows the elapsed time for selecting 1000 lines from a large (~ 40M lines) and a very
   large file(~ 300M lines) for each algorithm. We see that the algorithm 3 is much faster. As I
   mentioned before, the only assumption is that the lines should have approximately the same length.
   Algorithm            File 1 (~ 40M lines)    File 2 (~ 300M lines)
   random_sampler1.py   6.641s                  1m14.184s
   random_sampler2.py   50.406s                 6m51.078s
   random_sampler3.py   0.019s                  3.119s

