filename: python_text-data-cleaning-steps_20170630.txt
https://www.analyticsvidhya.com/blog/2014/11/text-data-cleaning-steps-python/

Steps for effective text data cleaning (with case study using Python)
November 16, 2014

   The days when one would get data in tabulated spreadsheets are truly behind us. A moment of silence
   for the data residing in the spreadsheet pockets. Today, more than 80% of the data is unstructured â€“
   it is either present in data silos or scattered around the digital archives. Data is being produced
   as we speak â€“ from every conversation we make in the social media to every content generated from
   news sources. In order to produce any meaningful actionable insight from data, it is important to
   know how to work with it in its unstructured form. As a Data Scientist at one of the fastest growing
   Decision Sciences firm, my bread and butter comes from deriving meaningful insights from unstructured
   text information.

Mining Twitter Data
   One of the first steps in working with text data is to pre-process it. It is an essential step before
   the data is ready for analysis. Majority of available text data is highly unstructured and noisy in
   nature â€“ to achieve better insights or to build better algorithms, it is necessary to play with clean
   data. For example, social media data is highly unstructured â€“ it is an informal communication â€“
   typos, bad grammar, usage of slang, presence of unwanted content like URLs, Stopwords, Expressions
   etc. are the usual suspects.

   In this blog, therefore I discuss about these possible noise elements and how you could clean them
   step by step. I am providing ways to clean data using Python.

   As a typical business problem, assume you are interested in finding:  which are the features of an
   iPhone which are more popular among the fans. You have extracted consumer opinions related to iPhone
   and here is a tweet you extracted:
   [gears-b.png]

   â€œI luv my &lt;3 iphone &amp; youâ€™re awsm apple. DisplayIsAwesome, sooo happppppy ğŸ™‚
   http://www.apple.comâ€

Steps for data cleaning:
   Here is what you do:
    1. Escaping HTML characters: Data obtained from web usually contains a lot of html entities like
       &lt; &gt; &amp; which gets embedded in the original data. It is thus necessary to get rid of
       these entities. One approach is to directly remove them by the use of specific regular
       expressions. Another approach is to use appropriate packages and modules (for example htmlparser
       of Python), which can convert these entities to standard html tags. For example: &lt; is
       converted to â€œ<â€ and &amp; is converted to â€œ&â€.

   Snippet:
          import HTMLParser
html_parser = HTMLParser.HTMLParser()
tweet = html_parser.unescape(original_tweet)

   Output:
   >> â€œI luv my <3 iphone & youâ€™re awsm apple. DisplayIsAwesome, sooo happppppy ğŸ™‚ http://www.apple.comâ€


    2. Decoding data: Thisis the process of transforming information from complex symbols to simple and
       easier to understand characters. Text data may be subject to different forms of decoding like
       â€œLatinâ€, â€œUTF8â€ etc. Therefore, for better analysis, it is necessary to keep the complete data in
       standard encoding format. UTF-8 encoding is widely accepted and is recommended to use.

   Snippet:
tweet = original_tweet.decode("utf8").encode(â€˜asciiâ€™,â€™ignoreâ€™)

   Output:
   >>  â€œI luv my <3 iphone & youâ€™re awsm apple. DisplayIsAwesome, sooo happppppy ğŸ™‚ http://www.apple.comâ€

    3. Apostrophe Lookup: To avoid any word sense disambiguation in text, it is recommended to maintain
       proper structure in it and to abide by the rules of context free grammar. When apostrophes are
       used, chances of disambiguation increases.

   For example â€œitâ€™s is a contraction for it is or it hasâ€.

   All the apostrophes should be converted into standard lexicons. One can use a lookup table of all
   possible keys to get rid of disambiguates.

   Snippet:
APPOSTOPHES = {â€œ's" : " is", "'re" : " are", ...} ## Need a huge dictionary
words = tweet.split()
reformed = [APPOSTOPHES[word] if word in APPOSTOPHES else word for word in words]
reformed = " ".join(reformed)

   Outcome:
   >>  â€œI luv my <3 iphone & you are awsm apple. DisplayIsAwesome, sooo happppppy ğŸ™‚
   http://www.apple.comâ€

    4. Removal of Stop-words: When data analysis needs to be data driven at the word level, the commonly
       occurring words (stop-words) should be removed. One can either create a long list of stop-words
       or one can use predefined language specific libraries.
    5. Removal of Punctuations: All the punctuation marks according to the priorities should be dealt
       with. For example: â€œ.â€, â€œ,â€,â€?â€ are important punctuations that should be retained while others
       need to be removed.
    6. Removal of Expressions: Textual data (usually speech transcripts) may contain human expressions
       like [laughing], [Crying], [Audience paused]. These expressions are usually non relevant to
       content of the speech and hence need to be removed. Simple regular expression can be useful in
       this case.

    7. Split Attached Words: We humans in the social forums generate text data, which is completely
       informal in nature. Most of the tweets are accompanied with multiple attached words like
       RainyDay, PlayingInTheCold etc. These entities can be split into their normal forms using simple
       rules and regex.

   Snippet:
cleaned = â€œ â€.join(re.findall(â€˜[A-Z][^A-Z]*â€™, original_tweet))

   Outcome:
   >>  â€œI luv my <3 iphone & you are awsm apple. Display Is Awesome, sooo happppppy ğŸ™‚
   http://www.apple.comâ€

    8. Slangs lookup: Again, social media comprises of a majority of slang words. These words should be
       transformed into standard words to make free text. The words like luv will be converted to love,
       Helo to Hello. The similar approach of apostrophe look up can be used to convert slangs to
       standard words. A number of sources are available on the web, which provides lists of all
       possible slangs, this would be your holy grail and you could use them as lookup dictionaries for
       conversion purposes.


   Snippet:
            tweet = _slang_loopup(tweet)

   Outcome:
   >>  â€œI love my <3 iphone & you are awesome apple. Display Is Awesome, sooo happppppy ğŸ™‚
   http://www.apple.comâ€

    9. Standardizing words: Sometimes words are not in proper formats. For example: â€œI looooveee youâ€
       should be â€œI love youâ€. Simple rules and regular expressions can help solve these cases.


   Snippet:
tweet = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))

   Outcome:
   >>  â€œI love my <3 iphone & you are awesome apple. Display Is Awesome, so happy ğŸ™‚
   http://www.apple.comâ€

   10. Removal of URLs: URLs and hyperlinks in text data like comments, reviews, and tweets should be
       removed.


   Final cleaned tweet:
   >>  â€œI love my iphone & you are awesome apple. Display Is Awesome, so happy!â€ , <3 , ğŸ™‚


Advanced data cleaning:
    1. Grammar checking: Grammar checking is majorly learning based, huge amount of proper text data is
       learned and models are created for the purpose of grammar correction. There are many online tools
       that are available for grammar correction purposes.
    2. Spelling correction: In natural language, misspelled errors are encountered. Companies like
       Google and Microsoft have achieved a decent accuracy level in automated spell correction. One can
       use algorithms like the Levenshtein Distances, Dictionary Lookup etc. or other modules and
       packages to fix these errors.

End Notes:
   Hope you found this article helpful. These were some tips and tricks, I have learnt while working
   with a lot of text data. If you follow the above steps to clean the data, you can drastically improve
   the accuracy of your results and draw better insights. Do share your views/doubts in the comments
   section and I would be happy to participate.

   Go Hack!
