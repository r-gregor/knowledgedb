filename: zig_Zigs-HashMap-multif_20250304.txt
https://www.openmymind.net/Zigs-HashMap-Part-1/

Zig's HashMap - Part 1
Jan 16, 2024

   This blog posts assumes that you're comfortable with Zig's implementation of Generics.

   Like most hash map implementations, Zig's std.HashMap relies on 2 functions, hash(key: K) u64 and
   eql(key_a: K, key_b: K) bool. The hash function takes a key and returns an unsigned 64 bit integer,
   known as the hash code. The same key always returns the same hash code. The hash function can produce
   the same hash code for two different keys which is one reason we also need eql: to determine if two
   keys are the same.

   This is all standard stuff, but Zig's implementation has a few specific details worth exploring. This
   is particularly true given the number of hash map types in the standard library as well as the fact
   that the documentation seems incomplete and confusing. Specifically, there are six hash map variants:
   std.HashMap, std.HashMapUnmanaged, std.AutoHashMap, std.AutoHashMapUnmanaged, std.StringHashMap,
   std.StringHashMapUnmanaged. One of those, std.HashMapUnmanaged, contains the bulk of the
   implementation. The other five are thin wrappers: they are composed of an std.HashMapUnmanaged. The
   documentation for those five variants is challenging because of this composition, which is not, in my
   opinion, handled well by the document generator.

   There's also a completely different ArrayHashMap which has different properties (e.g. preserving
   insertion order), which we won't be covering.

   If we look at the put method of std.HashMap, we'll see a pattern that's often repeated:

pub fn put(self: *Self, key: K, value: V) Allocator.Error!void {
	return self.unmanaged.putContext(self.allocator, key, value, self.ctx);
}

   As I said, most of the heavy lifting is done by std.HashMapUnmanaged, which the other variants wrap
   in a field named "unmanaged".

  [**1][https://www.openmymind.net/Zigs-HashMap-Part-1/#Unmanaged]Unmanaged

   "Unmanaged" types are sprinkled throughout the standard library. It's a naming convention which
   indicates that the type in question doesn't maintain an allocator. Any method that requires
   allocations takes an explicit allocator as a parameter. To see this in practice, consider this linked
   list:

pub fn LinkedList(comptime T: type) type {
	return struct {
		head: ?*Node = null,
		allocator: Allocator,

		const Self = @This();

		pub fn init(allocator: Allocator) Self {
			return .{
				.allocator = allocator,
			};
		}

		pub fn deinit(self: Self) void {
			var node = self.head;
			while (node) |n| {
				node = n.next;
				self.allocator.destroy(n);
			}
		}

		pub fn append(self: *Self, value: T) !void {
			const node = try self.allocator.create(Node);
			node.value = value;
			const h = self.head orelse {
				node.next = null;
				self.head = node;
				return;
			};
			node.next = h;
			self.head = node;
		}

		pub const Node = struct {
			value: T,
			next: ?*Node,
		};
	};
}

   Our init function takes and stores an std.mem.Allocator. This allocator is then used as needed in
   append and deinit. This is a common pattern in Zig. The "unmanaged" version of the above code is only
   slightly different:

pub fn LinkedListUnmanaged(comptime T: type) type {
	return struct {
		head: ?*Node = null,

		const Self = @This();

		pub fn deinit(self: Self, allocator: Allocator) void {
			var node = self.head;
			while (node) |n| {
				node = n.next;
				allocator.destroy(n);
			}
		}

		pub fn append(self: *Self, allocator: Allocator, value: T) !void {
		const node = try allocator.create(Node);
		// .. same as above
		}

		// Node is the same as above
		pub const Node = struct {...}
	};
}

   We no longer have an allocator field. The append and deinit functions both take an extra parameter:
   allocator. Because we no longer need to store the allocator, we're able to initialize a
   LinkedListUnmanaged(T) exclusively with default values (i.e. head: ?*Node = null) and are able to
   remove the init function altogether. This isn't a requirement of unamanged types, but it is common.
   To create a LinkedListUnmanaged(i32), you'd do:

var ll = LinkedListUnmanaged(i32){};

   It's a bit cryptic, but it's standard Zig. LinkedListUnmanaged(i32) returns a type, so the above is
   no different than doing: var user = User{} and relying on the default field values of User.

   You might be wondering what's the point of unamaged types?. But before we answer that, let's consider
   how easy it is to provide both managed and unmanaged versions of our LinkedList. We keep our
   LinkedListUnmanaged as-is, and change our LinkedList to wrap it:

pub fn LinkedList(comptime T: type) type {
	return struct {
		allocator: Allocator,
		unmanaged: LinkedListUnmanaged(T),

		const Self = @This();

		pub fn init(allocator: Allocator) Self {
			return .{
				.unmanaged = .{},
				.allocator = allocator,
			};
		}

		pub fn deinit(self: Self) void {
			self.unmanaged.deinit(self.allocator);
		}

		pub fn append(self: *Self, value: T) !void {
			return self.unmanaged.append(self.allocator, value);
		}

		pub const Node = LinkedListUnmanaged(T).Node;
	};
}

   This simple composition is, as we saw above, the same as how the various hash map types wrap an
   std.HashMapUnmanaged.

   There are a few benefits to unmanaged types. The most important is that they're more explicit. Rather
   than knowing that a type, like LinkList(T), will probably need to allocate memory at some point, the
   explicit API of the unmanaged variant identifies the specific methods that allocate/deallocate. This
   can help reduce surprises and gives greater control to the caller. A secondary benefit of unamanged
   types is that they save a few bytes of memory by not having a reference to the allocator. Some
   applications might need to store thousands or even millions of these structures, in which case it can
   add up.

   In the name of simplicity, the rest of this post won't mentioned unamanged. Anything we see about the
   StringHashMap or AutoHashMap or HashMap applies equally to their *Unmanaged variant.

HashMap vs AutoHashMap
   std.HashMap is a generic type which takes two type parameters: the type of the key and the type of
   the value. And, as we saw, hash maps require two functions: hash and eql. Combined, these functions
   are called the "context". Both functions operate on the key, and there isn't a single hash or eql
   function that'll work for all types. For example, for integer keys, eql is going to be a_key == b_key
   where as for []const u8 keys we want std.mem.eql(u8, a_key, b_key).

   When we use std.HashMap we need to provide the context (the two functions). We'll look at this
   shortly, but for now we can rely on std.AutoHashMap which automatically generate these functions for
   us. It might surprise you to know that AutoHashMap can even generate a context for more complex keys.
   The following works:

const std = @import("std");

pub fn main() !void {
	var gpa = std.heap.GeneralPurposeAllocator(.{}){};
	const allocator = gpa.allocator();

	var h = std.AutoHashMap(User, i32).init(allocator);
	try h.put(User{.id = 3, .state = .active}, 9001);
	defer h.deinit();
}

const User = struct{
	id: i32,
	state: State,

	const State = enum {
		active,
		pending,
	};
};

   But there's a limit to AutoHashMap's capabilities. Change our User struct to:

const User = struct{
	id: i32,
	state: State,
	login_ids: []i32,

	...
};

   And we get the following compilation error:

     std.hash.autoHash does not allow slices as well as unions and structs containing slices here
     (User) because the intent is unclear. Consider using std.hash.autoHashStrat or providing your own
     hash function instead.

   The reality is that there are types with ambiguous equality rules. Slices, like our login_ids above,
   are a good example. Should two slices pointing to different memory, but with the same length and same
   content be considered equal? It's application-specific. Similarly, I was surprised to find out that
   AutoHashMap won't allow floats (either directly or as part of a struct):

var h = std.AutoHashMap(f32, i32).init(allocator);
defer h.deinit();
try h.put(1.1, 9001);

   The above gives a compilation error: unable to hash type f32. It turns out that hashing floats
   isn't straightforward. It isn't that floats and slices can't be hashed or compared. It's that
   whatever implementation Zig picks would likely surprise some developers and that unexpectedness could
   result in misuse which could lead to bugs. In the end, if AutoHashMap can handle your key type, then
   use it. If not, use a HashMap and provide your own context (which we'll look at shortly).

AutoHashMap vs StringHashMap
   You'd be forgiven for thinking that StringHashMap(V) is an alias for AutoHashMap([]const u8, V). But,
   as we just saw, AutoHashMap doesn't support slice keys. We can confirm this. Trying to run:

const std = @import("std");

pub fn main() !void {
	var gpa = std.heap.GeneralPurposeAllocator(.{}){};
	const allocator = gpa.allocator();

	var h = std.AutoHashMap([]const u8, i32).init(allocator);
	try h.put("over", 9000);
	defer h.deinit();
}

   gives us the following error:

     error: std.auto_hash.autoHash does not allow slices here ([]const u8) because the intent is
     unclear. Consider using std.StringHashMap for hashing the contents of []const u8. Alternatively,
     consider using std.auto_hash.hash or providing your own hash function instead.

   As I said earlier, it isn't that slices can't be hashed or compared, it's that some cases might
   consider slices equal only if they reference the same memory, while others might consider two slices
   equal if their elements are the same. But, in the cases of strings, most people expect "teg" to equal
   "teg" regardless of where each is stored.

const std = @import("std");

pub fn main() !void {
	var gpa = std.heap.GeneralPurposeAllocator(.{}){};
	const allocator = gpa.allocator();

	const name1: []const u8 = &.{'T', 'e', 'g'};
	const name2 = try allocator.dupe(u8, name1);

	const eql1 = std.meta.eql(name1, name2);
	const eql2 = std.mem.eql(u8, name1, name2);
	std.debug.print("{any}\n{any}", .{eql1, eql2});
}

   The above program prints "false" followed by "true". std.meta.eql compares pointers using: a.ptr ==
   b.ptr and a.len == b.len. But, specifically for strings, most programmers probably expect the
   behavior of std.mem.eql, which compares the bytes within the strings.

   Therefore, just like AutoHashMap wraps HashMap with a auto-generated context, StringHashMap also
   wraps HashMap with a string-specific context. We'll look more closely at contexts next, but here's
   the context that StringHashMap uses:

pub const StringContext = struct {
	pub fn hash(self: @This(), s: []const u8) u64 {
		_ = self;
		return std.hash.Wyhash.hash(0, s);
	}

	pub fn eql(self: @This(), a: []const u8, b: []const u8) bool {
		_ = self;
		return std.mem.eql(u8, a, b);
	}
};

Custom Context
   We'll finish part 1 by looking at using HashMap directly, which means providing our own context.
   We'll start with a simple example: creating a HashMap for case-insensitive ascii strings. We want the
   following to output: Goku = 9000. Notice though that while we insert using the key "GOKU" we fetch
   using "goku":

const std = @import("std");

pub fn main() !void {
	var gpa = std.heap.GeneralPurposeAllocator(.{}){};
	const allocator = gpa.allocator();

	var h = std.HashMap([]const u8,
	    i32,
	    CaseInsensitiveContext,
	    std.hash_map.default_max_load_percentage
	).init(allocator);

	defer h.deinit();
	try h.put("Goku", 9000);
	std.debug.print("Goku = {d}\n", .{h.get("goku").?});
}

   Unlike the StringHashMap generic which only takes the value type, or AutoHashMap that takes both the
   key and value type, HashMap takes the key type, value type, context type and a fill factor. I'm not
   going to cover the fill factor; above we're using Zig's default fill factor (80). The part that we
   are interested in is the CaseInsensitiveContext. Let's look at its implementation:

const CaseInsensitiveContext = struct {
	pub fn hash(_: CaseInsensitiveContext, s: []const u8) u64 {
		var key = s;
		var buf: [64]u8 = undefined;
		var h = std.hash.Wyhash.init(0);

		while (key.len >= 64) {
			const lower = std.ascii.lowerString(buf[0..], key[0..64]);
			h.update(lower);
			key = key[64..];
		}

		if (key.len > 0) {
			const lower = std.ascii.lowerString(buf[0..key.len], key);
			h.update(lower);
		}
		return h.final();
	}

	pub fn eql(_: CaseInsensitiveContext, a: []const u8, b: []const u8) bool {
		return std.ascii.eqlIgnoreCase(a, b);
	}
};

   The first parameter to both functions is an instance of the context itself. This allows more advanced
   patterns where the context might have state. In many cases though, it isn't used.

   Our eql function uses the existing std.ascii.eqlIgnoreCase function to compare two keys in a
   case-insensitive manner. Straightforward.

   Our hash function can be broken down into two parts. The first is lower-casing the key. If we want
   "goku" and "GOKU" to be treated as equal, our hash function has to return the same hash code for
   both. We do this in batches of 64 bytes to avoid having to allocate a buffer to hold the lower-cased
   value. This is possible because our hashing function can be updated with new bytes (which is quite
   common).

   This leads us to the second part, what's std.hash.Wyhash? When talking about a hashing algorithm for
   hash maps (which are distinct from cryptographic hashing algorithms), we're looking for a number of
   properties, such as performance (every operation on a hash map requires hashing the key), uniform
   distribution (if our hash function returns an u64, then a random set of inputs should be well
   distributed within that range) and collision resistance (different values can result in the same hash
   code, but the less it happens the better). There are a number of algorithms, some specialized for
   specific inputs (e.g. short strings), some designed for specific hardware. WyHash is a popular option
   that works well across a number of inputs and characteristics. You essentially feed it bytes and,
   once done, get a u64 out (or u32 depending on the version).

const std = @import("std");

pub fn main() !void {
	{
		const name = "Teg";

		var h = std.hash.Wyhash.init(0);
		h.update(name);
		std.debug.print("{d}\n", .{h.final()});
	}

	{
		const name = "Teg";
		const err = @intFromError(error.OutOfMemory);

		var h = std.hash.Wyhash.init(0);
		h.update(name);
		h.update(std.mem.asBytes(&err));
		std.debug.print("{d}\n", .{h.final()});
	}
}

   This code outputs: 17623169834704516898 followed by 7758855794693669122. The numbers shouldn't mean
   anything. The goal is only to show how data can be fed into our hashing function to generate a hash
   code.

   Let's look at another example. Say we have a User that we'd like to use as a key in a hash map:

const User = struct {
	id: u32,
	name: []const u8,
};

   We can't use an AutoHashMap for this since it doesn't support slices (which our name is). Let's start
   with a skeleton:

const std = @import("std");

pub fn main() !void {
	var gpa = std.heap.GeneralPurposeAllocator(.{}){};
	const allocator = gpa.allocator();

	var h = std.HashMap(User, i32, User.HashContext, std.hash_map.default_max_load_percentage).init(allocator);
	defer h.deinit();
	try h.put(.{.id = 1, .name = "Teg"}, 100);
	try h.put(.{.id = 2, .name = "Duncan"}, 200);

	std.debug.print("{d}\n", .{h.get(.{.id = 1, .name = "Teg"}).?});
	std.debug.print("{d}\n", .{h.get(.{.id = 2, .name = "Duncan"}).?});
}

const User = struct {
	id: u32,
	name: []const u8,

	pub const HashContext = struct {
		pub fn hash(_: HashContext, u: User) u64 {
				// TODO
		}

		pub fn eql(_: HashContext, a: User, b: User) bool {
				// TODO
		}
	};
};

   We need to implement the hash and eql functions. eql, as is often the case, is straightforward:

pub fn eql(_: HashContext, a: User, b: User) bool {
	if (a.id != b.id) return false;
	return std.mem.eql(u8, a.name, b.name);
}

   And if you look at our other hash examples, you might be able to come up with its implementation:

pub fn hash(_: HashContext, u: User) u64 {
	var h = std.hash.Wyhash.init(0);
	h.update(u.name);
	h.update(std.mem.asBytes(&u.id));
	return h.final();
}

   Plug in the two functions and the above example should work.

Conclusion
   Hopefully you now have a better understanding of how Zig's hash maps are implemented and how to
   leverage them within your code. In most cases, std.StringHashMap or std.AutoHashMap are all you'll
   need. But knowing about the existence and purpose of the *Unmanaged variants, as well as the more
   generic std.HashMap, might prove useful. If nothing else, the documentation and their implementation
   should now be easier to follow.

   In the next part we'll dive deep into hash map keys and values, how they're stored and how they're
   managed.


---
https://www.openmymind.net/Zigs-HashMap-Part-2/

Zig's HashMap - Part 2
Jan 22, 2024

   In part 1 we explored how the six HashMap variants relate to each other and what each offered to
   developers. We largely focused on defining and initializing HashMaps for various data type and
   utilizing custom hash and eql functions for types not supported by the StringHashMap or AutoHashMap.
   In this part we'll focus on keys and values, how they're stored, exposed and our responsibility with
   respect to their lifetime.

   Loosely speaking, Zig's hash map are implemented using two slices: one for keys and one for values.
   The hash code returned from the hash function is used to find the ideal index of an entry within
   these arrays. To start simply, if we had this code:

var lookup = std.StringHashMap(i32).init(allocator);
defer lookup.deinit();

try lookup.put("Goku", 9001);
try lookup.put("Paul", 1234);

   We could visualize our hash map like so:

       keys:                values:
       --------             --------
       | Paul |             | 1234 |   @mod(hash("Paul"), 5) == 0
       --------             --------
       |      |             |      |
       --------             --------
       |      |             |      |
       --------             --------
       | Goku |             | 9001 |  @mod(hash("Goku"), 5) == 3
       --------             --------
       |      |             |      |
       --------             --------

   When we hash our keys and apply a modulo operation with the length of our array (5 above), we get the
   ideal index for the entry. I say "ideal" because our hash function can return the same hash code for
   two different keys; the chance of such collisions increase dramatically when we map the hash code to
   one of 5 available slots via @mod. But if we ignore possible collisions, this is a reasonable view of
   our hash map.

   Once our hash map fills up to a certain point (in part 1, we briefly talked about the fill factor and
   mentioned that Zig defaults to 80%), it needs to grow to accommodate new values and to maintain the
   constant-time performance of lookups. Growing a hash map is like growing a dynamic array, we allocate
   a new array and copy the values from the original to the new (a simple algorithm would be making it
   2x larger). For this to work with a hash map though, we can't simply copy the keys and values to the
   same index of the new slices. We need to re-calculate their index. Why? Because the location of an
   entry has to be consistent and predictable. We can't insert a key-value pair using one algorithm,
   e.g. @mod(hash("Goku"), 5), and expect to find it using a different one, e.g. @mod(hash("Goku"), 10)
   (notice the 5 changed to 10, because our array grew).

   This basic visualization is going to serve as a foundation for much of this post. Further, the fact
   that entries can be moved from one backing array to another (i.e. when the hash map fills up and
   needs to grow) is also something we'll keep revisiting.

Values
   If we extend the above snippet and call lookup.get("Paul"), the return value will be 1234. The return
   type of get is a ?i32, or more generically, ?V. The optional return type allows get to inform us that
   the key wasn't found. If you've looked through Zig's documentation, you've probably noticed another
   similar method: getPtr(key) with a slightly different return type: ?*V.

   Since it's difficult to appreciate the difference between the two methods when talking about
   primitive types like our i32, consider this version which swaps our i32 value for a User:

const std = @import("std");

pub fn main() !void {
	var gpa = std.heap.GeneralPurposeAllocator(.{}){};
	const allocator = gpa.allocator();

	var lookup = std.StringHashMap(User).init(allocator);
	defer lookup.deinit();

	try lookup.put("Goku", .{
		.id = 9000,
		.name = "Goku",
		.super = false,
	});

	var user = lookup.get("Goku").?;

	user.super = true;
	std.debug.print("{any}\n", .{lookup.get("Goku").?.super});
}

const User = struct {
	id: i32,
	name: []const u8,
	super: bool,
};

   Even though we set user.super = true, the value of the User in lookup is still false. This is
   because, in Zig, assignment are done by copy. If we keep the code as-is, but change lookup.get to
   lookup.getPtr, it'll work. We're still doing an assignment, thus still copying a value, but the value
   we're copying is the address of the User in our hash map, not the user itself.

   getPtr allows us to get a reference to the value within the hash map. As we see above, this has
   behavioral significance; we can directly modify the value stored in our hash map. It also has a
   performance implication as copying large values can be expensive. But consider our above
   visualization and remember that, as the hash table fills up, values can be relocated. With that in
   mind, can you explain why this code crashes?:

const std = @import("std");

pub fn main() !void {
	var gpa = std.heap.GeneralPurposeAllocator(.{}){};
	const allocator = gpa.allocator();

	// change the type, just to make it easier to write this snippet
	// the same would happen with our above StringHashMap(User)
	var lookup = std.AutoHashMap(usize, usize).init(allocator);
	defer lookup.deinit();

	try lookup.put(100, 100);
	const first = lookup.getPtr(100).?;

	for (0..50) |i| {
		try lookup.put(i, i);
	}
	first.* = 200;
}

   If the first.* = 200; syntax is a confusing, we're dereferencing the pointer and writing a value to
   it. Our pointer is the address of an index in our values array, so what this syntax does is write
   directly into our array. It crashes because our inserting-loop has forced the hash table to grow,
   causing the underlying key and values to be re-allocated and all keys and values to be moved. The
   pointer returned by getPtr is no longer valid. At the time of this writing, the default hash map size
   is 8 with a fill factor of 80%. If we looped 0..5 the code would work, but one more iteration (0..6)
   causes the growth and thus our crash. With typical usage, this issue isn't normally a problem; you
   won't hold a reference to an entry while modifying the hash map. But understanding that it can happen
   and understanding why it happens will help us better utilize other hash map features that return
   value and key pointers.

   Going back to our User example, what if we changed the type of lookup from std.StringHashMap(User) to
   std.StringHashMap(*User)? The biggest impact will be with respect to the lifetime of our values. With
   our original std.StringHashMap(User), we could say that the lookup owns the values - the users we
   insert are embedded with the hash map's value array. This makes lifetime management easy, when we
   deinit our lookup the backing keys and values arrays are freed.

   Our User has a name: []const u8 field. Our examples use a string literal, which statically exist
   throughout the lifetime of the program. If our name was dynamically allocated though, we would have
   to free it explicitly. We'll cover that as we explore pointer values in more detail.

   Using a *User breaks that ownership. Our hash map stores pointers, but it doesn't own what they point
   to. Despite the call to lookup.deinit, this code leaks the user:

const std = @import("std");

pub fn main() !void {
	var gpa = std.heap.GeneralPurposeAllocator(.{}){};
	const allocator = gpa.allocator();

	var lookup = std.StringHashMap(*User).init(allocator);
	defer lookup.deinit();

	const goku = try allocator.create(User);
	goku.* = .{
		.id = 9000,
		.name = "Goku",
		.super = false,
	};
	try lookup.put("Goku", goku);
}

const User = struct {
	id: i32,
	name: []const u8,
	super: bool,
};

   Let's visualize it:
 ===============================
 ║  keys:        values:       ║
 ║  ---------   -------------  ║
 ║  | Goku* |   | 1024d4000 | ----> -------------
 ║  ---------   -------------  ║    |    9000   |
 ║  |       |   |           |  ║    -------------
 ║  ---------   -------------  ║    | 1047300e4 |---> -----------------
 ===============================    -------------     | G | o | k | u |
                                    |     4     |     -----------------
                                    -------------
                                    |    false  |
                                    -------------

   We'll talk about keys in the next section, for now we use "Goku" for simplicity.

   The double-lined box is our lookup, and represents the memory it owns and is responsible for.
   References we put in our hash map will point to values outside that box. This has a number of
   implications. Most importantly though, it means the lifetime of the values is detached from the
   lifetime of the hash map, and calling lookup.deinit won't free them.

   There is a common case where we want to use pointers and associate the lifetime of the value with the
   hash map. Recall our crashing program, when a pointer to our hash map value became invalid. As I
   said, that isn't normally a problem, but in more advanced scenarios, you might want to have different
   parts of code referencing a value that also exists in a hash map. Let's re-examine the above
   visualization and think about what happens if our hash map grows and relocates the keys and values
   arrays:

 ===============================
 ║  keys:        values:       ║
 ║  ---------   -------------  ║
 ║  |       |   |           |  ║
 ║  ---------   -------------  ║
 ║  ---------   -------------  ║
 ║  |       |   |           |  ║
 ║  ---------   -------------  ║
 ║  ---------   -------------  ║
 ║  | Goku* |   | 1024d4000 | ----> -------------
 ║  ---------   -------------  ║    |    9000   |
 ║  |       |   |           |  ║    -------------
 ║  ---------   -------------  ║    | 1047300e4 |---> -----------------
 ===============================    -------------     | G | o | k | u |
                                    |     4     |     -----------------
                                    -------------
                                    |    false  |
                                    -------------

   The two arrays have grown, been reallocated and our entry indexes have been re-calculated, but our
   actual User still resides at the same place in the heap (memory location 1047300e4). Just like deinit
   doesn't alter anything outside our double-lined boxes, other changes such as growth, won't alter
   them.

   Generally speaking, it'll be obvious if you should be storing values or pointer to values. This is
   largely because methods like getPtr make it possible to efficiently retrieve and modify values
   directly from our hash map. Either way, we can have our performance cake, so performance isn't the
   main consideration. What does matter is whether values need to outlive the hash map and/or whether
   references to values need to exist (and thus remain valid) while the hash map undergoes changes.

   In cases where where the hash map and the referenced values should be linked, we need to iterate
   through the values and clean them up before we call lookup.deinit:

defer {
	var it = lookup.valueIterator();
	while (it.next()) |value_ptr| {
		allocator.destroy(value_ptr.*);
	}
	lookup.deinit();
}

   If the dereferencing (value_ptr.*) doesn't seem right, go back to the visualization. Our
   valueIterator is giving us a pointer to the value in the array, and the value in the array is a
   *User. Thus, value_ptr is a **User.

   Whether we're storing a User or a *User, any allocated fields within the value are always our
   responsibility. In a real application, your user's name wouldn't be string literals, they would be
   dynamically allocated. In that case, our while loop above would have to change to:

while (it.next()) |value_ptr| {
	const user = value_ptr.*;
	allocator.free(user.name);
	allocator.destroy(user);
}

   Even if our value is a User, its fields are our responsibility (it's a bit silly to think
   lookup.deinit would know how/what needs to be freed anyways):

while (it.next()) |value_ptr|
	allocator.free(value_ptr.name);
}

   In this last case, since we're storing User, our value_ptr is a *User (a pointer to a User, not a
   pointer to a pointer to a User as before).

Keys
   We could start and end this section by saying: everything we said about values applies equally to
   keys. That is 100% true, but it's somehow less intuitive. Most developers quickly understand that a
   heap-allocated User stored within a hash map has its own lifetime and need to be explicitly
   managed/freed. But for some reason, it isn't quite so obvious with keys.

   Like values, if our keys are primitive types we don't have to do anything special. A key such as an
   integer is stored directly within our hash map's key array and thus has its lifetime and memory tied
   to the hash map. This is a very common case. But another common case is using string keys with the
   std.StringHashMap. This often trips up developers new to Zig, but you need to guarantee that string
   keys are valid for as long as they're used by the hash map. And, if they're dynamically allocated,
   you need to make sure they're freed when no longer used. This means doing for keys exactly what we
   did for values.

   Let's visualize our hash map again, but this time properly represent a string key:

 ===================================
 ║   keys:          values:        ║
 ║   -------------  -------------  ║
 ║   | 1047300e4 |  | 1024d4000 | ----> -------------
 ║   -------------  -------------  ║    |    9000   |
 ║   |           |  |           |  ║    -------------
 ║   -------------  -------------  ║    | 1047300e4 |---> -----------------
 ===================================    -------------     | G | o | k | u |
                                        |     4     |     -----------------
                                        -------------
                                        |    false  |
                                        -------------

   In this example, our key is actually the user.name. Having the key be part of the value is pretty
   common. Here's what that might look like:

const user = try allocator.create(User);
user.* = . {
	.id = 9000,
	.super = false,
	// simulate a name that comes from a dynamic source, like a DB
	.name = try allocator.dupe(u8, "Goku"),
};
try lookup.put(user.name, user);

   In this case, our previous cleanup code is sufficient since we were already freeing user.name which
   is our key:

defer {
	var it = lookup.valueIterator();
	while (it.next()) |value_ptr| {
		const user = value_ptr.*;
		allocator.free(user.name);
		allocator.destroy(user);
	}
	lookup.deinit();
}

   But for cases where the key isn't part of the value, we need to iterate and free the keys. In many
   cases, you'll want to iterate both the keys and values and free both. We can simulate this by freeing
   the name as referenced by the key instead of the user:

defer {
	var it = lookup.iterator();
	while (it.next()) |kv| {
		// This..
		allocator.free(kv.key_ptr.*);

		// Is the same as the following, but only because user.name is our key
		// allocator.free(user.name);

		allocator.destroy(kv.value_ptr.*);
	}
	lookup.deinit();
}

   Instead of iteratorValue() we're using iterator() to get access to both a key_ptr and value_ptr.

   The last thing to consider is how to remove values from our lookup. This code leaks both the name/key
   and the heap-allocated user User despite using our improved cleanup logic:

var lookup = std.StringHashMap(*User).init(allocator);

defer {
	var it = lookup.iterator();
	while (it.next()) |kv| {
		allocator.free(kv.key_ptr.*);
		allocator.destroy(kv.value_ptr.*);
	}
	lookup.deinit();
}

const user = try allocator.create(User);
user.* = . {
	.id = 9000,
	.super = false,
	// simulate a name that comes from a dynamic source, like a DB
	.name = try allocator.dupe(u8, "Goku"),
};
try lookup.put(user.name, user);

// We added this!
_ = lookup.remove(user.name);

   The last line removes the entry from our hash map, so our cleanup routine no longer iterates over it
   and doesn't free name or user. We need to use fetchRemove instead of remove to get the key and value
   that was removed:

if (lookup.fetchRemove(user.name)) |kv| {
	allocator.free(kv.key);
	allocator.destroy(kv.value);
}

   fetchRemove doesn't return key and value pointers, it returns the actual key and value. That doesn't
   change how we use it, but it should be obvious why the key and value are returned as opposed to a
   pointer to the key and value. With the items removed from the hash map, there is no valid pointer to
   the keys and values within the hash map - they've been removed.

   All of this assumes that your values and keys need to be freed/invalidated when removed from the hash
   map. There can be cases where the lifetimes of your values (and more rarely your keys) are completely
   unrelated to their presence within the hash map. In those cases, you need to free the memory when it
   makes sense for your application to do so. There's no general pattern/guidance that applies.

   For most cases, when dealing with non primitive keys or values, the takeway is that when you call
   deinit on your hash map, any allocations you made for your keys and/or values won't be magically
   freed; you'll need to do that yourself.

getOrPut
   While there are a number of implications to what we've talked about so far, for me, one of the best
   benefits that comes from exposing key and value pointers directly is the getOrPut method.

   If I asked you to store named counters in a map in Go, or most languages, you'd end up with something
   like:

count, exists := counters[name]
if exists == false {
	counters[name] = 1
} else {
	counters[name] = count + 1;
}

   This code requires two lookups. While we've been trained not to look beyond the fact that hash map
   access is O(1), the reality is that fewer operations are faster than more, calculating the hash code
   isn't the cheapest operation (and its performance varies depending on the key type and length), and
   collisions add overhead to the entire process. The getOrPut method solves this problem by returning a
   value pointer and a boolean indicating whether or not the value was found.

   In other words, with getOrPut we either get a pointer to the found value, or we get a pointer to
   where the item should go. We also get a boolean indicating which of the two scenarios e have. sThis
   allows the above kind of upsert to be written with a single lookup:

const gop = try counters.getOrPut(name);
if (gop.found_existing) {
	gop.value_ptr.* += 1;
} else {
	gop.value_ptr.* = 1;
}

   Of course, like any other value or key pointer, value_ptr should only be considered valid so long as
   no changes to the hash map is made. This, by the way, also applied to the iterated keys and values we
   get from iterator(), valueIterator and keyIterator(), for the same reason.

Conclusion
   Hopefully you're now more comfortable with using std.HashMap, std.AutoHashMap and std.StringHashMap
   and, if appropriate, their unmanaged variants. While you might never have to provide your own context
   (hash and eql function), it's good to know that it's an option. For day to day programming, I find it
   immensely useful to visualize data, particularly when pointers are used and levels of indirection are
   added. Whenever I'm dealing with a value_ptr or key_ptr, I think of those two slices and the
   difference between the value or key and the address of the value or key in those slices.

---
https://www.openmymind.net/Zigs-HashMap-Part-3/

Zig's HashMap - Part 3
Aug 07, 2024

  Previously On...
   In Part 1 we saw how Zig's StringHashMap and AutoHashMap are wrappers around a HashMap. HashMap
   works across different key types by requiring a Context. Here's the built-in context that
   StringHashMap uses:

pub const StringContext = struct {
	pub fn hash(_: StringContext, s: []const u8) u64 {
		return std.hash.Wyhash.hash(0, s);
	}
	pub fn eql(_: StringContext, a: []const u8, b: []const u8) bool {
		return std.mem.eql(u8, a, b);
	}
};

   The Context is a simple interface which makes HashMap usable for any key. As long as you can provide
   a hash and eql implementation, you can use any type of key.

  Concurrency
   The purpose of this post is not to make Zig's HashMap thread-safe, but let's consider a common trick
   used to support multiple concurrent writers: using shards/buckets. If you aren't familiar with this
   pattern, it involves wrapping multiple HashMaps + Mutex (often called "Buckets" or "Shards"). The
   wrapper can then direct operations to the bucket "responsible" for the key.

pub fn ConcurrentStringHashMap(comptime V: type) type {
	return struct {
		buckets: [4]Bucket(V),

		// ...

		pub fn get(self: *Self, key: []const u8) ?V {
			const hash_code = std.hash.Wyhash.hash(0, key);
			// because we have 4 buckets
			const bucket_index = @mod(hash_code, 4);
			return self.buckets[bucket_index].get(key);
		}
	};
}

   hash_code & 3 might perform better than @mod(hash_code, 4). This is because N & (D-1) == N % D, but
   only when D is a power or 2

   Where Bucket(V) is its own wrapper for a HashMap and RwLock:

fn Bucket(comptime V: type) type {
	return struct {
		lock: std.Thread.RwLock,
		hash_map: std.StringHashMap(V),

		// init, deinit, ...

		pub fn get(self: *Self, key: []const u8) ?V {
			self.lock.lockShared();
			defer self.lock.unlockShared();
			return self.hash_map.get(key);
		}
	};
}

   But there's something that always annoys me about this implementation: every operation requires
   hashing the key twice. First we calculate the hash code in getIndex to determine the correct bucket.
   The second time takes place within StringContext.hash. This gnaws at me; hash maps work because hash
   functions are deterministic. We have the hash_code in our outer get, do we really need to calculate
   it again?

   One option is to use a custom key and context for our Bucket:

const Key = struct {
	key: []const u8,
	hash_code: u64,

	const Context = struct {
		pub fn hash(_: Context, k: Key) u64 {
			return k.hash_code;
		}
		pub fn eql(_: Context, a: Key, b: Key) bool {
			return std.mem.eql(u8, a.key, b.key);
		}
	};
};

   Our get method becomes:

pub fn get(self: *Self, key: []const u8) ?V {
	const hash_code = std.hash.Wyhash.hash(0, key);
	const bucket_index = @mod(hash_code, 4);
	return self.buckets[bucket_index].get(.{.key = key, .hash_code = hash_code});
}

   This works, but at the cost of having to store a larger key in our HashMap. While it's tempting to
   think this'll just use a bit more RAM, the reality is that it could negatively impact performance by
   increasing CPU-cache misses. Is there a solution that doesn't require this trade off?

Adapted Context
   What we want is for our key to be just the string, []const u8, and our Context to use our
   pre-generated hash code. At an extreme, we could say that we want our Context to be independent from
   our key. This is possible through *Adapted variants of the HashMap methods you're already using, such
   as getAdapted. The *Adapted variants, take an explicit Context per call (much like the Unmanged
   HashMap variants take an explicit Allocator per call).

   Our ConcurrentStringHashMap(V).get only used the calculate hash code to decide which bucket to use.
   Now we'll pass this to the bucket as well:

pub fn get(self: *Self, key: []const u8) ?V {
	const hash_code = std.hash.Wyhash.hash(0, key);
	const bucket_index = @mod(hash_code, 4);
	return self.buckets[bucket_index].get(key, hash_code);
}

   Bucket(V).get can use this hash code, along with a custom context in order to skip the duplicate key
   hashing:

fn get(self: *Self, key: []const u8, hash_code: u64) ?V {
	self.lock.lockShared();
	defer self.lock.unlockShared();
	return self.hash_map.getAdapted(key, BucketContext{.hash_code = hash_code});
}

   With BucketContext being similar to the built-in StringContext:

const BucketContext = struct {
	hash_code: u64,

	// We no longer need to re-calcualte the hash code!
	pub fn hash(self: BucketContext, key: []const u8) u64 {
		return self.hash_code;
	}

	// Same as StringContext.eql
	pub fn eql(_: BucketContext, a: []const u8, b: []const u8) bool {
		return std.mem.eql(u8, a, b);
	}
};

   You could say that we're overwriting the default (stateless) StringContext, with our own stateful
   BucketContext on a per-call basis. In fact, many of the HashMap method you use every day are thin
   wrappers around *Adapted variant, where the default context is used.

   However, not every method has a direct *Adapted variant. For example, there's no putAdapted. This is
   likely because put is itself a wrapper around getOrPut and there is a getOrPutAdapted. So using the
   *Adapted variants means that, sometimes, we'll need to dig a little deeper to find the right method.
   Here's our Bucket(V).put:

fn put(self: *Self, key: []const u8, hash_code: u64, value: V) !void {
	self.lock.lock();
	defer self.lock.unlock();

	const result = try self.hash_map.getOrPutAdapted(key, BucketContext{.hash_code = hash_code});
	result.key_ptr.* = key;
	result.value_ptr.* = value;
}

   When we use map.get or map.put, the type of the key is the generic parameter of our HashMap (a
   []const u8 in the case of a StringHashMap). But for the *Adapted variants, both the key and context
   are anytype. This isn't something we're utilizing here, but it provides enormous flexibility by
   decoupling not just the data, but also the types of the Context with respect to the HashMap.

   In fact, get is a wrapper around getContext which is a wrapper around getAdapted. Most of the methods
   follow this pattern. Like getAdapted, getContext takes an explicit Context. Unlike getAdapted the
   getContext context isn't anytype, it's of type Context, e.g. StringContext for a StringHashMap.

Rehasing
   As items are inserted into a HashMap, it'll start to fill up. At some point (known as the load
   factor), the HashMap has to grow its underlying storage and re-hash all its entries. This can
   introduce challenges when using adapted contexts. For example, imagine we insert "a" and provide an
   adapted context of BucketContext{.hash_code = 2941419223392617777}. Next we insert "b" with its own
   adapted context of BucketContext{.hash_code = 16239767293555273624}. But when we try to insert "c",
   the HashMap needs to grow. At this point, our previously inserted entries with the keys "a" and "b"
   need to be re-hashed, but those explicitly provided adapted contexts are long gone.

   In our specific cases, where Bucket(V) is a wrapper for StringHashMap(V), this isn't actually a
   problem. Why? Because the re-hashing step will use the default context, aka the StringContext at the
   top of this post. For a given string, the hash method of StringContext produces the same result as
   what we're storing in the hash_code field. To be sure of this, instead of using
   std.hash.Wyhash.hash(0, key) directly, we should probably use std.hash_map.hashString(key). This
   would ensure that our hash is always consistent with StringContext.

   However, if you were doing something really unique in your adapted context, it could be a problem
   (don't ask me what kind of thing, because I can't think of any). However, all is not lost, at the
   very end of this rabbit hole is a getOrPutContextAdapted which accept two adapted contexts! The first
   is the adapted context to use when looking up the key we want to get or put (just like the adapted
   context we passed to getOrPutAdapted). The additional one is the adapted context to use when
   re-hashing.

Conclusion
   If you've tried to navigate the HashMap documentation, you know that Zig's HashMaps are built on
   abstractions. StringHashMap and AutoHashMap are fairly thin wrappers around HashMap, which itself is
   wrapper around HashMapUnmanaged. Now we see that many of the standard methods, like get are
   abstractions around *Adapted variants - if not directly then through other layers of abstractions.

   My initial encounter with Zig's various HashMaps wasn't great. I didn't understand; I could hardly
   navigate the code. While there's still some things I don't understand, I'm starting to grasp the why.
   I understand why we have managed and unmanaged types and why we have *Adapted variants.

   You might never needed to use adapted contexts, but it's good to know that they're available. It
   might seem unrelated, but for me, it's like positive/negative look-ahead/look-behind in regular
   expressions. I've used these once or twice, long ago, but now only retain a vague sense of how they
   can be useful - I certainly don't remember the syntax. But that vague awareness is enough to be
   useful. If I do run into a problem where adapted contexts or negative look-ahead might be useful,
   there's a good chance I'll be able to recognize it and then can refresh/deepen my understanding.


---
[**1]
https://www.openmymind.net/learning_zig/generics/

Generics

   In the previous part we built a bare-boned dynamic array called IntList. The goal of the data
   structure was to store a dynamic number of values. Although the algorithm we used would work for any
   type of data, our implementation was tied to i64 values. Enter generics, the goal of which is to
   abstract algorithms and data structures from specific types.

   Many languages implement generics with special syntax and generic-specific rules. With Zig, generics
   are less of a specific feature and more of an expression of what the language is capable of.
   Specifically, generics leverage Zig's powerful compile-time metaprogramming.

   We'll begin by looking at a silly example, just to get our bearings:

const std = @import("std");

pub fn main() !void {
	var arr: IntArray(3) = undefined;
	arr[0] = 1;
	arr[1] = 10;
	arr[2] = 100;
	std.debug.print("{any}\n", .{arr});
}

fn IntArray(comptime length: usize) type {
	return [length]i64;
}

   The above prints { 1, 10, 100 }. The interesting part is that we have a function that returns a type
   (hence the function is PascalCase). Not just any type either, but a type based on a function
   parameter. This code only worked because we declared length as comptime. That is, we require anyone
   who calls IntArray to pass a compile-time known length parameter. This is necessary because our
   function returns a type and types must always be compile-time known.

   A function can return any type, not just primitives and arrays. For example, with a small change, we
   can make it return a structure:

const std = @import("std");

pub fn main() !void {
	var arr: IntArray(3) = undefined;
	arr.items[0] = 1;
	arr.items[1] = 10;
	arr.items[2] = 100;
	std.debug.print("{any}\n", .{arr.items});
}

fn IntArray(comptime length: usize) type {
	return struct {
		items: [length]i64,
	};
}

   It might seem odd, but arr's type really is an IntArray(3). It's a type like any other type and arr
   is a value like any other value. If we called IntArray(7) that would be a different type. Maybe we
   can make things neater:

const std = @import("std");

pub fn main() !void {
	var arr = IntArray(3).init();
	arr.items[0] = 1;
	arr.items[1] = 10;
	arr.items[2] = 100;
	std.debug.print("{any}\n", .{arr.items});
}

fn IntArray(comptime length: usize) type {
	return struct {
		items: [length]i64,

		fn init() IntArray(length) {
			return .{
				.items = undefined,
			};
		}
	};
}

   At first glance that might not look neater. But besides being nameless and nested in a function, our
   structure's looking like every other structure we've seen so far. It has fields, it has functions.
   You know what they say, if it looks like a duck.... Well, this looks, swims and quacks like a normal
   structure, because it is.

   We've taken this route to get comfortable with a function that returns a type and the accompanying
   syntax. To get a more typical generic, we need to make one last change: our function has to take a
   type. In reality, this is a small change, but type can feel more abstract than usize, so we took it
   slowly. Let's make a leap and modify our previous IntList to work with any type. We'll start with a
   skeleton:

fn List(comptime T: type) type {
	return struct {
		pos: usize,
		items: []T,
		allocator: Allocator,

		fn init(allocator: Allocator) !List(T) {
			return .{
				.pos = 0,
				.allocator = allocator,
				.items = try allocator.alloc(T, 4),
			};
		}
	};
}

   The above struct is almost identical to our IntList except i64 has been replaced with T. That T might
   seem special, but it's just a variable name. We could have called it item_type. However, following
   Zig's naming convention, variables of type type are PascalCase.
   For good or bad, using a single letter to represent a type parameter is much older than Zig. T is a
   common default in most languages, but you will see context-specific variations, such as hash maps
   using K and V for their key and value parameter types.

   If you aren't sure about our skeleton, consider the two places we use T: items: []T and
   allocator.alloc(T, 4). When we want to use this generic type, we'll create an instance using:
var list = try List(u32).init(allocator);

   When the code gets compiled, the compiler creates a new type by finding every T and replacing it with
   u32. If we use List(u32) again, the compiler will re-use the type it previous created. If we specify
   a new value for T, say List(bool) or List(User), new types will be created.

   To complete our generic List we can literally copy and paste the rest of the IntList code and replace
   i64 with T. Here's a full working example:
const std = @import("std");
const Allocator = std.mem.Allocator;

pub fn main() !void {
	var gpa = std.heap.GeneralPurposeAllocator(.{}){};
	const allocator = gpa.allocator();

	var list = try List(u32).init(allocator);
	defer list.deinit();

	for (0..10) |i| {
		try list.add(@intCast(i));
	}

	std.debug.print("{any}\n", .{list.items[0..list.pos]});
}

fn List(comptime T: type) type {
	return struct {
		pos: usize,
		items: []T,
		allocator: Allocator,

		fn init(allocator: Allocator) !List(T) {
			return .{
				.pos = 0,
				.allocator = allocator,
				.items = try allocator.alloc(T, 4),
			};
		}

		fn deinit(self: List(T)) void {
			self.allocator.free(self.items);
		}

		fn add(self: *List(T), value: T) !void {
			const pos = self.pos;
			const len = self.items.len;

			if (pos == len) {
				// we've run out of space
				// create a new slice that's twice as large
				var larger = try self.allocator.alloc(T, len * 2);

				// copy the items we previously added to our new space
				@memcpy(larger[0..len], self.items);

				self.allocator.free(self.items);

				self.items = larger;
			}

			self.items[pos] = value;
			self.pos = pos + 1;
		}
	};
}

   Our init function returns a List(T), and our deinit and add functions take a List(T) and *List(T). In
   our simple class, that's fine, but for large data structures, writing the full generic name can
   become a little tedious, especially if we have multiple type parameters (e.g. a hash map that takes a
   separate type for its key and value). The @This() builtin function returns the innermost type from
   where it's called. Most likely, our List(T) would be written as:

fn List(comptime T: type) type {
	return struct {
		pos: usize,
		items: []T,
		allocator: Allocator,

		// Added
		const Self = @This();

		fn init(allocator: Allocator) !Self {
			// ... same code
		}

		fn deinit(self: Self) void {
			// .. same code
		}

		fn add(self: *Self, value: T) !void {
			// .. same code
		}
	};
}

   Self isn't a special name, it's just a variable, and it's PascalCase because its value is a type. We
   can use Self where we had previously used List(T).

   We could create more complex examples, with multiple type parameters and more advanced algorithms.
   But, in the end, the core generic code would be no different than the simple examples above. In the
   next part we'll touch on generics again when we look at the standard library's ArrayList(T) and
   StringHashMap(V).


---

