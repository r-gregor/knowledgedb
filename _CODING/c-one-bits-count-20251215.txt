filename: c-one-bits-count-20251215.txt
https://xania.org/202512/11-pop-goes-the-weasel-er-count

Pop goes the...population count?

   Written by me, proof-read by an LLM.
   Details at end.

   Who among us hasn't looked at a number and wondered, "How many one bits are in there?" No? Just me
   then?

   Actually, this "population count" operation can be pretty useful in some cases like data compression
   algorithms, cryptography, chess, error correction, and sparse matrix representations. How
   might one write some simple C to return the number of one bits in an unsigned 64 bit value?

   One way might be to loop 64 times, checking each bit and adding one if set. Or, equivalently,
   shifting that bit down and adding it to a running count: sometimes the population count operation is
   referred to as a "horizontal add" as you're adding all the 64 bits of the value together,
   horizontally. There are "divide and conquer" approaches too, see the amazing Stanford Bit
   Twiddling Hacks page for a big list.

   My favourite way is to loop while the value is non-zero, and use a cute trick to "clear the bottom
   set bit". The loop count is then the number of set bits. How do you clear the bottom set bit? You and
   a value with itself decremented!
value       : 11010100
subtract 1  : 11010011
& value     : 11010000

   If you try some examples on paper, you'll see that subtracting one always moves the bottom set bit
   down by one place, setting all the bits from there down. Everything else is left the same. Then when
   you and, the bottom set bit is guaranteed to be and-ed with zero, but everything else remains. Great
   stuff!

   All right, let's see what the compiler makes of this:

#include <cstdint>

unsigned population_count(std::uint64_t value) {
	unsigned result = 0;
	while (value) {
		// Clear bottom set bit
		value &= value - 1;
		++result;
	}
	return result;
}

x86-64 gcc (15.2) -O2 -Wall -Wextra:
population_count(unsigned long):
	xor edx, edx
	test rdi, rdi
	je .L1
.L3:
	lea rax, [rdi-1]
	add edx, 1
	and rdi, rax
	jne .L3
.L1:
	mov eax, edx
	ret

   The core loop is pretty much what we'd expect, using the
   [**1][https://xania.org/202512/02-adding-integers]lea trick to get value - 1, anding and counting:

.L3:
  lea rax, [rdi-1]          ; rax = value - 1
  add edx, 1                ; ++result
  and rdi, rax              ; value &= value - 1
  jne .L3                   ; ...while (value)

   Great stuff, but we can do better. By default gcc and clang both target some kind of "generic"
   processor which influences which instructions they can use. We're compiling for Intel here, and gcc's
   default is somewhere around Intel's "nocona" architecture, from 2004. Unless you are running vintage
   hardware you can probably change it to something better. Let's pick the super up-to-date "westmere"
   (from 2010...) using -march=westmere and see what happens:

#include <cstdint>

unsigned population_count(std::uint64_t value) {
	unsigned result = 0;
	while (value) {
		// Clear bottom set bit
		value &= value - 1;
		++result;
	}
	return result;
}

x86-64 gcc (15.2) -march=westmere -O2 -Wall -Wextra -Wpedantic -Wconversion -Wsign-conversion -Werror -std=c++3c:
population_count(unsigned long):
	popcnt rax, rdi
	ret

   Wow! The entire routine has been replaced with a single instruction - popcnt rax, rdi. When I
   first saw this optimisation I was blown away: the compiler recognises a relatively complex loop as
   being functionally equivalent to a single instruction. Both gcc and clang can do this, and within
   Compiler Explorer you can use the optimisation pipeline viewer in clang to see that clang's "loop
   deletion pass" is responsible for this trick:

   [https://xania.org/202512/11-popcount-clang-opt.png]Screenshot of CE showing the opt pipeline viewer with
   the loop being replaced with a call to @llvm.ctpop.i64

   Compilers canonicalise code too, so some similar population count code will also be turned into a
   single instruction, though sadly not all. In this case, it's probably better to actually use a
   standard C++ routine to guarantee the right instruction as well as reveal your intention:
   std::popcount. But even if you don't, the compiler might just blow your mind with a single
   instruction anyway.

   See [https://youtu.be/Hu0vu1tpZnc]the video that accompanies this post.


---
[**1]https://xania.org/202512/02-adding-integers

Addressing the adding situation

   Yesterday we saw how compilers zero registers efficiently. Today let's look at something a tiny
   bit less trivial (though not by much): adding two integers. What do you think a simple x86 function
   to add two ints would look like? An add, right? Let's take a look!

int add(int x, int y) {
	return x + y;
}

x86-64 gcc (15.2):
add(int, int):
	lea eax, [rdi+rsi]
	ret

   Probably not what you were thinking, right? x86 is unusual in mostly having a maximum of two operands
   per instruction. There's no add instruction to add edi to esi, putting the result in eax. On an
   ARM machine this would be a simple add r0, r0, r1 or similar, as ARM has a separate destination
   operand. On x86, things like add are not result = lhs + rhs but lhs += rhs. This can be a limitation,
   as we don't get to control which register the result goes into, and we in fact lose the old value of
   lhs.

   So how do compilers work around this limitation? The answer lies in an unexpected place - the
   sophisticated memory addressing system of the x86. Nearly every operand can be a memory reference -
   there's no specific "load" or "store"; a mov can just refer to memory directly. Those memory
   references are pretty rich: you can refer to memory addressed by a constant, relative to a register,
   or relative to a register plus an offset (optionally multiplied by 1, 2, 4 or 8). Something like add
   eax, word ptr [rdi + rsi * 4 + 0x1000] is still a single instruction!

   Sometimes you don't want to access the memory at one of these complex addresses, you just want to calculate
   what the address would be. Sort of like C's "address-of" (&) operator. That's what lea
   ([https://www.felixcloutier.com/x86/lea]Load Effective Address) does: it calculates the address without
   touching memory.

   Why is this useful for addition? Well, if we're not actually accessing memory, we can abuse the
   addressing hardware as a calculator! That complex addressing mode with its
   register-plus-register-times-scale is really just shifting and adding - so lea becomes a cheeky way
   to do three-operand addition.

   The compiler writes our simple addition in terms of the address of memory at rdi offset by rsi. We
   get a full add of two registers and we get to specify the destination too. You'll notice that the
   operands are referenced as rdi and rsi (the 64-bit version) even though we only wanted a 32-bit add:
   because we are using the memory addressing system it unconditionally calculates a 64-bit address.
   However, in this case it doesn't matter; those top bits are discarded when the result is
   written to the 32-bit eax.

   Using lea often saves an instruction, is useful if both of the operands are still needed later on in
   other calculations (as it leaves them unchanged), and can execute on x86's multiple execution
   units in the same cycle. Compilers know this though, so you don't have to worry!

   See [https://youtu.be/BOvg0sGJnes]the video that accompanies this post.


---

