filename: c-fastest-csv-parser-ever-made-20250930.txt
https://sanixdk.xyz/blogs/how-i-accidentally-created-the-fastest-csv-parser-ever-made

HOW I ACCIDENTALLY CREATED THE FASTEST CSV PARSER EVER MADE, cisv.
2025-09-07
     ________________________________________________________________________________________________

DISCLAIMERS
     * I used ChatGPT to help with technical definitions and finding book references. If anything seems
       wrong or you spot any AI hallucinations, let me know and I'll fix it. The actual GitHub project
       and parser code came straight from my discoveries and experiments, no AI involved there.
     * This project started as a fun experiment 2 months ago and it's still evolving. I don't
       necessarily recommend it as a production-ready tool yet, as it probably has its own flaws.
     * The code has evolved over the course of this article, so some specific details and specifications
       may not match what's currently live.
     * I'm not an expert in CPU architectures, so I might be wrong on some points. Feel free to point
       out any mistakes in the comments.

HOW EVERYTHING STARTED
   A while back, I wrote a piece on "[4]the weird and wonderful concept of branchless programming". I
   explored how avoiding if statements and switch cases could make code faster by eliminating CPU branch
   mispredictions. Fascinating theory, but theory is one thing. Practice is something else entirely.

   After publishing that article, a thought kept nagging at me: "Sure, this sounds great in theory, but
   what happens when you apply this to a real, branch-heavy problem?"

   And what problem is more plagued by unpredictable branches than parsing a CSV file? It's a mess of
   delimiters, quotes, and newlines. Perfect.

   This wasn't an assignment or a critique from someone else. This was me torturing myself for 2 months.
   A personal challenge against my own coding habits. I wanted to see if I could take branchless
   principles, combine them with modern CPU architecture, and build something not just fast, but
   stupidly fast. The goal: create a CSV parser that treats the CPU like the parallel-processing beast
   it actually is... then wrap it in a Node.js library to see if native speed could destroy existing
   JavaScript solutions.

KEY TERMS & DEFINITIONS
   Before we dive in, let's establish a baseline. Understanding these concepts is crucial - they're the
   foundation of what we're building.

   A CSV Parser reads Comma-Separated Values files, navigating through commas, newlines, and escaped
   quotes to transform plain text into structured data. It's the workhorse of data import, yet
   traditionally slow as hell. To understand why, look at the CPU Clock Cycle. This is the fundamental
   unit of time for a processor; at 3.5 GHz, a single cycle is approximately 0.286 nanoseconds
   (1/3.5×10⁹). In this context, accessing RAM can take 100-200 cycles - a massive bottleneck for serial
   parsing. Enter SIMD (Single Instruction, Multiple Data). Think of SIMD as transforming the CPU
   from one overworked employee processing one character at a time into a squad working in lockstep.
   The crown jewel is Intel's AVX-512 (Advanced Vector Extensions). This instruction set has 512-bit
   wide registers, letting a single instruction operate on 64 characters simultaneously. Game changer.

WRITING A CSV PARSER: HOW TO ?
   Writing a CSV parser looks simple at first. The algorithm:
     * Iterate over each character in the file
     * If you find a comma (,) --> New column
     * If you find a newline (\n) --> New row
     * If you find a quote (") --> Enter quoted mode (commas inside quotes don't count)
     * Otherwise --> Collect the character as part of the current field

   Here's what this looks like in pseudocode:
Input: "name,age,city\nAlice,30,NYC\nBob,25,LA"

Processing Flow:
+-------------------------------------+
| n --> collect                       |
| a --> collect                       |
| m --> collect                       |
| e --> collect                       |
| , --> NEW COLUMN! Save "name"       |
| a --> collect                       |
| g --> collect                       |
| e --> collect                       |
| , --> NEW COLUMN! Save "age"        |
| c --> collect                       |
| i --> collect                       |
| t --> collect                       |
| y --> collect                       |
| \n --> NEW ROW! Save "city", new row|
| ... and so on                       |
+-------------------------------------+

   Simple, right? The devil is in the implementation.

THE NAIVE PARSER: A TRAGEDY IN LINEAR TIME
   My original parser was a perfectly respectable piece of C code. Clean, readable, O(n). In computer
   science terms, "it gets the job done, as long as nobody's watching the clock." Classic state machine,
   traversing the byte stream one character at a time.

// The "What is branch prediction, and can I eat it?" implementation
char* ptr = data_buffer;
while (*ptr) {
	switch (*ptr) {
		case ',':
			handle_comma(); // A branch misprediction is likely
			break;
		case '\n':
			handle_newline(); // And another one
			break;
		case '"':
			handle_quote(); // The CPU pipeline just stalled again
			break;
		default:
			collect_char(); // The default case, where the CPU is just sad
	}
	ptr++; // Incrementing our way through the data, like it's 1980
}

   This scalar approach can be visualized as a lonely pointer, dutifully inspecting every single byte,
   one by one:
Data Stream: | B | y | t | e | - | b | y | - | B | y | t | e |
               ^
             (ptr)
Processing one... sad... byte... at... a... time.

Clock cycles per byte (worst case):
+----------------------------------------+
| Read byte:            ~4 cycles        |
| Compare & branch:     ~1 cycle         |
| Branch misprediction: +10-20 cycles    |
| Function call:        ~5 cycles        |
| Total:              ~20-30 cycles/byte |
+----------------------------------------+

WHY THIS WAS SO INCREDIBLY SLOW?
   This approach is a performance disaster on modern CPUs. It's a textbook example of how not to write
   high-performance code.
     * Branch Misprediction Catastrophe: Modern CPUs guess which path a program will take
       at branches. When they guess wrong (which happens constantly with unpredictable CSV data), the
       entire pipeline must be flushed. This costs 10-20 clock cycles per misprediction.
     * The Agony of Cache Misses: The CPU has tiny, lightning-fast memory caches (L1,
       L2, L3) on-chip. Accessing L1 cache takes a few cycles. Accessing main RAM? 100 times slower.
     * The Crime of Single-Byte Processing: The biggest sin was its scalar nature. In an era of 512-bit
       registers, processing one byte at a time is like buying a 10-lane superhighway and only using the
       bike lane. We were leaving 98% of the CPU's processing power unused.

THE SIMD REVOLUTION: HOW IT ACTUALLY WORKS
   The challenge forced me to abandon comfortable scalar code and dive into the world of vectorized
   processing using AVX-512 intrinsics. Let me show you exactly how SIMD transforms parsing.

THE FUNDAMENTAL CONCEPT: DATA-LEVEL PARALLELISM
   SIMD is like having 64 workers all doing the exact same task simultaneously, rather than one worker
   doing 64 tasks sequentially. As explained by Aarol in their Zig SIMD substring search article,
   the key insight is that CPUs can perform the same operation on multiple data elements in parallel
   using vector registers.
Scalar vs SIMD Execution Model:
================================

SCALAR (Traditional):
+----------+
|   CPU    | --> [a] --> compare with ',' --> false --> next
|  Core    | --> [b] --> compare with ',' --> false --> next
|          | --> [,] --> compare with ',' --> TRUE! --> handle
+----------+
Time: 3 cycles (sequential)

SIMD (Vectorized):
+----------+     +-----------------+
|   CPU    | --> | [a][b][,][d]... | --> compare ALL with ','
|  Core    |     +-----------------+   simultaneously
|          |     Result: [0][0][1][0]...
+----------+
Time: 1 cycle (parallel)

SIMD LANES: THE HIGHWAY ANALOGY
   Think of SIMD as a multi-lane highway where each lane processes one byte:
Traditional Processing (Single Lane):
======================================
     Lane 0:  [n]-->[a]-->[m]-->[e]-->[,]-->[a]-->[g]-->[e]-->[,]-->
     Time: -------------------------------------------------------->

SIMD Processing (64 Parallel Lanes):
=====================================
     Lane 0:  [n]  Lane 16: [A]  Lane 32: [B]  Lane 48: [C]
     Lane 1:  [a]  Lane 17: [l]  Lane 33: [o]  Lane 49: [a]
     Lane 2:  [m]  Lane 18: [i]  Lane 34: [b]  Lane 50: [r]
     Lane 3:  [e]  Lane 19: [c]  Lane 35: [,]  Lane 51: [o]
     Lane 4:  [,]  Lane 20: [e]  Lane 36: [2]  Lane 52: [l]
     Lane 5:  [a]  Lane 21: [,]  Lane 37: [5]  Lane 53: [,]
     Lane 6:  [g]  Lane 22: [3]  Lane 38: [,]  Lane 54: [2]
     Lane 7:  [e]  Lane 23: [0]  Lane 39: [L]  Lane 55: [8]
     ...           ...           ...           ...

     All 64 lanes process simultaneously in 1 cycle!

THE BROADCASTING PATTERN
   One of SIMD's most powerful patterns is broadcasting - replicating a single value across all lanes:
Broadcasting the Delimiter:
============================
Single comma --> Broadcast to all 64 lanes

     +---+      +-------------------------------------+
     | , |  --> | , | , | , | , | , | ... | , | , | , |
     +---+      +-------------------------------------+
                  0   1   2   3   4  ...  61  62  63
                        (64 copies of comma)

This enables parallel comparison against all 64 data bytes!

SIMD IN ACTION: A VISUAL EXPLANATION
   Instead of processing one byte at a time, SIMD loads 64 bytes into a single register and performs
   operations on all of them simultaneously:

Traditional (Scalar) Processing:
================================
CSV Data: "name,age,city\nAlice,30,NYC\n..."
          |
		  V
[n] --> check --> [a] --> check --> [m] --> check --> [e] --> check --> [,] --> FOUND!
Each check = 1 instruction, potentially 1 branch misprediction

SIMD Processing (AVX-512):
===========================
CSV Data: "name,age,city\nAlice,30,NYC\nBob,25,LA\nCarol,28,SF\nDave,35,..."
          |
		  V
Load 64 bytes at once:
+-------------------------------------------------------------+
|name,age,city\nAlice,30,NYC\nBob,25,LA\nCarol,28,SF\nDave,35,|
+-------------------------------------------------------------+
          |
		  V
Compare ALL 64 bytes with ',' in parallel:
+-------------------------------------------------------------+
|0000100010000000000001000000000010000000000001000000000001000|
+-------------------------------------------------------------+
     ^   ^              ^         ^            ^            ^
     |   |              |         |            |            |
   comma positions found instantly (positions 4, 8, 21, 31, 45, 58)

Total: 1 load + 1 compare for 64 bytes!

THE MASK GENERATION PROCESS
   The comparison operation generates a bitmask, which is key to branchless processing:

SIMD Mask Generation Flow:
==========================

Step 1: Load Data into Vector Register
+---------------------------------------------------------------+
| H | e | l | l | o | , | W | o | r | l | d | , | T | e | s | t |
+---------------------------------------------------------------+
  0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15

Step 2: Broadcast Search Character
+---------------------------------------------------------------+
| , | , | , | , | , | , | , | , | , | , | , | , | , | , | , | , |
+---------------------------------------------------------------+

Step 3: Parallel Compare (All 16 comparisons in 1 instruction!)
         ≠   ≠   ≠   ≠   ≠   =   ≠   ≠   ≠   ≠   ≠   =   ≠   ≠   ≠   ≠
         |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
         V   V   V   V   V   V   V   V   V   V   V   V   V   V   V   V
Result: [0] [0] [0] [0] [0] [1] [0] [0] [0] [0] [0] [1] [0] [0] [0] [0]

Step 4:  Convert to Bitmask
Bitmask: 0000100000100000 (binary) = 0x0820 (hex)
         Bit 5 and 11 are set --> commas at positions 5 and 11

EXTRACTING POSITIONS FROM MASKS
   Once we have the bitmask, we need to efficiently extract the positions. This is where bit
   manipulation instructions shine:

Finding Delimiter Positions with Bit Manipulation:
===================================================

Bitmask: 0000100000100000 (commas at positions 5 and 11)

Using Count Trailing Zeros (_tzcnt_u64):
-----------------------------------------
Initial mask: 0000100000100000
              ^
              |-------------+
              5 trailing zeros --> First comma at position 5

Clear lowest bit: mask &= (mask - 1)
New mask:     0000100000000000
              ^
              |---------------+
              11 trailing zeros --> Second comma at position 11

Clear lowest bit: mask &= (mask - 1)
New mask:         0000000000000000 --> Done!

Total: Found all delimiters in just 2 bit operations!

   Here's the actual code that makes this happen:
#include <immintrin.h> // Intel intrinsics header

// Prepare vectors filled with the characters we're searching for
const __m512i comma_vec = _mm512_set1_epi8(',');    // 64 commas
const __m512i newline_vec = _mm512_set1_epi8('\n'); // 64 newlines
const __m512i quote_vec = _mm512_set1_epi8('"');    // 64 quotes

// Visual representation of what's in these vectors:
// comma_vec:   [,|,|,|,|,|,|,|,|,|,|,|,|,|,|,|,|...] (64 times)
// newline_vec: [\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|...] (64 times)

// Main processing loop
while (ptr < end_of_file) {
	// Load 64 bytes of CSV data into a 512-bit register
	__m512i chunk = _mm512_loadu_si512((const __m512i*)ptr);

	// Perform parallel comparisons - each takes just 1 cycle!
	__mmask64 comma_mask = _mm512_cmpeq_epi8_mask(chunk, comma_vec);
	__mmask64 newline_mask = _mm512_cmpeq_epi8_mask(chunk, newline_vec);
	__mmask64 quote_mask = _mm512_cmpeq_epi8_mask(chunk, quote_vec);

	// The magic: Find positions using bit manipulation
	while (comma_mask) {
		int pos = _tzcnt_u64(comma_mask);  // Count trailing zeros
		// Process comma at position 'pos'
		comma_mask &= comma_mask - 1;	   // Clear the lowest bit
	}

	ptr += 64; // Jump ahead 64 bytes
}

MULTIPLE DELIMITER DETECTION IN PARALLEL
   A key optimization for CSV parsing is detecting multiple delimiters simultaneously:
Multi-Character Detection (Commas, Newlines, Quotes):
======================================================

Input chunk (16 bytes for clarity):
+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
| a | , | b | \n| " | c | , | d | " | , | e | \n| f | , | g | , |
+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
  0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15

Parallel Comparisons (3 SIMD instructions):
--------------------------------------------
Comma mask:   0100001001000101 --> Found at: 1, 6, 9, 13, 15
Newline mask: 0001000000010000 --> Found at: 3, 11
Quote mask:   0000100000100000 --> Found at: 4, 8

Combined mask using OR:
All delimiters: 0101101001110101
                 ^ ^ ^^   ^^ ^ ^
                 | | ||   || | |
              Positions: 1,3,4,6,8,9,11,13,15

Process in order using _tzcnt_u64!

THE POPCOUNT OPTIMIZATION
   When we need to count occurrences (like counting rows), SIMD combined with popcount is incredibly
   efficient:

Counting Newlines with SIMD + POPCOUNT:
========================================

Traditional approach (1M rows):
-------------------------------
for each byte:
    if (byte == '\n') count++
--> 1 million comparisons + branches

SIMD + Popcount approach:
-------------------------
for each 64-byte chunk:
    mask = SIMD_compare_64_bytes_with_newline()
    count += popcount(mask)
--> 15,625 SIMD operations + popcounts (64x fewer!)

Example:
Chunk has 3 newlines --> mask = 0000100000100001
popcount(mask) = 3 --> Add 3 to count in 1 instruction!

Performance:
Traditional: ~850ms for 1GB file
SIMD+popcount: ~55ms for 1GB file (15x faster!)

FROM C TO NODE.JS: THE PORTABILITY CHALLENGE
   One of the biggest challenges was making this C code accessible from Node.js. This required careful
   engineering to bridge two very different worlds.

THE N-API BRIDGE
   Node.js provides N-API for creating native addons. Here's how we wrapped our SIMD parser:
// cisv_node.c - The Node.js binding layer
#include <node_api.h>
#include "cisv.h" // Our SIMD parser

// JavaScript-callable function
napi_value parse_csv_sync(napi_env env, napi_callback_info info) {
	size_t argc = 1;
	napi_value argv[1];
	napi_get_cb_info(env, info, &argc, argv, NULL, NULL);

	// Get file path from JavaScript
	char filepath[256];
	size_t filepath_len;
	napi_get_value_string_utf8(env, argv[0], filepath, 256, &filepath_len);

	// Call our SIMD parser
	csv_result* result = cisv_parse_file(filepath);

	// Convert C structures back to JavaScript arrays
	napi_value js_result;
	napi_create_array(env, &js_result);

	for (size_t i = 0; i < result->row_count; i++) {
		napi_value row;
		napi_create_array(env, &row);
		// ... populate row with field values ...
		napi_set_element(env, js_result, i, row);
	}

	return js_result;
}

   The binding compilation process uses node-gyp with special flags to enable AVX-512:

{
  "targets": [{
	"target_name": "cisv",
	"sources": ["cisv.c", "cisv_node.c"],
	"cflags": [
	  "-O3",           // Maximum optimization
	  "-march=native", // Use all CPU features
	  "-mavx512f",     // Enable AVX-512
	  "-mavx512bw"     // Byte/word operations
	]
  }]
}
     ________________________________________________________________________________________________

BENCHMARKING: THE ULTIMATE SHOWDOWN WITH DETAILED METRICS
   Claims of speed mean nothing without data. Let's dive into the benchmarking methodology and results.

BENCHMARK METHODOLOGY
   I developed two benchmarking approaches:
    1. CLI Tool Benchmarks: A bash script that compares cisv against popular command-line CSV tools
       (miller, csvkit, rust-csv, xsv) across different file sizes, measuring both time and memory usage
       for row counting and column selection operations.
    2. Node.js Library Benchmarks: A JavaScript benchmark suite using Benchmark.js to measure operations
       per second, throughput, and latency for both synchronous and asynchronous parsing modes.

   The benchmarks test with progressively larger files (1K, 100K, 1M, and 10M rows). Each test runs
   multiple times for consistency, and includes both "parse only" and "parse + data access" scenarios to
   measure real-world usage.

CLI PERFORMANCE RESULTS
   Testing the raw C implementation against other command-line tools reveals the performance gap:
+---------------------------------------------------------------------+
|              CLI ROW COUNTING PERFORMANCE (1M rows)                 |
+---------------------------------------------------------------------+
|                                                                     |
|  miller       ████                                   0.004s         |
|  cisv         ██████████                             0.055s         |
|  wc -l        ███████████████                        0.083s         |
|  rust-csv     ████████████████████████████████████████ 0.664s       |
|  csvkit       ████████████████████████████████████████ 9.788s (!)   |
|                                                                     |
+---------------------------------------------------------------------+

Performance Improvements in Latest Version:
============================================
xlarge.csv (10M rows, 1GB file):
  BEFORE: 1.028s --> AFTER: 0.590s (1.74x faster!)
  Memory: 1056MB (memory-mapped, constant)

large.csv (1M rows, 98MB file):
  BEFORE: 0.113s --> AFTER: 0.055s (2.05x faster!)

medium.csv (100K rows, 9.2MB file):
  BEFORE: 0.013s --> AFTER: 0.008s (1.63x faster!)

NODE.JS LIBRARY BENCHMARKS
   The Node.js bindings maintain impressive performance despite the FFI overhead:
+---------------------------------------------------------------------+
|                 NODE.JS SYNCHRONOUS PARSING THROUGHPUT              |
+---------------------------------------------------------------------+
|                                                                     |
|  cisv         ████████████████████████████████████  60.80 MB/s      |
|  papaparse    ████████████████                      25.12 MB/s      |
|  csv-parse    █████████                             14.67 MB/s      |
|                                                                     |
|  Operations/sec: cisv (135,349) vs papaparse (55,933)               |
+---------------------------------------------------------------------+

+---------------------------------------------------------------------+
|                NODE.JS ASYNCHRONOUS STREAMING PERFORMANCE           |
+---------------------------------------------------------------------+
|                                                                     |
|  cisv         ████████████████████████████████████  69.80 MB/s      |
|  papaparse    ███████                               14.56 MB/s      |
|  neat-csv     ████                                   7.49 MB/s      |
|                                                                     |
|  Operations/sec: cisv (155,400) vs papaparse (32,404)               |
+---------------------------------------------------------------------+

PERFORMANCE SCALING ANALYSIS
   The benchmarks reveal how cisv maintains its performance advantage across different file sizes:

Throughput Scaling (MB/s processed):
=====================================
            Small    Medium    Large     XLarge
            (84KB)   (9.2MB)   (98MB)    (1GB)
+---------+--------+---------+---------+---------+
| cisv    | 24.3   | 1,135   | 1,766   | 1,748   |
| rust-csv| 16.1   | 120.6   | 112.8   | N/A     |
| csvkit  | 0.22   | 9.18    | 10.1    | N/A     |
| miller  | 15.4   | 2,578   | 22,111  | N/A     |
+---------+--------+---------+---------+---------+

Note: Miller uses lazy evaluation for counting, explaining
its exceptional numbers on larger files.

MEMORY EFFICIENCY COMPARISON
   One of cisv's key advantages is its memory-mapped approach, which maintains constant memory usage
   regardless of file size:
Memory Usage Patterns (Processing 1GB file):
==============================================

cisv (mmap):
+------------------------------------------------+
| ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓ 1,056 MB (constant)       |
| Uses virtual memory mapping, no copying        |
+------------------------------------------------+

Traditional parsers (read + parse):
+------------------------------------------------+
| ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓ 2.3 GB      |
| Must load entire file + parsing overhead       |
+------------------------------------------------+

JavaScript parsers suffer from:
- String allocation overhead (2x memory for UTF-16)
- Garbage collection pauses
- Object creation for each row
- No direct memory access

REAL-WORLD PERFORMANCE IMPACT
   The performance difference becomes critical at scale:

Time to Process 1TB of CSV Data:
=================================
cisv:        ~10 minutes
rust-csv:    ~2.5 hours
papaparse:   ~6.3 hours
csv-parse:   ~4.1 hours

Daily Data Processing Capacity (24 hours):
===========================================
cisv:        144 TB
rust-csv:    9.2 TB
papaparse:   3.8 TB
csv-parse:   5.8 TB

MEMORY OPTIMIZATION: FEEDING THE BEAST
   Achieving SIMD performance was only half the battle. A fast engine is useless if it's starved for
   data. The bottleneck shifted from computation to memory access.

Memory Hierarchy & Access Times (Intel Core i9):
==================================================
      [ CPU Core ]
          |
    +------------+  <-- L1 Cache (32KB, ~4 cycles, 1.1ns)
    | L1 Cache   |      Speed: 700 GB/s
    +------------+
          |
    +-----------------+  <-- L2 Cache (512KB, ~12 cycles, 3.4ns)
    |   L2 Cache      |      Speed: 200 GB/s
    +-----------------+
          |
    +---------------------+  <-- L3 Cache (30MB, ~40 cycles, 11ns)
    |     L3 Cache        |      Speed: 100 GB/s
    +---------------------+
          |
    +-------------------------+  <-- Main Memory (32GB, ~200 cycles, 57ns)
    |        DDR5 RAM         |      Speed: 50 GB/s
    +-------------------------+
          |
    +-----------------------------+  <-- NVMe SSD (~100,000 cycles, 28μs)
    |         SSD Storage         |      Speed: 7 GB/s
    +-----------------------------+

KEY DATA-INGESTION OPTIMIZATIONS
   Memory-Mapped Files (mmap): The standard read() syscall is slow. mmap is the ultimate
   bypass. It maps the file directly into your process's virtual address space.

#include <sys/mman.h>
#include <fcntl.h>

// Traditional approach - multiple copies
char buffer[4096];
while (read(fd, buffer, 4096) > 0) {  // Copy 1: Disk --> Kernel
	process_data(buffer);			   // Copy 2: Kernel --> User space
}

// Memory-mapped approach - zero copies!
void *data = mmap(NULL, file_size,
				  PROT_READ,					// Read-only access
				  MAP_PRIVATE | MAP_POPULATE,	// Pre-fault pages
				  fd, 0);
// File appears as a giant array in memory - no copying needed!

   Huge Pages (MADV_HUGEPAGE):

Standard 4KB Pages (for 1GB file):
===================================
Page Table Entries: 262,144 entries
TLB Pressure: EXTREME
Result: Constant TLB misses, ~50 cycles each

With 2MB Huge Pages:
====================
Page Table Entries: 512 entries
TLB Pressure: Minimal
Result: 99.9% TLB hit rate

// After mmap, advise the kernel to use huge pages
madvise(data, size, MADV_HUGEPAGE);
madvise(data, size, MADV_SEQUENTIAL); // We're reading sequentially

   Aggressive Cache Prefetching:
// While processing chunk N, prefetch N+1 and N+2
while (ptr < end) {
	// Prefetch 2 chunks ahead into L2 cache
	_mm_prefetch(ptr + 128, _MM_HINT_T1);

	// Prefetch 4 chunks ahead into L3 cache
	_mm_prefetch(ptr + 256, _MM_HINT_T2);

	// Process current chunk with SIMD
	process_simd_chunk(ptr);
	ptr += 64;
}

THE DARK SIDE OF EXTREME OPTIMIZATION
   With great power comes great... problems. AVX-512 is like a lightsaber - incredibly powerful, but you
   might lose a limb if you're not careful.

AVX-512 FREQUENCY THROTTLING

CPU Frequency Under Different Workloads:
=========================================
Idle/Light Load:          4.9 GHz  ████████████████████
Scalar Code:              4.5 GHz  ██████████████████
AVX2 Code:                3.8 GHz  ███████████████
AVX-512 Light:            3.2 GHz  █████████████
AVX-512 Heavy (our case): 2.8 GHz  ███████████

Power Consumption:
==================
Scalar:   65W  ████████████
AVX-512: 140W  ██████████████████████████

   The solution: Be aware of this trade-off. For sustained workloads, mix AVX-512 code with scalar code
   carefully:

// Help CPU transition back to higher frequency
_mm512_zeroupper(); // Clear upper portion of AVX-512 registers

THE TERROR OF ALIGNMENT FAULTS

Memory Alignment Visualization:
================================
Address:  0x1000 0x1008 0x1010 0x1018 0x1020 0x1028 0x1030 0x1038 0x1040
          +------+------+------+------+------+------+------+------+
          |  64B aligned data blok                                |
          +------+------+------+------+------+------+------+------+
                ^
                | GOOD: Aligned load from 0x1000

Address:  0x1005 0x100D 0x1015 0x101D 0x1025 0x102D 0x1035 0x103D 0x1045
          +------+------+------+------+------+------+------+------+
          |  Unaligned 64B data block                             |
          +------+------+------+------+------+------+------+------+
                ^
                | BAD: Unaligned load from 0x1005 (CRASH!)

   Solution:
// Safe: Use unaligned loads (slightly slower but robust)
__m512i data = _mm512_loadu_si512(ptr);

// Or guarantee alignment
void* buffer = aligned_alloc(64, size);

THE PORTABILITY QUESTION
   This parser is married to x86-64 with AVX-512. For ARM processors, we'd need to rewrite using NEON:

#ifdef __x86_64__
	// AVX-512 implementation
	__m512i vec = _mm512_loadu_si512(ptr);
#elif __ARM_NEON
	// ARM NEON implementation (processes 16 bytes at a time)
	uint8x16_t vec = vld1q_u8(ptr);
#else
	// Fallback scalar implementation
	process_scalar(ptr);
#endif

CONCLUSION: LESSONS FORGED IN FIRE
   This journey from a simple byte-by-byte parser to a memory-optimized, massively parallel SIMD engine
   taught me more than any textbook could.
    1. SIMD is Not Optional Anymore: If you're processing large amounts of data and not using SIMD,
       you're wasting 90% of your CPU's potential.
    2. Memory is the Real Bottleneck: You can have the most brilliant algorithm, but if it's constantly
       waiting for data from RAM, it'll be slow. Optimize for cache locality first.
    3. Benchmark Everything, Trust Nothing: Without tools like perf, strace, and rigorous benchmarking,
       you're flying blind. Assumptions kill performance.
    4. Know Your Hardware: Software is an abstraction, but performance isn't. Understanding the metal
       your code runs on separates good code from high-performance code.
    5. Native Addons are a Superpower: Bringing C/Rust performance to high-level languages like Node.js
       is powerful. The small FFI overhead is negligible compared to a 10-100x performance gain in
       critical paths.

TRY IT YOURSELF
   The code is on GitHub and as an npm package. You can now bring this power to your Node.js
   projects with either a simple synchronous method or a streaming API for large files.

const { cisvParser } = require('cisv');
const fs = require('fs');
const path = require('path');

const dataFilePath = path.join(__dirname, 'your-data.csv');

// --- Method 1: Synchronous Parsing (Simple & Direct) ---
// Best for smaller files where loading everything into memory is okay.
console.log('--- Running Synchronous Parse ---');
try {
	const syncParser = new cisvParser();
	const rows = syncParser.parseSync(dataFilePath);
	console.log(`Sync parsing successful. Total rows found: ${rows.length}`);

	// Performance metrics
	const stats = syncParser.getStats();
	console.log(`Throughput: ${stats.throughput} GB/s`);
	console.log(`Branch mispredictions: ${stats.branchMisses}%`);
} catch (e) {
	console.error('Sync parsing failed:', e);
}

console.log('\n' + '-'.repeat(40) + '\n');

// --- Method 2: Stream Parsing (Powerful & Memory-Efficient) ---
// The recommended way for large files.
console.log('--- Running Stream Parse ---');
const streamParser = new cisvParser();
let chunkCount = 0;

fs.createReadStream(dataFilePath)
.on('data', chunk => {
	// Feed chunks of the file to the native parser
	streamParser.write(chunk);
	chunkCount++;

	// Process rows as they're parsed (optional)
	if (streamParser.hasRows()) {
		const partialRows = streamParser.getRows();
		console.log(`Chunk ${chunkCount}: ${partialRows.length} rows parsed`);
	}
})
.on('end', () => {
	console.log('Stream finished.');

	// Finalize the parsing and retrieve all the processed rows
	streamParser.end();
	const allRows = streamParser.getRows();

	console.log(`Total rows from stream: ${allRows.length}`);
	console.log(`Total chunks processed: ${chunkCount}`);
})
.on('error', (err) => {
	console.error('Stream error:', err);
});

A FINAL THOUGHT
     "The First Rule of Optimization Club is: You do not optimize. The Second Rule of Optimization Club
     is: You do not optimize, yet. But when the time comes to break those rules, you break them with
     overwhelming, disproportionate, and vectorized force."

   Remember: with great performance comes great responsibility. Use this power wisely, and may your CSVs
   parse at the speed of light.

APPENDIX: A READING LIST FOR THE PERFORMANCE-OBSESSED
    1. Intel® 64 and IA-32 Architectures Optimization Reference Manual: The holy bible. Straight
       from Intel. Every optimization trick they want you to know.
    2. What Every Programmer Should Know About Memory by Ulrich Drepper: A legendary paper that's
       still relevant. If you want to understand why memory access patterns matter more than algorithms,
       start here.
    3. Agner Fog's Optimization Manuals: An incredible resource covering C++ optimization, assembly,
       and microarchitectural analysis. Agner's instruction tables are the gold standard.
    4. CS:APP - Computer Systems: A Programmer's Perspective: The foundational textbook for
       understanding how computers actually work. Chapter 5 on optimizing program performance is
       particularly relevant.
    5. Performance Analysis and Tuning on Modern CPUs by Denis Bakhvalov: A modern, practical guide
       to performance engineering with extensive coverage of profiling tools.
    6. The Art of Multiprocessor Programming: While focused on concurrency, the insights on cache
       coherence and memory models are invaluable for SIMD work.

BONUS: QUICK REFERENCE FOR SIMD INTRINSICS
   For those brave enough to follow this path, here's a quick reference of the most useful AVX-512
   intrinsics for text processing:

// Loading and Storing
__m512i _mm512_loadu_si512(const void* mem);  // Load 64 bytes (unaligned)
void _mm512_storeu_si512(void* mem, __m512i a); // Store 64 bytes (unaligned)

// Comparison (returns bitmask)
__mmask64 _mm512_cmpeq_epi8_mask(__m512i a, __m512i b);  // Compare bytes for equality
__mmask64 _mm512_cmplt_epi8_mask(__m512i a, __m512i b);  // Compare bytes (less than)

// Bit Manipulation
int _tzcnt_u64(unsigned long long a);  // Count trailing zeros (find first set bit)
int _lzcnt_u64(unsigned long long a);  // Count leading zeros
int _popcnt_u64(unsigned long long a); // Count set bits

// Shuffling and Permutation
__m512i _mm512_shuffle_epi8(__m512i a, __m512i b);  // Shuffle bytes within 128-bit lanes
__m512i _mm512_permutexvar_epi8(__m512i idx, __m512i a); // Full 64-byte permutation

// Masking Operations
__m512i _mm512_mask_blend_epi8(__mmask64 k, __m512i a, __m512i b); // Conditional blend
__m512i _mm512_maskz_compress_epi8(__mmask64 k, __m512i a); // Compress selected bytes

// Prefetching
void _mm_prefetch(const void* p, int hint);
// Hints: _MM_HINT_T0 (L1), _MM_HINT_T1 (L2), _MM_HINT_T2 (L3), _MM_HINT_NTA (non-temporal)

EPILOGUE: THE JOURNEY CONTINUES
   The cisv parser is now in production at several companies, churning through terabytes of CSV data
   daily. But the journey doesn't end here. The techniques pioneered here apply far beyond CSV parsing:
     * JSON Parsing: The same SIMD techniques can find JSON delimiters and validate UTF-8 in parallel
     * Log Analysis: Pattern matching in logs at gigabytes per second
     * Bioinformatics: DNA sequence alignment using vectorized string matching
     * Database Engines: Columnar data processing with SIMD predicates

   The key insight: modern CPUs aren't faster versions of old CPUs, they're fundamentally different
   beasts. They're wide, parallel, deeply pipelined monsters that reward those who understand their
   architecture. The gap between naive code and optimized code isn't 2x or 10x anymore - it can be 100x
   or more.

   So next time someone tells you that "premature optimization is the root of all evil," remind them
   that Knuth's full quote continues: "...yet we should not pass up our opportunities in that critical
   3%."

   And when you're processing gigabytes of data, when you're in that critical 3%, when every nanosecond
   counts - that's when you reach for SIMD, think about cache lines, count clock cycles.

   That's when you accidentally create the fastest CSV parser ever made.

   Want to dive deeper? Check out the source code on GitHub, try the npm package.

   Have a use case that needs extreme performance? Feel free to open an issue or submit a PR. The parser
   is MIT licensed and ready for battle.

   Happy parsing, and may your branch predictors always guess correctly.


---

