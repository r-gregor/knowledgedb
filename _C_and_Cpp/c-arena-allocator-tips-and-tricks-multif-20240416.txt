filename: c_arena_allocator_tips_and_tricks_multif_20240416.txt
https://nullprogram.com/blog/2023/09/27/

Arena allocator tips and tricks
September 27, 2023

   Over the past year I've refined my approach to arena allocation. With practice, it's effective,
   simple, and fast; typically as easy to use as garbage collection but without the costs. Depending on
   need, an allocator can weigh just 7-25 lines of code - perfect when lacking a runtime. With the
   core details of my own technique settled, now is a good time to document and share lessons learned.
   This is certainly not the only way to approach arena allocation, but these are practices I've worked
   out to simplify programs and reduce mistakes.

   An arena is a memory buffer and an offset into that buffer, initially zero. To allocate an object,
   grab a pointer at the offset, advance the offset by the size of the object, and return the pointer.
   There's a little more to it, such as ensuring alignment and availability. We'll get to that. Objects
   are not freed individually. Instead, groups of allocations are freed at once by restoring the offset
   to an earlier value. Without individual lifetimes, you don't need to write destructors, nor do your
   programs need to walk data structures at run time to take them apart. You also no longer need to
   worry about memory leaks.

   A minority of programs inherently require general purpose allocation, at least in part, that linear
   allocation cannot fulfill. This includes, for example, most programming language runtimes. If you
   like arenas, avoid accidentally create such a situation through an over-flexible API that allows
   callers to assume you have general purpose allocation underneath.

   To get warmed up, here's my style of arena allocation in action that shows off multiple features:

typedef struct {
	uint8_t  *data;
	ptrdiff_t len;
} str;

typedef struct {
	strlist *next;
	str      item;
} strlist;

typedef struct {
	str head;
	str tail;
} strpair;

// Defined elsewhere
void towidechar(wchar_t *, ptrdiff_t, str);
str  loadfile(wchar_t *, arena *);
strpair cut(str, uint8_t);

strlist *getlines(str path, arena *perm, arena scratch) {
	int max_path = 1<<15;
	wchar_t *wpath = new(&scratch, wchar_t, max_path);
	towidechar(wpath, max_path, path);

	strpair pair = {0};
	pair.tail = loadfile(wpath, perm);

	strlist *head = 0;
	strlist **tail = &head;
	while (pair.tail.len) {
		pair = cut(pair.tail, '\n');
		*tail = new(perm, strlist, 1);
		(*tail)->item = pair.head;
		tail = &(*tail)->next;
	}
	return head;
}

   Take note of these details, each to be later discussed in detail:
	 * getlines takes two arenas, "permanent" and "scratch". The former is for objects that will be
	   returned to the caller. The latter is for temporary objects whose lifetime ends when the function
	   returns. They have stack lifetimes just like local variables.
	 * Objects are not explicitly freed. Instead, all allocations from a scratch arena are implicitly
	   freed upon return. This would include error return paths automatically.
	 * The scratch arena is passed by copy - i.e. a copy of the "header" not the memory region itself.
	   Allocating only changes the local copy, and so cannot survive the return. The semantics are
	   obvious to callers, so they're less likely to get mixed up.
	 * While wpath could be an automatic local variable, it's relatively large for the stack, so it's
	   allocated out of the scratch arena. A scratch arena safely permits large, dynamic allocations
	   that would never be safe on the stack. In other words, a sane alloca! Same for variable-length
	   arrays (VLAs). A scratch arena means you'll never be tempted to use either of these terrible
	   ideas.
	 * The second parameter to new is a type, so it's obviously a macro. As you will see momentarily,
	   this is not some complex macro magic, just a convenience one-liner. There is no implicit cast,
	   and you will get a compiler diagnostic if the type is incorrect.
	 * Despite all the allocation, there is not a single sizeof operator nor size computation. That's
	   because size computations are a major source of defects. That job is handled by specialized code.
	 * Allocation failures are not communicated by a null return. Lifting this burden greatly simplifies
	   programs. Instead such errors are handled non-locally by the arena.
	 * All allocations are zero-initialized by default. This makes for simpler, less error-prone
	   programs. When that's too expensive, this can become an opt-out without changing the default.

  An arena implementation
   An arena suitable for most cases can be this simple:

typedef struct {
	char *beg;
	char *end;
} arena;

void *alloc(arena *a, ptrdiff_t size, ptrdiff_t align, ptrdiff_t count) {
	ptrdiff_t padding = -(uintptr_t)a->beg & (align - 1);
	ptrdiff_t available = a->end - a->beg - padding;
	if (available < 0 || count > available/size) {
		abort();  // one possible out-of-memory policy
	}

	void *p = a->beg + padding;
	a->beg += padding + count*size;
	return memset(p, 0, count*size);
}

   Yup, just a pair of pointers! When allocating, all sizes are signed just as they ought to be.
   Unsigned sizes are another historically common source of defects, and offer no practical advantages
   in return.

   The align parameter allows the arena to handle any unusual alignments, something that's surprisingly
   difficult to do with libc. It's difficult to appreciate its usefulness until it's convenient.

   The uintptr_t business may look unusual if you've never come across it before. To align beg, we need
   to compute the number of bytes to advance the address (padding) until the alignment evenly divides
   the address. The modulo with align computes the number of bytes it's since the last alignment:

extra = addr % align

   We can't operate numerically on an address like this, so in the code we first convert to uintptr_t.
   Alignment is always a power of two, which notably excludes zero, so no worrying about division by
   zero. That also means we can compute modulo by subtracting one and masking with AND:

extra = addr & (align - 1)

   However, we want the number of bytes to advance to the next alignment, which is the inverse:

padding = -addr & (align - 1)

   Add the uintptr_t cast and you have the code in alloc.

   The if tests if there's enough memory and simultaneously for overflow on size*count. If either fails,
   it invokes the out-of-memory policy, which in this case is abort. I strongly recommend that, at least
   when testing, always having something in place to, at minimum, abort when allocation fails, even when
   you think it cannot happen. It's easy to use more memory than you anticipate, and you want a reliable
   signal when it happens.

   An alternative policy is to longjmp to a "handler", which with GCC and Clang doesn't even require
   runtime support. In that case add a jmp_buf to the arena:

typedef struct {
	char  *beg;
	char  *end;
	void **jmp_buf;
} arena;

void *alloc(...) {
	// ...
	if (/* out of memory */) {
		__builtin_longjmp(a->jmp_buf, 1);
	}
	// ...
}

bool example(..., arena scratch) {
	void *jmp_buf[5];
	if (__builtin_setjmp(jmp_buf)) {
		return 0;
	}
	scratch.jmp_buf = jmp_buf;
	// ...
	return 1;
}

   example returns failure to the caller if it runs out of memory, without needing to check individual
   allocations and, thanks to the implicit free of scratch arenas, without needing to clean up. If
   callees receiving the scratch arena don't set their own jmp_buf, they'll return here, too. In a real
   program you'd probably wrap the setjmp setup in a macro.

   Suppose zeroing is too expensive or unnecessary in some cases. Add a flag to opt out:

void *alloc(..., int flags) {
	// ...
	return flag&NOZERO ? p : memset(p, 0, total);
}

   Similarly, perhaps there's a critical moment where you're holding a non-memory resource (lock, file
   handle), or you don't want allocation failure to be fatal. In either case, it's important that the
   out-of-memory policy isn't invoked. You could request a "soft" failure with another flag, and then do
   the usual null pointer check:

void *alloc(..., int flags) {
	// ...
	if (/* out of memory */) {
		if (flags & SOFTFAIL) {
			return 0;
		}
		abort();
	}
	// ...
}

   Most non-trivial programs will probably have at least one of these flags.

   In case it wasn't obvious, allocating an arena is simple:

arena newarena(ptrdiff_t cap) {
	arena a = {0};
	a.beg = malloc(cap);
	a.end = a.beg ? a.beg+cap : 0;
	return a;
}

   Or make a direct allocation from the operating system, e.g. mmap, VirtualAlloc. Typically arena
   lifetime is the whole program, so you don't need to worry about freeing it. (Since you're using
   arenas, you can also turn off any memory leak checkers while you're at it.)

   If you need more arenas then you can always allocate smaller ones out of the first! In multi-threaded
   applications, each thread may have at least its own scratch arena.

  The new macro
   I've shown alloc, but few parts of the program should be calling it directly. Instead they have a
   macro to automatically handle the details. I call mine new, though of course if you're writing C++
   you'll need to pick another name (make? PushStruct?):

#define new(a, t, n)  (t *)alloc(a, sizeof(t), _Alignof(t), n)

   The cast is an extra compile-time check, especially useful for avoiding mistakes in levels of
   indirection. It also keeps normal code from directly using the sizeof operator, which is easy to
   misuse. If you added a flags parameter, pass in zero for this common case. Keep in mind that the goal
   of this macro is to make common allocation simple and robust.

   Often you'll allocate single objects, and so the count is 1. If you think that's ugly, you could make
   variadic version of new that fills in common defaults. In fact, that's partly why I put count last!

#define new(...)             newx(__VA_ARGS__,new4,new3,new2)(__VA_ARGS__)
#define newx(a,b,c,d,e,...)  e
#define new2(a, t)           (t *)alloc(a, sizeof(t), alignof(t), 1, 0)
#define new3(a, t, n)        (t *)alloc(a, sizeof(t), alignof(t), n, 0)
#define new4(a, t, n, f)     (t *)alloc(a, sizeof(t), alignof(t), n, f)

   Not quite so simple, but it optionally makes for more streamlined code:

thing *t   = new(perm, thing);
thing *ts  = new(perm, thing, 1000);
char  *buf = new(perm, char, len, NOZERO);

   Side note: If sizeof should be avoided, what about array lengths? That's part of the problem! Hardly
   ever do you want the size of an array, but rather the number of elements. That includes char arrays
   where this happens to be the same number. So instead, define a countof macro that uses sizeof to
   compute the value you actually want. I like to have this whole collection:

#define sizeof(x)    (ptrdiff_t)sizeof(x)
#define countof(a)   (sizeof(a) / sizeof(*(a)))
#define lengthof(s)  (countof(s) - 1)

   Yes, you can convert sizeof into a macro like this! It won't expand recursively and bottoms out as an
   operator. countof also, of course, produces a less error-prone signed count so users don't fumble
   around with size_t. lengthof statically produces null-terminated string length.

char msg[] = "hello world";
write(fd, msg, lengthof(msg));

#define MSG "hello world"
write(fd, MSG, lengthof(MSG));

  Enhance alloc with attributes
   At least for GCC and Clang, we can further improve alloc with three function attributes:

__attribute((malloc, alloc_size(2, 4), alloc_align(3)))
void *alloc(...);

   malloc indicates that the pointer returned by alloc does not alias any existing object. Enables some
   significant optimizations that are otherwise blocked, most often by breaking potential loop-carried
   dependencies.

   alloc_size tracks the allocation size for compile-time diagnostics and run-time assertions
   (__builtin_object_size). This generally requires a non-zero optimization level. In other words,
   you will get a compiler warnings about some out bounds accesses of arena objects, and with Undefined
   Behavior Sanitizer you'll get run-time bounds checking. It's a great complement to fuzzing.

   In theory alloc_align may also allow better code generation, but I've yet to observe a case. Consider
   it optional and low-priority. I mention it only for completeness.

  Arena size and growth
   How large an arena should you allocate? The simple answer: As much as is necessary for the program to
   successfully complete. Usually the cost of untouched arena memory is low or even zero. Most programs
   should probably have an upper limit, at which point they assume something has gone wrong. Arenas
   allow this case to be handled gracefully, simplifying recovery and paving the way for continued
   operation.

   While a sufficient answer for most cases, it's unsatisfying. There's a common assumption that
   programs should increase their memory usage as much as needed and let the operating system respond if
   it's too much. However, if you've ever tried this yourself, you probably noticed that mainstream
   operating systems don't handle it well. The typical results are system instability - thrashing,
   drivers crashing - possibly necessitating a reboot.

   If you insist on this route, on 64-bit hosts you can reserve a gigantic virtual address space and
   gradually commit memory as needed. On Linux that means leaning on overcommit by allocating the
   largest arena possible at startup, which will automatically commit through use. Use MADV_FREE to
   decommit.

   On Windows, VirtualAlloc handles reserve and commit separately. In addition to the allocation offset,
   you need a commit offset. Then expand the committed region ahead of the allocation offset as it
   grows. If you ever manually reset the allocation offset, you could decommit as well, or at least
   MEM_RESET. At some point commit may fail, which should then trigger the out-of-memory policy, but the
   system is probably in poor shape by that point - i.e. use an abort policy to release it all quickly.

  Pointer laundering (filthy hack)
   While allocations out of an arena don't require individual error checks, allocating the arena itself
   at startup requires error handling. It would be nice if the arena could be allocated out of .bss and
   punt that job to the loader. While you could make a big, global char[] array to back your arena, it's
   technically not permitted (strict aliasing). A "clean" .bss region could be obtained with a bit of
   assembly - .comm plus assembly to get the address into C without involving an array. I wanted a
   more portable solution, so I came up with this:

arena getarena(void) {
	static char mem[1<<28];
	arena r = {0};
	r.beg = mem;
	asm ("" : "+r"(r.beg));  // launder the pointer
	r.end = r.beg + countof(mem);
	return r;
}

   The asm accepts a pointer and returns a pointer ("+r"). The compiler cannot "see" that it's actually
   empty, and so returns the same pointer. The arena will be backed by mem, but by laundering the
   address through asm, I've disconnected the pointer from its origin. As far the compiler is concerned,
   this is some foreign, assembly-provided pointer, not a pointer into mem. It can't optimize away mem
   because it's been given to a mysterious assembly black box.

   While inappropriate for a real project, I think it's a neat trick.

  Arena-friendly container data structures

   In my initial example I used a linked list to stores lines. This data structure is great with arenas.
   It only takes a few of lines of code to implement a linked list on top of an arena, and no "destroy"
   code is needed. Simple.

   What about [**1][https://nullprogram.com/blog/2023/09/30/]arena-backed associative arrays?
   Or [**2][https://nullprogram.com/blog/2023/10/05/]arena-backed dynamic arrays?

   See these follow-up articles for details!
     * [https://nullprogram.com/tags/c/]c



---
[**1]
https://nullprogram.com/blog/2023/09/30/

An easy-to-implement, arena-friendly hash map
September 30, 2023

   My last article had tips for for arena allocation. This next article demonstrates a technique for
   building bespoke hash maps that compose nicely with arena allocation. In addition, they're fast,
   simple, and automatically scale to any problem that could reasonably be solved with an in-memory hash
   map. To avoid resizing - both to better support arenas and to simplify implementation - they have
   slightly above average memory requirements. The design, which we're calling a hash-trie, is the
   result of fruitful collaboration with NRK, whose sibling article includes benchmarks. It's my new
   favorite data structure, and has proven incredibly useful. With a couple well-placed acquire/release
   atomics, we can even turn it into a lock-free concurrent hash map.

   I've written before about MSI hash tables, a simple, very fast map that can be quickly implemented
   from scratch as needed, tailored to the problem at hand. The trade off is that one must know the
   upper bound a priori in order to size the base array. Scaling up requires resizing the array - an
   impedance mismatch with arena allocation. Search trees scale better, as there's no underlying array,
   but tree balancing tends to be finicky and complex, unsuitable to rapid, on-demand implementation. We
   want the ease of an MSI hash table with the scaling of a tree.

   I'll motivate the discussion with example usage. Suppose we have an array of pointer+length strings,
   as defined last time:

typedef struct {
	uint8_t  *data;
	ptrdiff_t len;
} str;

   And we need a function that removes duplicates in place, but (for the moment) we're not worried about
   preserving order. This could be done naively in quadratic time. Smarter is to sort, then look for
   runs. Instead, I've used a hash map to track seen strings. It maps str to bool, and it is represented
   as type strmap and one insert+lookup function, upsert.

// Insert/get bool value for given str key.
bool *upsert(strmap **, str key, arena *);

ptrdiff_t unique(str *strings, ptrdiff_t len, arena scratch) {
	ptrdiff_t count = 0;
	strmap *seen = 0;
	while (count < len) {
		bool *b = upsert(&seen, strings[count], &scratch);
		if (*b) {
			// previously seen (discard)
			strings[count] = strings[--len];
		} else {
			// newly-seen (keep)
			count++;
			*b = 1;
		}
	}
	return count;
}

   In particular, note:
	 * A null pointer is an empty hash map and initialization is trivial. As discussed in the last
	   article, one of my arena allocation principles is default zero-initializion. Put together, that
	   means any data structure containing a map comes with a ready-to-use, empty map.
	 * The map is allocated out of the scratch arena so it's automatically freed upon any return. It's
	   as care-free as garbage collection.
	 * The map directly uses strings in the input array as keys, without making copies nor worrying
	   about ownership. Arenas own objects, not references. If I wanted to carve out some fixed keys
	   ahead of time, I could even insert static strings.
	 * upsert returns a pointer to a value. That is, a pointer into the map. This is not strictly
	   required, but usually makes for a simple interface. When an entry is new, this value will be
	   false (zero-initialized).

   So, what is this wonderful data structure? Here's the basic shape:

typedef struct {
	hashmap *child[4];
	keytype  key;
	valtype  value;
} hashmap;

   They child and key fields are essential to the map. Adding a child to any data structure turns it
   into a hash map over whatever field you choose as the key. In other words, a hash-trie can serve as
   an intrusive hash map. In several programs I've combined intrusive lists and hash maps to create an
   insert-ordered hash map. Going the other direction, omitting value turns it into a hash set. (Which
   is what unique really needs!)

   As you probably guessed, this hash-trie is a 4-ary tree. It can easily be 2-ary (leaner but slower)
   or 8-ary (bigger and usually no faster), but 4-ary strikes a good balance, if a bit bulky. In the
   example above, keytype would be str and valtype would be bool. The most general form of upsert looks
   like this:

valtype *upsert(hashmap **m, keytype key, arena *perm) {
	for (uint64_t h = hash(key); *m; h <<= 2) {
		if (equals(key, (*m)->key)) {
			return &(*m)->value;
		}
		m = &(*m)->child[h>>62];
	}
	if (!perm) {
		return 0;
	}
	*m = new(perm, hashmap);
	(*m)->key = key;
	return &(*m)->value;
}

   This will take some unpacking. The first argument is a pointer to a pointer. That's the destination
   for any newly-allocated element. As it travels down the tree, this points into the parent's child
   array. If it points to null, then it's an empty tree which, by definition, does not contain the key.

   We need two "methods" for keys: hash and equals. The hash function should return a uniformly
   distributed integer. As is usually the case, less uniform fast hashes generally do better than
   highly-uniform slow hashes. For hash maps under ~100K elements a 32-bit hash is fine, but larger maps
   should use a 64-bit hash state and result. Hash collisions revert to linear, linked list performance
   and, per the birthday paradox, that will happen often with 32-bit hashes on large hash maps.

   If you're worried about pathological inputs, add a seed parameter to upsert and hash. Or maybe even
   use the address m as a seed. The specifics depend on your security model. It's not an issue for most
   hash maps, so I don't demonstrate it here.

   The top two bits of the hash are used to select a branch. These tend to be higher quality for
   multiplicative hash functions. At each level two bits are shifted out. This is what gives it its
   name: a trie of the hash bits. Though it's un-trie-like in the way it deposits elements at the first
   empty spot. To make it 2-ary or 8-ary, use 1 or 3 bits at a time.

   I initially tried a Multiplicative Congruential Generator (MCG) to select the next branch at each
   trie level, instead of bit shifting, but NRK noticed it was consistently slower than shifting.

   While "delete" could be handled using gravestones, many deletes would not work well. After all, the
   underlying allocator is an arena. A combination of uniformly distributed branching and no deletion
   means that rebalancing is unnecessary. This is what grants it its simplicity!

   If no arena is provided, it reverts to a lookup and returns null when the key is not found. It allows
   one function to flexibly serve both modes. In unique, pure lookups are unneeded, so this condition
   could be skipped in its strmap.

   Sometimes it's useful to return the entire hashmap object itself rather than an internal pointer,
   particularly when it's intrusive. Use whichever works best for the situation. Regardless, exploit
   zero-initialization to detect newly-allocated elements when possible.

   In some cases we may deep copy the key in its arena before inserting it into the map. The provided
   key may be a temporary (e.g. sprintf) which the map outlives, and the caller doesn't want to allocate
   a longer-lived key unless it's needed. It's all part of tailoring the map to the problem, which we
   can do because it's so short and simple!

  Fleshing it out

   Putting it all together, unique could look like the following, with strmap/upsert renamed to
   strset/ismember:

uint64_t hash(str s) {
	uint64_t h = 0x100;
	for (ptrdiff_t i = 0; i < s.len; i++) {
		h ^= s.data[i];
		h *= 1111111111111111111u;
	}
	return h;
}

bool equals(str a, str b) {
	return a.len==b.len && !memcmp(a.data, b.data, a.len);
}

typedef struct {
	strset *child[4];
	str     key;
} strset;

bool ismember(strset **m, str key, arena *perm) {
	for (uint64_t h = hash(key); *m; h <<= 2) {
		if (equals(key, (*m)->key)) {
			return 1;
		}
		m = &(*m)->child[h>>62];
	}
	*m = new(perm, strset);
	(*m)->key = key;
	return 0;
}

ptrdiff_t unique(str *strings, ptrdiff_t len, arena scratch) {
	ptrdiff_t count = 0;
	for (strset *seen = 0; count < len;) {
		if (ismember(&seen, strings[count], &scratch)) {
			strings[count] = strings[--len];
		} else {
			count++;
		}
	}
	return count;
}

   The FNV hash multiplier is 19 ones, my favorite prime. I don't bother with an xorshift finalizer
   because the bits are used most-significant first. Exercise for the reader: Support retaining the
   original input order using an intrusive linked list on strset.

  Relative pointers?
   As mentioned, four pointers per entry - 32 bytes on 64-bit hosts - makes these hash-tries a bit
   heavier than average. It's not an issue for smaller hash maps, but has practical consequences for
   huge hash maps.

   In attempt to address this, I experimented with relative pointers (example: markov.c). That is,
   instead of pointers I use signed integers whose value indicates an offset relative to itself. Because
   relative pointers can only refer to nearby memory, a custom allocator is imperative, and arenas fit
   the bill perfectly. Range can be extended by exploiting memory alignment. In particular, 32-bit
   relative pointers can reference up to 8GiB in either direction. Zero is reserved to represent a null
   pointer, and relative pointers cannot refer to themselves.

   As a bonus, data structures built out of relative pointers are position independent. A collection of
   them - perhaps even a whole arena - can be dumped out to, say, a file, loaded back at a different
   position, then continue to operate as-is. Very cool stuff.

   Using 32-bit relative pointers on 64-bit hosts cuts the hash-trie overhead in half, to 16 bytes. With
   an arena no larger than 8GiB, such pointers are guaranteed to work. No object is ever too far away.
   It's a compounding effect, too. Smaller map nodes means a larger number of them are in reach of a
   relative pointer. Also very cool.

   However, as far as I know, no generally available programming language implementation supports this
   concept well enough to put into practice. You could implement relative pointers with language
   extension facilities, such as C++ operator overloads, but no tools will understand them - a major
   bummer. You can no longer use a debugger to examine such structures, and it's just not worth that
   cost. If only arena allocation was more popular...

  As a concurrent hash map
   For the finale, let's convert upsert into a concurrent, lock-free hash map. That is, multiple threads
   can call upsert concurrently on the same map. Each must still have its own arena, probably per-thread
   arenas, and so no implicit locking for allocation.

   The structure itself requires no changes! Instead we need two atomic operations: atomic load
   (acquire), and atomic compare-and-exchange (acquire/release). They operate only on child array
   elements and the tree root. To illustrate I will use GCC atomics, also supported by Clang.

valtype *upsert(map **m, keytype key, arena *perm) {
	for (uint64_t h = hash(key);; h <<= 2) {
		map *n = __atomic_load_n(m, __ATOMIC_ACQUIRE);
		if (!n) {
			if (!perm) {
				return 0;
			}
			arena rollback = *perm;
			map *new = new(perm, map, 1);
			new->key = key;
			int pass = __ATOMIC_RELEASE;
			int fail = __ATOMIC_ACQUIRE;
			if (__atomic_compare_exchange_n(m, &n, new, 0, pass, fail)) {
				return &new->value;
			}
			*perm = rollback;
		}
		if (equals(n->key, key)) {
			return &n->value;
		}
		m = n->child + (h>>62);
	}
}

   First an atomic load retrieves the current node. If there is no such node, then attempt to insert one
   using atomic compare-and-exchange. The ABA problem is not an issue thanks again to lack of
   deletion: Once set, a pointer never changes. Before allocating a node, take a snapshot of the arena
   so that the allocation can be reverted on failure. If another thread got there first, continue
   tumbling down the tree as though a null was never observed.

   On compare-and-swap failure, it turns into an acquire load, just as it began. On success, it's a
   release store, synchronizing with acquire loads on other threads.

   The key field does not require atomics because it's synchronized by the compare-and-swap. That is,
   the assignment will happen before the node is inserted, and keys do not change after insertion. The
   same goes for any zeroing done by the arena.

   Loads and stores through the returned pointer are the caller's responsibility. These likely require
   further synchronization. If valtype is a shared counter then an atomic increment is sufficient. In
   other cases, upsert should probably be modified to accept an initial value to be assigned alongside
   the key so that the entire key/value pair inserted atomically. Alternatively, break it into two
   steps. The details depend on the needs of the program.

   On small trees there will much contention near the root of the tree during inserts. Fortunately, a
   contentious tree will not stay small for long! The hash function will spread threads around a large
   tree, generally keeping them off each other's toes.

   A complete demo you can try yourself: concurrent-hash-trie.c. It returns a value pointer like
   above, and store/load is synchronized by the thread join. Each thread is given a per-thread subarena
   allocated out of the main arena, and the final tree is built from these subarenas.

   For a practical example: a multithreaded rainbow table to find hash function collisions. Threads
   are synchronized solely through atomics in the shared hash-trie.

   A complete fast, concurrent, lock-free hash map in under 30 lines of C sounds like a sweet deal to
   me!
	 * [https://nullprogram.com/tags/optimization/]optimization



---
[**2]
https://nullprogram.com/blog/2023/10/05/

A simple, arena-backed, generic dynamic array for C
October 05, 2023

   Previously I presented an arena-friendly hash map applicable to any programming language where one
   might use arena allocation. In this third article I present a generic, arena-backed dynamic array.
   The details are specific to C, as the most appropriate mechanism depends on the language (e.g.
   templates, generics). Just as in the previous two articles, the goal is to demonstrate an idea so
   simple that a full implementation fits on one terminal pager screen - a concept rather than a
   library.

   Unlike a hash map or linked list, a dynamic array - a data buffer with a size that varies during run
   time - is more difficult to square with arena allocation. They're contiguous by definition, and we
   cannot resize objects in the middle of an arena, i.e. realloc. So while convenient, they come with
   trade-offs. At least until they stop growing, dynamic arrays are more appropriate for shorter-lived,
   temporary contexts, where you would use a scratch arena. On average they consume about twice the
   memory of a fixed array of the same size.

   As before, I begin with a motivating example of its use. The guts of the generic dynamic array
   implementation are tucked away in a push() macro, which is essentially the entire interface.

typedef struct {
	int32_t  *data;
	ptrdiff_t len;
	ptrdiff_t cap;
} int32s;

int32s fibonacci(int32_t max, arena *perm) {
	static int32_t init[] = {0, 1};
	int32s fib = {0};
	fib.data = init;
	fib.len = fib.cap = countof(init);

	for (;;) {
		int32_t a = fib.data[fib.len-2];
		int32_t b = fib.data[fib.len-1];
		if (a+b > max) {
			return fib;
		}
		*push(&fib, perm) = a + b;
	}
}

   Anyone familiar with Go will quickly notice a pattern: int32s looks an awful lot like a Go slice.
   That was indeed my inspiration, and there is enough context that you could infer similar
   semantics. I will even call these "slice headers." Initially I tried a design based on stretchy
   buffers, but I didn't like the macros nor the ergonomics.

   I wouldn't write a fibonacci this way in practice, but it's useful for highlighting certain features.
   Of particular note:
	 * The dynamic array initially wraps a static array, yet I can append to it as though it were a
	   dynamic allocation. If I don't append at all, it still works. (Though of course the caller then
	   shouldn't modify the elements.)
	 * push() operates on any object which is slice-shaped. That is it has a pointer field named data, a
	   ptrdiff_t length field named len, a ptrdiff_t capacity field named cap, and all in that order.
	 * push() evaluates to a pointer to the newly-pushed element. In my example I immediately
	   dereference and assign a value.
	 * An element is zero-initialized the first time it's pushed. I say "first time" because you can
	   truncate an array by reducing len, and "pushing" afterward will simply reveal the original
	   elements.
	 * The name int32s is intended to evoke plurality. I'll use this convention again in a moment.
	 * The arena passed to push() is only used if the array needs to grow. The new backing array will be
	   allocated out of this arena regardless of the original backing array.
	 * Resizes always change the backing array address, and the old array remains valid. This is also
	   just like slices in Go.
	 * Despite the name perm, I expect it points to the caller's scratch arena. It's "permanent" only
	   relative to the fibonacci call. Otherwise I might build the array in a scratch arena, then create
	   a final copy in a permanent arena.

   For a slightly more realistic example: rendering triangles. Suppose we need data in array format for
   OpenGL, but we don't know the number of vertices ahead of time. A dynamic array is convenient,
   especially if we discard the array as soon as OpenGL is done with it. We could build up entire scenes
   like this for each display frame.

typedef struct {
	 GLfloat x, y, z;
} GLvert;

typedef struct {
	GLvert    *data;
	ptrdiff_t len;
	ptrdiff_t cap;
} GLverts;

void renderobj(char *buf, ptrdiff_t len, arena scratch) {
	GLverts vs = {0};
	objparser parser = newobjparser(buf, len);
	for (...) {
		*push(&vs, &scratch) = nextvert(&parser);
	}
	glVertexPointer(3, GL_FLOAT, 0, vs.data);
	glDrawArrays(GL_TRIANGLES, 0, vs.len);
}

   As before, GLverts is slice-shaped. This time it's zero-initialized, which is a valid empty dynamic
   array. As with maps, that means any object with such a field comes with a ready-to-use empty dynamic
   array. Putting it together, here's an example that gradually appends vertices to named dynamic
   arrays, randomly accessed by string name:

typedef struct {
	map    *child[4];
	str    name;
	GLverts verts;
} map;

verts *upsert(map **, str, arena *);  // from the last article

map *example(..., arena *perm) {
	map *m = 0;
	for (...) {
		str name = ...;
		vert v = ...;
		verts *vs = upsert(&m, name, perm);
		*push(vs, perm) = v;
	}
	return m;
}

   That's what Go would call map[str][]vert, but allocated entirely out of an arena. Ever thought C
   could do this so simply and conveniently? The memory allocator (~15 lines), map (~30 lines), dynamic
   array (~30 lines), constructors (0 lines), and destructors (0 lines) that power this total to ~75
   lines of zero-dependency code!

  Implementation details
   I despise macro abuse, and programs substantially implemented in macros are annoying. They're
   difficult to understand and debug. A good dynamic array implementation will require a macro, and one
   of my goals was to keep it as simple and minimal as possible. The macro's job is to:
	1. Check the capacity and maybe grow the array via function call.
	2. Smuggle type information (i.e. sizeof) to that function.
	3. Compute a pointer of the proper type to the new element.

   Here's what I came up with:

#define push(s, arena) \
	((s)->len >= (s)->cap \
		? grow(s, sizeof(*(s)->data), arena), \
		  (s)->data + (s)->len++ \
		: (s)->data + (s)->len++)

   The macro will be used as an expression, so it cannot use statements like if. The condition is
   therefore a ternary operator. If it's full, it calls the supporting grow function. In either case, it
   computes the result from data. In particular, note that the grow branch uses a comma operator to
   sequence growth before pointer derivation, as grow will change the value of data as a side effect.

   To be generic, the grow function uses memcpy-based type punning:

static void grow(void *slice, ptrdiff_t size, arena *a) {
	struct {
		void     *data;
		ptrdiff_t len;
		ptrdiff_t cap;
	} replica;
	memcpy(&replica, slice, sizeof(replica));

	replica.cap = replica.cap ? replica.cap : 1;
	ptrdiff_t align = 16;
	void *data = alloc(a, 2*size, align, replica.cap);
	replica.cap *= 2;
	if (replica.len) {
		memcpy(data, replica.data, size*replica.len);
	}
	replica.data = data;

	memcpy(slice, &replica, sizeof(replica));
}

   The slice header is copied over a local replica, avoiding conflicts with strict aliasing. This is the
   archetype slice header. It still requires that different pointers have identical memory
   representation. That's virtually always true, and certainly true anywhere I'd use an arena.

   If the capacity was zero, it behaves as though it was one, and so, through doubling, zero-capacity
   arrays become capacity-2 arrays on the first push. It's better to let alloc - whose definition, you
   may recall, included an overflow check - handle size overflow so that it can invoke the out of memory
   policy, so instead of doubling cap, which would first require an overflow check, it doubles the
   object size. This is a small constant (i.e. from sizeof), so doubling it is always safe.

   Copying over old data includes a special check for zero-length inputs, because, quite
   frustratingly, memcpy does not accept null even when the length is zero. I check for zero length
   instead of null so that it's more sensitive to defects. If the pointer is null with a non-zero
   length, it will trip Undefined Behavior Sanitizer, or at least crash the program, rather than
   silently skip copying.

   Finally the updated replica is copied over the original slice header, updating it with the new data
   pointer and capacity. The original backing array is untouched but is no longer referenced through
   this slice header. Old slice headers will continue to function with the old backing array, such as
   when the arena is reset to a point where the dynamic array was smaller.

	int32s vals = {0};
	*push(&vals, &scratch) = 1;  // resize: cap=2
	*push(&vals, &scratch) = 2;
	*push(&vals, &scratch) = 3;  // resize: cap=4
	{
		arena tmp = scratch;  // scoped arena
		int32s extended = vals;
		*push(&extended, &tmp) = 4;
		*push(&extended, &tmp) = 5;  // resize: cap=8
		example(extended);
	}
	// vals still works, cap=4, extension freed

   In practice, a dynamic array comes from old backing arrays whose total size adds up just shy of the
   current array capacity. For example, if the current capacity is 16, old arrays are size 2+4+8 = 14.

   If you're worried about misuse, such as slice header fields being in the wrong order, a couple of
   assertions can quickly catch such mistakes at run time, typically under the lightest of testing. In
   fact, I planned for this by using the more-sensitive len>=cap instead of just len==cap, so that it
   would direct execution towards assertions in grow:

	assert(replica.len >= 0);
	assert(replica.cap >= 0);
	assert(replica.len <= replica.cap);

   This also demonstrates another benefit of signed sizes: Exactly half the range is invalid and so
   defects tend to quickly trip these assertions.

  Alignment
   Alignment is unfortunately fixed, and I picked a "safe" value of 16. In my new() macro I used
   _Alignof to pass type information to alloc. Due to an oversight, unlike sizeof, _Alignof cannot be
   applied to expressions, and so it cannot be used in dynamic arrays. GCC and Clang support _Alignof on
   expressions just like sizeof, as it's such an obvious idea, but Microsoft chose to strictly follow
   the oversight in the standard. To support MSVC, I've deliberately limited the capabilities of push.
   If that doesn't matter, fixing it is easy:

--- a/example.c
+++ b/example.c
@@ -2,3 +2,3 @@
	 ((s)->len >= (s)->cap \
-		 ? grow(s, sizeof(*(s)->data), arena), \
+		 ? grow(s, sizeof(*(s)->data), _Alignof(*(s)->data), arena), \
		   (s)->data + (s)->len++ \
@@ -6,3 +6,3 @@

-static void grow(void *slice, ptrdiff_t size, arena *a)
+static void grow(void *slice, ptrdiff_t size, ptrdiff_t align, arena *a)
 {
@@ -16,3 +16,2 @@
	 replica.cap = replica.cap ? replica.cap : 1;
-	 ptrdiff_t align = 16;
	 void *data = alloc(a, 2*size, align, replica.cap);

   Though while you're at it, if you're already using extensions you might want to switch push to a
   statement expression so that the slice header s does not get evaluated more than once - i.e. so
   that upsert() in my example above could be used inside the push() expession.

#define push(s, a) ({ \
	typeof(s) s_ = (s); \
	typeof(a) a_ = (a); \
	if (s_->len >= s_->cap) { \
		grow(s_, sizeof(*s_->data), _Alignof(*s_->data), a_); \
	} \
	s_->data + s_->len++; \
})

   So far this approach to dynamic arrays has been useful on a number of occasions, and I'm quite happy
   with the results. As with arena-friendly hash maps, I've no doubt they'll become a staple in my C
   programs.

  Addendum: extend the last allocation
   Dennis Schön suggests a check if the array ends at the next arena allocation and, if so, extend the
   array into the arena in place. grow() already has the necessary information on hand, so it needs only
   the additional check:

static void grow(void *slice, ptrdiff_t size, ptrdiff_t align, arena *a) {
	struct {
        char      *data;
		ptrdiff_t len;
		ptrdiff_t cap;
	} replica;
	memcpy(&replica, slice, sizeof(replica));

	if (!replica.data) {
		replica.cap = 1;
		replica.data = alloc(a, 2*size, align, replica.cap);
	} else if (a->beg == replica.data + size*replica.cap) {
		alloc(a, size, 1, replica.cap);
	} else {
		void *data = alloc(a, 2*size, align, replica.cap);
		memcpy(data, replica.data, size*replica.len);
		replica.data = data;
	}

	replica.cap *= 2;
	memcpy(slice, &replica, sizeof(replica));
}

   Because that's yet another check for null, I've split it out into an independent third case:
	1. If the data pointer is null, make an initial allocation.
	2. If the array ends at the next arena allocation, extend it.
	3. Otherwise allocate a fresh array and copy.

   Not quite as simple, but it improves the most common case.



---

