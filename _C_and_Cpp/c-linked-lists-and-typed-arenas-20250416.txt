filename: c-linked-lists-and-typed-arenas-20250416.txt
https://bumbershootsoft.wordpress.com/2025/01/11/linked-lists-and-typed-arenas/

Linked Lists and Typed Arenas
   When I wrote the logic last year for the shooting gallery on the Amiga, I ultimately used an
   array of longwords to represent all the shots currently in flight. I mentioned at the time that while
   arrays like this were inefficient to delete from, overall its performance characteristics were fine
   and its correctness was far easier to verify. That said, I did end up implementing a solid chunk of a
   linked list-based system back then, and there's some interesting things to see in that work.

   There isn't much terribly exciting about singly-linked lists in and of themselves these days-they're
   one of the first things you learn in any data structures class-but I did run into some interesting,
   dustier corners when poking around setting them up on a 68000 or 6502 CPU. This week I'll be looking
   at how to deal with not having a real operating system or C runtime library to help manage your
   memory, and then over the rest of the month I'll dedicate a brief article to the quirks of getting
   them working on each of those chips.

Working With Linked Lists in C
   Linked lists, like all collections, don't exist in a vaccuum. They have to be lists of something, and
   for this article I'll just be using the four-byte integers that the shooting gallery project was
   using for its missile array. That means our list node's payload will just be an integer:

typedef struct ListNode_s {
	struct ListNode_s *next;
	int val;
} ListNode;

   (Since I was originally doing this stuff on the Amiga, I'll be assuming that both pointers and the
   int type are 4 bytes long; these won't be true everywhere, but the C-level details won't change.) The
   next pointer points to the next element in the list if it exists, or is a NULL pointer if there is no
   such element. Iterating through the list is a matter of starting at the front of the list and
   following the next pointer until you can't. For example, to sum up all the values in a list:

int result = 0;
ListNode *i = start;
while (i != NULL) {
	result += i->val;
	i = i->next;
}

   Or, since C's for loops are actually just disguised while loops:

int result = 0;
for (ListNode *i = start; i; i = i->next) result += i->val;

Looking at Other Languages
   The expressions i->val and i->next here are strongly analogous to the car and cdr operations in Lisp,
   and we can easily imagine a version of cons to go with it:

ListNode *cons(int car, ListNode *cdr) {
	ListNode *result = malloc(sizeof(ListNode));
	if (result) {
		result->next = cdr;
		result->val = car;
	}
	return result;
}

   This will get us in trouble. Lisp can get away with semantics like this because each node in the list
   is garbage-collected independently-it's completely fine to write something like:

(define a '(1 2 3))
(define b (cons 'a a))
(define c (cons 4 a))

   This will give us three variables named a, b, and c with values (1 2 3), (a 1 2 3), and (4 1 2 3)
   respectively, but the 1 2 3 parts of all three of them are the same chunk of memory. If b or c are
   deleted, the list node representing the first element of their lists will be collected immediately,
   but the rest will stick around until all three variables are gone.

   That doesn't happen with the sort of manual deletion we use in C. If we free up a list and its nodes
   were also being used in other lists, those lists will lead an iterator off into deallocated memory as
   soon as they hit the parts that were deleted. The solution here is to look away from Lisp and and
   turn instead towards languages like Java, C++, and C#. The list classes in these languages uniquely
   own their own data. External code only uses Lists to refer to the entire collection, and programmers
   making use of the list library will never directly touch a ListNode pointer themselves. We'll be
   bending that rule here, but it's worth looking at why.

Bringing Iterators Into Our C Implementation
   Java provides multiple implementations of lists, including a linked list, but it hides away the
   internals. Summing the elements of a List of Integers looks quite different from our C version:

int result = 0;
java.util.Iterator<Integer> i = list.iterator();
while (i.hasNext()) result += i.next().intValue();

   The principle Java is using here is that code like this really shouldn't have to care about what list
   implementation is being used, or, indeed, if this is an iteration at all; the core type here is
   actually java.lang.Collection, which includes not only lists but also arrays and sets.

   C++'s Standard Template Library is similar but it tries to keep its iterator types as unobtrusive as
   possible, by a very particular and narrow definition of "unobtrusive":

int result = 0;
for (std::list<int>::iterator i = list.begin(); i != list.end(); ++i) result += *i;

   If you squint, and in particular if you rely more heavily on type deduction so that you declare i as
   auto without having to restate the types of the collection, its elements, and its const status, this
   looks a bit like the C code that you'd write to iterate over an array using a pointer as your loop
   variable instead of an array index. To get this effect, C++ is relying extremely heavily on its
   operator-overload functionality.

   C doesn't have that operator overload functionality, though, and so we can't necessarily take direct
   inspiration from C++ here. The Java approach, however, looks more promising. We could create a very
   simple List structure and Java's iteration functions are almost all trivial:

typedef struct List_s { ListNode *start; } List;
typedef ListNode* Iterator;

Iterator iterator(List *l) { return l->start; }
bool hasNext(Iterator n) { return n != NULL; }
int next(Iterator *i) { int v = (*i)->val; *i = (*i)->next; return v; }

   This isn't bad at all; if anything, the main critique of it is that except for next(), the types and
   functions are so trivial that there's little sense in making them functions. The next() function
   itself is also a bit awkward because it needs an extra layer of indirection for the i argument since
   it needs to advance the iterator as well as returning the value the iterator previously was waiting
   on. Instead of having iterator() or hasNext() functions, or indeed having a type alias where we
   rename ListNode pointers as Iterator, we're frankly better off just dealing with ListNode pointers
   ourselves and extracting them from the List type or doing null-pointer checks directly. That's much
   friendlier than trying to create equivalents to C++'s begin() and end() markers. However, now that
   we're dealing with raw ListNode pointers, we can use i->val and i = i->next as drop-in replacements
   for C++'s *i and ++i operations. Despite being a "lower level" language, C's happiest place here ends
   up neatly straddling the C++ and Java designs.

   We can simplify even further, though. We'll just need to assemble a few basic facts about working
   with data structures in C:
     * Most operations we'll want to do on List or Iterator types will occasionally need to modify the
       internal values. Prepending an element to a list rewrites the start field, and we already saw how
       next() needed to advance the iterator as a side effect.
     * C has no notion of pass-by-reference; the way to "modify an argument" in C is to pass a pointer
       to that argument instead and write through that pointer as needed. Again, we saw this in next().
     * A pointer to a struct is also a pointer to the first field in that struct.

   The first two facts mean that any function-level API for our lists is going to operate on arguments
   of type List * or Iterator *. But the third fact means that List * and Iterator * are the same type:
   ListNode **. If we define our lists as particular variables of type ListNode *, then our list
   manipulation functions can operate on entire lists or points in the middle of it with no changes in
   its API.

   Better yet, for the kinds of work we've been looking at on this blog, all these facts are as true in
   assembly-language programming environments as they are in C. All of these lessons may be applied
   directly to machine code. It's not quite enough, however, to get us out of C's grip completely.

Custom Allocators
   I casually invoked the C standard library function malloc() in my proposed definition of cons above.
   In an ordinary C program, that's fine. However, 8-bit console development or 100%-assembler
   programmers won't have those functions. Replicating them is also quite a lot of work; C allocators
   are pretty heavyweight because they need to be able to handle lots of allocations of wildly different
   sizes, need to worry about allocations being inconveniently located so that we start running out of
   free space large enough to hold new allocations, and much more. That level of power isn't always
   necessary, and even in the modern world it's not uncommon to use simplified custom allocators to
   avoid paying for power that you don't use.

   Happily, in recent years some of these alternatives have become less obscure. The Zig programming
   language is motivated by a desire to avoid hidden control flow or memory allocations, and as a result
   it prominently surfaces memory allocation strategies. While it includes a general purpose allocator
   that's on par with C, C++, or any of the managed-memory programming languages, it also offers three
   lower-level strategies:
     * A page allocator that offers access to the underlying operating system's memory-allocation
       system. On Unix-like systems this will revolve around the brk() system call, while Windows has
       VirtualAlloc which offers a different set of capabilities and restrictions. More sophisticated
       1980s-era computers like the Amiga and Atari ST have their own idiosyncratic system calls.
     * A fixed buffer allocator that doesn't touch the OS at all, but instead assigns memory from a
       pre-existing range of memory. Zig's fixed-buffer allocator simply steps through the buffer
       handing out space as it goes; for all practical purposes it cannot actually free the elements
       that it allocates. In practice, if you're using one of these, you set up the buffer, allocate
       everything you need, and then when you're done with all the things you allocate you free them all
       at once by freeing the backing buffer itself.
     * The "allocate a big buffer and dole out memory from it, never freeing until you free it all in
       one go" concept is incredibly useful in long-running systems and it has a name: arena allocators.
       Zig also offers an explicit arena allocator that wraps other allocators to provide this
       capability with automatic management of growth or recycling of backing buffers.

   Of these, the fixed buffer allocator is the most promising lead for managing the shots of the
   shooting gallery project as a linked list. We have a meaningful cap on the total possible number of
   shots, so we can put our fixed buffer in the uninitialized data segment and work with it directly as
   needed. The larger data requirements of the list over the array I went with means we'll need twice as
   much memory as otherwise, but at those limits we had room for that even with the Atari 2600's 128
   bytes of RAM. We can't use its approach exactly, though, because shots are short-lived and are
   constantly being allocated and deallocated as they fly off the screen or collide with targets.
   Objects are constantly created and destroyed, and there's no consistent ordering on object lifespans;
   we can not assert that objects created earlier are destroyed earlier, nor can we say that objects
   created earlier are destroyed later. This is the kind of situation that usually calls for a
   general-purpose allocator.

   The simplest general purpose allocators address this by maintaining a free list independently of the
   actually allocated memory. This arranges the blocks of unallocated memory into a singly-linked list.
   Objects are allocated by assigning memory for the earliest block in the list that fits, much like the
   fixed-buffer allocator. Deallocated objects become new empty blocks of unallocated memory and put
   back in the free list. The actual data required to maintain the free list can be kept in the parts of
   the underlying memory buffers that aren't currently in use; it's our memory, after all.

   Now, there are a large number of problems with relying on a simple general-purpose allocator like the
   one I describe above; any decently long-running program with a lot of memory churn will cause it to
   collapse under its own weight and be unable to satisfy memory requests that in principle it should
   absolutely be able to handle. However, none of those problems apply to us because we have two
   enormous advantages:
     * Every element we ever allocate is an object of the ListNode type. We don't need to treat our
       fixed buffer as some kind of amorphous blob of memory that we have to efficiently use; we can
       simply have the buffer be an array of ListNodes.
     * The ListNode type is itself intended to implement a linked list. That means that we can implement
       the free list as if it were any other linked list we intend to use.

   That gives us a straightforward allocation system that never touches the standard library and could
   be ported to pretty much any sort of system new or old:

static ListNode node_pool[POOL_SIZE];

void init_list_allocator(void) {
	for (int i = 1; i < POOL_SIZE; ++i) {
		node_pool[i-1].next = &node_pool[i];
	}
	node_pool[POOL_SIZE-1].next = NULL;
	free_list = node_pool;
}

ListNode *node_alloc(int32_t val) {
	ListNode *result = free_list;
	if (result) {
		free_list = free_list->next;
		result->next = NULL;
		result->val = val;
	}
	return result;
}

void node_free(ListNode *node) {
	node->next = free_list;
	free_list = node;
}

   In this design, node_alloc and node_free are extremely fast, and init_list_allocator is a linear scan
   through memory up through POOL_SIZE entries. The shooting gallery project had a POOL_SIZE of 16,
   which makes the cost trivial. For added fun, we can even treat it as an arena allocator; calling
   init_list_allocator after we allocate a bunch of nodes effectively restores everything ever allocated
   to the free list.

   If POOL_SIZE is very large, though, this linear cost gets unreasonable fast. If our arena was a
   gigabyte, we'd have to iterate through hundreds of millions of elements just to, effectively, do
   nothing at all. Worse, by stepping through and writing each of these elements-and with unique and
   important values, no less-every bit of memory that is notionally allocated by the OS has to be
   actually assigned and preserved, potentially requiring the OS to start swapping things to disk or
   otherwise ruining performance further. The solution to that is to borrow even more heavily from the
   simplistic general-purpose allocator: the ListNode structure includes an integer value, so we can
   have free-list entries use that value to specify how large a block they describe. Now initializing or
   resetting the free-list is a matter of taking the first element, having it represent every block in
   the pool, and setting its next field to NULL. Allocation has to make sure to propagate updated
   information to the next element in the array, though, so that gets a little more complex:

ListNode *alloc_node(Arena *arena, ListNode *next, int val) {
	ListNode *result = arena->free_list;
	if (result) {
		if (result->val > 1) {
			arena->free_list++;
			arena->free_list->next = result->next;
			arena->free_list->val = result->val-1;
		} else {
			arena->free_list = result->next;
		}
		result->next = next;
		result->val = val;
	}
	return result;
}

   Even so, setting up, resetting, or deleting a repository with a full billion nodes was basically
   instantaneous with this, and allocating every last node into a linked-list turned out to take about
   six and a half seconds on my laptop, compared to seventeen seconds for a malloc()-based solution. A
   simplified allocator will absolutely buy you something even here in 2025.

Coming Up Next
   Most of the things we looked at here are wildly overkill for the three-kilobyte Amiga program that
   inspired the tests. Next time we'll actually distill some of these adventures down into something
   that can meaningfully collide with the Motorola 68000 processor-and also maybe see what kind of
   restrictions and guarantees C's infamously weak type system can nevertheless impose or provide.


---

