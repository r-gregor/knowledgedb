filename: c-who-owns-the-memory-multif-20260106.txt
https://lukefleed.xyz/posts/who-owns-the-memory-pt1/

Who Owns the Memory? Part 1: What is an Object?
20 Dec, 2025

This is the first article in a nnn-part series exploring how C, C++, and Rust manage memory at a low level.
We begin where the hardware does: with bytes. From there, we build up to objects, storage duration, lifetime,
and aliasing, the vocabulary required to understand ownership.

You can discuss this article on Lobsters, Reddit (r/rust and r/programming) and Hacker News

Memory Is Just Bytes
A 64-bit processor sees memory as a flat array of 2^{64} addressable bytes. It does not know what a
struct is. It does not know what an int is. When we execute mov rax, [rbx], the CPU fetches 8 bytes starting
at the address in rbx, shoves them into rax, and moves on. The semantic meaning of those bytes, whether they
represent a pointer, a floating-point number, or part of a UTF-8 string, exists only in our source code and
the instructions we generate.

The machinery we build atop this substrate, effective types in C, object lifetime in C++, validity invariants
in Rust, exists to help compilers reason about what the hardware cannot see. These abstractions enable
optimization: if the compiler knows two pointers cannot alias, it keeps values in registers instead of
reloading from memory. If it knows a reference is never null, it elides null checks. If it knows an object's
lifetime has ended, it reuses the storage.

Virtual Address Space
Modern operating systems do not give processes direct access to physical RAM. Instead, each process operates
within its own virtual address space, a fiction maintained by the MMU (Memory Management Unit) that maps
virtual addresses to physical frames. The C standard captures this abstraction explicitly: pointers in C
reference virtual memory, and the language makes no guarantees about physical layout.

This abstraction buys us two properties. First, isolation: a pointer in process A cannot reference memory in
process B. Dereferencing an unmapped address triggers a page fault, typically terminating the process. This
is crucial for process-level security, since a compromised or buggy process cannot read credentials from our
browser or corrupt our kernel's data structures. Second, portability: code does not need to know the physical
memory topology of the machine it runs on.

From our perspective, virtual memory means that the addresses we work with are translated by hardware before
reaching DRAM. This translation has performance implications. TLB misses are expensive, but the abstraction
holds: we operate on a contiguous address space that the OS manages for us.

Alignment
Not all byte addresses are equal. [DEL:On x86-64, loading a uint64_t from an address that is not divisible by
8 incurs a penalty. On stricter architectures like ARM (without unaligned access support) or older SPARC, it
causes a hardware trap.:DEL] On x86-64, misaligned access to a uint64_t is handled transparently by the CPU
with negligible overhead for most workloads. Intel has optimized this path since Sandy Bridge (2011). The
penalty becomes significant only when a load straddles a cache line boundary (64 bytes) or, worse, a page
boundary. On stricter architectures like ARM (without unaligned access support) or older SPARC, it causes a
hardware trap.

The reason is mechanical. DRAM is accessed in aligned chunks. When the CPU requests data at address 0x1003,
but the memory bus fetches 8-byte-aligned blocks, the memory controller must fetch two blocks (0x1000-0x1007
and 0x1008-0x100F), extract the relevant bytes, and reassemble them. This costs cycles.

The C standard formalizes this through the concept of alignment:

#include <stdalign.h>
#include <stdio.h>

int main(void) {
	printf("alignof(int) = %zu\n", alignof(int));                 // typically 4
	printf("alignof(double) = %zu\n", alignof(double));           // typically 8
	printf("alignof(max_align_t) = %zu\n", alignof(max_align_t)); // typically 16
}

The alignof operator (C11/C23) returns the required alignment for a type. Accessing an object at an address
that violates its alignment is undefined behavior,~~ not because the standard is being pedantic, but because
the hardware cannot reliably execute it~~. This allows the compiler to assume aligned access and emit
instructions that would trap or produce wrong results on misaligned addresses. On x86-64, misaligned scalar
loads work but may cross cache lines; on stricter architectures like ARM, they trap.

Consider this concrete failure case from Modern C:

union {
	unsigned char bytes[32];
	complex double val[2];
} overlay;

// complex double typically requires 16-byte alignment (sizeof is 16)
complex double *p = (complex double *)&overlay.bytes[4];  // misaligned
*p = 1.0 + 2.0*I;  // undefined behavior

On x86-64 with alignment checking enabled, or on ARM, this crashes with a bus error. The pointer arithmetic
is legal C, but the resulting address violates the alignment requirement of complex double. The hardware
refuses.

Cache Lines and Memory Bandwidth
Alignment interacts with another hardware reality: cache lines. On modern x86-64 processors, the L1 cache
operates on 64-byte lines. When we read a single byte, the CPU actually fetches 64 bytes. If our data
structures are laid out poorly, we waste bandwidth fetching bytes we never use.

Worse, if a single logical datum spans two cache lines, every access requires two cache fetches. For a struct
that straddles a 64-byte boundary, this doubles memory traffic.

This is why compilers insert padding between struct fields. Consider:

struct Bad {
	char a;     // 1 byte
	// 7 bytes padding
	double b;   // 8 bytes
	char c;     // 1 byte
	// 7 bytes padding (to align the whole struct)
};

struct Good {
	double b;   // 8 bytes
	char a;     // 1 byte
	char c;     // 1 byte
	// 6 bytes prdding
};

sizeof(struct Bad) is 24 bytes. sizeof(struct Good) is 16 bytes. The compiler cannot reorder fields in C (the
standard guarantees fields appear in declaration order with increasing addresses), so we must consider layout
ourselves.

	             struct Bad                                           struct Good
	     +----------------------------+                    +------------------------------+
	0x00 |XXXXXXX    a: car           |               0x00 |                              |
	     +----------------------------+               0x01 |                              |
	0x01 |                            |                ... |          b: double           |
	 ... |         padding            |                ... |                              |
	 ... |                            |                ... |                              |
	0x07 |                            |               0x07 |                              |
	     +----------------------------+                    +------------------------------+
	0x08 |                            |               0x08 |XXXXXXXX     a: char          |
	 ... |                            |                    +------------------------------+
	0x09 |        b: double           |               0x09 |XXXXXXXX     c: char          |
	 ... |                            |                    +------------------------------+
	 ... |                            |               0x0A |                              |
	 ... |                            |                ... |          padding             |
	 ... |                            |                ... |                              |
	0x0F |                            |               0x0F |                              |
	     +----------------------------+                    +------------------------------+
	0x10 |XXXXXXX     c: char         |                          16 bytes, algn: 8
	     +----------------------------+
	0x11 |                            |
	 ... |         padding            |
	 ... |                            |
	0x17 |                            |
	     +----------------------------+
	         24 bytes, align: 8

	                   [fig1: Memory Layout of 'struct Bad' vs 'struct Good']


The Allocator's View
When we call malloc(n), we do not receive exactly n bytes of usable memory. The allocator maintains metadata
like chunk headers, size fields, and free-list pointers that live adjacent to our allocation. In glibc's
ptmalloc2, an allocated chunk looks roughly like this:

+------------------+
| prev_size        |  (8 bytes, used only if previous chunk is free)
+------------------+
| size       |N|M|P|  (8 bytes, includes flags in low 3 bits)
+------------------+
| user data...     |  <- pointer returned by malloc
|                  |
+------------------+

The size field stores the chunk size with three flag bits: P (previous chunk in use), M (chunk obtained via
mmap), and A (non-main arena). The actual usable size is size & ~0x7.

This has several implications. Every allocation has overhead, and small allocations suffer proportionally
more: a 16-byte allocation requires at least 32 bytes of actual memory (16 bytes data + 16 bytes metadata,
depending on the allocator). The allocator also imposes its own alignment; malloc guarantees alignment
suitable for any primitive type (max_align_t), which is 16 bytes on most 64-bit platforms. Finally, memory is
not truly free after free(). The allocator tracks allocated regions, and free() does not necessarily return
memory to the OS. It typically returns it to a free list for reuse.

When we discuss ownership and resource management in later sections, keep this in mind: deallocating memory
at the language level means returning bytes to the allocator. The allocator decides when (if ever) to return
pages to the operating system.

Objects Are Bytes
The C standard makes this explicit: every object can be viewed as an array of unsigned char:

int x = 42;
unsigned char *bytes = (unsigned char *)&x;
for (size_t i = 0; i < sizeof(int); i++) {
	printf("%02x ", bytes[i]);
}
// Output on little-endian x86-64: 2a 00 00 00

This is the object representation, the actual bytes in memory. The semantic interpretation (that these bytes
represent the integer 42) is layered on top by the type system.

Rust and C++ inherit this model. When we say a Rust i32 occupies 4 bytes with alignment 4, we mean exactly
what C means: 4 contiguous bytes at an address divisible by 4. The type systems differ dramatically in what
operations they permit on those bytes, but the physical representation is identical.

This is the starting point. We have bytes in a virtual address space, aligned for hardware access, managed by
an allocator. Everything that follows (effective types, ownership, lifetimes) is a layer of abstraction over
this physical reality.

From Bytes to Objects
A region of memory becomes an object when we impose a type interpretation on it. The type dictates how many
bytes participate, what their alignment must be, and what operations are valid. But the three languages
differ fundamentally in when and how this imposition occurs, and what invariants the type carries.

C: Effective Type Rules
In C, the relationship between memory and type is established through the concept of effective type. The
effective type of an object determines how it may be accessed.

For declared variables, the effective type is simply the declared type:
int x = 42;  // effective type of x is int

The variable x occupies sizeof(int) bytes at some address, and those bytes must be accessed as int or as
unsigned char. Accessing them as float* is undefined behavior:

int x = 42;
float *fp = (float *)&x;
float f = *fp;  // undefined behavior: access through incompatible type

This is not a runtime check. The compiler does not insert code to verify the access. Instead, the rule exists
to enable optimization. When the compiler sees a write through int* and a read through float*, the strict
aliasing rule permits it to assume these pointers reference different objects. The compiler can then reorder
loads and stores, keep values in registers across the write, and eliminate redundant accesses.

The rule has a critical asymmetry. Any object can be viewed as an array of unsigned char:

int x = 42;
unsigned char *bytes = (unsigned char *)&x;
for (size_t i = 0; i < sizeof(int); i++) {
	printf("%02x ", bytes[i]);  // valid: char access is always permitted
}

But the reverse is undefined:

unsigned char buffer[sizeof(int)] = {0};
int *p = (int *)buffer;
int val = *p;  // undefined behavior

The buffer's effective type is unsigned char[4]. Accessing it through int* violates the effective type rule.
The fact that the bytes happen to form a valid int representation is irrelevant. The compiler is entitled to
assume this access cannot happen, and may generate code that produces garbage or crashes.

For dynamically allocated memory, the situation is different. Memory returned by malloc has no effective type
until we write to it:

void *p = malloc(sizeof(double));
double *dp = p;
*dp = 3.14;  // this write sets the effective type to double

After this write, the allocated region has effective type double. Subsequent reads through double* are valid.
Reading through int* would again be undefined.

The effective type machinery exists purely for optimization. The compiler uses it to reason about aliasing.
It provides no runtime safety.

C++: Object Lifetime
C++ inherits C's effective type rules but adds a distinct concept: object lifetime. An object's lifetime is
the interval during which accessing the object is well-defined.

The C++ standard specifies the boundaries precisely. For an object of type T, lifetime begins when storage
with proper alignment and size is obtained and initialization (if any) is complete. Lifetime ends when the
destructor call starts (for class types) or when the object is destroyed (for non-class types), or when the
storage is released or reused.

Consider placement new:

struct Widget {
	int value;
	Widget(int v) : value(v) { }
	~Widget() { std::cout << "destroyed\n"; }
};

alignas(Widget) unsigned char buffer[sizeof(Widget)];
Widget* w = new (buffer) Widget(42);  // lifetime begins here
w->~Widget();                          // lifetime ends here
// buffer still contains bytes, but no Widget object exists

Between placement new and the destructor call, a Widget object exists at that address. Before placement new
and after the destructor, the bytes exist but no Widget does. Accessing w->value after w->~Widget() is
undefined behavior, even though the bytes are still there and unchanged.

The destructor call does not free memory. It ends the object's lifetime while the storage remains intact.
This is what placement new and explicit destructor calls rely on: the ability to construct an object in
pre-existing storage, use it, destroy it, and potentially construct a different object in the same storage.

For trivial types (those without constructors, destructors, or virtual functions), C++ objects behave
essentially like C objects. For class types with nontrivial special member functions, the lifetime boundaries
become significant. A std::string accessed after destruction will likely read freed memory or corrupted
pointers, because the destructor deallocated the internal buffer.

C++20 also introduced implicit object creation. Certain operations, such as std::malloc, implicitly create
objects of implicit-lifetime types if doing so would give the program defined behavior:

struct Point { int x, y; };  // implicit-lifetime type (trivial)

Point* p = (Point*)std::malloc(sizeof(Point));
p->x = 1;  // in C++20, this is well-defined
p->y = 2;  // malloc implicitly created the Point object

This was added to retroactively make well-defined the code patterns that had been common but technically
undefined.

Rust: Validity Invariants
Rust imposes a stronger requirement. Every type has validity invariants, and producing a value that violates
its type's invariant is immediate undefined behavior. The compiler's optimizer assumes these invariants hold
unconditionally.

The Rust Reference defines validity per type:

// A bool must be 0x00 (false) or 0x01 (true)
let b: bool = unsafe { std::mem::transmute(2u8) };  // UB: invalid bool

// A reference must be non-null, aligned, and point to a valid value
let r: &i32 = unsafe { std::mem::transmute(0usize) };  // UB: null reference

// A char must be a valid Unicode scalar value (not a surrogate)
let c: char = unsafe { std::mem::transmute(0xD800u32) };  // UB: surrogate

// An enum must have a valid discriminant
enum Status { Active = 0, Inactive = 1 }
let s: Status = unsafe { std::mem::transmute(2u8) };  // UB: invalid discriminant

// The never type must never exist
let n: ! = unsafe { std::mem::zeroed() };  // UB

The moment an invalid value is produced, undefined behavior has occurred. The Rust compiler assumes that all
values produced during program execution are valid; producing an invalid value is therefore immediate UB.

In C, you can have an int variable containing any 32-bit pattern, and as long as you do not read it in
certain ways, no UB occurs. In Rust, if a bool contains the bit pattern 0x02, UB has already happened at the
point of creation, regardless of whether you subsequently read it.

Consider references. In C and C++, a pointer can be null, and dereferencing it is UB. But the pointer itself
can exist and be passed around. In Rust:

let ptr: *const i32 = std::ptr::null();  // valid: raw pointer can be null
let r: &i32 = unsafe { &*ptr };          // UB occurs HERE, at reference creation

The UB does not occur when we read through the reference. It occurs when the reference is created. A &T
carries an invariant: non-null, properly aligned, pointing to a valid T. Violating this invariant at any
point is UB, regardless of what we do with the reference afterward.

This strictness enables more aggressive optimization. When the compiler sees a &T, it emits dereferenceable
and nonnull annotations to LLVM. A match expression on a bool need not generate a default case for values
2-255. These optimizations would be unsound if invalid values could exist.

The cost is that more operations require unsafe. You cannot create a reference to potentially-invalid memory,
even temporarily. You must use raw pointers and convert to references only when validity is guaranteed:

let ptr: *const i32 = some_ffi_function();

if !ptr.is_null() && ptr.is_aligned() {
	let r: &i32 = unsafe { &*ptr };  // sound: we verified validity
	println!("{}", *r);
}

Object Representation vs. Value
All three languages distinguish between an object's representation (its bytes in memory) and its value (the
semantic interpretation of those bytes). But they draw the line differently, and understanding where each
language draws it determines what low-level manipulations are sound.

Every object occupies a contiguous sequence of bytes. The size of a type is how many bytes; the alignment
constrains where those bytes can start. A type with alignment 8 must be stored at an address divisible by 8.
These constraints reflect how the memory bus fetches data, as we saw in the alignment section.

In C, we can freely inspect any object as unsigned char[]:

double d = 3.14159;
unsigned char *bytes = (unsigned char *)&d;
// bytes[0..7] contain the IEEE 754 representation

The bytes are the representation. The value is what those bytes mean according to IEEE 754. C permits
examining bytes without caring about their semantic meaning. C++ inherits this but adds constraints around
object lifetime: we can inspect the bytes of a live object, but accessing bytes after the destructor has run
is undefined, even if the storage has not been reused.

Rust permits byte-level inspection through raw pointers and transmutation, but imposes validity constraints
that C and C++ do not:

let x: i32 = 42;
let bytes: [u8; 4] = unsafe { std::mem::transmute(x) };
// bytes contains the little-endian representation: [42, 0, 0, 0]

// Going the other direction requires care:
let bytes: [u8; 1] = [2];
let b: bool = unsafe { std::mem::transmute(bytes) };  // UB: 2 is not a valid bool

The asymmetry mirrors C's effective type rule. Converting a typed value to bytes is generally safe.
Converting bytes to a typed value requires that the bytes constitute a valid value of that type, and Rust's
validity invariants, as we saw earlier, are stricter than C's.

This distinction matters when we consider struct layout. C guarantees fields appear in declaration order; the
compiler inserts padding but cannot reorder. C++ inherits this for standard-layout types. Rust makes minimal
guarantees by default: the repr(Rust) layout allows the compiler to reorder fields to minimize padding.
Consider:

struct A {
	a: u8,
	b: u32,
	c: u16,
}

Rust might lay this out as (b, c, a, padding) to achieve size 8 instead of the naive 12. Different generic
instantiations of the same struct may have different layouts. For interoperability with C, Rust provides #
[repr(C)], which guarantees C-compatible layout: fields in declaration order, padding computed by the
standard algorithm.

The layout algorithm for repr(C) is deterministic. Start with offset 0. For each field in declaration order:
add padding until the offset is a multiple of the field's alignment, record the field's offset, advance by
the field's size. Finally, round the struct's total size up to its alignment. This is exactly how our struct
Bad ended up at 24 bytes while struct Good achieved 16. The algorithm is mechanical, but field ordering is
our responsibility.

When is mem::transmute sound? Size must match (the compiler enforces this). Alignment must be compatible:
transmuting &u8 to &u64 is unsound even if sizes matched, because the u8 may not be 8-byte aligned. And
validity must be preserved: the bytes must constitute a valid value of the target type. This last constraint
is what Rust adds beyond C. The repr(transparent) attribute creates a type with identical layout to its
single non-zero-sized field, making transmutation between them sound and enabling zero-cost newtypes.

Storage Duration
Every object resides somewhere in memory. The storage duration of an object determines when that memory is
allocated and when it becomes invalid. All three languages recognize the same fundamental categories, though
they use different terminology and provide different guarantees about deallocation.

The Four Categories
C defines four storage durations. C++ inherits the same four. Rust maps onto an equivalent model, though the
language specification does not use identical terminology.

  * Static storage duration: The object exists for the entire execution of the program. In C and C++, this
    includes global variables, variables declared with static, and string literals. In Rust, this includes
    static items and string literals (which have type &'static str). The memory for these objects is
    typically placed in the .data or .rodata segment of the executable and requires no runtime allocation.

  * Thread storage duration: The object exists for the lifetime of a thread. C11 introduced _Thread_local
    (spelled thread_local since C23), C++11 introduced thread_local, and Rust provides thread_local! macro.
    Each thread gets its own instance of the variable, allocated when the thread starts and deallocated when
    it terminates.

  * Automatic storage duration: The object exists within a lexical scope, typically a function body or block.
    When execution enters the scope, space is reserved; when execution leaves, the space is released. In C
    and C++, local variables without static or thread_local have automatic storage. In Rust, all local
    bindings have automatic storage. This is typically implemented via the stack.

  * Allocated (dynamic) storage duration: The object's lifetime is controlled explicitly by the program. In
    C, this means malloc/free. In C++, this means new/delete or allocator-aware containers. In Rust, this
    means Box, Vec, String, and other heap-allocating types.

Stack
Automatic storage is almost universally implemented using a call stack. When a function is called, the
compiler reserves space for its local variables by adjusting the stack pointer. On x86-64 following the
System V ABI, this looks like:

my_function:
	push rbp
	mov rbp, rsp
	sub rsp, 48          ; reserve 48 bytes for locals
	; ... function body ...
	mov rsp, rbp
	pop rbp
	ret

The sub rsp, 48 instruction allocates space for all local variables in a single operation. The compiler
computes the required size at compile time by summing the sizes of all locals (accounting for alignment).
Deallocation is equally cheap: mov rsp, rbp releases all that space instantly.

This has two consequences. First, allocation and deallocation of automatic storage is O(1)O(1)O(1) regardless
of how many objects are involved. A function with 100 local variables pays the same cost as one with 2.
Second, the space is not initialized. After sub rsp, 48, those 48 bytes contain whatever was previously on
the stack. In C, reading an uninitialized automatic variable is undefined behavior (the value is
indeterminate). In C++, the same rule applies. In Rust, the compiler enforces definite initialization: you
cannot read a variable before assigning to it.

    In C, reading an uninitialized automatic variable is undefined behavior (the value is indeterminate). In
    C++ prior to C++26, the same rule applies. C++26 introduces erroneous behavior, a new category: the value
    is still indeterminate but reading it is not UB-implementations may diagnose or zero-initialize. Clang's
    -ftrivial-auto-var-init=zero has provided this behavior for years.

fn example() {
	let x: i32;
	println!("{}", x);  // error: borrow of possibly-uninitialized variable
}

The Rust compiler tracks initialization state through control flow and rejects programs that might read
uninitialized memory. This is a compile-time check with no runtime cost.

Heap
Dynamic allocation is fundamentally different. When we call malloc(n), the allocator must find a contiguous
region of at least n bytes that is not currently in use, mark that region as allocated, and return a pointer
to it. When we call free(p), the allocator must determine the size of the allocation (stored in metadata
adjacent to the user data), mark that region as available for future allocations, and possibly coalesce
adjacent free regions to reduce fragmentation.

This involves data structure manipulation, potential system calls (if the allocator needs more memory from
the OS), and can have variable latency depending on heap state. The cost is not O(1)O(1)O(1).

In C, heap allocation is explicit:

int* p = malloc(sizeof(int) * 100);
if (p == NULL) {
	// allocation failed
}
// ... use p ...
free(p);

We are responsible for checking for allocation failure, calling free exactly once, not using the pointer
after free, and not freeing the same pointer twice. Violating any of these causes undefined behavior or
memory leaks. The language provides no assistance.

In C++, dynamic allocation can be explicit (new/delete) or managed through RAII:

// Explicit (dangerous)
int* p = new int[100];
delete[] p;

// RAII (safer)
auto v = std::make_unique<int[]>(100);
// v automatically deleted when it goes out of scope

std::unique_ptr wraps a raw pointer and calls delete in its destructor. When v goes out of scope, the
destructor runs, the memory is freed. We do not call delete manually.

This is opt-in. You can still use raw new/delete. You can still have dangling pointers. The compiler does not
verify correctness.

In Rust, heap allocation is handled through owning types:
let v: Vec<i32> = Vec::with_capacity(100);
// v automatically deallocated when it goes out of scope

Vec<T> owns heap-allocated memory. When v goes out of scope, Vec's Drop implementation runs, calling the
allocator to free the buffer. There is no way to forget to free, no way to double-free, and no way to use
after free (the compiler rejects such programs).

The difference from C++ is that Rust's ownership is not opt-in. Every heap allocation is owned by exactly one
binding. Transferring ownership is a move. After a move, the original binding is unusable:

let v1 = vec![1, 2, 3];
let v2 = v1;           // v1 moved to v2
println!("{:?}", v1);  // error: borrow of moved value

Who Calls Free?
The central difference between C, C++, and Rust in their treatment of dynamic storage is responsibility for
deallocation.

In C, we decide when to call free. The language does not track ownership. If you pass a pointer to a
function, the function might free it, or it might not. The only way to know is documentation or convention.

void process(int* data) {
	// Does this function free data? You have to read the docs.
}

In C++, RAII shifts responsibility to destructors. If you use unique_ptr, the destructor frees. If you use
shared_ptr, the destructor decrements a reference count and frees when it reaches zero. But you can still use
raw pointers, and the compiler cannot tell you which convention a given codebase follows.

void process(int* data) {
	// Raw pointer: who owns this? Still ambiguous.
}

void process(std::unique_ptr<int[]> data) {
	// Ownership transferred: this function will free when done.
}

In Rust, the type system encodes ownership:

fn process(data: Vec<i32>) {
	// This function owns data. It will be freed when process returns.
}

fn process_ref(data: &Vec<i32>) {
	// This function borrows data. The caller retains ownership.
}

fn process_mut(data: &mut Vec<i32>) {
	// Mutable borrow. Caller retains ownership. No other access allowed during this call.
}

The signature tells you everything. Vec<i32> means ownership transfer. &Vec<i32> means immutable borrow. &mut
Vec<i32> means mutable borrow. The compiler enforces these semantics. You cannot pass a Vec and then continue
using it; the move would be rejected.

Stack Allocation in Rust
Rust provides fine-grained control over whether data lives on the stack or heap. By default, local bindings
are stack-allocated:

let x: [i32; 1000] = [0; 1000];  // 4000 bytes on the stack

This works until the array is too large for the stack (typically 1-8 MB depending on platform). For large
allocations, use Box:

let x: Box<[i32; 1000000]> = Box::new([0; 1000000]);  // heap

Box<T> is a pointer to a heap allocation. It has the same size as a raw pointer (8 bytes on 64-bit),
implements Deref so you can use it like a reference, and frees the allocation in its Drop implementation.

The memory layout of Box<T> is a single pointer:

use std::mem::size_of;
assert_eq!(size_of::<Box<[i32; 1000]>>(), 8);  // just a pointer

Unlike C++ unique_ptr, which may carry a deleter, Box<T> always uses the global allocator and has no space
overhead. The deallocation function is known statically.

Object Lifetime
Storage duration determines when memory is allocated and deallocated. Object lifetime determines when
accessing that memory is well-defined. In C, these are identical. In C++, they can differ. In Rust, lifetime
becomes a compile-time property tracked through constraint propagation over the control-flow graph.

C: Lifetime Is Storage Duration
C does not distinguish between storage exists and object is alive. An object with automatic storage duration
is alive from when execution enters its block of definition until execution leaves. An object with static
storage duration is alive for the entire program. An object with allocated storage duration is alive from
malloc to free.

The consequence is that C has no notion of storage exists but object is not yet constructed. When the stack
frame is created, all automatic variables exist. Whether they are initialized is a separate question:

int* dangling(void) {
	int x = 42;
	return &x;
}

int main(void) {
	int* p = dangling();
	printf("%d\n", *p);  // undefined behavior
}

This program compiles. The function dangling returns a pointer to x, but x has automatic storage duration.
When dangling returns, its stack frame is deallocated. The pointer p now points to memory that no longer
belongs to any live object. Dereferencing it is undefined behavior.

The C standard does not require the compiler to reject this. A conforming implementation may emit a warning,
but the program is syntactically valid. The burden falls entirely on us.

C++: Lifetime Within Storage
C++ introduces a distinction. Storage duration determines when memory is allocated and deallocated. Object
lifetime determines when the object can be accessed. For class types, lifetime begins when the constructor
completes and ends when the destructor starts.

Consider placement new:

struct Widget {
	std::string name;
	Widget(const char* n) : name(n) { }
	~Widget() { std::cout << "destroyed\n"; }
};

alignas(Widget) unsigned char buffer[sizeof(Widget)];
// Storage exists. No Widget object exists.

Widget* w = new (buffer) Widget("test");
// Constructor has run. Widget object now exists.

w->~Widget();
// Destructor has run. Widget object no longer exists.
// Storage still exists.

Between placement new and the explicit destructor call, the Widget object is alive. Before placement new and
after the destructor, the bytes in buffer exist but no Widget does. Accessing w->name after the destructor
call is undefined behavior. The bytes are there. The object is not.

The C++ standard formalizes this. For an object of type T, lifetime begins when storage with proper alignment
and size is obtained and initialization is complete. Lifetime ends when the destructor call starts (for class
types) or when the object is destroyed (for non-class types).

For trivial types (no user-defined constructor, no destructor, no virtual functions), C++ behaves like C. For
class types with nontrivial constructors or destructors, the distinction matters.

The dangling reference problem persists:

int& dangling() {
	int x = 42;
	return x;
}

This compiles. A good compiler warns. The standard does not require rejection.

Rust: Lifetimes as Named Regions
Rust prevents dangling references through compile-time analysis. The equivalent code does not compile:

fn dangling() -> &i32 {
	let x = 42;
	&x
}

The compiler rejects this:

error[E0106]: missing lifetime specifier
 --> src/lib.rs:1:18
  |
1 | fn dangling() -> &i32 {
  |                  ^ expected named lifetime parameter

We cannot return a reference without specifying its lifetime. If we try to add one:

fn dangling<'a>() -> &'a i32 {
	let x = 42;
	&x
}

error[E0515]: cannot return reference to local variable `x`
 --> src/lib.rs:3:5
  |
3 |     &x
  |     ^^ returns a reference to data owned by the current function

The compiler determines that x does not live long enough to satisfy the lifetime 'a. How does it know this?
The answer lies in region inference.

Regions and the Control-Flow Graph
A lifetime in Rust is a region: a set of points in the control-flow graph where a reference must be valid.
The borrow checker computes these regions through constraint propagation.

Consider this code:

let x = 0;
let y = &x;
let z = &y;

Each let binding introduces an implicit scope. The borrow checker infers the minimal region for each
reference. Desugared (using notation that is not valid Rust syntax, but illustrates the structure):

'a: {
	let x: i32 = 0;
	'b: {
		let y: &'b i32 = &'b x;
		'c: {
			let z: &'c &'b i32 = &'c y;
		}
	}
}

The reference y has lifetime 'b because that is the smallest region that covers its usage. The reference z
has lifetime 'c. The borrow checker minimizes lifetimes to the extent necessary.

When a reference is passed to an outer scope, the borrow checker infers a larger lifetime:

let x = 0;
let z;
let y = &x;
z = y;

Desugared:

'a: {
	let x: i32 = 0;
	'b: {
		let z: &'b i32;
		'c: {
			let y: &'b i32 = &'b x;  // must use 'b, not 'c
			z = y;
		}
	}
}

Because y is assigned to z, and z lives in scope 'b, the reference must be valid for 'b. The borrow checker
propagates this requirement.

Region Inference in rustc
The borrow checker operates on MIR (Mid-level Intermediate Representation), a simplified form of Rust code.
The process has two phases:

Phase 1: replace_regions_in_mir
The compiler identifies universal regions (those appearing in the function signature, such as 'a in fn foo
<'a>(x: &'a u32)) and replaces all other regions with fresh inference variables. Universal regions are free
in the function body. They represent constraints from the caller.

Phase 2: compute_regions
The compiler runs a type checker on MIR to collect constraints between regions. It then performs constraint
propagation to compute the value of each inference variable.

A region's value is a set. The set contains:
 1. Locations in the MIR control-flow graph: Each location is a pair (basic block, statement index). This
    identifies the point on entry to that statement.

 2. End markers for universal regions: If region 'a outlives region 'b, then end('b) is in the set for 'a.
    The element end('b) represents the portion of the caller's control-flow graph after the current function
    returns.

 3. end('static): Represents program execution after the function returns, extending to program termination.

The two main constraint types are:
Outlives constraints: If 'a: 'b (region 'a outlives region 'b), all elements of 'b plus end('b) must be added
to 'a.

Liveness constraints: A region must contain all points where it can be used.

Constraint Propagation
Consider this function:

fn bad<'a, 'b>(x: &'a usize) -> &'b usize {
	x
}

This should not compile. We have no guarantee that 'a outlives 'b. If 'a is shorter than 'b, the return value
would be a dangling reference.

The compiler introduces inference variables. Let '#1 correspond to 'a, '#3 correspond to 'b, and '#2
correspond to the expression x. Let L1 be the location of x.

Initial state from liveness constraints:

Region Contents
'#1    (empty)
'#2    L1
'#3    L1

The return statement creates an outlives constraint '#2: '#3 (the returned reference must outlive the return
type's region). Propagating:

Region   Contents
'#1      L1
'#2      L1, end('#3)
'#3      L1

The parameter creates an outlives constraint '#1: '#2 (the input flows to the expression). Propagating:

Region        Contents
'#1           L1, end('#2), end('#3)
'#2           L1, end('#3)
'#3           L1

Now the compiler checks: does '#1 contain any end('x) that is not justified by a where clause or implied
bound? Yes. '#1 contains end('#3), but we have no where clause stating 'a: 'b. This is an error.

The RegionInferenceContext in rustc stores:
  * constraints: all outlives constraints
  * liveness_constraints: all liveness constraints
  * universal_regions: the set of regions from the function signature
  * universal_region_relations: known relationships between universal regions (from where clauses)

The solve method performs propagation, then check_universal_regions verifies that no universal region grew to
contain end markers it cannot justify.

Non-Lexical Lifetimes
Before Rust 2018, lifetimes were lexical: a reference was live until the end of its lexical scope. This
rejected valid programs:

let mut data = vec![1, 2, 3];
let x = &data[0];
println!("{}", x);
data.push(4);  // error in old Rust: x still in scope

With lexical lifetimes, x would be considered live until the closing brace, conflicting with the mutable
borrow for push.

Non-lexical lifetimes (NLL) compute liveness from the control-flow graph. A reference is live from its
creation to its last use, not to the end of its scope:

let mut data = vec![1, 2, 3];
let x = &data[0];
println!("{}", x);  // last use of x
data.push(4);       // ok: x is no longer live

The borrow of data for x extends to the println! call. After that, the borrow ends. The mutable borrow for
push does not conflict.

There are subtleties. If a type has a destructor, the destructor counts as a use. The destructor runs at
scope end, extending the lifetime:

struct Wrapper<'a>(&'a i32);

impl Drop for Wrapper<'_> {
	fn drop(&mut self) { }
}

let mut data = vec![1, 2, 3];
let x = Wrapper(&data[0]);
println!("{:?}", x);
data.push(4);  // error: destructor of x runs at scope end

The Drop impl means x is used at scope end. The borrow extends to that point, conflicting with push. To fix
this, we can call drop(x) explicitly before push.

Lifetimes can have holes. A variable can be reborrowed:

let mut data = vec![1, 2, 3];
let mut x = &data[0];

println!("{}", x);  // last use of first borrow
data.push(4);       // ok: first borrow ended
x = &data[3];       // new borrow starts
println!("{}", x);

The borrow checker sees two distinct borrows tied to the same variable. The first ends after the first
println!. The second starts at the reassignment.

Control flow matters. Different branches can have different last uses:

fn condition() -> bool { true }

let mut data = vec![1, 2, 3];
let x = &data[0];

if condition() {
	println!("{}", x);  // last use in this branch
	data.push(4);       // ok
} else {
    data.push(5);       // ok: x not used in this branch
}

In the if branch, x is used before push. In the else branch, x is never used, so the borrow effectively ends
at x's creation.

Lifetimes Across Function Boundaries
Within a function, the borrow checker has complete information. It knows every use of every reference. Across
function boundaries, this information is lost. Function signatures must declare the relationships between
input and output lifetimes.

fn first_word(s: &str) -> &str {
	// returns a slice of the input
}

The signature says: the output lifetime equals the input lifetime. The returned slice borrows from s. Callers
cannot use the returned slice after s is invalidated.

When signatures are ambiguous, the compiler requires explicit annotation:

fn longest(x: &str, y: &str) -> &str {
	if x.len() > y.len() { x } else { y }
}

This does not compile. The return could come from x or y. The compiler does not know which. We must tell it:

fn longest<'a>(x: &'a str, y: &'a str) -> &'a str {
	if x.len() > y.len() { x } else { y }
}

The signature now says: both inputs must be valid for at least 'a, and the output is valid for 'a. Callers
must ensure both inputs outlive the result.

Lifetime Elision
Common patterns do not require explicit annotation. Elision rules infer lifetimes:

Rule 1: Each input reference gets its own lifetime.

fn f(x: &i32, y: &i32)
// becomes
fn f<'a, 'b>(x: &'a i32, y: &'b i32)

Rule 2: If there is exactly one input lifetime, it is assigned to all outputs.

fn f(x: &i32) -> &i32
// becomes
fn f<'a>(x: &'a i32) -> &'a i32

Rule 3: If one input is &self or &mut self, its lifetime is assigned to all outputs.

impl Foo {
	fn method(&self, x: &i32) -> &i32
	// becomes
	fn method<'a, 'b>(&'a self, x: &'b i32) -> &'a i32
}

When elision rules do not determine all lifetimes, explicit annotation is required.

'static

The lifetime 'static means valid for the [DEL:entire program:DEL] remainder of program execution. String
literals have type &'static str because they are stored in the binary's read-only data segment:

let s: &'static str = "hello";

The bound T: 'static means T contains no non-static references. This is required for spawning threads, since
the spawned thread may outlive the current stack frame:

fn spawn<F>(f: F) where F: FnOnce() + Send + 'static

A closure that captures &x where x is a local variable does not satisfy 'static. The reference would dangle
when the spawning function returns.

Uninitialized Memory
All runtime-allocated memory begins as uninitialized. The bytes exist but contain whatever values were left
there by previous use. The question is: what happens when we read those bytes before writing to them?

C says the value is indeterminate. C++ adds the concept of vacuous initialization. Rust says reading
uninitialized memory is undefined behavior, full stop, and provides MaybeUninit<T> as the controlled
mechanism for working with such memory.

C: Indeterminate Values
In C, reading an uninitialized automatic variable produces an indeterminate value. The C23 standard
distinguishes this from a non-value representation (previously called trap representation), which is a bit
pattern that does not correspond to any valid value of the type.

void example(void) {
	int x;
	printf("%d\n", x);  // indeterminate value
}

What happens here? The standard does not say the program crashes. It does not say the program continues with
some arbitrary value. It says the behavior is undefined. The compiler is permitted to assume this code never
executes.

The distinction between indeterminate values and non-value representations matters for types where not all
bit patterns are valid. The bool type has only two valid values: 0 (false) and 1 (true). A bool occupies at
least 8 bits. Setting other bits produces a non-value representation:

bool b;
memset(&b, 0xFF, sizeof(b));  // all bits set
if (b) { /* ... */ }          // undefined behavior

The compiler may test for zero, or it may test the least significant bit. Different optimization levels may
produce different results. The behavior is not merely unspecified (implementation-defined but consistent); it
is undefined (the compiler may assume it cannot happen).

For types without non-value representations (most integer types on modern hardware), reading an indeterminate
value is still undefined behavior because the compiler cannot reason about what value it will see. The
optimizer may propagate contradictory assumptions through the program.

C++: Vacuous Initialization and Implicit Object Creation

C++ inherits C's rules for scalar types but adds complexity for class types. A variable has vacuous
initialization if it is default-initialized and its class type has a trivial default constructor.

struct Trivial {
	int x;
	int y;
};

void example() {
	Trivial t;  // vacuous initialization: x and y are indeterminate
}

The object t exists. Its lifetime has begun. But its members x and y contain indeterminate values. Reading
them is undefined behavior, just as in C.

For class types with nontrivial constructors, default initialization runs the constructor:

struct Nontrivial {
	int x;
	Nontrivial() : x(0) { }
};

void example() {
	Nontrivial n;  // constructor runs, x is 0
}

C++20 introduced implicit object creation. Certain operations (allocation functions, memmove, memcpy,
creation of unsigned char or std::byte arrays) implicitly create objects of implicit-lifetime types within
their storage region. This retroactively makes some previously-undefined patterns well-defined:

struct X { int a, b; };

X* make_x() {
	X* p = (X*)std::malloc(sizeof(struct X));
	p->a = 1;  // pre-C++20: UB (no X object exists)
	p->b = 2;  // C++20: ok (X implicitly created)
	return p;
}

The C++ standard specifies that malloc implicitly creates objects of implicit-lifetime types if doing so
would give the program defined behavior. This was a pragmatic fix for code that had been written for decades
under the assumption that malloc plus assignment creates an object.

Rust: Immediate Undefined Behavior
Rust takes the strictest position. Reading uninitialized memory is undefined behavior at the point of the
read, regardless of type. The Rust Reference states that integers, floating point values, and raw pointers
must be initialized and must not be obtained from uninitialized memory.

This applies even to u8, which has no invalid bit patterns. The reasoning is that the compiler must be able
to assume all values are initialized. Without this guarantee, the optimizer cannot propagate values,
eliminate dead stores, or make any assumptions about the contents of memory.

Rust enforces this at compile time for safe code through definite initialization analysis:

fn example() {
	let x: i32;
	println!("{}", x);  // error: use of possibly uninitialized `x`
}

The analysis tracks initialization state through control flow:

fn example(condition: bool) {
	let x: i32;
	if condition {
		x = 1;
	}
	println!("{}", x);  // error: `x` is possibly uninitialized
}

Even though the if branch initializes x, the else branch does not. The compiler rejects the program.

The analysis understands control flow but not values:

fn example() {
	let x: i32;
	if true {
		x = 1;
	}
	println!("{}", x);  // error: compiler doesn't evaluate `true`
}

The compiler does not evaluate true at analysis time. It sees a conditional with only one branch that
initializes x. The program is rejected.

Loops require care:

fn example() {
	let x: i32;
	loop {
		if true {
			x = 0;
			break;
		}
	}
	println!("{}", x);  // ok: compiler knows break is reached
}

The compiler understands that execution cannot reach println! without passing through the break, and the
break is preceded by initialization. The program compiles.

Move Semantics and Uninitialization
In Rust, moving a value out of a variable leaves that variable logically uninitialized:

fn example() {
	let x = Box::new(42);
	let y = x;           // x is moved, now logically uninitialized
	println!("{}", x);   // error: use of moved value
}

For Copy types, this does not apply. The value is copied, and both variables remain initialized:

fn example() {
	let x: i32 = 42;
	let y = x;           // copy, not move
	println!("{}", x);   // ok: x is still initialized
}

A variable can be reinitialized after being moved from:

fn example() {
	let mut x = Box::new(42);
	let y = x;           // x is moved
	x = Box::new(43);    // x is reinitialized
	println!("{}", x);   // ok
}

The mut is required because the compiler considers reinitialization to be mutation.

MaybeUninit<T>: The Escape Hatch

When performance requires working with uninitialized memory, Rust provides MaybeUninit<T>. This is a union
type that can hold either an initialized T or uninitialized bytes:

use std::mem::MaybeUninit;
let x: MaybeUninit<i32> = MaybeUninit::uninit();

The key property: dropping a MaybeUninit<T> does nothing. It does not run T's destructor. This is essential
because the value might not be initialized, and dropping an uninitialized value would be undefined behavior.

To initialize, we write to the MaybeUninit:

let mut x: MaybeUninit<i32> = MaybeUninit::uninit();
x.write(42);  // now initialized

To extract the initialized value, we call assume_init:

let mut x: MaybeUninit<i32> = MaybeUninit::uninit();
x.write(42);
let value: i32 = unsafe { x.assume_init() };

The assume_init call is unsafe because the compiler cannot verify that we actually initialized the value. We
are asserting to the compiler: trust me, it is initialized. If we lied, behavior is undefined.

Array Initialization
Safe Rust does not permit partial array initialization. We must initialize all elements at once:

let arr: [i32; 4] = [1, 2, 3, 4];       // ok
let arr: [i32; 1000] = [0; 1000];       // ok, all zeros

For dynamic initialization, MaybeUninit provides a path:

use std::mem::{self, MaybeUninit};
const SIZE: usize = 10;

let arr: [Box<u32>; SIZE] = {
	// Create array of uninitialized MaybeUninit
	let mut arr: [MaybeUninit<Box<u32>>; SIZE] = [const { MaybeUninit::uninit() }; SIZE];

	// Initialize each element
	for i in 0..SIZE {
		arr[i] = MaybeUninit::new(Box::new(i as u32));
	}

	// Transmute to initialized type
	unsafe { mem::transmute::<_, [Box<u32>; SIZE]>(arr) }
};

The transmute is sound because MaybeUninit<T> has the same layout as T. We initialized every element, so the
array now contains valid Box<u32> values.

A critical detail: the assignment arr[i] = MaybeUninit::new(...) does not drop the old value. MaybeUninit<T>
has no Drop implementation. If we had written to a regular Box<u32> array, the assignment would drop the old
value, which would be undefined behavior for uninitialized memory.

Raw Pointer Writes
When MaybeUninit::new is not suitable, we use raw pointer operations. The ptr module provides:
  * ptr::write(ptr, val): Writes val to ptr without reading or dropping the old value
  * ptr::copy(src, dest, count): Copies count elements from src to dest (like memmove)
  * ptr::copy_nonoverlapping(src, dest, count): Like copy, but assumes no overlap (like memcpy)

These functions do not drop the destination. They overwrite the bytes. This is correct for uninitialized
memory but dangerous for initialized memory containing values with destructors. We can use
std::mem::needs_drop::<T>() to check at compile time whether a type requires drop glue.

A subtlety: the destination pointer must have provenance for the target allocation. Rust's memory model
tracks not just addresses but also which allocation a pointer is permitted to access. A pointer synthesized
from an integer (via ptr::from_exposed_addr) has weaker provenance guarantees than one derived from a
reference.

use std::ptr;

let mut x: MaybeUninit<String> = MaybeUninit::uninit();
unsafe {
	ptr::write(x.as_mut_ptr(), String::from("hello"));
}
let s = unsafe { x.assume_init() };

We cannot use &mut to get a reference to the uninitialized String because creating a reference to an invalid
value is undefined behavior. The as_mut_ptr method returns a raw pointer without creating a reference.

For struct fields, we use raw reference syntax to avoid creating intermediate references:

use std::{ptr, mem::MaybeUninit};

struct Demo {
	field: bool,
}

let mut uninit = MaybeUninit::<Demo>::uninit();
let field_ptr = unsafe { &raw mut (*uninit.as_mut_ptr()).field };
unsafe { field_ptr.write(true); }
let demo = unsafe { uninit.assume_init() };

The &raw mut syntax creates a raw pointer without creating a reference. This is important because &mut
(*uninit.as_mut_ptr()).field would create a reference to an uninitialized bool, which is undefined behavior.

The Vec Pattern
The most common use of uninitialized memory in Rust is building collections. Vec<T> internally uses
MaybeUninit (via RawVec) to manage its buffer. When we call vec.reserve(n), the vector allocates space for n
additional elements without initializing them.

A performance pattern for filling a vector from an external source:

fn read_into_vec(src: &[u8], count: usize) -> Vec<u8> {
	let mut v: Vec<u8> = Vec::with_capacity(count);

	unsafe {
		// Get raw pointer to uninitialized buffer
		let ptr = v.as_mut_ptr();

		// Copy from source (assumes src.len() >= count)
		std::ptr::copy_nonoverlapping(src.as_ptr(), ptr, count);

		// Tell Vec the elements are now initialized
		v.set_len(count);
	}

	v
}

The set_len call is unsafe because we are asserting that the first count elements are initialized. The vector
trusts us. If we lie, dropping the vector will attempt to drop uninitialized values, which is undefined
behavior for types with destructors.

For u8, there is no destructor, so the immediate danger is reading garbage. But the compiler may still
optimize based on the assumption that all values are initialized.

The safe alternative uses resize or extend:

fn read_into_vec_safe(src: &[u8], count: usize) -> Vec<u8> {
	let mut v: Vec<u8> = Vec::with_capacity(count);
	v.extend_from_slice(&src[..count]);
	v
}

This initializes each element as it is added. For large buffers, the unsafe version can be measurably faster
because it avoids redundant initialization. Whether the speedup matters depends on the workload.

The Deprecated mem::uninitialized
Older Rust code uses mem::uninitialized::<T>() to create uninitialized values. This function is deprecated
and should not be used in new code. The problem is that it returns a T, which means the caller receives an
initialized value of type T that actually contains garbage:

// DON'T DO THIS
let x: bool = unsafe { std::mem::uninitialized() };
// x is now an "initialized" bool with garbage bits
// Any use of x is undefined behavior

The compiler believes x is initialized. It may propagate this "value" through the program. The result is
unpredictable.

MaybeUninit solves this by wrapping the uninitialized state in a type that the compiler understands. The
value inside is not accessible until we call assume_init. This prevents the compiler from making false
assumptions about initialization state.

Aliasing
Two pointers alias when they refer to overlapping regions of memory. This matters because aliasing constrains
what the compiler can optimize. If the compiler cannot prove that two pointers refer to different memory, it
must assume that a write through one may affect a read through the other. This forces conservative code
generation: values must be reloaded from memory instead of kept in registers, stores cannot be reordered, and
entire classes of optimization become impossible.

Aliasing rules exist for optimization, not for safety. C and C++ have aliasing rules that, when violated,
result in undefined behavior. The compiler is not checking these rules to protect us. It is assuming we
follow them, and optimizing accordingly. Violate the assumption and the optimizer generates incorrect code.

Rust makes the aliasing rule explicit and compiler-checked. The &T/&mut T distinction encodes a simple
invariant: you can have many shared references or one mutable reference, but never both simultaneously. The
borrow checker enforces this at compile time.

Why the Compiler Cares
Consider this function:

fn compute(input: &u32, output: &mut u32) {
	if *input > 10 {
		*output = 1;
	}
	if *input > 5 {
		*output *= 2;
	}
}

We would like the compiler to optimize this to:

fn compute(input: &u32, output: &mut u32) {
	let cached_input = *input;
	if cached_input > 10 {
		*output = 2;
	} else if cached_input > 5 {
		*output *= 2;
	}
}

The optimization caches *input in a register and eliminates the redundant read in the second condition. It
also recognizes that if *input > 10, the final value will always be 2 (set to 1, then doubled), so it writes
2 directly.

This optimization is only valid if input and output do not alias. If they point to the same memory, the write
*output = 1 changes what *input reads:

let mut x: u32 = 20;
compute(&x, &mut x);  // input and output both point to x

With aliasing, the original function produces 1:
  * *input is 20, so *output = 1 (now x is 1)
  * *input is 1, so the second condition is false
  * Result: 1

The optimized function produces 2:
  * cached_input is 20, so *output = 2
  * Result: 2

In Rust, the call compute(&x, &mut x) is rejected at compile time. The borrow checker sees an attempt to
create both a shared reference and a mutable reference to x simultaneously. The program does not compile.

In C, the equivalent code compiles and the optimizer may generate the wrong result.

C: Type-Based Alias Analysis
The C standard specifies the effective type rule. An object must be accessed through its effective type or
through a character type. Accessing an object through a pointer of incompatible type is undefined behavior.

int x = 42;
float* fp = (float*)&x;
float f = *fp;  // undefined behavior: accessing int through float*

This is often called strict aliasing. The compiler assumes that pointers of different types do not alias. An
int* and a float* cannot point to the same object (with narrow exceptions for character types and unions).
This assumption enables Type-Based Alias Analysis (TBAA): the compiler tracks pointer types and assumes
incompatible types refer to disjoint memory.

Consider:

void update(int* pi, float* pf) {
	*pi = 1;
	*pf = 2.0f;
	printf("%d\n", *pi);
}

Under strict aliasing, the compiler may assume pi and pf do not alias. The write to *pf cannot affect *pi, so
the compiler can print 1 without reloading from memory. It may even reorder the stores or keep *pi in a
register.

If we pass pointers that actually alias:

union { int i; float f; } u;
update(&u.i, &u.f);  // undefined behavior

The optimizer's assumption is violated. The generated code may print 1 even though the memory now contains
the bit pattern of 2.0f. Or it may print something else entirely. The behavior is undefined.

The classic strict aliasing violation involves type punning without unions:

uint32_t float_bits(float f) {
	return *(uint32_t*)&f;  // undefined behavior
}

This attempts to read the bit representation of a float by casting its address to uint32_t*. The effective
type of the object is float. Accessing it through uint32_t* violates the effective type rule.

The correct approach uses a union:

uint32_t float_bits(float f) {
	union { float f; uint32_t u; } converter = { .f = f };
	return converter.u;
}

Or memcpy:

uint32_t float_bits(float f) {
	uint32_t result;
	memcpy(&result, &f, sizeof(result));
	return result;
}

Modern compilers optimize memcpy of small sizes to register moves. The resulting assembly is identical to the
undefined type-punning version, but the semantics are well-defined.

The restrict Qualifier
Type-based alias analysis only helps when pointer types differ. Two int* pointers may alias, and the compiler
must assume they do unless told otherwise.

C99 introduced the restrict qualifier. A restrict-qualified pointer is a promise from us: during the lifetime
of this pointer, no other pointer will be used to access the same memory.

void add_arrays(int* restrict dest, const int* restrict src, size_t n) {
	for (size_t i = 0; i < n; i++) {
		dest[i] += src[i];
	}
}

The restrict qualifier tells the compiler that dest and src do not overlap. The compiler can vectorize the
loop, load multiple elements of src at once, and store multiple elements to dest without worrying that a
store to dest[i] might affect a subsequent load from src[j].

Without restrict:

void add_arrays(int* dest, const int* src, size_t n) {
	for (size_t i = 0; i < n; i++) {
		dest[i] += src[i];
	}
}

The compiler must assume dest and src might overlap. If dest == src + 1, each store to dest[i] affects the
next load from src[i+1]. The loop cannot be vectorized. Each iteration must complete before the next begins.

The restrict qualifier is a promise, not a constraint. The compiler does not check it. If we lie and pass
overlapping pointers, the optimized code produces wrong results.

Standard library functions use restrict extensively:

void* memcpy(void* restrict dest, const void* restrict src, size_t n);
void* memmove(void* dest, const void* src, size_t n);

memcpy promises non-overlapping regions. memmove handles overlap correctly but cannot be optimized as
aggressively.

Rust: Shared XOR Mutable
Rust's aliasing model is simpler and compiler-enforced. The rule is: at any point in time, a piece of memory
can have either many shared references (&T) or one mutable reference (&mut T), but not both.

This is sometimes written as shared XOR mutable, or aliasing XOR mutation. The key insight is that aliasing
is only dangerous when combined with mutation. Many readers can safely read the same memory. A single writer
can safely modify memory if no one else is reading. The problem arises when reads and writes can interleave
unpredictably.

let mut x = 5;

let r1 = &x;      // shared reference
let r2 = &x;      // another shared reference, ok
println!("{} {}", r1, r2);

let r3 = &mut x;  // mutable reference
*r3 += 1;
println!("{}", r3);

This compiles because the shared references r1 and r2 are no longer used after the println!. Their lifetimes
end before the mutable reference r3 is created.

let mut x = 5;

let r1 = &x;
let r3 = &mut x;  // error: cannot borrow `x` as mutable
println!("{}", r1);

This does not compile. The shared reference r1 is still live when we attempt to create the mutable reference
r3. The borrow checker rejects the program.

The Rust compiler annotates references with LLVM attributes based on this invariant. A &T receives noalias
for read operations. A &mut T receives noalias unconditionally, telling LLVM that no other pointer accesses
this memory for the reference's lifetime. This enables the same optimizations that C achieves through
restrict, but the guarantee is compiler-verified rather than programmer-promised.

What the Borrow Checker Sees
The borrow checker does not understand the semantic meaning of operations. It does not know that &data[0] and
&data[1] are disjoint. It sees borrows and tracks their lifetimes.

Consider this code that the borrow checker rejects:

let mut v = vec![1, 2, 3];
let x = &v[0];    // immutable borrow of v
v.push(4);         // mutable borrow of v for push
println!("{}", x);

The borrow checker sees that &v[0] creates a reference with some lifetime, that this reference x must live
until println!, that v.push(4) requires &mut v, that at the point of push the reference x is still live, and
therefore there is a conflict: we cannot have &mut v while &v exists.

The borrow checker does not know that push might reallocate the vector, invalidating x. It simply enforces
the aliasing rule. As a consequence, it prevents the iterator invalidation bug.

The borrow checker also does not understand that &mut v[0] and &mut v[1] are disjoint:

let mut arr = [1, 2, 3];
let a = &mut arr[0];
let b = &mut arr[1];  // error: cannot borrow `arr[_]` as mutable more than once

The indexing operation arr[i] desugars to a method call that borrows the entire array. The borrow checker
sees two mutable borrows of arr, not two borrows of disjoint elements.

Splitting Borrows
The borrow checker understands struct fields as disjoint:

struct Point { x: i32, y: i32 }

let mut p = Point { x: 0, y: 0 };
let px = &mut p.x;
let py = &mut p.y;  // ok: different fields
*px = 1;
*py = 2;

This works because the compiler knows that p.x and p.y occupy different memory. They can be borrowed mutably
at the same time.

For slices and arrays, the standard library provides split_at_mut:

let mut arr = [1, 2, 3, 4];
let (left, right) = arr.split_at_mut(2);
// left is &mut [1, 2], right is &mut [3, 4]
left[0] = 10;
right[0] = 30;

The implementation of split_at_mut uses unsafe code:

pub fn split_at_mut(&mut self, mid: usize) -> (&mut [T], &mut [T]) {
	let len = self.len();
	let ptr = self.as_mut_ptr();

	assert!(mid <= len);

	unsafe {
		(std::slice::from_raw_parts_mut(ptr, mid),
		std::slice::from_raw_parts_mut(ptr.add(mid), len - mid))
	}
}

The unsafe block constructs two mutable slices from raw pointers. We assert that the slices do not overlap.
The safe interface guarantees this by construction: the slices cover [0, mid) and [mid, len). This is a very
Rusty pattern: building safe abstractions over unsafe primitives. Users of split_at_mut cannot violate the
aliasing rules. The borrow checker verifies that the two returned slices are used correctly.

From Rules to Registers
We saw that aliasing information lets the compiler keep values in registers instead of reloading from memory.
But the payoff extends beyond eliminating redundant loads. Consider what happens when the compiler tries to
vectorize a loop.

void scale(float* dest, const float* src, float factor, size_t n) {
	for (size_t i = 0; i < n; i++) {
		dest[i] = src[i] * factor;
	}
}

If dest and src might overlap, each iteration depends on the previous. A write to dest[i] might modify src
[i+1]. The compiler must execute iterations sequentially:

.loop:
    movss   xmm1, [rsi]           ; load one float from src
    mulss   xmm1, xmm0            ; multiply by factor
    movss   [rdi], xmm1           ; store one float to dest
    add     rsi, 4
    add     rdi, 4
    dec     rcx
    jnz     .loop

One element per iteration. Almost any modern x86-64 CPU has 256-bit AVX registers that can hold eight floats.
We are using 32 bits of that capacity. The other 224 bits sit idle.

Add restrict to promise non-overlap:

void scale(float* restrict dest, const float* restrict src, float factor, size_t n) {
	for (size_t i = 0; i < n; i++) {
		dest[i] = src[i] * factor;
	}
}

Now the compiler knows iterations are independent:

    vbroadcastss ymm0, xmm0       ; broadcast factor to all 8 lanes
.loop:
    vmovups ymm1, [rsi]           ; load 8 floats from src
    vmulps  ymm1, ymm1, ymm0      ; multiply all 8
    vmovups [rdi], ymm1           ; store 8 floats to dest
    add     rsi, 32
    add     rdi, 32
    sub     rcx, 8
    jnz     .loop

Eight elements per iteration. For large arrays, this can approach an 8x speedup in some workloads.

In Rust, the equivalent function:

fn scale(dest: &mut [f32], src: &[f32], factor: f32) {
	for (d, s) in dest.iter_mut().zip(src.iter()) {
		*d = *s * factor;
	}
}

The signature encodes the non-aliasing constraint. The borrow checker verifies at call sites that dest and
src do not overlap. The compiler passes noalias to LLVM, and LLVM generates the same vectorized loop.

In C, restrict is a promise we can break. In Rust, the borrow checker enforces it. The generated code is
identical. The safety guarantee is not.

This is why aliasing rules exist. They are the information the optimizer needs to use the hardware
effectively. C provides this through type rules and programmer annotations. Rust provides it through static
analysis. The CPU does not care which language we used. It cares whether the instructions match the actual
memory access patterns.


---
https://lukefleed.xyz/posts/who-owns-the-memory-pt2/

Who Owns the Memory? Part 2: Who Calls Free?
24 Dec, 2025

In the first part of this series, we saw that objects occupy storage, that storage has duration, and that the
type system imposes structure on raw bytes. But we have sidestepped a question that dominates systems
programming in practice: when heap-allocated memory must be released, who bears responsibility for releasing
it?

Ownership: Who Calls Free?
The stack is self-managing. When a function returns, the stack pointer moves and automatic storage vanishes.
There is no decision to make, no function to call, no possibility of error. The heap is different. Memory
obtained through malloc persists until someone calls free. The allocator cannot know when we are finished
with an allocation; only our program logic knows that. And so the burden falls on us.

This burden carries severe consequences, often exploitable ones. If we free too early, subsequent accesses
read garbage or, worse, data that a new allocation has placed there (a use-after-free, the vulnerability
class behind a substantial fraction of remote code execution exploits). If we free twice, we corrupt the
allocator's metadata; an attacker who controls the timing can often leverage this into arbitrary write
primitives. If we never free, we leak, and the process grows until the operating system intervenes.

The question is how programming languages help us manage this responsibility, or whether they help at all.

C: Manual Management
C offers two primitives: malloc to acquire and free to release. Everything else is convention.

Consider a function that opens a file and allocates a read buffer:

typedef struct {
	int fd;
	char *buffer;
	size_t capacity;
} FileReader;

FileReader *filereader_open(const char *path, size_t buffer_size) {
	FileReader *reader = malloc(sizeof(FileReader));
	if (!reader) return NULL;

	reader->buffer = malloc(buffer_size);
	if (!reader->buffer) {
		free(reader);
		return NULL;
	}

	reader->fd = open(path, O_RDONLY);
	if (reader->fd < 0) {
		free(reader->buffer);
		free(reader);
		return NULL;
	}

	reader->capacity = buffer_size;
	return reader;
}

We acquire three resources: the struct itself, the buffer, and the file descriptor. Each acquisition can
fail, and each failure path must release everything acquired before it. The code above handles this
correctly. But observe the shape: the cleanup logic mirrors the acquisition logic in reverse, and we must
write it by hand for every function that acquires resources.

The corresponding cleanup function:

void filereader_close(FileReader *reader) {
	if (!reader) return;
	if (reader->fd >= 0) close(reader->fd);
	free(reader->buffer);
	free(reader);
}

What happens if we call filereader_close twice on the same pointer? The first call closes the descriptor and
frees the memory. The second call passes the same address to free, corrupting the allocator's free list. The
compiler does not warn us.

What happens if, between filereader_open and filereader_close, we reassign reader->buffer without freeing the
old buffer? We leak. The original allocation becomes unreachable.

The fundamental problem is that C pointers carry no ownership semantics. When a function declares void
process(FileReader *reader), nothing in that signature tells us whether process will free the reader, expects
us to free it afterward, or assumes it will remain valid for some longer duration. The type FileReader *
means only "address of a FileReader"; it says nothing about responsibility.

Large C codebases develop conventions to manage this. The Linux kernel uses reference counting for shared
structures, with _get and _put suffixes indicating acquisition and release. GLib uses _new for allocation,
_free for deallocation, and _ref/_unref for reference-counted objects. These conventions work, but they are
conventions, patterns enforced by code review rather than by the compiler. Every violation is a latent bug.

C++: Binding Cleanup to Scope
C++ exploits a property that C has but does not leverage: local variables have a well-defined scope, and when
that scope ends, the variable ceases to exist. The language allows us to attach custom cleanup logic to that
moment through destructors.

A destructor is a special member function, denoted ~ClassName(), that the compiler calls automatically when
an object's lifetime ends. The call is not optional. It happens regardless of how control flow exits the
scope: normal return, early return, exception propagation.

class FileReader {
	public:
		explicit FileReader(const char *path, size_t buffer_size)
			: buffer_(new char[buffer_size])
			, capacity_(buffer_size)
			, fd_(::open(path, O_RDONLY))
	{
		if (fd_ < 0) {
			delete[] buffer_;
			throw std::system_error(errno, std::generic_category());
		}
	}

		~FileReader() {
			if (fd_ >= 0) ::close(fd_);
			delete[] buffer_;
		}

		FileReader(const FileReader &) = delete;
		FileReader &operator=(const FileReader &) = delete;

	private:
		char *buffer_;
		size_t capacity_;
		int fd_;
};

When we write:

void process_file(const char *path) {
	FileReader reader(path, 4096);
	do_something(reader);
}

the compiler generates an implicit call to ~FileReader() at the closing brace, regardless of whether
do_something returns normally or throws. We did not write this call; the language guarantees it.

The destruction sequence is precise. After executing the destructor body and destroying any automatic objects
declared within it, the compiler destroys all non-static data members in reverse order of their declaration,
then all direct base classes in reverse order of construction. This reverse ordering matters: later-declared
members may depend on earlier ones, so we tear down in the opposite order we built up.

Consider what this means for exception safety. If any operation between resource acquisition and release
throws, the destructor still runs during stack unwinding. We do not need explicit cleanup code on every exit
path. The resource management logic is written once, in the destructor, and the compiler inserts the call at
every point where it is needed.

The standard library provides RAII wrappers for common resources: std::unique_ptr for exclusive ownership of
heap memory, std::shared_ptr for reference-counted shared ownership, std::lock_guard for mutexes,
std::fstream for files. Using these types, we rarely write new or delete directly.

But RAII in C++ is opt-in. Nothing prevents us from writing:

void leaky() {
	int *p = new int[1000];
	// forgot delete[]
}

Nothing prevents extracting a raw pointer and misusing it:

void dangling() {
	int *raw;
	{
		auto owner = std::make_unique<int>(42);
		raw = owner.get();
	}  // owner destroyed here
	*raw = 10;  // use-after-free
}

The compiler does not track which pointers own and which merely observe. We can bypass RAII entirely with raw
new and delete. We can hold raw pointers past their owners' lifetimes. We can delete through a base class
pointer when the destructor is not virtual, which is undefined behavior even if no resources would leak,
because the derived destructor never runs.

C++ gives us the machinery for safe resource management. Using that machinery is a choice the language cannot
enforce. A codebase mixing raw pointers, unique_ptr, shared_ptr, and manual new/delete requires reasoning
about ownership at every function boundary. The answer lives in programmers' heads, in comments, in coding
standards. This is better than C, where even the machinery does not exist. But it remains insufficient to
eliminate memory safety bugs from large codebases.

Rust: Ownership in the Type System
Rust takes the RAII pattern and embeds it into the type system as a non-negotiable rule: every value has
exactly one owner, and when that owner goes out of scope, the value is dropped. The compiler verifies this
property statically, and no amount of programmer intent can bypass it.

Consider what happens when we allocate a vector:

fn example() {
	let v = vec![1, 2, 3];
}

The binding v owns the heap-allocated buffer. When v goes out of scope at the closing brace, Rust calls drop
on the Vec, which deallocates the buffer. We cannot accidentally forget to free the memory.

The critical mechanism is the move. When we assign a value to another binding, ownership transfers:

let v1 = vec![1, 2, 3];
let v2 = v1;

After this assignment, v1 is no longer valid. Any attempt to use it is a compile-time error. This is not a
shallow copy followed by invalidation of the source, as in C++ move semantics where the moved-from object
remains in a "valid but unspecified state" that we can still inspect and call methods on. In Rust, ownership
transfer means the source binding ceases to exist as far as the type system is concerned. The compiler marks
it as uninitialized. There is no moved-from state.

Why does this matter? Consider what would happen without this rule. If both v1 and v2 were valid after the
assignment, both would attempt to free the same buffer when they went out of scope. The move rule makes
double-free impossible: exactly one binding owns the allocation at any time, and exactly one drop occurs.

The same logic applies to function calls. When we pass a value to a function, ownership transfers to the
function's parameter:

fn consume(v: Vec<i32>) {
	// v is owned here; dropped at end of function
}

fn main() {
	let data = vec![1, 2, 3];
	consume(data);
	// data is no longer valid here
}

The function signature fn consume(v: Vec<i32>) declares that consume takes ownership. The caller cannot use
data after the call because ownership has moved. Compare this to fn borrow(v: &Vec<i32>), which borrows
without taking ownership, or fn mutate(v: &mut Vec<i32>), which borrows mutably. The type encodes the
ownership relationship.

The Drop Trait and Recursive Destruction
When a value goes out of scope, Rust runs its destructor. For types that implement the Drop trait, this means
calling the drop method:

struct FileHandle {
	fd: i32,
}

impl Drop for FileHandle {
	fn drop(&mut self) {
		unsafe { libc::close(self.fd); }
	}
}

The drop method receives &mut self, not self. We cannot move out of self during drop because drop takes a
mutable reference. This prevents us from returning the inner file descriptor to avoid closing it. When drop
returns, the value is gone.

After drop executes, Rust recursively drops all fields of the struct. This is automatic and unavoidable. If
we have:

struct Connection {
	socket: TcpStream,
	buffer: Vec<u8>,
}

and Connection does not implement Drop, Rust still drops socket and buffer when a Connection goes out of
scope. The compiler generates this "drop glue" for every type that needs it. We do not write boilerplate to
drop children; the language handles it. If Connection does implement Drop, Rust first calls our drop method,
then drops the fields. We cannot prevent the recursive field drops. After our drop returns, the fields will
be dropped regardless of what we did.

The destruction order is deterministic and specified by the language. For local variables, drop order is the
reverse of declaration order: later declarations are dropped first. The rationale is that later variables may
hold references to earlier ones, so we must destroy the borrowers before the borrowed. For struct fields,
drop order is declaration order (not reversed). For tuples, elements drop in order. For arrays, elements drop
from index 0 to the end. For enums, only the active variant's fields are dropped. Closure captures by move
are dropped in an unspecified order, so if destruction order among captured values matters, do not rely on
closure drop order.

ManuallyDrop and Suppressing Automatic Destruction
The recursive drop behavior creates a problem when we need fine-grained control over destruction. Consider a
type that wraps a Box and wants to deallocate the contents in a custom way:

struct SuperBox<T> {
	my_box: Box<T>,
}

impl<T> Drop for SuperBox<T> {
	fn drop(&mut self) {
		unsafe {
			// Deallocate the box's contents ourselves
			let ptr = Box::into_raw(self.my_box);  // ERROR: cannot move out of &mut self
			std::alloc::dealloc(ptr as *mut u8, Layout::new::<T>());
		}
	}
}

This does not compile. We cannot move self.my_box out of &mut self. And even if we could, after our drop
returns, Rust would try to drop my_box again, causing a double-free.

One solution is Option:
struct SuperBox<T> {
	my_box: Option<Box<T>>,
}

impl<T> Drop for SuperBox<T> {
	fn drop(&mut self) {
		if let Some(b) = self.my_box.take() {
			// Handle b ourselves; self.my_box is now None
			// When Rust drops self.my_box, it drops None, which does nothing
		}
	}
}

This works, but it pollutes our type with Option semantics. A field that should always be Some must be
declared as Option solely because of what happens in the destructor. Every access to my_box elsewhere in the
code must handle the None case that should never occur outside of drop.

ManuallyDrop<T> provides a cleaner solution. It is a wrapper that suppresses automatic drop for its contents:

use std::mem::ManuallyDrop;

struct SuperBox<T> {
	my_box: ManuallyDrop<Box<T>>,
}

impl<T> Drop for SuperBox<T> {
	fn drop(&mut self) {
		unsafe {
			// Take ownership of the inner Box
			let b = ManuallyDrop::take(&mut self.my_box);
			// Now we own b and can do whatever we want
			// When our drop returns, Rust will "drop" self.my_box,
			// but ManuallyDrop's drop is a no-op
		}
	}
}

ManuallyDrop<T> has the same size and alignment as T. It implements Deref and DerefMut, so we can use the
wrapped value normally. But when Rust drops a ManuallyDrop<T>, nothing happens. The inner T is not dropped.
We are responsible for dropping it manually via ManuallyDrop::drop(&mut x) or taking ownership via
ManuallyDrop::take(&mut x).

This is useful beyond custom destructors. When we need to move a value out of a context where Rust would
normally drop it, ManuallyDrop lets us suppress that drop and handle the value ourselves. The unsafe is
required because we are taking responsibility for ensuring the value is eventually dropped (or intentionally
leaked).

Drop Flags and Conditional Initialization
Rust tracks initialization state at compile time when possible. But consider:

let x;
if condition {
	x = Box::new(0);
}

At the end of scope, should Rust drop x? It depends on whether condition was true. When the compiler cannot
determine initialization state statically, it inserts a runtime drop flag: a hidden boolean that tracks
whether the value was initialized. At scope exit, Rust checks the flag before calling drop.

For straight-line code and branches that initialize consistently, the compiler can eliminate these flags
through static analysis. The flags exist only when genuinely necessary, and the generated code checks them
only at points where the initialization state is ambiguous.

You Cannot Call Drop Directly

Rust prevents explicit calls to the Drop::drop method:

let v = vec![1, 2, 3];
v.drop();  // error: explicit use of destructor method

If we could call drop directly, the value would still be in scope afterward. When the scope ends, Rust would
call drop again. Instead, we use std::mem::drop for early cleanup:

let v = vec![1, 2, 3];
drop(v);  // v is moved into drop() and dropped there

The drop function takes ownership by value: fn drop<T>(_: T) {}. The value moves into the function and is
dropped when the function returns. Since ownership moved, the original binding is invalid and no second drop
occurs.

Drop Check: When Lifetimes and Destructors Collide
We now arrive at a subtle interaction between Rust's lifetime system and its destructor semantics. Consider
this seemingly innocuous code:

struct Inspector<'a>(&'a u8);

struct World<'a> {
	inspector: Option<Inspector<'a>>,
	days: Box<u8>,
}

fn main() {
	let mut world = World {
		inspector: None,
		days: Box::new(1),
	};
	world.inspector = Some(Inspector(&world.days));
}

This compiles. The Inspector holds a reference to days, both are fields of World, and when world goes out of
scope, both are dropped. The fact that days does not strictly outlive inspector does not matter here because
neither has a destructor that could observe the other.

But watch what happens when we add a Drop impl to Inspector:

struct Inspector<'a>(&'a u8);

impl<'a> Drop for Inspector<'a> {
	fn drop(&mut self) {
		println!("I was only {} days from retirement!", self.0);
	}
}

struct World<'a> {
	inspector: Option<Inspector<'a>>,
	days: Box<u8>,
}

fn main() {
	let mut world = World {
		inspector: None,
		days: Box::new(1),
	};
	world.inspector = Some(Inspector(&world.days));
}

This does not compile:

error[E0597]: `world.days` does not live long enough

What changed? The Drop implementation. When Inspector has a destructor, that destructor might access the
reference it holds. If days is dropped before inspector, the destructor would dereference freed memory. The
borrow checker must now enforce that any data borrowed by Inspector outlives the Inspector itself, because
the destructor could observe that data.

This is the drop checker (dropck). It enforces a stricter rule when types have destructors: for a generic
type to soundly implement drop, its generic arguments must strictly outlive it. Not live at least as long as,
but strictly outlive. The difference is subtle. Without a destructor, two values can go out of scope
simultaneously because neither observes the other during destruction. With a destructor, the type with the
destructor might observe its borrowed data, so that data must still be valid when the destructor runs.

Only generic types need to worry about this. If a type is not generic, the only lifetimes it can contain are
'static, which truly lives forever. The problem arises when a type is generic over a lifetime or a type
parameter, and it implements Drop, and that Drop could potentially access the borrowed data.

The #[may_dangle] Escape Hatch
The drop checker is conservative. It assumes that any Drop impl for a generic type might access data of the
generic parameter. But this is often not true. Consider Vec<T>:

impl<T> Drop for Vec<T> {
	fn drop(&mut self) {
		// Deallocate the buffer
		// We DO drop each T element, but we don't "use" T in the sense
		// of accessing references that T might contain
	}
}

When we drop a Vec<&'a str>, we deallocate the buffer. We do call drop on each &'a str element, but &'a str
has no destructor; its drop is a no-op. The Vec's drop does not actually dereference those &'a str values. It
does not read the strings. It just deallocates the backing memory.

Yet the drop checker does not know this. It sees impl<'a> Drop for Vec<&'a str> and concludes that 'a must
strictly outlive the Vec. This prevents code like:

fn main() {
	let mut v: Vec<&str> = Vec::new();
	let s: String = "Short-lived".into();
	v.push(&s);
	drop(s);  // s dropped while v still holds a reference
}

This is correctly rejected. But it also rejects:

fn main() {
	let mut v: Vec<&str> = Vec::new();
	let s: String = "Short-lived".into();
	v.push(&s);
}  // s and v dropped here, but in what order?

The second example should be fine. Both v and s are dropped at the end of main. Variables are dropped in
reverse declaration order, so v drops first, then s. By the time s drops, v is already gone. The references
in v are never dereferenced during v's destruction.

To allow this pattern, Rust provides an unstable, unsafe attribute: #[may_dangle]. It tells the drop checker
"I promise not to access this generic parameter in my destructor":

unsafe impl<#[may_dangle] T> Drop for Vec<T> {
	fn drop(&mut self) {
		// ...
	}
}

The #[may_dangle] on T is a promise that the Drop impl does not access T values in a way that requires them
to be valid. The drop checker relaxes its requirements: T may now dangle (be a reference to freed memory)
when the Vec is dropped.

But this is a lie, or at least an incomplete truth. Vec<T>'s destructor does drop each T element. If T has a
destructor that accesses borrowed data, that data must still be valid. The promise is more precisely: "I do
not access T myself, but I may trigger T's destructor." This distinction matters for the soundness of the
overall system.

PhantomData and Drop Check Interaction
When we use #[may_dangle], we are opting out of the drop checker's conservative assumptions. But we must opt
back in for cases where we do transitively drop T. This is where PhantomData enters the picture.

Consider the actual implementation of Vec:

struct Vec<T> {
	ptr: *const T,	// raw pointer, no ownership semantics
	len: usize,
	cap: usize,
}

A raw pointer *const T does not imply ownership. The drop checker does not assume that Vec owns T values just
because it contains a *const T. If we write:

unsafe impl<#[may_dangle] T> Drop for Vec<T> {
	fn drop(&mut self) {
		// drop elements, deallocate buffer
	}
}

we have told the drop checker that T may dangle. But Vec does own T values and does drop them. If T is
PrintOnDrop<'a> (a type with a destructor that dereferences 'a), then 'a must be valid when Vec drops,
because Vec will invoke T's destructor.

We need to tell the drop checker: "T may dangle, except if T itself has drop glue that would observe borrowed
data." The mechanism for this is PhantomData<T>:

use std::marker::PhantomData;

struct Vec<T> {
	ptr: *const T,
	len: usize,
	cap: usize,
	_marker: PhantomData<T>,
}

PhantomData<T> is a zero-sized type that tells the compiler "act as if this struct owns a T." It affects
variance, auto-trait inference, and critically, drop check. When the drop checker sees PhantomData<T> in a
struct, it knows that dropping the struct may involve dropping T values.

The interaction is:
 1. #[may_dangle] T on the Drop impl says "I promise not to access T directly in my destructor."
 2. PhantomData<T> in the struct says "but I do own T values and will drop them."
 3. The drop checker combines these: T itself may dangle (its references may be invalid), but if T has drop
    glue, that glue will run, and whatever T's glue accesses must still be valid.

This is subtle. Suppose T is &'a str. The type &'a str has no destructor (dropping a reference is a no-op).
So PhantomData<&'a str> does not introduce additional constraints. The #[may_dangle] applies fully, and 'a
may dangle.

Now suppose T is PrintOnDrop<'a>, a type with a destructor that dereferences 'a. The PhantomData<PrintOnDrop
<'a>> tells the drop checker that Vec will drop PrintOnDrop<'a> values. The #[may_dangle] says Vec itself
won't access 'a. But dropping PrintOnDrop<'a> will access 'a. So 'a must still be valid when Vec is dropped,
despite the #[may_dangle].

The rules compose correctly. The #[may_dangle] is not a blanket permission to let everything dangle. It is
permission for the specific Drop impl to not access the parameter, combined with the PhantomData indicating
that transitive drops may still occur.

Without #[may_dangle], implementing a collection like Vec would be unnecessarily restrictive. Every Vec<&'a
T> would require 'a to strictly outlive the Vec, even when the Vec is dropped before the borrowed data goes
out of scope. The combination of #[may_dangle] and PhantomData allows the standard library to express the
precise ownership semantics: "we will drop T values, but we do not otherwise observe them in our destructor."

Leaks Are Safe
Here we encounter a design decision that surprises many: leaking memory is safe in Rust. The function
std::mem::forget takes ownership of a value and does not run its destructor:

let v = vec![1, 2, 3];
std::mem::forget(v);
// v's destructor never runs; the heap allocation is leaked

This is a safe function. It does not require unsafe. The reasoning is that safe in Rust means cannot cause
undefined behavior. Leaking memory is wasteful, but it does not corrupt memory, create dangling pointers, or
cause data races. A program that leaks is buggy but not undefined.

Moreover, leaks can occur in safe code without calling forget. The simplest example is a reference cycle with
Rc:

use std::cell::RefCell;
use std::rc::Rc;

struct Node {
	next: Option<Rc<RefCell<Node>>>,
}

fn create_cycle() {
	let a = Rc::new(RefCell::new(Node { next: None }));
	let b = Rc::new(RefCell::new(Node { next: Some(a.clone()) }));
	a.borrow_mut().next = Some(b.clone());
}

When create_cycle returns, the Rcs go out of scope, but each has a reference count of 2 due to the cycle. The
count decrements to 1, not 0. The nodes are never freed. This is safe code with no unsafe blocks, and it
leaks.

Since leaks are possible in safe code, the language cannot assume destructors always run. This has profound
implications for unsafe code. Any type whose safety invariants depend on the destructor running is unsound in
the presence of mem::forget or cycles.

The standard library learned this lesson with thread::scoped, an API that allowed spawning threads
referencing stack data. It relied on a guard whose destructor joined the thread:

pub fn scoped<'a, F>(f: F) -> JoinGuard<'a>
where F: FnOnce() + Send + 'a

The guard's lifetime tied it to the borrowed data. When the guard dropped, it joined the thread, ensuring the
thread finished before the data went out of scope. But mem::forget(guard) prevented the destructor from
running. The thread would continue executing while the stack data it referenced was freed. Use-after-free
from safe code. The API was unsound and had to be removed.

The correct design principle is that unsafe code cannot rely on destructors running to maintain safety
invariants. Safe abstractions must account for the possibility that destructors are skipped. The standard
library's Vec::drain is a good example. Draining a vector moves elements out one at a time. If the drain
iterator is forgotten mid-iteration, some elements have been moved out and the vector's length is wrong.
Rather than leaving the vector in an inconsistent state, Drain sets the vector's length to zero at the start
of iteration. If Drain is forgotten, the remaining elements leak (their destructors do not run, their memory
is not reused), but the vector is in a valid state (empty). Leaks amplify leaks, but undefined behavior does
not occur.

Beyond RAII
We have built a comprehensive picture of ownership. Rust makes ownership tracking mandatory and statically
verified. Yet even Rust's model has limits.

The Single-Destructor Problem
The RAII model, whether in C++ or Rust, shares a structural limitation: the destructor is a single,
parameterless function that returns nothing. When an object goes out of scope, exactly one action occurs. We
cannot choose between alternatives. We cannot pass runtime information to the cleanup logic. We cannot
receive a result from it.

For many resources this constraint is invisible. A file handle has one sensible cleanup action: close the
descriptor. A heap allocation has one cleanup action: free the memory. A mutex guard has one cleanup action:
unlock the mutex. The destructor does the obvious thing, and the single-destructor model works well.

But consider a database transaction. A transaction must eventually either commit (make changes permanent) or
rollback (discard changes). These are fundamentally different operations with different semantics, different
failure modes, and often different parameters. A commit might require a priority level. A rollback might need
to log the reason for abandonment. The destructor cannot accommodate this. It must pick one.

The standard workaround in C++ and Rust is to default to rollback and provide an explicit commit method:

class Transaction {
	public:
		explicit Transaction(Database& db) : db_(db), committed_(false) {
			db_.begin();
		}

		void commit() {
			db_.commit();
			committed_ = true;
		}

		~Transaction() {
			if (!committed_) {
				db_.rollback();
			}
		}

	private:
		Database& db_;
		bool committed_;
};

We call commit() when the transaction succeeds and let the destructor rollback on error paths or early
returns. Exception safety falls out naturally; if an exception propagates, the destructor runs, and
uncommitted transactions rollback.

The problem is that forgetting to call commit() is not a compile-time error. If we write a function that
successfully completes its work but neglects to call commit(), the destructor happily rolls back. The program
is wrong, but the compiler cannot tell us. We have traded one category of bug (forgetting to cleanup) for
another (forgetting to finalize). The second category is arguably more insidious because the cleanup happens,
silently doing the wrong thing.

Rust's ownership system does not help here:

struct Transaction<'a> {
	db: &'a mut Database,
	committed: bool,
}

impl<'a> Transaction<'a> {
	fn commit(mut self) {
		self.db.commit();
		self.committed = true;
	}
}

impl<'a> Drop for Transaction<'a> {
	fn drop(&mut self) {
		if !self.committed {
			self.db.rollback();
		}
	}
}

The commit method takes self by value, consuming the transaction. After calling commit, the binding is gone.
But if we never call commit, the transaction goes out of scope, drop runs, and we rollback. No compiler
error. The type system tracked ownership but not obligation.

Defer: Explicit Scope-Bound Cleanup
Some languages take a different approach. Rather than binding cleanup to object destruction, they provide
explicit defer statements that execute at scope exit.

Zig's defer runs an expression unconditionally when control leaves the enclosing block:

fn processFile(path: []const u8) !void {
	const file = try std.fs.cwd().openFile(path, .{});
	defer file.close();

	const buffer = try allocator.alloc(u8, 4096);
	defer allocator.free(buffer);

	// work with file and buffer
	// both are cleaned up when we exit, regardless of how
}

The cleanup code sits immediately after the acquisition code. We see allocation and deallocation together,
which aids comprehension. Zig extends this with errdefer, which executes only if the function returns an
error, separating error-path cleanup from success-path transfer.

The defer model has a structural limitation that RAII does not: the cleanup is scope-bound. We cannot return
a resource to our caller the way we return an RAII object. The defer runs when the current scope exits,
period. RAII is more flexible; we can return an RAII object, store it in a data structure, transfer ownership
to another thread. The cleanup travels with the object. Defer is local; RAII is transferable.

But defer has an advantage in simplicity. We do not need to define a type, implement a trait, worry about
drop order among struct fields. We write the cleanup code inline, at the point of acquisition. For resources
that do not leave the current function, defer is often cleaner.

Linear Types: Enforcing Explicit Consumption
The transaction example shows a gap in RAII's guarantees. RAII ensures that cleanup happens, but not that we
made an explicit decision about which cleanup to perform. The destructor chooses for us, silently.

Linear types close this gap. A linear type must be explicitly consumed; it cannot simply go out of scope. If
we try to let a linear value fall out of scope without passing it to a consuming function, the compiler
rejects the program.

Consider a hypothetical extension to Rust:

// Hypothetical syntax
#[linear]
struct Transaction { db: Database }

fn commit(txn: Transaction) -> Result<(), Error> {
	txn.db.commit()?;
	destruct txn;  // explicitly consume
	Ok(())
}

fn rollback(txn: Transaction, reason: &str) {
	txn.db.rollback();
	destruct txn;  // explicitly consume
}

fn do_work(db: Database) {
	let txn = Transaction { db };
	// ERROR: `txn` goes out of scope without being consumed
	// must call either commit(txn) or rollback(txn, ...)
}

The transaction cannot be ignored. Forgetting to decide is a compile-time error, not a runtime silent
rollback. Languages with linear types (Vale, Austral, and to some extent Haskell with LinearTypes) can
express patterns that RAII cannot.

Why Rust Does Not Have Linear Types
Rust's types are affine, not linear. An affine type can be used at most once; a linear type must be used
exactly once. The difference matters when panics unwind the stack.

Rust permits silent dropping because Drop::drop must always be callable. When a scope exits, when a panic
unwinds, when we reassign a variable, Rust calls drop. The entire language assumes that any value can be
dropped at any time.

Linear types would break this assumption. If a value must be explicitly consumed, what happens when a panic
unwinds through a function holding that value? The unwinding code cannot know which consuming function to
call, what parameters to pass, what to do with the return value. Every generic container, every iterator
adapter, every function that might discard a value would need to be reconsidered.

Rust chose affine types, accepting that the compiler cannot enforce explicit consumption but gaining a
simpler model where any value can be dropped. The #[must_use] attribute provides a weaker guarantee: a
warning (not an error) if a value is unused. It catches some mistakes but does not provide the hard guarantee
that linear types would.

When Things Go Wrong
RAII ensures cleanup happens when scope ends. But it says nothing about how we signal failures. When a
function cannot complete its task, it must communicate this to the caller, and the caller must have a chance
to respond. The three languages take fundamentally different approaches to this problem, and the differences
reveal deep assumptions about what error handling should look like.

C: Return Codes and errno
C library functions indicate failure through their return values, but the convention varies by function and
must be looked up individually. The C standard defines several patterns. Functions like fopen return a null
pointer on failure; the null serves as an out-of-band sentinel that cannot be confused with a valid result.
Functions like puts and fclose return EOF, a special error code, on failure. Functions like fgetpos and
fsetpos return a nonzero value on failure, where zero indicates success and the actual return value is
otherwise unneeded. Functions like thrd_create return a special success code, and any other value indicates a
specific failure condition. Functions like printf return a negative value on failure, where success produces
a positive count.

if (puts("hello world") == EOF) {
	perror("can't output to terminal:");
	exit(EXIT_FAILURE);
}

The perror function prints a diagnostic message based on the current value of errno, a thread-local variable
that library functions set when they fail. The combination of checking the return value and consulting errno
provides both detection and diagnosis. But errno is fragile. It must be checked immediately after the failing
call. Any intervening function call, even a successful one, may modify errno. If we need to preserve error
information across other operations, we must copy it:

FILE *f = fopen(path, "r");
if (!f) {
	int saved = errno;
	log_message("fopen failed");   // might modify errno
	errno = saved;
	return -1;
}

The more serious problem is propagation. Consider a function that opens a file, allocates a buffer, reads
data, and processes it. Each step can fail. Each failure requires releasing resources acquired in previous
steps:

int process_file(const char *path) {
	FILE *f = fopen(path, "r");
	if (!f) return -1;

	char *buf = malloc(4096);
	if (!buf) {
		fclose(f);
		return -1;
	}

	if (fread(buf, 1, 4096, f) == 0 && ferror(f)) {
		free(buf);
		fclose(f);
		return -1;
	}

	// process buf...

	free(buf);
	fclose(f);
	return 0;
}

The cleanup code is duplicated at each error site. If we add a fourth resource, we must update three error
paths. The duplication invites mistakes. Miss one cleanup and we leak.

The goto statement consolidates cleanup into a single location:

int process_file(const char *path) {
	int result = -1;
	FILE *f = NULL;
	char *buf = NULL;

	f = fopen(path, "r");
	if (!f) goto cleanup;

	buf = malloc(4096);
	if (!buf) goto cleanup;

	if (fread(buf, 1, 4096, f) == 0 && ferror(f)) goto cleanup;

	// process buf...
	result = 0;

cleanup:
	free(buf);
	if (f) fclose(f);
	return result;
}

All resources must be initialized to null at the top. The cleanup block must handle partially-initialized
state; free(NULL) is defined to do nothing, but fclose(NULL) is undefined behavior, hence the explicit check.
The pattern requires discipline. Reviewers must verify that every resource acquired before a goto is released
in the cleanup block, and that the cleanup block handles every possible partial state.

For errors that cannot be handled locally, C provides setjmp and longjmp. The setjmp macro marks a location
in the code; longjmp transfers control directly to that location, unwinding the call stack without executing
intervening code. This is C's mechanism for non-local jumps, used when an error deep in a call chain must
abort the entire operation:

#include <setjmp.h>

jmp_buf error_handler;

void deep_function(void) {
	if (catastrophic_failure()) {
		longjmp(error_handler, 1);  // never returns
	}
}

int main(void) {
	if (setjmp(error_handler) != 0) {
		// longjmp landed here
		fprintf(stderr, "operation aborted\n");
		return EXIT_FAILURE;
	}

	deep_function();
	return EXIT_SUCCESS;
}

The longjmp function never returns to its caller. Control transfers directly to the setjmp site as if setjmp
had just returned, but with a nonzero value indicating which longjmp triggered. This bypasses all
intermediate stack frames. Local variables in those frames are not cleaned up; their destructors (in the C
sense of cleanup code we might have written) do not run. Resources acquired between setjmp and longjmp leak
unless we manually track and release them. The jmp_buf must remain valid when longjmp is called; if the
function containing setjmp has returned, the behavior is undefined.

An int return code provides limited information. We can distinguish success from failure, and errno codes
like ENOENT or EACCES give some context, but rich error information requires either out-parameters, custom
error structs, or global state.

C++: Exceptions and Hidden Control Flow
C++ exceptions address the propagation problem directly. When a function cannot fulfill its contract, it
executes throw with an exception object. Control transfers immediately to the nearest enclosing catch block
whose type matches the thrown object, skipping all intermediate code but calling destructors along the way.

std::string read_file(const std::string& path) {
	std::ifstream f(path);
	if (!f) {
		throw std::runtime_error("cannot open: " + path);
	}
	std::stringstream buf;
	buf << f.rdbuf();
	return buf.str();
}

void process() {
	try {
		auto content = read_file("data.txt");
		// use content...
	} catch (const std::runtime_error& e) {
		std::cerr << e.what() << '\n';
	}
}

The try block delimits code where exceptions may be caught. The catch clause specifies a type; if the thrown
object's type matches (including derived classes), that handler executes. Multiple catch clauses can handle
different exception types; the runtime tries them in order and executes the first match. Catching by const
reference avoids copying and preserves polymorphism for exception hierarchies.

When an exception propagates through a scope, the compiler ensures that destructors run for all local objects
in reverse order of construction. This is stack unwinding. RAII objects release their resources automatically
on error paths:

void safe_operation() {
	auto resource = std::make_unique<Widget>();
	might_throw();
}   // resource destroyed whether or not might_throw() throws

The destructor runs regardless of how we exit the scope. We wrote no cleanup code; the compiler inserted it.

Here lies the problem with exceptions: every function call is a potential exit point. In the code above, if
might_throw() throws, control leaves safe_operation immediately. This is convenient when we want automatic
cleanup. It is treacherous when we are maintaining invariants across multiple operations.

void transfer(Account& from, Account& to, int amount) {
	from.withdraw(amount);  // might throw
	to.deposit(amount);     // might throw
}

If deposit throws after withdraw succeeds, the money has left from but never arrived at to. The program is in
an inconsistent state. The strong exception guarantee (if an operation fails, state is unchanged) requires
explicit effort:

void transfer(Account& from, Account& to, int amount) {
	int withdrawn = from.withdraw(amount);  // might throw
	try {
		to.deposit(amount);
	} catch (...) {
		from.deposit(withdrawn);  // rollback
		throw;
	}
}

Now we are back to writing manual error handling, but with a twist: the error paths are invisible. Looking at
a function that calls other functions, we cannot tell which calls might throw without examining their
declarations (or their transitive callees). The C code with goto cleanup was verbose, but every exit point
was visible in the source.

Destructors must not throw. Since C++11, destructors are implicitly noexcept. If a destructor throws during
stack unwinding from another exception, the program has two active exceptions with no way to handle both;
std::terminate is called. This constraint shapes class design: destructors perform only operations that
cannot fail, or they catch and suppress exceptions internally.

The noexcept specifier declares that a function does not throw. If it does throw, std::terminate is called
rather than unwinding. Move constructors and move assignment operators should be noexcept when possible; this
enables std::vector to move elements during reallocation rather than copying them. If a move constructor
might throw, the vector must copy to preserve the strong exception guarantee. Copying is dramatically slower
for types with expensive copy operations. The performance of your vector depends on whether your move
constructor is marked noexcept.

The exception safety guarantees classify behavior when exceptions occur. The nothrow guarantee means the
function never throws; destructors and swap functions provide this. The strong guarantee means if an
exception is thrown, the program state is unchanged; std::vector::push_back provides this. The basic
guarantee means if an exception is thrown, the program is in a valid state with no leaks, but the state may
have changed. Every function in a C++ codebase implicitly has one of these guarantees, whether the author
thought about it or not.

C++23: std::expected and the Missing Operator
C++23 introduced std::expected<T, E>, a vocabulary type that holds either a value of type T or an error of
type E. The design is explicitly modeled on Rust's Result type and Haskell's Either.

#include <expected>

std::expected<int, std::errc> parse_int(std::string_view s) {
	int value;
	auto [ptr, ec] = std::from_chars(s.data(), s.data() + s.size(), value);
	if (ec != std::errc{}) {
		return std::unexpected(ec);
	}
	return value;
}

The std::unexpected wrapper distinguishes error values from success values. Callers inspect the result:

auto result = parse_int("42");
if (result) {
	use(*result);
} else {
    handle(result.error());
}

C++23 added monadic operations for chaining. The and_then method applies a function if a value is present,
where the function returns another std::expected. The transform method applies a function that returns a
plain value, wrapping the result. The or_else method handles the error case:

auto result = parse(input)
	.and_then(validate)
	.transform([](int n) { return n * 2; });

The type exists. The monadic operations exist. What C++ lacks is the operator that makes the pattern
ergonomic.

Consider a function that calls two fallible operations and combines their results. With exceptions, C++ is
concise:

auto strcat(int i) -> std::string {
	return std::format("{}{}", foo(i), bar(i));
}

We do not even need to know that foo and bar might throw. Errors propagate invisibly.

With std::expected, the same logic becomes:

auto strcat(int i) -> std::expected<std::string, E> {
	auto f = foo(i);
	if (!f) {
		return std::unexpected(f.error());
	}

	auto b = bar(i);
	if (!b) {
		return std::unexpected(b.error());
	}

	return std::format("{}{}", *f, *b);
}

The manual error propagation dominates the function. We give names to the expected objects (f, b) rather than
to the values we care about. We access values through *f and *b. The ceremony obscures the logic.

Rust solves this with the ? operator:

fn strcat(i: i32) -> Result<String, E> {
	Ok(format!("{}{}", foo(i)?, bar(i)?))
}

One character per fallible operation. The happy path reads as naturally as the exception version. The error
path is explicit but minimal.

C++ cannot adopt Rust's ? syntax. The conditional operator ?: creates ambiguity: in a ? *b ? *c : d, the
parser cannot determine whether ? begins a conditional or is a postfix propagation operator. P2561R0 proposes
?? as an alternative, but as of C++26, no propagation operator has been adopted.

The monadic operations provide a workaround:

auto strcat(int i) -> std::expected<std::string, E> {
	return foo(i).and_then([&](int f) {
		return bar(i).transform([&](int b) {
			return std::format("{}{}", f, b);
		});
	});
}

This works. It is also nested, lambda-heavy, and harder to read than either the exception version or the Rust
version. The ergonomics gap matters. If the safe pattern is more verbose than the dangerous pattern,
programmers will choose the dangerous pattern. Codebases that adopt std::expected must fight against the
syntax to use it consistently.

A function using std::expected that calls code using exceptions must catch and convert at the boundary:

std::expected<Data, Error> safe_wrapper() {
	try {
		return legacy_function_that_throws();
	} catch (const std::exception& e) {
		return std::unexpected(Error{e.what()});
	}
}

A function using exceptions that calls code returning std::expected must check and throw:

Data wrapper_for_exception_code() {
	auto result = modern_function();
	if (!result) {
		throw std::runtime_error(result.error().message());
	}
	return *result;
}

Mixing styles is awkward but inevitable. Real codebases will contain both patterns indefinitely. The boundary
code is boilerplate that adds nothing but conversion overhead.

Rust: Result and the ? Operator
Rust represents recoverable errors as values. Operations that can fail return Result<T, E>, and the type
system requires handling both possibilities.

use std::fs::File;
use std::io::{self, Read};

fn read_file(path: &str) -> Result<String, io::Error> {
	let mut f = File::open(path)?;
	let mut contents = String::new();
	f.read_to_string(&mut contents)?;
	Ok(contents)
}

The ? operator is syntactic sugar for early return on error. Applied to a Result, it extracts the Ok value or
returns the Err from the enclosing function. The desugaring is approximately:

match expr {
	Ok(val) => val,
	Err(e) => return Err(From::from(e)),
}

The From::from(e) call enables automatic error type conversion. A function returning Result<T, MyError> can
use ? on any Result whose error type implements From<OriginalError> for MyError.

On x86-64, the ? operator compiles to a test of the discriminant and a conditional jump:

    test    eax, eax
    jne     .Lerror

The cost is just one comparison and one conditional branch per ?. This is the same cost as the manual if (!
result) check in C++, but without the syntactic overhead.

Because Result is marked #[must_use], ignoring a return value triggers a compiler warning. Errors cannot be
silently dropped. The type system forces acknowledgment of every failure mode.

The ? operator also works on Option<T>:

fn get_username(id: UserId) -> Option<String> {
	let user = users.get(&id)?;
	let profile = user.profile()?;
	Some(profile.name.clone())
}

The same syntax handles both "might fail with error" (Result) and "might be absent" (Option). Conversion
between them is explicit: result.ok() discards the error to produce an Option, while option.ok_or(err)
attaches an error to produce a Result.

Panics and Unwinding
Panics handle unrecoverable errors: violated assertions, out-of-bounds access, explicit panic!() calls. By
default, panics unwind the stack, calling Drop implementations for local values. The mechanism uses platform
unwinding libraries, the same infrastructure as C++ exceptions.

fn get(v: &[i32], i: usize) -> i32 {
	v[i]  // panics if i >= v.len()
}

Unlike exceptions, panics are not meant to be caught routinely. The std::panic::catch_unwind function can
intercept a panic, but its purpose is FFI boundaries and thread isolation, not error handling:

use std::panic;

let result = panic::catch_unwind(|| {
		panic!("oops");
		});
assert!(result.is_err());

A panic reaching an extern "C" boundary is undefined behavior. Rust code that might panic must catch the
panic before calling into C. Compiling with panic=abort eliminates unwinding entirely; panics terminate the
process immediately.

The distinction between Result and panic corresponds to recoverable versus unrecoverable errors. A missing
file is recoverable: try a different path, prompt the user, report and continue. An index out of bounds is a
bug: assumptions are violated, continuing risks corruption. Recoverable errors return Result. Bugs panic. The
type system encodes this distinction.

Exception Safety Through Control Flow
The ? operator achieves exception safety through normal control flow. When ? encounters an error, it returns
from the function. RAII objects drop as part of that return. The compiler generates the same cleanup code for
early returns via ? as for normal returns.

fn safe_operation() -> Result<(), Error> {
	let resource = acquire()?;
	might_fail()?;
	Ok(())
}

If might_fail() returns Err, the function returns early. Before returning, resource drops. This happens
through the normal return path, not unwinding. The generated code is a conditional branch to the function
epilogue.

Every potential exit point is visible in the source. The ? marks where control might leave. There are no
hidden exits, no invisible propagation. We can read the function and see exactly where early returns occur.
The C++ exception model hides exit points; the Rust model makes them explicit while keeping them
syntactically light.

Rust code achieves the basic exception safety guarantee automatically: early returns via ? release all local
resources. The strong guarantee requires the same care as in C++; modifying state before a fallible operation
requires rollback logic on error. But the explicit control flow makes reasoning about state easier.

Panic Safety in Unsafe Code
Safe Rust cannot violate memory safety through panics. Unsafe code can create intermediate states that
violate invariants; if a panic occurs in that window, destructors may observe invalid state.

Consider extending a vector by cloning elements from a slice:

impl<T: Clone> Vec<T> {
	unsafe fn extend_unchecked(&mut self, src: &[T]) {
		self.reserve(src.len());
		let old_len = self.len();
		self.set_len(old_len + src.len());  // length updated first

		for (i, x) in src.iter().enumerate() {
			// clone() might panic!
			self.as_mut_ptr().add(old_len + i).write(x.clone());
		}
	}
}

If clone() panics after set_len, the vector claims more initialized elements than exist. When it drops, it
calls drop on uninitialized memory. The fix is to update length after initialization, or use a guard:

struct SetLenOnDrop<'a, T> {
	vec: &'a mut Vec<T>,
	len: usize,
}

impl<T> Drop for SetLenOnDrop<'_, T> {
	fn drop(&mut self) {
		unsafe { self.vec.set_len(self.len); }
	}
}

impl<T: Clone> Vec<T> {
	fn extend_from_slice(&mut self, src: &[T]) {
		self.reserve(src.len());
		let mut guard = SetLenOnDrop {
vec: self,
		len: self.len()
		};

		for x in src {
			unsafe {
				self.as_mut_ptr().add(guard.len).write(x.clone());
			}
			guard.len += 1;  // only increment after successful write
		}

		std::mem::forget(guard);  // don't run Drop, length is correct
	}
}

The guard tracks successfully initialized elements. If clone() panics, the guard drops, setting the vector's
length to the count of actually initialized elements. The invariant holds across the panic. Safe Rust cannot
violate memory safety through panics. Unsafe code must maintain invariants across every potential panic
point.

Move Semantics
All this discussion about ownership assumes that ownership can be transferred efficiently. When we say a
value "moves" from one binding to another, what actually happens to the bytes? And why is this operation
preferable to copying?

The answer varies dramatically across C, C++, and Rust, and the differences expose fundamental assumptions
each language makes about values, identity, and resources.

The Cost of Copies

Consider what happens when we pass a value to a function. In the simplest model, the caller's value is copied
into the callee's parameter. For a 32-bit integer, this means copying 4 bytes, negligible. But what about a
dynamically-sized container?

A std::vector<int> in C++ or a Vec<i32> in Rust has a particular structure: a pointer to heap-allocated
storage, a length, and a capacity. The struct itself is small (typically 24 bytes on 64-bit systems), but it
can own a large heap allocation.

struct Vec {
	int* data;     // 8 bytes
	size_t len;    // 8 bytes
	size_t cap;    // 8 bytes
};
// sizeof(Vec) == 24, but data might point to way more memory

If we copy this struct byte-for-byte, we produce two Vec instances pointing to the same heap allocation. This
is a shallow copy. It creates a problem: both instances believe they own the memory. When one destructor
frees it, the other's pointer dangles. When the second destructor runs, it double-frees.

The alternative is a deep copy: allocate new heap storage, copy all data, and update the new struct's
pointer. This is correct but expensive. Copying takes time. More importantly, it allocates memory, which
involves a system call (or at minimum, allocator bookkeeping) and pollutes the cache with data we may never
touch again.

The overhead becomes prohibitive when values pass through function boundaries repeatedly. A function that
takes a vector by value, processes it, and returns a new vector might copy millions of bytes twice: once on
entry, once on return. In performance-sensitive code, we want to avoid passing large objects by value
entirely, preferring pointers or references. But this complicates APIs and obscures ownership.

The Shallow Copy Escape
We can observe however that when we pass a vector to a function that consumes it, the caller no longer needs
its copy. The bytes in the caller's stack frame become dead immediately after the call. If we could somehow
transfer ownership without physically duplicating the heap data, we would get the clarity of value semantics
with the efficiency of pointer passing.

This is exactly what move semantics provide. Instead of copying the entire data structure, we copy only the
struct (the pointer, length, and capacity), then invalidate the source somehow so it does not attempt
cleanup.

The somehow is where languages diverge.

In C, there is no language-level support. We must implement this manually:

typedef struct {
	int* data;
	size_t len;
	size_t cap;
} Vec;

void transfer(Vec* dest, Vec* src) {
	*dest = *src;           // shallow copy the struct
	src->data = NULL;       // invalidate source
	src->len = 0;
	src->cap = 0;
}

After transfer, the destination owns the heap memory, and the source is in a moved-from state (nulled
pointers, zero sizes). This works, but nothing enforces it. We can still access src->data after the transfer.
However, we would read NULL, or worse, the invalidation was forgotten and we would read a pointer that
someone else will free.

C++11: Rvalue References and Move Semantics

Before C++11, the language had one kind of reference: the lvalue reference, written T&. An lvalue is roughly
"something with a name and an address", a variable, a dereferenced pointer, an array element. Lvalue
references bind to lvalues and provide aliased access to existing objects.

But consider a temporary:

std::vector<int> make_vector() {
	return std::vector<int>{1, 2, 3, 4, 5};
}

void consume(std::vector<int> v);

consume(make_vector());  // the argument is a temporary

The return value of make_vector() is not an lvalue, it has no name, no persistent storage, no address we can
take. It is an rvalue, a temporary that will be destroyed at the end of the full expression. Before C++11,
passing this temporary to consume meant copying it, even though the original was about to disappear. We
duplicated data that would be destroyed moments later.

C++11 introduced rvalue references, written T&&. An rvalue reference binds to rvalues; temporaries,
expressions, values returned by functions. The type system now distinguishes "I want to observe this object"
(const T&) from "I want to steal from this object" (T&&).

This distinction enables overloading. A class can define two versions of a constructor:

class vector {
	public:
		// Copy constructor: source is const lvalue reference
		vector(const vector& other) {
			data_ = new int[other.cap_];
			std::copy(other.data_, other.data_ + other.len_, data_);
			len_ = other.len_;
			cap_ = other.cap_;
		}

		// Move constructor: source is rvalue reference
		vector(vector&& other) noexcept {
			data_ = other.data_;
			len_ = other.len_;
			cap_ = other.cap_;
			// Invalidate source
			other.data_ = nullptr;
			other.len_ = 0;
			other.cap_ = 0;
		}
};

When the argument is an rvalue (a temporary, or the result of std::move), overload resolution selects the
move constructor. We perform the shallow copy and invalidate the source, exactly as in the C version, but now
the selection happens automatically based on value category.

The key insight is that std::move does not move anything. It is a cast:

template<typename T>
constexpr std::remove_reference_t<T>&& move(T&& t) noexcept {
	return static_cast<std::remove_reference_t<T>&&>(t);
}

It takes a reference (of any kind, due to reference collapsing) and returns an rvalue reference to the same
object. The object does not move. We simply change how the type system categorizes it.

This cast enables us to move from named variables:

std::vector<int> v1{1, 2, 3};
std::vector<int> v2 = std::move(v1);  // v1 cast to rvalue, move constructor called

After this line, v2 owns the heap allocation that v1 previously owned. v1 is in a moved-from state: its data
pointer is null, its length and capacity are zero. The destructor will run when v1 goes out of scope, but it
will do nothing because there is nothing to free.

The Implicitly-Declared Move Constructor
The compiler can generate move constructors automatically. If a class has no user-declared copy constructor,
copy assignment operator, move assignment operator, or destructor, the compiler declares an implicit move
constructor. This implicit version performs member-wise move: for each non-static data member, it moves that
member using the member's own move constructor (or copies it, for types without move constructors).

struct Wrapper {
	std::vector<int> data;
	std::string name;
	int id;
	// implicit move constructor:
	// Wrapper(Wrapper&& other) noexcept
	//     : data(std::move(other.data))
	//     , name(std::move(other.name))
	//     , id(other.id) {}  // int is trivially movable (just copied)
};

For trivially movable types (essentially, types compatible with C), the move constructor copies the object
representation as if by std::memmove. No per-member move occurs at runtime; the operation reduces to a memory
copy of the struct's bytes.

The rule about user-declared special member functions is important. If we declare a destructor (even a
defaulted one), the implicit move constructor is not generated:

struct C {
	std::vector<int> data;
	~C() {}  // destructor declared, even though empty
};

C c1;
C c2 = std::move(c1);  // calls copy constructor, not move!

This behavior exists because a user-declared destructor suggests the class manages resources in ways the
compiler cannot infer. The conservative choice is to fall back to copying. We can force generation of the
move constructor with = default:

struct D {
	std::vector<int> data;
	~D() {}
	D(D&&) = default;  // explicitly request move constructor
};

Move Assignment
The same pattern applies to assignment. The move assignment operator takes an rvalue reference and transfers
ownership:

vector& operator=(vector&& other) noexcept {
	if (this != &other) {
		delete[] data_;        // free our current storage
		data_ = other.data_;
		len_ = other.len_;
		cap_ = other.cap_;
		other.data_ = nullptr;
		other.len_ = 0;
		other.cap_ = 0;
	}
	return *this;
}

The self-assignment check is necessary because std::move can be applied to any lvalue, including the
left-hand side of the assignment itself (though this would be perverse).

The destructor of the moved-from object still runs. Move semantics transfer ownership of resources, but the
source object continues to exist until its scope ends. The moved-from state must be valid enough for the
destructor to execute safely. For vector, that means null pointer and zero lengths,the destructor checks for
null before freeing, or simply has no work to do.

The noexcept specification on move constructors matters for optimization. std::vector needs to relocate
elements when it grows. If the element type's move constructor is noexcept, the vector can move elements to
the new buffer. If it might throw, the vector must copy instead to preserve the strong exception guarantee.
If an exception occurs during relocation, the original vector must remain intact. The difference can be
dramatic for vectors of vectors.

Value Categories in Depth
C++11 refined the notion of value categories beyond the simple lvalue/rvalue split. We have three categories:

  * An lvalue designates an object with identity that persists beyond a single expression. Variables,
    function returns by reference, dereferenced pointers. The address can be taken.

  * A prvalue (pure rvalue) is a temporary with no identity. Literals, function returns by value (before
    binding), results of arithmetic expressions. These initialize objects or compute values, but have no
    persistent address.

  * An xvalue (expiring value) has identity but can be moved from. The result of std::move(), the result of a
    cast to rvalue reference, the return value of a function returning T&&. The object exists, has an
    address, but we have permission to transfer its resources.

Overload resolution uses these categories:

void f(Widget& w);        // lvalue reference overload
void f(const Widget& w);  // const lvalue reference overload
void f(Widget&& w);       // rvalue reference overload

Widget w;
const Widget cw;
f(w);                // calls f(Widget&)
f(cw);               // calls f(const Widget&)
f(Widget{});         // calls f(Widget&&) - prvalue
f(std::move(w));     // calls f(Widget&&) - xvalue

When both const T& and T&& overloads exist, rvalues (prvalues and xvalues) prefer the T&& overload. Lvalues
can only bind to the lvalue reference overloads. If only const T& is provided, it accepts everything; rvalues
bind to const lvalue references, which is why copying was the fallback before C++11.

This machinery operates entirely at compile time. By the time we reach machine code, there are no value
categories, no rvalue references, just addresses and data. The type system's job was to select the right
constructor or operator; having done so, the generated code performs the memory operations we specified.

The Moved-From Problem
We have seen that in C++ rvalue references enable overloading, move constructors transfer resources,
std::move casts to rvalue. But we glossed over a critical detail. After a move, what happens to the source
object?

The source object still exists. It has a name, an address, a type. The destructor will run when it goes out
of scope. We can call methods on it, read its fields, pass it to functions. The move constructor transferred
its resources, but the object itself remains.

The C++ standard describes moved-from objects as being in a valid but unspecified state. Here Valid means the
object satisfies the invariants of its type sufficiently to be destroyed and to have certain operations
performed on it (typically assignment and, for some types, queries like empty()). Unspecified means we cannot
know what values its members hold without inspecting them.

For std::unique_ptr, the moved-from state is fully specified: the pointer becomes null. We can observe this:

std::unique_ptr<int> p = std::make_unique<int>(42);
std::unique_ptr<int> q = std::move(p);

// p still exists, and we can use it
if (p) {
	std::cout << *p;  // does not execute
} else {
	std::cout << "p is null";  // this executes
}

int* raw = p.get();  // returns nullptr
p.reset(new int(7)); // we can even reuse p

The moved-from unique_ptr is a perfectly functional object. It holds a null pointer, knows it holds a null
pointer, and behaves consistently. The get() method returns null. The bool conversion returns false. We can
reset() it with a new pointer and continue using it. This is well-defined behavior.

For std::vector, the situation is murkier. The standard guarantees only that the moved-from vector is in a
valid but unspecified state. In practice, most implementations leave it empty:

std::vector<int> v1{1, 2, 3, 4, 5};
std::vector<int> v2 = std::move(v1);

std::cout << v1.size();  // likely prints 0, but not guaranteed

The output is likely zero because implementations typically null out the source's data pointer and set its
size to zero. But the standard does not require this. An implementation could leave v1 with garbage values, a
dangling pointer, or some other state that satisfies valid (meaning the destructor and assignment still work)
without being predictable.

Here is where the design becomes problematic. The compiler does not prevent us from using moved-from objects.
There is no warning, no error, nothing. If we forget that we moved from a variable and try to use it, the
code compiles and runs:

void process(std::vector<int> data);

void example() {
	std::vector<int> v{1, 2, 3, 4, 5};
	process(std::move(v));

	// Bug: v has been moved from
	for (int x : v) {         // compiles fine
		std::cout << x << " "; // prints nothing, or garbage, or crashes
	}
}

This is not undefined behavior in the strict sense. Iterating over an empty vector is well-defined, but it is
almost certainly a bug. The programmer intended to iterate over the original data and forgot that process
consumed it. The program silently does the wrong thing.

The cppreference example demonstrates this explicitly:

A a1 = f(A());
std::cout << "Before move, a1.s = " << std::quoted(a1.s)
          << " a1.k = " << a1.k << '\n';
A a2 = std::move(a1);
std::cout << "After move, a1.s = " << std::quoted(a1.s)
          << " a1.k = " << a1.k << '\n';

Output:

Before move, a1.s = "test" a1.k = -1
After move, a1.s = "" a1.k = 0

The moved-from object is accessible, observable, and the program continues without complaint. The std::string
member is empty; the int member is zero.

The fundamental issue here is that C++ chose to preserve the moved-from object's existence for backward
compatibility and flexibility. Some use cases genuinely benefit from reusing moved-from objects, reassigning
to them, or swapping with another object. The cost of this flexibility is that the type system cannot enforce
the discipline of "don't use it after moving."

Static analyzers and compilers can sometimes detect use-after-move, but they cannot do so reliably in all
cases. The analysis is flow-sensitive and context-dependent, and function boundaries obscure the dataflow. A
function that takes T&& might move from its parameter, or it might not, the caller cannot tell from the
signature alone.

Rust: Moves Without Ghosts
Rust takes a different approach entirely. When a value moves, the source binding becomes invalid. Not valid
but unspecified, invalid. The compiler rejects any subsequent use:

fn process(data: Vec<i32>);

fn example() {
	let v = vec![1, 2, 3, 4, 5];
	process(v);

	for x in v {         // error: use of moved value: `v`
		println!("{}", x);
	}
}

The error message is unambiguous:

error[E0382]: use of moved value: `v`
 --> src/main.rs:7:14
  |
4 |     let v = vec![1, 2, 3, 4, 5];
  |         - move occurs because `v` has type `Vec<i32>`, which does not implement the `Copy` trait
5 |     process(v);
  |             - value moved here
6 |
7 |     for x in v {
  |              ^ value used here after move

There is no moved-from state to observe because there is no way to observe it. The binding v is not null, not
empty, not unspecified. It simply does not exist from the compiler's perspective after the move. The name
remains in scope (you can shadow it with a new binding), but the compiler's initialization tracking marks it
as uninitialized.

At the assembly level, the actual data movement is nearly identical to C++. The Vec's three words (pointer,
length, capacity) are copied from one stack location to another, or into registers for a function call. There
is no heap allocation, no deep copy, just 24 bytes shuffled around. The difference is purely a compile-time
concept: Rust tracks that the source is no longer valid.

fn example() {
	let v1 = vec![1, 2, 3];
	let v2 = v1;           // v1 moved to v2
	println!("{:?}", v1);  // error: borrow of moved value
}

The generated assembly for let v2 = v1 is a memcpy of the struct, essentially identical to what C++ would
generate. But where C++ would let us access v1 afterward (finding it in some "valid but unspecified" state),
Rust stops compilation.

This tracking happens through dataflow analysis in the compiler. Each variable has an initialization state
that the compiler updates as it processes statements. When v1 is assigned to v2, the compiler marks v1 as
uninitialized. Any subsequent use of v1 is an error, as if we had declared let v1: Vec<i32>; without
initializing it.

What about reinitialization? A moved-from variable can be assigned a new value:

fn example() {
	let mut v = vec![1, 2, 3];
	let v2 = v;            // v is now uninitialized
	v = vec![4, 5, 6];     // v is reinitialized
	println!("{:?}", v);   // ok: prints [4, 5, 6]
}

The compiler's tracking is flow-sensitive. After the move, v is uninitialized. After the reassignment, v is
initialized with a new value. The mut is required because reinitialization is a form of mutation in Rust's
model.

Control flow complicates the analysis. If a move occurs in one branch but not another, the variable's
initialization state depends on which path was taken:

fn example(condition: bool) {
	let v = vec![1, 2, 3];
	if condition {
		drop(v);           // v moved into drop()
	}
	println!("{:?}", v);   // error: v might have been moved
}

The compiler cannot statically determine which branch executes, so it conservatively assumes v might be
uninitialized. This occasionally forces us to restructure code or use Option<T> to represent maybe moved
states explicitly.

For cases where the compiler cannot determine initialization statically, Rust uses drop flags. These are
runtime boolean values, typically stored on the stack, that track whether a value has been moved. When the
variable goes out of scope, the generated code checks the flag before calling the destructor:

fn example(condition: bool) {
	let x;
	if condition {
		x = Box::new(0);
		println!("{}", x);
	}
	// x goes out of scope: compiler generates code to check if x was initialized
}

The drop flag mechanism tells us something about the design trade-off Rust accepted. In straight-line code,
the compiler knows exactly which bindings are initialized at every point, and generates direct drops with no
runtime overhead. But conditional moves force a choice: either reject some valid programs (overly
conservative static analysis), or emit a runtime check. Rust chose the latter for flexibility, keeping the
flag on the stack where it costs a byte and a conditional branch at scope exit. For hot loops, we can
restructure code to ensure static initialization tracking; for cold paths, the flag is negligible.

What we cannot do in safe Rust is observe a moved-from binding. The asymmetry with C++ lies entirely in what
the compiler permits us to write, since both languages copy the same bytes at runtime and both leave the
source's memory untouched until the stack frame is reclaimed. C++ allows the moved-from object to participate
in subsequent expressions; Rust does not.

Copy, Move, Clone
We have been speaking of "move" as if it were a single concept, but Rust distinguishes three related
operations: implicit copy, move, and explicit clone. Understanding when each applies requires understanding
what the type system knows about the data.

An i32 is 4 bytes. When we write let y = x where x: i32, the compiler generates a mov instruction that copies
those 4 bytes. After the assignment, both x and y hold independent copies of the same value. We can use both.
This is a copy.

A Vec<i32> is 24 bytes on the stack (pointer, length, capacity), but those 24 bytes control an arbitrarily
large heap allocation. When we write let y = x where x: Vec<i32>, the compiler generates the same kind of mov
instructions to copy those 24 bytes. But now both x and y would point to the same heap allocation. If we
allowed both to be used, we would have aliasing, and when both go out of scope, we would have double-free. So
after the assignment, x is invalidated. This is a move.

At the machine level, copy and move are identical. Both copy the bytes that constitute the value. The
difference is in what the compiler permits afterward. For Copy types, the source remains valid. For non-Copy
types, the source is invalidated.

Rust uses the Copy trait to mark types where this byte-for-byte duplication is semantically complete. If
copying the bytes gives us two independent, fully functional values, the type can be Copy. Integers, floats,
bool, char, raw pointers, and tuples or arrays of Copy types are all Copy. The defining characteristic is
that there is no additional resource management beyond the bytes themselves.

The Copy trait has a constraint: a type cannot implement both Copy and Drop. If a type has a destructor,
duplicating its bytes creates two values that will both try to run cleanup. For Vec, this means double-free.
For File, this means closing the same file descriptor twice. The mutual exclusion between Copy and Drop is
enforced by the compiler:

#[derive(Copy, Clone)]
struct Point { x: i32, y: i32 }  // ok: no Drop, all fields Copy

#[derive(Copy, Clone)]
struct Wrapper(Vec<i32>);        // error: Vec is not Copy

The error message is direct:

error[E0204]: the trait `Copy` cannot be implemented for this type
 --> src/main.rs:4:10
  |
4 | #[derive(Copy, Clone)]
  |          ^^^^
5 | struct Wrapper(Vec<i32>);
  |                -------- this field does not implement `Copy`

C++ has a parallel concept in trivially copyable types. The C++ standard defines a trivially copyable class
as one where each eligible copy constructor, move constructor, copy assignment operator, and move assignment
operator is trivial, and the destructor is trivial and non-deleted. "Trivial" here means the
compiler-generated default does the right thing, which for these operations means bitwise copy. A struct
containing only integers and other trivially copyable types is trivially copyable.

The difference is enforcement. In C++, std::is_trivially_copyable_v<T> is a compile-time query we can use in
static_assert or SFINAE, but the language does not prevent us from memcpy-ing a non-trivially-copyable
object. We might get away with it if the object has no internal pointers or virtual functions. We might
corrupt memory if it does. In Rust, attempting to derive Copy on a non-qualifying type is a hard error.

Clone is the explicit deep copy operation. Where Copy happens implicitly on assignment, Clone::clone() must
be called explicitly. The implementation can do anything: allocate new memory, copy all elements, increment
reference counts, whatever is appropriate for the type. For Vec<T>, clone() allocates a new buffer and clones
each element.

The relationship between Copy and Clone is that Copy is a supertrait of Clone. Every Copy type must also
implement Clone, and for Copy types, clone() is equivalent to a byte copy. This might seem redundant, but it
allows generic code to work uniformly:

fn duplicate<T: Clone>(x: &T) -> T {
	x.clone()
}

This function works for both i32 (where clone compiles to a simple load) and String (where clone allocates
and copies). The call site is uniform; the generated code is not.

When we see .clone() in Rust code, we know that something potentially expensive is happening. The Rust
philosophy is that expensive operations should be visible. Making us write .clone() forces acknowledgment of
this cost.

C++ takes the opposite approach for copy constructors. Given std::vector<int> v2 = v1;, this invokes the copy
constructor, which allocates and copies all elements. The syntax is identical to copying an int. We must know
that vector has an expensive copy constructor; the code does not tell us. Move semantics (v2 = std::move(v1))
were added in C++11 partly to make expensive operations more visible, but copy remains implicit.

One subtlety: Rust's clone() is not always a deep copy in the intuitive sense. For Rc<T>, calling clone()
increments the reference count and returns a new Rc pointing to the same allocation. The data is shared, not
duplicated. This is the correct semantics for Rc, since the entire point of reference counting is to share
data. But it means we cannot assume that clone() produces an independent copy. The trait's contract is
weaker: clone() produces a value that is semantically equivalent to the original for the purposes of the
type's interface.

Elision: When Neither Happens
We have seen that moving a value copies its bytes from source to destination. For a 24-byte Vec struct, this
means three 8-byte writes. But what about returning a Vec from a function? Naively, we might expect: the
function constructs a Vec in its stack frame, then on return, the Vec is moved to the caller's stack frame,
then the caller receives the returned value. This would mean writing those 24 bytes twice.

The answer to whether we can avoid this lies in how function returns work at the ABI level. The Itanium C++
ABI, which governs calling conventions on most Unix-like systems, distinguishes between trivial and
non-trivial return types. A type is non-trivial for purposes of calls if it has a non-trivial destructor or a
non-trivial copy or move constructor. For non-trivial return types, the ABI specifies that the caller passes
an address as an implicit parameter, and the callee constructs the return value directly into this address.

The Itanium ABI goes further. The address passed need not point to temporary memory on the caller's stack.
Copy elision may cause it to point anywhere: to a local variable's storage, to global memory, to
heap-allocated memory. The pointer is passed as if it were the first parameter in the function prototype,
preceding all other parameters including this. If the return type has a non-trivial destructor, the caller is
responsible for destroying the object after, and only after, the callee returns normally. If an exception is
thrown after construction but before the return completes, the callee must destroy the return value before
propagating the exception.

This machinery enables what C++ calls copy elision. The returned object is constructed directly in its final
location. Two forms are commonly discussed:

RVO (Return Value Optimization) applies when we return a prvalue, a temporary with no name:

std::vector<int> make_vector() {
	return std::vector<int>{1, 2, 3};
}

The vector is constructed directly into the caller-provided address. No temporary exists in make_vector's
stack frame.

NRVO (Named Return Value Optimization) applies when we return a named local variable:

std::vector<int> make_vector() {
	std::vector<int> v{1, 2, 3};
	v.push_back(4);
	return v;
}

Here the compiler can allocate v directly in the caller-provided space from the start. If it does, there is
no copy or move on return. If it does not (perhaps because control flow makes this impossible), the return
invokes the move constructor.

Before C++17, both forms of elision were permitted but not guaranteed. A conforming compiler could choose not
to elide, and the program would fall back to copy or move constructors. Code relying on elision for
correctness, such as returning a non-copyable non-movable type, was non-portable.

C++17 changed this for prvalues through a reformulation of value categories. The standard now specifies that
prvalues are not materialized until needed. A prvalue does not create a temporary object; it initializes a
target object directly. The C++ standard states that the materialization of a temporary object is generally
delayed as long as possible to avoid creating unnecessary temporary objects. The result is guaranteed copy
elision for prvalues. The statement std::vector<int> v = make_vector(); constructs the vector directly into
v, mandated by the standard rather than left as an optional optimization.

NRVO remains optional. The standard permits but does not require it. In practice, every major compiler
performs NRVO when control flow permits. Multiple return statements returning different named variables
typically defeat NRVO because the compiler cannot know at the function's entry which variable will be
returned.

How does this relate to Rust? Rust does not have copy elision as a named language concept because the problem
is different. Rust moves are defined as bitwise copies that invalidate the source. There is no move
constructor, no user code that might run, no observable side effects to elide. Moving a Vec copies 24 bytes
regardless of context.

Rust does perform the same underlying ABI-level optimization. When a function returns a value that cannot fit
in registers, the caller passes a hidden pointer in rdi (System V) or rcx (Microsoft x64), and the callee
writes directly to that location. But Rust does not need language-level elision rules because there is no
observable difference. Bitwise copy is bitwise copy; constructing directly into the destination versus
constructing locally and then copying produces identical bit patterns.

fn make_vec() -> Vec<i32> {
	vec![1, 2, 3, 4, 5]
}

fn caller() {
	let v = make_vec();
}

With optimizations, make_vec receives the hidden pointer in rdi and writes the Vec's three fields (pointer,
capacity, length) directly to that address. There is no intermediate Vec on make_vec's stack frame that gets
copied out. The heap allocation happens once, the Vec header is written once, directly to where caller wants
it.

For types that fit in registers, both languages return values in RAX and RDX. A function returning (i32, i32)
involves no memory operations for the return itself.

The consequence is that returning large values by value in Rust is not expensive because of the return
mechanism. The cost is in constructing the data structure: the allocations, the element initialization, the
potential reallocations. The mechanics of getting the result back to the caller add nothing beyond what the
ABI already requires for any struct return.

Smart Pointers and Reference Counting
Move semantics solve the problem of transferring exclusive ownership efficiently. But some data structures
cannot express their sharing patterns through exclusive ownership alone. A graph where multiple edges
reference the same node, a cache that outlives any single user, a callback that must remain valid for
multiple callers: these require shared ownership, where the resource is released only when the last owner
disappears.

We have seen how C++ and Rust handle exclusive heap ownership through unique_ptr and Box. The move semantics
we examined apply directly: unique_ptr leaves a nullptr after move, Box leaves no valid binding at all. Now
we turn to the harder problem of shared ownership, where multiple owners must coordinate to determine when
deallocation occurs.

Shared Ownership: shared_ptr
When multiple parts of a program need to share ownership of heap-allocated data, we need reference counting.
C++'s shared_ptr implements this with a control block: a separate heap allocation that stores metadata about
the shared object.

A typical shared_ptr<T> contains two pointers: one to the managed object (T*), and one to the control block.
The control block contains:
  * A strong reference count (the number of shared_ptrs pointing to the object)
  * A weak reference count (the number of weak_ptrs, plus one if the strong count is nonzero)
  * A pointer to the managed object (or the object itself, if allocated together)
  * The deleter (type-erased, since different shared_ptrs with different deleters can share ownership)
  * The allocator (also type-erased)

When we copy a shared_ptr, the strong count is incremented atomically. When a shared_ptr is destroyed, the
strong count is decremented. If the strong count reaches zero, the managed object is destroyed (the deleter
is invoked). The control block itself is not destroyed until the weak count also reaches zero, because
weak_ptrs need to query the control block to determine whether the object still exists.

auto p = std::make_shared<Widget>();  // one allocation: control block + Widget
auto q = p;                           // strong count: 2
q.reset();                            // strong count: 1
// p destroyed, strong count: 0, Widget destroyed

The make_shared function is preferred over shared_ptr<T>(new T) because it performs a single allocation for
both the control block and the object, improving cache locality and reducing allocation overhead. The
downside is that the memory for the object cannot be released until all weak references are also gone, since
the control block and object share a single allocation.

Shared Ownership: Rc<T> and Arc<T>
Rust separates the single-threaded and multi-threaded cases into distinct types. Rc<T> (reference counted)
uses non-atomic operations and is not thread-safe. Arc<T> (atomically reference counted) uses atomic
operations and can be shared across threads.

The layout of Rc<T> is similar in spirit to shared_ptr, but simpler. An Rc<T> is a single pointer to a heap
allocation containing:
  * A strong count (Cell<usize>, non-atomic)
  * A weak count (Cell<usize>, non-atomic)
  * The data (T)

There is no separate deleter or allocator. Rc<T> always uses the global allocator and always drops T in
place. This means Rc<T> is a single pointer, 8 bytes, not two pointers like shared_ptr.

use std::rc::Rc;

let a = Rc::new(Widget::new());  // allocates RcBox containing counts + Widget
let b = Rc::clone(&a);           // strong count: 2
drop(b);                         // strong count: 1
// a dropped, strong count: 0, Widget dropped, RcBox deallocated

The convention is to write Rc::clone(&a) rather than a.clone(). Both work identically, but the explicit form
makes clear that we are incrementing a reference count, not performing a deep copy.

Arc<T> has the same layout, with the counts replaced by AtomicUsize:

use std::sync::atomic::AtomicUsize;

pub struct ArcInner<T> {
	strong: AtomicUsize,
	weak: AtomicUsize,
	data: T,
}

The atomic operations add overhead. Incrementing an atomic counter requires synchronization at the hardware
level: a lock prefix on x86, load-linked/store-conditional sequences on ARM. In high-contention scenarios,
cache lines bounce between cores. If shared ownership is needed but thread safety is not, Rc avoids this cost
entirely.

Rust enforces this separation at compile time. Rc<T> does not implement Send or Sync. Attempting to move an
Rc across thread boundaries is a type error:

use std::rc::Rc;
use std::thread;

let rc = Rc::new(42);
thread::spawn(move || {
	println!("{}", rc);  // error: `Rc<i32>` cannot be sent between threads safely
});

The error is not a runtime panic. The code does not compile. We cannot accidentally introduce data races by
using the wrong smart pointer type.

Reference Count Synchronization
The increment operation in Arc::clone uses Ordering::Relaxed. This might seem dangerous for a multi-threaded
primitive, but the reasoning is precise that incrementing the count establishes no ordering relationship with
any other memory access. We already have access to the Arc, which means we already have a valid reference to
the data. We are simply recording that one more owner exists. The only requirement is atomicity itself,
ensuring that two concurrent increments do not lose a count.

impl<T> Clone for Arc<T> {
	fn clone(&self) -> Arc<T> {
		let inner = unsafe { self.ptr.as_ref() };
		inner.rc.fetch_add(1, Ordering::Relaxed);
		Arc { ptr: self.ptr, phantom: PhantomData }
	}
}

The decrement is where the complexity lies. Consider what happens when the last owner drops its Arc. At that
moment, no other thread holds a reference, and we must deallocate. But we must first ensure that all writes
performed by any previous owner are visible to us. If thread A writes to arc.data and then drops its Arc, and
thread B subsequently drops the last Arc, thread B must see thread A's writes before running the destructor.

The standard solution uses release-acquire synchronization:

impl<T> Drop for Arc<T> {
	fn drop(&mut self) {
		let inner = unsafe { self.ptr.as_ref() };
		if inner.rc.fetch_sub(1, Ordering::Release) != 1 {
			return;
		}
		std::sync::atomic::fence(Ordering::Acquire);
		unsafe { drop(Box::from_raw(self.ptr.as_ptr())); }
	}
}

Every fetch_sub with Release ordering guarantees that all prior writes in that thread are visible to any
thread that subsequently performs an Acquire operation on the same atomic variable and observes the stored
value. When thread B's fetch_sub returns 1, indicating it was the last owner, the subsequent Acquire fence
synchronizes with all the Release decrements that preceded it. Thread B now sees every write that any
previous owner performed before dropping.

On x86, the Release ordering on fetch_sub compiles to a simple lock xadd instruction, which already provides
the necessary ordering guarantees due to x86's strong memory model. The Acquire fence compiles to nothing, as
x86 loads already have acquire semantics. On ARM, the situation is different: Release requires a dmb ish
barrier before the store, and Acquire requires a barrier after the load.

Using Relaxed for the decrement would allow the deallocating thread to see stale data, potentially reading
uninitialized memory or seeing partially-written values. Using SeqCst everywhere would be correct but
unnecessarily expensive, adding full memory barriers where weaker ordering suffices. The release-acquire
pattern is the minimum synchronization required for correctness.

Raw Pointers, Variance, and the Drop Checker
An Arc internally holds a raw pointer to the heap allocation. A naive definition might use *mut ArcInner<T>,
but this creates two problems that NonNull<T> and PhantomData<T> solve.

Raw pointers are invariant in their type parameter. If we have an Arc<&'static str>, we should be able to use
it where an Arc<&'a str> is expected (assuming 'static: 'a), because a longer-lived reference can substitute
for a shorter-lived one. But *mut T does not allow this substitution. NonNull<T> is a pointer wrapper that is
covariant in T, restoring the expected subtyping relationship.

The second problem concerns the drop checker. When the compiler analyzes whether a type can be safely
dropped, it needs to know what the type logically owns. A raw pointer carries no ownership information; the
compiler assumes *mut T does not own a T. But Arc<T> does own a T, at least when it is the last owner. If we
do not communicate this to the compiler, code that should be rejected might compile:

fn bad<'a>(arc: Arc<&'a str>) {
	let local = String::from("local");
	// Without PhantomData, the compiler might not realize
	// that dropping arc could access the &str
}

Adding PhantomData<ArcInner<T>> tells the drop checker that Arc<T> behaves as if it contains an ArcInner<T>,
which contains a T. The drop checker then ensures that T outlives the Arc, preventing dangling references in
the destructor.

The final structure:

pub struct Arc<T> {
	ptr: NonNull<ArcInner<T>>,
	phantom: PhantomData<ArcInner<T>>,
}

This is still a single pointer in size, 8 bytes. PhantomData is a zero-sized type; it affects the type system
without occupying memory. NonNull is a #[repr(transparent)] wrapper around *const T, also pointer-sized. The
layout is identical to a raw pointer, but the semantics are correct for a reference-counted smart pointer.

Weak References and the Control Block Lifetime
A weak_ptr or Weak does not keep the managed object alive. It holds a pointer to the control block (or inner
allocation), not to the object itself. When we attempt to upgrade a weak reference to a strong one, the
operation must atomically check whether the object still exists and, if so, increment the strong count before
another thread can decrement it to zero.

Consider the race: thread A holds the last shared_ptr and is about to drop it. Thread B holds a weak_ptr and
calls lock(). If B reads the strong count as 1, then A decrements it to 0 and deallocates, then B increments
it to 1, we have a use-after-free. The upgrade must be atomic with respect to the decrement.

In C++, weak_ptr::lock() performs a compare-and-swap loop. It loads the strong count, checks if it is zero
(returning an empty shared_ptr if so), then attempts to atomically increment it from the observed value. If
another thread modified the count in the meantime, the CAS fails and the loop retries. The implementation
looks roughly like:

shared_ptr<T> weak_ptr<T>::lock() const noexcept {
	long count = control_block->strong_count.load(std::memory_order_relaxed);
	while (count != 0) {
		if (control_block->strong_count.compare_exchange_weak(
					count, count + 1, std::memory_order_acq_rel)) {
			return shared_ptr<T>(/* from control block */);
		}
		// count was updated by compare_exchange_weak on failure
	}
	return shared_ptr<T>();  // empty
}

Rust's Weak::upgrade() follows the same pattern. The acq_rel ordering on the successful CAS ensures that if
we observe a nonzero count, our subsequent access to the data sees all writes that happened before any
previous owner released their reference.

The control block has a two-phase destruction. When the strong count reaches zero, the managed object is
destroyed: its destructor runs, and if allocated separately from the control block, its memory is freed. The
control block itself survives. Only when the weak count also reaches zero is the control block deallocated.
This separation allows weak references to safely query whether the object exists.

With make_shared, the object and control block occupy a single allocation. The object's destructor runs when
the strong count hits zero, but the memory cannot be freed until the weak count also hits zero, because the
control block metadata lives in the same allocation. Long-lived weak references to make_shared objects keep
the entire allocation alive, even though the object has been destroyed. Separate allocation via shared_ptr<T>
(new T) avoids this: the object's memory is freed immediately when the strong count hits zero, and only the
smaller control block remains for weak reference bookkeeping.

The Cost of Atomics in Reference Counting
Arc::clone compiles to a lock xadd instruction on x86. The lock prefix asserts exclusive ownership of the
cache line containing the reference count, forcing all other cores to invalidate their copies. On a system
with 8 cores all cloning the same Arc, each clone causes 7 cache invalidations. The reference count
ping-pongs between cores, and each increment waits for the cache line to arrive in the exclusive state.

; Arc::clone on x86-64
mov     rax, qword ptr [rdi]        ; load pointer to ArcInner
lock    xadd qword ptr [rax], 1     ; atomic increment of refcount

The lock xadd instruction alone takes roughly 15-30 cycles on modern x86 when uncontended, rising to 100+
cycles under contention as cores compete for the cache line. On ARM, the equivalent uses load-exclusive/
store-exclusive pairs:

; Arc::clone on ARM64
.retry:
    ldxr    x1, [x0]          ; load-exclusive refcount
    add     x1, x1, #1
    stxr    w2, x1, [x0]      ; store-exclusive
    cbnz    w2, .retry        ; retry if store failed

The store-exclusive fails if another core modified the cache line between the load and store, requiring a
retry. Under high contention, threads spin in this loop, wasting cycles.

The decrement in Arc::drop has the same atomic cost, plus the Acquire fence when we observe the count
reaching zero. On x86, the fence is free because lock xadd already has acquire-release semantics. On ARM, it
expands to a dmb ish (data memory barrier, inner shareable domain), which stalls the pipeline until all prior
memory operations complete.

Beyond instruction costs, reference counting interacts poorly with modern cache hierarchies. The reference
count and the data occupy the same allocation, often the same cache line. If threads read the data while
another thread clones or drops the Arc, false sharing occurs: the reading threads' cache lines are
invalidated by the write to the reference count, even though the data itself is unchanged. This is
unavoidable without splitting the count into a separate allocation, which would add indirection and defeat
the single-pointer layout.

Rc<T> avoids all of this. With no threading, the increment is a simple add instruction, the decrement a
simple sub. No cache coherence traffic, no memory barriers, no retry loops. The type system's guarantee that
Rc is not Send lets us omit the synchronization entirely.

; Rc::clone on x86-64
mov     rax, qword ptr [rdi]        ; load pointer to RcBox
inc     qword ptr [rax]             ; non-atomic increment

C's Approach: Manual Reference Counting
C has no built-in reference-counted smart pointers, but the pattern is pervasive. The Linux kernel's kref,
GLib's GObject, and COM's IUnknown all implement reference counting manually, each with its own conventions
and failure modes.

A single-threaded implementation is pretty straightforward:

struct Widget {
	int refcount;
	// ... data ...
};

struct Widget *widget_ref(struct Widget *w) {
	w->refcount++;
	return w;
}

void widget_unref(struct Widget *w) {
	if (--w->refcount == 0) {
		widget_destroy(w);
		free(w);
	}
}

Threading requires atomic operations. Before C11, these were compiler or platform-specific: GCC's
__sync_fetch_and_add, MSVC's InterlockedIncrement, or inline assembly. C11 standardized atomics, but the
memory ordering must still be chosen correctly:

#include <stdatomic.h>

struct Widget {
	atomic_int refcount;
	// ... data ...
};

struct Widget *widget_ref(struct Widget *w) {
	atomic_fetch_add_explicit(&w->refcount, 1, memory_order_relaxed);
	return w;
}

void widget_unref(struct Widget *w) {
	if (atomic_fetch_sub_explicit(&w->refcount, 1, memory_order_release) == 1) {
		atomic_thread_fence(memory_order_acquire);
		widget_destroy(w);
		free(w);
	}
}

The ordering arguments mirror what we saw in Arc: relaxed for increment (we already have a valid reference),
release for decrement (publish our writes), acquire fence before destruction (synchronize with all
releasers). Getting any of these wrong introduces data races that manifest as sporadic corruption,
use-after-free in specific timing windows, or crashes that appear once per million operations.

The Linux kernel's kref wraps this pattern into a small struct with kref_get and kref_put operations. This
discipline is not enforced, nothing prevents calling kref_get on an object whose count has already reached
zero, or forgetting to call kref_put on an error path. Static analysis tools like Sparse and Coverity catch
some bugs; the rest are found through testing, fuzzing, or production crashes.

Every usage site must remember to call widget_ref when acquiring a reference and widget_unref when releasing
one. There is no automatic cleanup on scope exit, no destructor to invoke at the end of a block. If a
function returns early due to an error, every acquired reference must be explicitly released. Missing a
single unref leaks memory; an extra unref corrupts the count or triggers a double-free.



---
https://lukefleed.xyz/posts/who-owns-the-memory-pt3/

Who Owns the Memory? Part 3: How Big Is your Type?
29 Dec, 2025

This is the third part of a series exploring how C, C++, and Rust manage memory at a low level. In Part I we
examined how memory is organized at the hardware level. Part II explored ownership and lifetime, showing how
the three languages answer the question of who is responsible for freeing memory and when access to that
memory is valid.

Part III turns to representation: how abstract types become concrete bit patterns, and how polymorphism, the
ability to write code that operates on many types, is implemented.

You can discuss this article on Lobsters, Reddit (r/rust and r/programming) and Hacker News

Type Layout and Memory Representation
In Part I we established that every type has a size and an alignment, and that compilers insert padding to
satisfy alignment constraints. We showed how a poorly ordered struct in C could waste 8 bytes of padding
where a well-ordered one used only 2. But we sidestepped a question: who decides the order?

In C and C++, the answer is straightforward: we do. The compiler lays out fields in declaration order,
inserting padding as the alignment algorithm dictates. This predictability is essential for binary
compatibility, memory-mapped I/O, and network protocols where byte offsets must match external
specifications. It is also a constraint: we bear responsibility for field ordering, and a careless
declaration can bloat a frequently-allocated struct.

Rust makes a different choice. By default, the compiler reserves the right to reorder fields.

The C Layout Algorithm

C specifies a deterministic algorithm for struct layout. Start with a current offset of zero. For each field
in declaration order: add padding until the offset is a multiple of the field's alignment, record the field's
offset, advance by the field's size. Finally, round the struct's total size up to its alignment.

struct Example {
	char a;      // offset 0, size 1
	// 7 bytes padding (align to 8 for double)
	double b;    // offset 8, size 8
	char c;      // offset 16, size 1
	// 7 bytes padding (struct size must be multiple of 8)
};
// sizeof(struct Example) == 24

The algorithm is mechanical. Given field types and their order, the layout is fully determined. This property
is what makes C the lingua franca of FFI: any language that implements the same algorithm can share data
structures with C code.

C++ inherits this layout for standard-layout types. The Itanium ABI, which governs most non-Windows C++
implementations, extends the algorithm to handle base classes, virtual functions, and virtual inheritance.
For our purposes, a C++ struct without inheritance, virtual functions, or access specifiers mixing fields
follows the same layout as C.

The [[no_unique_address]] attribute (C++20) allows the compiler to overlap a data member with another if the
member has no members of its own. This enables empty base optimization for member objects:

struct Empty {};

struct WithoutAttribute {
	Empty e;
	int x;
};
// sizeof(WithoutAttribute) == 8 on typical platforms
// (Empty requires 1 byte in C++, padded to 4 for int alignment)

struct WithAttribute {
	[[no_unique_address]] Empty e;
	int x;
};
// sizeof(WithAttribute) == 4

Without the attribute, Empty occupies one byte (C++ mandates that different objects have different addresses,
so even empty classes have nonzero size). With the attribute, e can share storage with padding or with x
itself, reducing the struct to just sizeof(int).

repr(Rust): The Compiler's Prerogative
Rust's default representation, repr(Rust), makes minimal guarantees. The specification states only:
 1. Fields are properly aligned.
 2. Fields do not overlap (except for ZSTs, which may share addresses).
 3. The struct's alignment is at least the maximum alignment of its fields.

There is no guarantee about field order. The compiler may reorder fields to minimize padding, and different
compilations of the same source can produce different layouts. The same generic struct instantiated with
different type parameters will typically have different field orderings.

Consider:

struct Foo<T, U> {
	count: u16,
	data1: T,
	data2: U,
}

For Foo<u32, u16>, an efficient layout places count and data2 (both 2 bytes) adjacent, followed by data1 (4
bytes):

// Possible layout of Foo<u32, u16>: size 8, align 4
// count:  offset 0, size 2
// data2:  offset 2, size 2
// data1:  offset 4, size 4

For Foo<u16, u32>, the same reordering achieves the same efficiency:

// Possible layout of Foo<u16, u32>: size 8, align 4
// count:  offset 0, size 2
// data1:  offset 2, size 2
// data2:  offset 4, size 4

If Rust preserved declaration order, Foo<u32, u16> would require padding after count:

// Declaration-order layout of Foo<u32, u16>: size 12, align 4
// count:  offset 0, size 2
// _pad:   offset 2, size 2
// data1:  offset 4, size 4
// data2:  offset 8, size 2
// _pad:   offset 10, size 2

The reordering saves 4 bytes per instance without any annotation or programmer attention.

The trade-off is unpredictability. We cannot assume field offsets. We cannot cast a *const Foo<A, B> to a
byte pointer and read at a known offset to extract a field. We cannot send a repr(Rust) struct across a
network or write it to a file and expect another compilation (or even the same compilation with different
flags) to interpret it correctly.

repr(C): Predictable Layout for FFI
When we need C-compatible layout, we annotate the type with #[repr(C)]:

#[repr(C)]
struct ThreeInts {
	first: i16,
	second: i8,
	third: i32,
}

With repr(C), Rust applies the C layout algorithm. Fields appear in declaration order. Padding follows the
standard rules. The resulting layout is compatible with a C struct declared with the same field types and
order:

struct ThreeInts {
	int16_t first;
	int8_t second;
	int32_t third;
};

repr(C) is necessary for FFI correctness. It is also useful when we need stable layout for unsafe code that
relies on field offsets, or when serializing data to a known binary format. The trade-off is that we accept
whatever padding the declaration order implies.

For enums, repr(C) produces a layout compatible with C unions plus a tag. The exact representation depends on
whether the enum has fields:

#[repr(C)]
enum MyEnum {
	A(u32),
	B(f32, u64),
	C { x: u32, y: u8 },
	D,
}

This is laid out as a repr(C) union of repr(C) structs, where each struct begins with a discriminant of the
platform's C int size:

// Equivalent repr(C) layout:
#[repr(C)]
union MyEnumRepr {
	a: MyVariantA,
	b: MyVariantB,
	c: MyVariantC,
	d: MyVariantD,
}

#[repr(C)]
struct MyVariantA { tag: u64, value: u64 }

#[repr(C)]
struct MyVariantB { tag: u64, _pad: u64, value0: f64, _pad2: u64, value1: u64 }
// ... and so on

The discriminant size for a fieldless repr(C) enum matches the C ABI's enum size, which is
implementation-defined. On most platforms, this is int (4 bytes), though some ABIs use smaller types for
small enums.

Primitive Representations for Enums
When we need precise control over the discriminant, we can specify an integer type:

#[repr(u8)]
enum Opcode {
	Nop = 0,
	Load = 1,
	Store = 2,
	// ... up to 255 variants
}

This enum occupies exactly 1 byte. The discriminant is stored as a u8. This is essential for binary protocols
where the tag must fit a specific field width.

For enums with fields, repr(u8) or similar sets the discriminant size but still uses C-style layout for the
variant data:

#[repr(u8)]
enum Packet {
	Ping,
	Data([u8; 64]),
	Error(u32),
}
// size: 72 bytes (1 byte tag + 7 padding + 64 data)
// The discriminant is guaranteed to be 1 byte

Combining repr(C) and a primitive representation, like #[repr(C, u8)], specifies both C layout and a specific
discriminant type.

Adding an explicit repr to an enum with fields has a consequence: it suppresses niche optimization. This
becomes clear after we discuss zero-sized types.

repr(packed): Eliminating Padding
Sometimes we need minimum size regardless of alignment. Network packet headers, binary file formats, and
memory-mapped hardware registers often require tightly packed data. repr(packed) removes inter-field padding:

#[repr(packed)]
struct PackedExample {
	a: u8,
	b: u32,
	c: u8,
}
// size: 6 bytes (1 + 4 + 1), no padding
// alignment: 1 byte

Compare with the default layout, which would require 12 bytes (1 + 3 padding + 4 + 1 + 3 padding). The packed
version is half the size.

The penalty is that fields may be misaligned. On x86-64, misaligned loads incur a performance penalty. On
stricter architectures like ARM without unaligned access support or older SPARC, they cause hardware
exceptions. Even on tolerant architectures, misaligned atomics are often incorrect.

Rust addresses part of this problem by making it illegal to create a reference to a misaligned field:

#[repr(packed)]
struct Packed {
	a: u8,
	b: u32,  // may be at offset 1, misaligned
}

let p = Packed { a: 1, b: 2 };
let r: &u32 = &p.b;  // ERROR: reference to packed field

The compiler rejects this because a &u32 must point to a 4-byte-aligned address, but p.b may not be. The
workaround is to copy the value first:

let value = p.b;         // copies the unaligned bytes into a local
let r: &u32 = &value;    // value is properly aligned on the stack

Or use raw pointer operations with explicit unaligned reads:

let ptr: *const u32 = std::ptr::addr_of!(p.b);
let value = unsafe { ptr.read_unaligned() };

repr(packed(n)) generalizes this by setting the maximum field alignment to n. Fields with natural alignment
less than n are laid out normally; fields with natural alignment greater than n are treated as if their
alignment were n. This allows partial packing, trading some space for better access patterns.

For FFI work, combining repr(C, packed) gives C-compatible layout with minimal size. This matches #pragma
pack(1) in many C compilers.

repr(align): Preventing False Sharing
While repr(packed) reduces alignment, repr(align(n)) increases it. The attribute forces the type to have
alignment of at least n bytes, where n must be a power of two.

Why would we want more alignment than necessary? The answer lies in cache architecture. On x86-64, the L1
cache operates on 64-byte cache lines. When one core writes to an address, the entire cache line containing
that address is invalidated in all other cores' caches. This is the MESI protocol (or variants like MESIF,
MOESI) maintaining coherence.

Consider two atomic counters that different threads increment concurrently:

use std::sync::atomic::{AtomicU64, Ordering};

struct Counters {
	a: AtomicU64,  // offset 0, 8 bytes
	b: AtomicU64,  // offset 8, 8 bytes
}

Both counters fit in a single 64-byte cache line. When thread 1 increments a, it invalidates thread 2's cache
line. When thread 2 increments b, it invalidates thread 1's cache line. Neither thread is accessing the
other's data, yet they are constantly invalidating each other's caches. This is false sharing, and it can
degrade performance by an order of magnitude on contended workloads.

The fix is to ensure each counter occupies its own cache line:

use std::sync::atomic::{AtomicU64, Ordering};

#[repr(align(64))]
struct CacheAligned(AtomicU64);

struct Counters {
	a: CacheAligned,  // offset 0, padded to 64 bytes
	b: CacheAligned,  // offset 64, padded to 64 bytes
}

Now sizeof::<Counters>() is 128 bytes instead of 16, but a and b cannot share a cache line. Each thread's
writes affect only its own cache line, and the coherence protocol stops bouncing the line between cores.

The trade-off is memory consumption. Padding a 8-byte counter to 64 bytes is an 8x increase. For a handful of
hot counters in a concurrent data structure, this is negligible. For an array of thousands of counters, it
becomes prohibitive. The choice depends on access patterns: if threads access disjoint counters, alignment
helps. If threads frequently access the same counter, the contention is real, not false, and alignment does
nothing.

repr(align) can combine with other representations. #[repr(C, align(64))] gives C-compatible field ordering
with cache-line alignment. The align modifier applies to the struct as a whole, not to individual fields, so
all fields remain at their natural offsets within the enlarged, aligned struct.

repr(transparent): Zero-Cost Wrappers
Sometimes we want a new type that has identical layout to an existing type. The newtype pattern wraps a
single field:

struct Meters(f64);
struct Seconds(f64);

With repr(Rust), these types are not guaranteed to have the same layout or ABI as f64. A function returning
Meters might use different registers than a function returning f64, even though the underlying data is
identical.

repr(transparent) guarantees layout and ABI identity:

#[repr(transparent)]
struct Meters(f64);

#[repr(transparent)]
struct Seconds(f64);

Now Meters has exactly the same size, alignment, and function-call ABI as f64. We can pass a Meters where C
expects a double. We can transmute between Meters and f64 (in either direction) without undefined behavior.

repr(transparent) requires that the type has exactly one field with non-zero size, and may have any number of
zero-sized fields with alignment 1 (like PhantomData<T>).

This enables patterns like:

use std::marker::PhantomData;

#[repr(transparent)]
struct Id<T>(u64, PhantomData<T>);

Id<User> and Id<Post> are distinct types at compile time but have identical layout to u64. The PhantomData
participates in the type system (for variance and drop checking) but not in the layout.

Zero-Sized Types
Rust permits types with size zero:

struct Nothing;           // no fields
struct AlsoNothing { }    // empty struct
struct MoreNothing(());   // contains unit type
struct AndNothing([u8; 0]); // zero-length array

All of these have size 0 and alignment 1 (the minimum). They occupy no memory. An array [Nothing; 1000000]
has size 0.

ZSTs become useful in generic contexts. A HashMap<K, V> stores keys and values. What if we only care about
keys? We could duplicate the code as HashSet<K>, or we could define:

type HashSet<K> = HashMap<K, ()>;

Because () is a ZST, HashMap<K, ()> stores no value data. The compiler eliminates loads, stores, and
allocations for the values. We get a set implementation from a map implementation with no runtime overhead.

The standard library's HashSet is implemented exactly this way:

pub struct HashSet<T, S> {
	map: HashMap<T, (), S>,
}

Unsafe code must be careful with ZSTs. Pointer arithmetic on *const T where T is zero-sized is a no-op:
ptr.add(1) returns ptr unchanged. This breaks the assumption that advancing a pointer produces a different
address. Additionally, most allocators do not accept zero-size requests, so Box::new(ZST) uses special
handling rather than calling the allocator.

One subtle property: references to ZSTs must be non-null and properly aligned (alignment 1 means any address
is valid), but dereferencing them is defined to read zero bytes. A reference to a ZST at address 0x1 is
valid; a reference at address 0 (null) is undefined behavior even though no actual memory access occurs.

Empty Types
Zero-sized types can be instantiated. We can create a value of type () or struct Nothing; and pass it around.
Empty types go further: they cannot be instantiated at all.

enum Void {}

An enum with no variants has no valid values. We cannot construct a Void because there is no variant to
construct. The type exists at the type level but can never exist at the value level.

This enables type-level reasoning about impossibility. Consider a trait for data sources that might fail:

trait DataSource {
	type Error;
	fn fetch(&self) -> Result<Data, Self::Error>;
}

Most implementations have a meaningful error type: network sources fail with I/O errors, parsers fail with
syntax errors. But some sources are infallible. An in-memory cache cannot fail to read its own contents:

enum Infallible {}

struct MemoryCache { data: Data }

impl DataSource for MemoryCache {
	type Error = Infallible;
	fn fetch(&self) -> Result<Data, Infallible> {
		Ok(self.data.clone())
	}
}

The Error = Infallible communicates at the type level that fetch cannot return Err. Callers can use an
irrefutable pattern:

fn use_cache(cache: &MemoryCache) {
	let Ok(data) = cache.fetch();  // no Err case to handle
	process(data);
}

The compiler optimizes based on this knowledge. Result<T, Infallible> has the same layout as T because the
Err variant cannot exist and requires no discriminant:

use std::mem::size_of;
use std::convert::Infallible;

assert_eq!(size_of::<Result<u64, Infallible>>(), 8);  // same as u64

The standard library provides std::convert::Infallible for this purpose. It is defined as an empty enum and
used throughout the standard library to indicate operations that cannot fail.

Raw pointers to empty types are valid to construct but dereferencing them is undefined behavior. There is no
value to read, so the dereference cannot produce a valid result. This makes empty types unsuitable for
representing C's void*. The recommended approach for opaque C pointers is *const () or a newtype wrapper
around it, which can be safely dereferenced to read zero bytes.

Niche Optimization
An enum must somehow record which variant it currently holds. The naive approach stores a discriminant, a
small integer that identifies the variant, alongside the variant's data. For an enum with four variants, we
need at least 2 bits to distinguish them; in practice, the discriminant occupies 1, 2, or 4 bytes depending
on the number of variants and alignment constraints.

Consider Option<T>, which has two variants: Some(T) and None. The naive layout stores a discriminant plus the
T:

// Naive Option<u64> layout:
// discriminant: 1 byte (0 = None, 1 = Some)
// padding: 7 bytes (to align u64)
// value: 8 bytes
// total: 16 bytes

This is wasteful when T is something like &u64. A reference is 8 bytes, so Option<&u64> would be 16 bytes: 8
for the pointer, plus padding and discriminant overhead. But here is the insight that enables optimization: a
reference can never be null. Rust guarantees that &T always points to a valid T. The bit pattern consisting
of all zeros, the null pointer, can never represent a valid reference.

The compiler exploits this. For Option<&T>, there is no separate discriminant. The Some variant stores the
pointer as-is. The None variant is represented by the null pointer. Pattern matching becomes a null check:

// Actual Option<&u64> layout:
// pointer: 8 bytes
// total: 8 bytes
// None is represented as null (0x0000000000000000)
// Some(&x) is represented as the address of x

This is called niche optimization. A niche is a bit pattern that a type guarantees it will never hold. The
compiler uses niches to encode enum discriminants without allocating additional space.

Which types have niches? Any type that forbids certain bit patterns:
References (&T, &mut T) forbid null. NonNull<T> exists specifically to be a non-null pointer. The NonZeroU32
type (and its siblings NonZeroU8, NonZeroI64, etc.) forbid zero. Box<T> contains a non-null pointer
internally. Function pointers forbid null. bool only permits 0 and 1, so 254 other byte values are niches.

The optimization composes. Option<Box<T>> uses null for None. What about Option<Option<Box<T>>>? The outer
Option needs a niche to represent its None, and the inner Option already used null for its None. Can we
distinguish outer-None from Some(None)?

On x86-64 with 48-bit virtual addresses, pointers have constraints beyond non-nullness. The upper 16 bits
must be a sign extension of bit 47. Most user-space pointers have the top bits all zeros (canonical
lower-half addresses). The compiler can use a non-canonical address like 0x0000000000000001 (misaligned for
any type with alignment > 1) to represent additional None variants:

use std::mem::size_of;

assert_eq!(size_of::<Box<i32>>(), 8);
assert_eq!(size_of::<Option<Box<i32>>>(), 8);
assert_eq!(size_of::<Option<Option<Box<i32>>>>(), 8);

All three types fit in 8 bytes. The compiler found two niches in the pointer: null for the inner None, and
another invalid address for the outer None.

For types without niches, the discriminant requires additional space:

use std::mem::size_of;

assert_eq!(size_of::<u64>(), 8);
assert_eq!(size_of::<Option<u64>>(), 16);

Every bit pattern is a valid u64, so there is no niche. The compiler must store a separate discriminant, and
alignment padding expands the total to 16 bytes.

Now we can return to the point we deferred earlier: why does #[repr(u8)] on an enum suppress niche
optimization? The repr(u8) attribute guarantees that the discriminant is stored as an explicit u8 at a known
location. This is a layout guarantee for FFI and binary serialization. If the compiler used niche
optimization, there would be no explicit discriminant byte; the variant would be encoded in the payload's bit
pattern. These two requirements are incompatible. Explicit discriminant layout and niche optimization are
mutually exclusive:

use std::mem::size_of;

enum WithNiche { Some(Box<i32>), None }

#[repr(u8)]
enum WithoutNiche { Some(Box<i32>), None }

assert_eq!(size_of::<WithNiche>(), 8);      // niche used, no discriminant
assert_eq!(size_of::<WithoutNiche>(), 16);  // explicit u8 discriminant + padding + pointer

The repr(u8) forces a 1-byte discriminant to exist, which with 7 bytes of padding and the 8-byte pointer
yields 16 bytes total.

Visualizing Layouts
Rust provides std::mem::size_of::<T>() and std::mem::align_of::<T>() for inspecting type properties at
runtime (they are const functions, so compile-time evaluation is also possible). For detailed layout
information, the -Zprint-type-sizes flag on nightly rustc shows field ordering, padding, and discriminant
placement:

RUSTFLAGS=-Zprint-type-sizes cargo +nightly build --release

For an enum like:

enum E {
	A,
	B(i32),
	C(u64, u8, u64, u8),
	D(Vec<u32>),
}

The output shows:

print-type-size type: `E`: 32 bytes, alignment: 8 bytes
print-type-size     discriminant: 1 bytes
print-type-size     variant `D`: 31 bytes
print-type-size         padding: 7 bytes
print-type-size         field `.0`: 24 bytes, alignment: 8 bytes
print-type-size     variant `C`: 23 bytes
print-type-size         field `.1`: 1 bytes
print-type-size         field `.3`: 1 bytes
print-type-size         padding: 5 bytes
print-type-size         field `.0`: 8 bytes, alignment: 8 bytes
print-type-size         field `.2`: 8 bytes

The compiler has reordered variant C's fields to place the u8s before the padding, minimizing wasted space.
The discriminant is only 1 byte despite four variants (2 bits suffice, but alignment constraints mean 1 byte
is the minimum addressable unit).

Fat Pointers and Dynamically Sized Types
We saw how repr(Rust) gives the compiler freedom to reorder fields because it knows each field's size and
alignment at compile time. But this assumption does not always hold. Some types have sizes that can only be
determined at runtime, and the way Rust handles them differs sharply from C++.

The Problem with C Arrays
In C, when we pass an array to a function, the type system loses information.

void process(int arr[], size_t len);

int main(void) {
	int data[10] = {0};
	process(data, 10);  // must pass length separately
}

The parameter int arr[] is identical to int *arr. The array decays to a pointer, and the length vanishes from
the type. Nothing in the language connects the pointer to its length. If we pass the wrong length, we get
buffer overflows. If we forget to pass it entirely, the function cannot know where the array ends.

This decay happens implicitly. The expression data in the call site has type int[10], but by the time it
reaches process, it is just int*. The compiler erases information that the programmer must then track
manually.

Dynamically Sized Types in Rust
Rust solves this with dynamically sized types (DSTs), types whose size is not known at compile time. The
language has three built-in DST categories.

The slice type [T] represents a contiguous sequence of T values of unknown length. Unlike [T; N], which has
compile-time-known size N * size_of::<T>(), the type [T] could represent any number of elements. A [u8] might
be 10 bytes or 10,000 bytes.

The string slice str is semantically a [u8] with a validity invariant requiring UTF-8 encoding. It shares the
same dynamically-sized nature.

Trait objects dyn Trait represent values of unknown concrete type that implement Trait. A dyn Display might
be a String at 24 bytes, an i32 at 4 bytes, or a custom type of arbitrary size. The concrete type is erased;
only the interface remains.

However, all these types cannot exist directly on the stack or as struct fields (except as the last field).
We cannot write let x: [u8]; because the compiler cannot determine how much stack space to allocate. DSTs
exist only behind pointers.

Wide Pointers
A pointer to a sized type is a single machine word, 8 bytes on x86-64. A pointer to a DST must carry
additional information, making it twice as wide.

For slices, a &[T] stores a pointer to the first element paired with the number of elements.

use std::mem::size_of;

assert_eq!(size_of::<&u8>(), 8);        // thin pointer
assert_eq!(size_of::<&[u8]>(), 16);     // wide pointer
assert_eq!(size_of::<&str>(), 16);      // same representation as &[u8]

We can examine the actual representation using std::ptr::metadata:

let arr = [1i32, 2, 3, 4, 5];
let slice: &[i32] = &arr[1..4];

// The slice is (pointer to arr[1], length 3)
let ptr = slice.as_ptr();
let len = slice.len();

assert_eq!(len, 3);
assert_eq!(unsafe { *ptr }, 2);  // first element of slice

The wide pointer solves the C problem. When we pass a &[T], the length travels with the pointer. There is no
way to separate them, pass the wrong length or forget it.

The coercion from &[i32; 5] to &[i32] is an unsizing coercion. The compiler takes the thin pointer to the
array, combines it with the statically-known length, and produces a wide pointer. This happens implicitly at
coercion sites, including let bindings with explicit types, function arguments, and return values.

Trait Object Representation
For trait objects, the wide pointer contains different metadata. A &dyn Trait stores a pointer to the
concrete value paired with a pointer to a vtable (virtual method table).

use std::mem::size_of;

trait Drawable {
	fn draw(&self);
	fn area(&self) -> f64;
}

impl Drawable for i32 {
	fn draw(&self) { println!("{}", self); }
	fn area(&self) -> f64 { 0.0 }
}

assert_eq!(size_of::<&i32>(), 8);
assert_eq!(size_of::<&dyn Drawable>(), 16);

The vtable is a static data structure generated by the compiler for each (concrete type, trait) pair. It
contains function pointers for every method in the trait, plus metadata required for memory management.

For a trait Drawable with methods draw and area, the vtable looks roughly like this:

+0:   drop_in_place::<ConcreteType>
+8:   size_of::<ConcreteType>
+16:  align_of::<ConcreteType>
+24:  Drawable::draw for ConcreteType
+32:  Drawable::area for ConcreteType

The size and alignment entries are essential for dropping boxed trait objects. When we call drop on a Box<dyn
Drawable>, the runtime must know how many bytes to deallocate and what alignment the allocator expects.

Virtual Dispatch in Assembly
When we call a method on a trait object, the compiler generates an indirect call through the vtable. Consider
this function:

fn call_draw(obj: &dyn Drawable) {
	obj.draw();
}

On x86-64, this compiles to something like:

call_draw:
    ; rdi = data pointer (obj.data)
    ; rsi = vtable pointer (obj.vtable)
    mov     rax, [rsi + 24]    ; load draw function pointer from vtable
    mov     rdi, rdi           ; data pointer becomes first argument (self)
    jmp     rax                ; tail call to draw implementation

The vtable lookup adds one memory indirection compared to a direct call. More significantly, the indirect
call through a register prevents the CPU from predicting the branch target until the vtable load completes.
Modern CPUs have indirect branch predictors, but they are less effective than direct branch prediction.

Compare this to a generic function with static dispatch:

fn call_draw_static<T: Drawable>(obj: &T) {
	obj.draw();
}

Here the compiler monomorphizes the function for each concrete T, producing a direct call:

call_draw_static_for_i32:
    jmp     <i32 as Drawable>::draw

No vtable lookup, no indirect branch. The call target is known at compile time, enabling inlining. If draw is
small, the compiler can inline it entirely, eliminating the call overhead and enabling further optimizations
across the inlined code.

Performance Implications of Dynamic Dispatch
The overhead of virtual dispatch is not just the vtable lookup. The indirect call has cascading effects on
the CPU pipeline.

Modern CPUs predict branch targets to keep the instruction pipeline full. For direct calls, the target is
encoded in the instruction itself, and the predictor can fetch the target instructions immediately. For
indirect calls through a register, the CPU must wait for the register value to be computed (the vtable load),
then use an indirect branch predictor to guess the target. Indirect branch predictors maintain a table of
recent indirect branch targets, but they are less accurate than direct branch prediction, especially when a
call site dispatches to many different implementations.

When the predictor guesses wrong, the pipeline must be flushed and restarted from the correct target. On
modern x86-64, this costs roughly 15-20 cycles. If a tight loop calls a trait object method that alternates
between two implementations, the predictor may never stabilize, incurring the misprediction penalty on every
other iteration.

The vtable itself must be in cache for the lookup to be fast. A vtable is small (typically 32-64 bytes for a
trait with a few methods), but if we iterate over a heterogeneous collection of trait objects, each object
may point to a different vtable. With many distinct implementations, the vtables compete for cache space. The
first access to each vtable is a cache miss, adding 100+ cycles of memory latency.

Inlining is the most significant loss. When the compiler inlines a function, it can see both caller and
callee code simultaneously. This enables constant propagation, dead code elimination, loop fusion, and SIMD
vectorization across the boundary. In the general case, none of this is possible through a vtable; the
compiler cannot see through the indirection, so each call becomes an optimization barrier.

In C++, modern compilers can sometimes devirtualize virtual calls. If the concrete type is visible at the
call site (immediately after construction, or when the class is marked final), Clang/GCC may replace the
indirect call with a direct one. With LTO and -fwhole-program-vtables, C++ compilers can devirtualize when
only one implementation exists program-wide.

Rust's situation is less favorable. As of 2024, rustc does not provide the metadata LLVM needs for
whole-program devirtualization. Even when only a single type implements a trait in the entire binary, trait
object calls remain indirect. LLVM can devirtualize in trivial cases where the concrete type is immediately
visible (creating a trait object and calling a method in the same basic block), but this is rare in practice.
The tracking issue rust-lang/rust#68262 has seen little progress. For now, if you need devirtualization in
Rust, use generics or enums, the compiler will not rescue trait objects for you.

Consider a loop summing areas. Here we must be careful: the static and dynamic versions solve different
problems.

// Static dispatch: all elements must be the same concrete type
fn sum_areas_static<T: Drawable>(shapes: &[T]) -> f64 {
	shapes.iter().map(|s| s.area()).sum()
}

// Dynamic dispatch: elements can be different concrete types
fn sum_areas_dynamic(shapes: &[&dyn Drawable]) -> f64 {
	shapes.iter().map(|s| s.area()).sum()
}

The static version requires a homogeneous slice, all Circles or all Rectangles, never mixed. The dynamic
version accepts heterogeneous collections. These are not interchangeable; we choose based on whether we need
polymorphism over a closed or open set of types.

For the homogeneous case, the compiler can inline the entire loop body, unroll the loop, and potentially
vectorize with SIMD. For the heterogeneous case, each area() call is a function pointer load, an indirect
call, and a return. The loop cannot be vectorized because the compiler cannot prove anything about what area
() does.

When the set of types is closed and known at compile time, Rust offers a third approach that combines the
benefits of both: enum dispatch.

enum Shape {
	Circle(Circle),
	Rectangle(Rectangle),
}

impl Shape {
	fn area(&self) -> f64 {
		match self {
			Shape::Circle(c) => c.area(),
			Shape::Rectangle(r) => r.area(),
		}
	}
}

fn sum_areas_enum(shapes: &[Shape]) -> f64 {
	shapes.iter().map(|s| s.area()).sum()
}

The enum approach accepts a heterogeneous collection (circles and rectangles mixed), but the dispatch is a
compile-time match expression rather than a vtable lookup. The compiler can inline each branch, and modern
CPUs predict match arms more reliably than indirect calls. The slice is homogeneous at the type level (&
[Shape]), so it is cache-friendly, no pointer chasing, no scattered vtables.

The trade-off is extensibility. Adding a new shape to an enum requires modifying the enum definition and
every match. With trait objects, new types can implement the trait without touching existing code. Enums are
closed; traits are open.

The rule of thumb is to use generics with trait bounds for performance-critical code paths where all elements
share a concrete type. Use enums when the type set is closed and we need heterogeneous collections with
predictable dispatch. Reserve trait objects for open extensibility where the flexibility is worth the cost,
or for reducing compile times and binary size when performance is not critical.

The Vtable Location Trade-off
Rust and C++ made opposite design decisions about where to store the vtable pointer.

In C++, polymorphic objects embed a vptr directly in the object:

class Drawable {
	public:
		virtual void draw() = 0;
		virtual double area() = 0;
		virtual ~Drawable() = default;
		int x;
};

// Memory layout of a Drawable subclass instance:
// +0:  vptr (8 bytes, points to vtable)
// +8:  x (4 bytes)
// +12: padding (4 bytes)
// Total: 16 bytes

Every instance of a polymorphic class carries the vptr. A Drawable* or Drawable& is 8 bytes, but each object
is 8 bytes larger than it would be without virtual functions.

Rust places the vtable pointer in the reference, not the object:

struct Circle {
	radius: f64,
}

impl Drawable for Circle {
	fn draw(&self) { /* ... */ }
	fn area(&self) -> f64 { std::f64::consts::PI * self.radius * self.radius }
}

// Memory layout of Circle:
// +0: radius (8 bytes)
// Total: 8 bytes (no vptr)

// Memory layout of &dyn Drawable pointing to Circle:
// +0: data pointer (8 bytes)
// +8: vtable pointer (8 bytes)
// Total: 16 bytes

The trade-off is memory allocation versus reference size. With a million objects and one reference each, C++
uses 8 MB for vptrs embedded in objects; Rust uses 8 MB for vtable pointers in references. With a million
objects and ten references each, C++ still uses 8 MB; Rust uses 80 MB.

But Rust's design enables something C++ cannot do. The same object can be viewed through multiple trait
lenses simultaneously:

use std::fmt::Debug;

let circle = Circle { radius: 1.0 };

let drawable: &dyn Drawable = &circle;
let debug: &dyn Debug = &circle;
let any: &dyn std::any::Any = &circle;

Each reference carries its own vtable pointer, pointing to different vtables. The circle object itself is
unchanged. In C++, the vptr embedded in the object determines its dynamic type at construction time. A
Drawable* and a Base* pointing to the same object use the same embedded vptr (possibly adjusted for multiple
inheritance).

Dyn Compatibility
Not every trait can be used as a trait object. The rules for dyn compatibility (formerly called object
safety) restrict which traits work with dynamic dispatch.

A dyn-compatible trait must not require Self: Sized. Requiring Sized would contradict the purpose of trait
objects, which exist to handle values of unknown size. The trait must have no associated constants, since
constants require knowing the concrete type at compile time. It must have no associated types with generic
parameters, which would require instantiation at compile time.

All methods must be dispatchable or explicitly non-dispatchable. A dispatchable method must have a receiver
(&self, &mut self, Box<Self>, etc.), must not return Self by value, must not take Self by value as a
parameter, and must not have type parameters.

// Dyn compatible
trait Compatible {
	fn method(&self);
	fn returns_ref(&self) -> &str;
}

// NOT dyn compatible
trait Incompatible {
	fn returns_self(&self) -> Self;        // Self in return position
	fn takes_self(&self, other: Self);     // Self as parameter
	fn generic<T>(&self, x: T);            // type parameter
	const VALUE: i32;                       // associated constant
}

Dynamic dispatch requires a vtable entry for each method, and a vtable entry is a function pointer with a
fixed signature. If a method returns Self, the return type depends on the concrete type, which is erased. If
a method is generic over T, there would need to be infinitely many vtable entries, one for each T.

Methods that violate these rules can still exist on dyn-compatible traits if they have a where Self: Sized
bound, making them unavailable through trait objects but callable on concrete types:

trait PartiallyCompatible {
	fn dispatchable(&self);

	fn not_dispatchable(&self) -> Self where Self: Sized;
}

// This works:
let obj: &dyn PartiallyCompatible = &some_value;
obj.dispatchable();

// This does not compile:
// obj.not_dispatchable();

Auto Traits and Lifetime Bounds

Trait objects can include auto traits and lifetime bounds. Unlike regular traits, where only one non-auto
trait is allowed, auto traits can be added freely:

use std::fmt::Debug;

// All valid trait object types:
fn takes_debug(x: &dyn Debug) {}
fn takes_debug_send(x: &dyn Debug + Send) {}
fn takes_debug_send_sync(x: &(dyn Debug + Send + Sync)) {}
fn takes_debug_static(x: &(dyn Debug + 'static)) {}

The auto traits (Send, Sync, Unpin, etc.) do not add methods to the vtable. They are marker traits checked at
compile time when the trait object is created. If we try to create a &dyn Debug + Send from a type that is
not Send, the compiler rejects it:

use std::rc::Rc;
use std::fmt::Debug;

let rc: Rc<i32> = Rc::new(42);
// Error: Rc<i32> is not Send
// let obj: &(dyn Debug + Send) = &*rc;

Lifetime bounds constrain how long references inside the trait object can live. A dyn Trait + 'static
contains no non-'static references. A dyn Trait + 'a may contain references that live at least as long as 'a.
The default lifetime depends on context and follows elision rules.

Supertraits in the Vtable
When a trait has supertraits, the vtable includes method entries for the supertrait methods:

trait Shape {
	fn area(&self) -> f64;
}

trait Circle: Shape {
	fn radius(&self) -> f64;
}

The vtable for dyn Circle contains entries for both area and radius. When we call a supertrait method on a
trait object, it goes through the same vtable lookup:

fn print_area(c: &dyn Circle) {
	// This works because Shape::area is in the Circle vtable
	println!("Area: {}", c.area());
}

Trait object upcasting allows converting &dyn Circle to &dyn Shape. The compiler generates a different vtable
for dyn Shape that contains only area, and the conversion substitutes the vtable pointer:

fn use_as_shape(c: &dyn Circle) {
	let shape: &dyn Shape = c;  // upcasting coercion
	println!("Area: {}", shape.area());
}

The ?Sized Bound
By default, all type parameters have an implicit Sized bound:

fn foo<T>(x: T) { }
// is equivalent to:
fn foo<T: Sized>(x: T) { }

This makes sense. To pass x by value, the compiler must know its size to copy it onto the stack. But
sometimes we want to accept DSTs through references. The ?Sized bound relaxes the Sized requirement:

fn process<T: ?Sized>(x: &T) {
	// T might be a DST
	// x is a wide pointer if T is unsized
}

// Now this works:
process::<[u8]>(&[1, 2, 3][..]);
process::<str>("hello");
process::<dyn std::fmt::Debug>(&42);

The standard library uses ?Sized extensively. The signature of std::borrow::Borrow is:

pub trait Borrow<Borrowed: ?Sized> {
	fn borrow(&self) -> &Borrowed;
}

This allows String to implement Borrow<str>, returning &str, a reference to a DST.

In trait definitions, Self has an implicit Self: ?Sized bound, the opposite of type parameters. This allows
traits to be implemented for DSTs:

trait MyTrait {
	fn method(&self);
}

impl MyTrait for str {
	fn method(&self) {
		println!("length: {}", self.len());
	}
}

If traits required Self: Sized by default, we could not implement traits for str, [T], or dyn OtherTrait.

Custom DSTs
A struct can contain a DST as its last field, making the struct itself a DST:

struct MySlice {
	header: u32,
	data: [u8],  // DST as last field
}

MySlice is now a DST. We cannot create it directly on the stack. The only supported construction method is
through unsizing coercion from a sized variant:

struct MySized<const N: usize> {
	header: u32,
	data: [u8; N],
}

fn main() {
	let sized = MySized::<4> { header: 42, data: [1, 2, 3, 4] };
	let dynamic: &MySlice = unsafe {
		// Requires ptr::from_raw_parts or careful transmutation
		std::mem::transmute::<&MySized<4>, &MySlice>(&sized)
	};

	assert_eq!(dynamic.header, 42);
	assert_eq!(dynamic.data.len(), 4);
}

This is awkward and requires unsafe code. The ptr::from_raw_parts API (stabilized in Rust 1.79) provides a
safer way to construct custom DST pointers, but the ergonomics remain poor. Most code uses the built-in DST
types ([T], str, dyn Trait) rather than defining custom ones.

To construct a DST, we must provide the metadata (length or vtable pointer) that completes the type. But the
language provides no syntax for this at the value level. The standard library handles DST construction
internally through careful unsafe code and compiler magic for types like str and [T].

Invalid Metadata is Undefined Behavior
The metadata in a wide pointer is not merely informational. The compiler trusts it for safety-critical
operations. Providing invalid metadata is undefined behavior.

For slices, the length must not cause the slice to extend beyond its allocation:

let arr = [1, 2, 3];
let ptr = arr.as_ptr();
// UB: length 1000 exceeds the allocation
let bad_slice: &[i32] = unsafe {
	std::slice::from_raw_parts(ptr, 1000)
};

For trait objects, the vtable must be a valid vtable for the trait that matches the actual dynamic type of
the pointed-to object:

// UB: null is not a valid vtable
let data_ptr: *const () = &42i32 as *const i32 as *const ();
let vtable_ptr: *const () = std::ptr::null();
let bad: &dyn std::fmt::Debug = unsafe {
	std::mem::transmute((data_ptr, vtable_ptr))
};

The Rust Reference explicitly lists invalid wide pointer metadata as undefined behavior. Calling a method
through a corrupted vtable pointer could jump to arbitrary code. Using an invalid slice length could read or
write out of bounds.

Polymorphism: Monomorphization vs Vtables
We saw how Rust represents pointers to dynamically sized types. A &dyn Draw carries both a data pointer and a
vtable pointer, 16 bytes that enable runtime method dispatch. But this raises a question we have not yet
answered: why does Rust need two polymorphism mechanisms at all? Templates and generics already let us write
code that works across types. Why introduce vtables?

When we write a function that operates on "any type implementing trait X," the compiler must decide how to
generate code for it. Two strategies exist. The compiler can stamp out a separate copy of the function for
each concrete type that actually gets used, a process called monomorphization. Alternatively, the compiler
can generate a single copy of the function that dispatches method calls through a table of function pointers
at runtime. Both C++ and Rust support both strategies. C, lacking native generics, provides only workarounds
that approximate each approach with varying degrees of type safety.

Monomorphization eliminates runtime indirection entirely; every call is direct, every function body can be
inlined, the optimizer sees through abstraction boundaries. But the binary contains a separate copy of the
generic code for every type instantiation, which bloats both compile time and binary size. Dynamic dispatch
through vtables produces a single copy of the code regardless of how many implementing types exist, but every
method call requires loading a function pointer from memory and jumping through it, which the CPU cannot
predict well and which prevents inlining.

C Without Generics
C has no built-in parametric polymorphism. We cannot write a function that operates on "any comparable type"
and have the compiler generate type-safe, specialized versions. Historically, C programmers used three
workarounds.

Preprocessor macros perform textual substitution before the compiler ever sees the code:

#define MAX(a, b) ((a) > (b) ? (a) : (b))

int x = MAX(3, 5);           // expands to ((3) > (5) ? (3) : (5))
double y = MAX(1.5, 2.5);    // expands to ((1.5) > (2.5) ? (1.5) : (2.5))

Each use site expands to type-specific code, achieving something like monomorphization. But macros operate
outside the type system. The preprocessor has no concept of what a and b are; it pastes text. This leads to
the classic double-evaluation trap:

#define SQUARE(x) ((x) * (x))
int a = 5;
int b = SQUARE(a++);  // expands to ((a++) * (a++)), undefined behavior

The argument is evaluated twice. If it has side effects, the program's behavior becomes undefined. The macro
cannot evaluate its argument once and bind it to a local; macros do not have local variables.

C11 introduced _Generic, which provides compile-time type dispatch:

#define abs(x) _Generic((x),    \
	int: abs_int,               \
	long: abs_long,             \
	double: fabs,               \
	default: abs_int)(x)

int abs_int(int x) { return x < 0 ? -x : x; }
long abs_long(long x) { return x < 0 ? -x : x; }

The _Generic keyword examines the type of its first argument and selects the corresponding expression. This
is better than macros: the selection happens within the type system, and each branch is a proper function
that evaluates its argument once. But we must enumerate every supported type explicitly and write separate
implementations for each. We have not reduced code duplication; we have centralized dispatch.

For dynamic polymorphism, we can use function pointers with void*:

typedef int (*comparator)(const void*, const void*);

void qsort(void* base, size_t nmemb, size_t size, comparator cmp);

int compare_int(const void* a, const void* b) {
	return *(const int*)a - *(const int*)b;
}

int arr[] = {5, 2, 8, 1};
qsort(arr, 4, sizeof(int), compare_int);

The standard library's qsort treats the array as raw bytes and accepts a function pointer for comparison. The
type information is erased: the comparator receives void* and must cast internally. Nothing prevents passing
compare_int to sort an array of double. The compiler cannot verify correctness. If the programmer gets it
wrong, the program silently produces garbage or crashes.

C++ Templates
C++ templates let us define families of functions and classes parameterized over types:

template<typename T>
T max(T a, T b) {
	return (a > b) ? a : b;
}

int x = max(3, 5);           // instantiates max<int>
double y = max(1.5, 2.5);    // instantiates max<double>

When the compiler encounters max(3, 5), it deduces T = int and generates a specialized function max<int>. A
separate max<double> gets generated for the second call. Each instantiation is compiled independently,
producing code identical to what we would write by hand. There is no runtime overhead.

Templates use what is sometimes called duck typing: instantiation succeeds when the operations used in the
template body are valid for the concrete type; otherwise, the compiler emits an error. The problem is that
errors emerge from deep within the template instantiation, often producing notoriously verbose diagnostics
that obscure the root cause. The template's requirements are implicit; we discover at instantiation time
whether a type satisfies them.

This implicit checking enables SFINAE (Substitution Failure Is Not An Error), a mechanism where invalid
template substitutions silently remove candidates from the overload set rather than causing hard errors.
Before C++20, constraining templates required arcane metaprogramming with std::enable_if and type traits:

#include <type_traits>

template<typename T>
typename std::enable_if<std::is_integral<T>::value, T>::type
absolute(T x) {
	return x < 0 ? -x : x;
}

The enable_if machinery conditionally makes the return type valid or invalid depending on the type trait. If
T is not integral, the substitution fails, and this overload is removed from consideration. The code works
but is dense with machinery that obscures intent.

C++20 introduced concepts, which make constraints explicit:
#include <concepts>

template<typename T>
concept Comparable = requires(T a, T b) {
	{ a < b } -> std::convertible_to<bool>;
	{ a > b } -> std::convertible_to<bool>;
};

template<Comparable T>
T max(T a, T b) {
	return (a > b) ? a : b;
}

The Comparable concept declares what operations T must support. The template states its constraint explicitly
in the signature. If we instantiate max with a non-comparable type, the error message refers directly to the
violated concept rather than to some failed substitution buried in the template body.

Regardless of how constraints are expressed, templates monomorphize. For large templates like std::sort or
std::unordered_map, binary bloat becomes pretty significant. We can mitigate this effect with explicit
instantiation, where we declare in a single translation unit which instantiations to generate:

// header
template<typename T>
void process(T x);

// source
template<typename T>
void process(T x) { /* implementation */ }

template void process<int>(int);
template void process<double>(double);

Other translation units can use process<int> without triggering instantiation; they link against the
pre-generated code. This reduces compile time and binary size at the cost of flexibility.

C++ Virtual Functions
C++ supports runtime polymorphism through virtual functions. A class with at least one virtual function is
polymorphic:

class Shape {
public:
	virtual double area() const = 0;
	virtual ~Shape() = default;
};

class Circle : public Shape {
	double radius;
public:
	Circle(double r) : radius(r) {}
	double area() const override { return 3.14159 * radius * radius; }
};

class Rectangle : public Shape {
	double width, height;
public:
	Rectangle(double w, double h) : width(w), height(h) {}
	double area() const override { return width * height; }
};

When we call a virtual function through a base class pointer or reference, the actual function invoked
depends on the dynamic type of the object:

void print_area(const Shape& s) {
	std::cout << s.area() << "\n";  // virtual dispatch
}

Circle c(1.0);
Rectangle r(2.0, 3.0);
print_area(c);  // calls Circle::area
print_area(r);  // calls Rectangle::area

The compiler cannot know at compile time which implementation to call. The decision is deferred to runtime.

To implement this, the compiler inserts a hidden pointer (the vptr) into every object of a polymorphic class.
The vptr points to a static table (the vtable) shared by all objects of the same dynamic type. The Itanium
C++ ABI, used by GCC, Clang, and most non-Windows compilers, specifies the vtable layout precisely.

The vtable contains several components, laid out at specific offsets from an address point. Components before
the address point (at negative offsets) include virtual call offsets for adjusting this pointers in multiple
inheritance scenarios, virtual base offsets for locating virtual base subobjects, and the offset-to-top, a
ptrdiff_t giving the displacement from this vtable pointer location to the top of the complete object (used
by dynamic_cast<void*>). At the address point sits the RTTI pointer, pointing to type information for runtime
type identification. After the address point come the virtual function pointers themselves, in declaration
order.

For a simple class hierarchy without multiple inheritance, the layout simplifies. A Circle object in memory
looks like:

Circle object (16 bytes on x86-64):
+0:   vptr (8 bytes, points to Circle's vtable)
+8:   radius (8 bytes, double)

Circle's vtable:
-16:  offset-to-top (0)
-8:   RTTI pointer
 0:   &Circle::~Circle() (complete destructor)
+8:   &Circle::~Circle() (deleting destructor)
+16:  &Circle::area()

The vptr in every Circle instance points to offset 0 of this vtable, the address point. When we call s.area()
where s is a Shape&, the compiler generates:

; rdi = pointer to Shape object
mov     rax, [rdi]          ; load vptr from object
mov     rax, [rax + 16]     ; load area() pointer from vtable
call    rax                 ; indirect call

Two memory loads occur on every virtual call: one to fetch the vptr from the object, one to fetch the
function pointer from the vtable. More significantly, the call rax is an indirect branch. The CPU's branch
predictor must guess the target without knowing it until the register value is computed. If a call site
invokes many different implementations (iterating over a heterogeneous container of shapes), the predictor
may thrash, causing pipeline stalls.

The optimizer cannot inline through a virtual call. It does not know which function will be invoked, so it
cannot substitute the function body at the call site. This blocks constant propagation, dead code
elimination, and other interprocedural optimizations.

The advantage is code size. There is exactly one print_area function, regardless of how many shape types
exist. The vtable adds per-class overhead, not per-use overhead. For large class hierarchies, this can
dramatically reduce binary size compared to templated alternatives.

Rust Generics
constraints are declared upfront.

fn max<T: PartialOrd>(a: T, b: T) -> T {
	if a > b { a } else { b }
}

let x = max(3, 5);           // instantiates max::<i32>
let y = max(1.5, 2.5);       // instantiates max::<f64>

The bound T: PartialOrd states that T must implement the PartialOrd trait. The compiler checks this at the
call site: calling max with a type that does not implement PartialOrd produces an error that directly states
the unsatisfied bound.

Inside the function body, we can only use operations that PartialOrd guarantees. Attempting to call methods
not provided by the bound fails immediately, not at instantiation:

fn broken<T: PartialOrd>(a: T, b: T) -> T {
	println!("{}", a);  // error: T doesn't implement Display
	if a > b { a } else { b }
}

Rust checks the generic function against its declared bounds before instantiation. This differs from C++
templates, where the body is tentatively compiled against each concrete type, with errors emerging during
instantiation.

The monomorphization process itself works similarly. When the Rust compiler encounters generic function
calls, it records the concrete types. During code generation, the monomorphization collector traverses the
call graph to identify all required instantiations. Each generic function paired with each set of concrete
type arguments becomes a distinct mono item that gets compiled to machine code.

The collector partitions mono items into Codegen Units (CGUs). For incremental compilation, the partitioner
creates separate CGUs for stable non-generic code and for monomorphized instances. When only generic
instantiations change, the stable CGU can be reused.

The same binary size concerns apply. A generic function used with many types produces many copies. The cargo
llvm-lines tool shows which functions contribute most to generated LLVM IR. In large codebases, common
utility functions like Option::map and Result::map_err can get instantiated hundreds of times, dominating
code size.

The standard mitigation is the inner function pattern: move the bulk of the logic into a non-generic inner
function, leaving a thin generic wrapper:

pub fn read<P: AsRef<Path>>(path: P) -> io::Result<Vec<u8>> {
	fn inner(path: &Path) -> io::Result<Vec<u8>> {
		let mut file = File::open(path)?;
		let size = file.metadata().map(|m| m.len()).unwrap_or(0);
		let mut bytes = Vec::with_capacity(size as usize);
		io::default_read_to_end(&mut file, &mut bytes)?;
		Ok(bytes)
	}
	inner(path.as_ref())
}

The outer generic function calls as_ref() to convert P to &Path, then delegates to inner. Now inner compiles
once regardless of how many path types are used. The outer wrapper is tiny, its monomorphization overhead
minimal.

Rust Trait Objects
When monomorphization costs are prohibitive, Rust offers trait objects. We already covered the
representation: &dyn Trait is a wide pointer containing a data pointer and a vtable pointer. Calling a method
loads the function pointer from the vtable and invokes it indirectly.

The key difference from C++ is where the vtable pointer lives. In C++, the vptr is embedded in the object. A
Circle contains its own vptr; the Circle type is inherently polymorphic. In Rust, the vtable pointer is in
the reference, not the object. A plain Circle struct has no vptr. The vtable pointer appears only when we
view the Circle through a trait object:

let c = Circle { radius: 1.0 };
let shape: &dyn Shape = &c;  // wide pointer created here

The Circle struct occupies only the bytes its fields require. The 8-byte vtable pointer is added when we
create the &dyn Shape, not when we create the Circle. An object can be viewed through multiple different
trait objects, each with its own vtable, without the object itself containing any vtable pointers.

This design means Rust structs remain trivially copyable (if their fields are) even when they implement
traits with virtual methods. In C++, adding a single virtual function to a class makes it non-trivially
copyable and increases its size by the vptr. In Rust, implementing a trait never changes a struct's layout.

Choosing Between Strategies
Static dispatch through monomorphization is appropriate when the hot path demands maximum performance, when
the set of types is small and known, when inlining across the generic boundary matters, and when compile time
and binary size are acceptable costs.

Dynamic dispatch through vtables is appropriate when the concrete type cannot be known until runtime (plugin
systems, user-defined types loaded dynamically), when binary size is constrained, when compile time must be
minimized, and when the call overhead is negligible compared to the work the method performs.

There are common patterns that combine both. For example, we can expose a generic public API and then convert
to a trait object internally to avoid instantiation explosion:

pub fn process<W: Write>(writer: W) {
	process_dyn(&mut writer as &mut dyn Write)
}

fn process_dyn(writer: &mut dyn Write) {
	// large implementation, compiled once
}

The public API accepts any Write implementor. Internally, we immediately convert to a trait object.
process_dyn compiles only once. The cost is one virtual dispatch per method call within process_dyn, but the
binary contains only one copy of the implementation.

Under the Hood
Consider incrementing a counter through both dispatch strategies. Let's start with static dispatch:

trait Counter {
	fn increment(&mut self);
}

struct Simple(u64);
impl Counter for Simple {
	fn increment(&mut self) { self.0 += 1; }
}

fn inc_static<T: Counter>(c: &mut T) {
	c.increment();
}

For inc_static::<Simple>, monomorphization produces:

inc_static_Simple:
    add     qword ptr [rdi], 1
    ret

The entire method call is inlined to a single add instruction. The trait abstraction has zero runtime cost.

Now consider dynamic dispatch:

fn inc_dynamic(c: &mut dyn Counter) {
	c.increment();
}

The compiler generates:

inc_dynamic:
    mov     rax, [rsi + 24]    ; load increment from vtable
    mov     rdi, rdi           ; data pointer (already in rdi)
    jmp     rax                ; tail call through vtable

The function loads the method pointer from the vtable and jumps to it. The actual increment happens in the
target function, which cannot be inlined here. For a single increment, the difference is trivial. In a tight
loop incrementing millions of times, the static version avoids the vtable load and indirect branch on every
iteration.

The static version also enables further optimization. If the compiler can prove the counter is never observed
between increments, it can batch them. If the counter value is known, it can constant-fold. None of this is
possible through the vtable indirection.

Closures and Captures
We have seen two strategies for polymorphism: monomorphization produces specialized code for each concrete
type, while vtables enable a single function to operate on values of unknown type through indirect dispatch.
Both mechanisms deal with code that varies. But what happens when we need a function that carries state?

Consider a sorting function that accepts a comparison predicate. The predicate must know how to compare,
which is pure code. But suppose we want to sort by distance from some reference point. Now the predicate
needs access to the reference point's coordinates, data that exists outside the function itself. The
predicate is no longer pure code; it is code plus environment.

This is the closure problem. A closure closes over variables from its enclosing scope, capturing them for
later use. The three languages approach this problem with characteristic differences. C lacks closures
entirely and requires manual workarounds. C++ introduced lambda expressions that desugar to anonymous structs
with an overloaded call operator. Rust closures work similarly but integrate with the ownership system, with
the Fn, FnMut, and FnOnce traits encoding how the closure interacts with its captured state.

C: Function Pointers and the Context Pattern
C has function pointers, not closures. A function pointer is an address of executable code; it contains no
data beyond the address itself.

int compare_ints(const void *a, const void *b) {
	return *(const int*)a - *(const int*)b;
}

qsort(array, n, sizeof(int), compare_ints);

This works when comparison needs no external state. When it does, C libraries adopt a convention: pass a
void* context alongside the function pointer, and the callback receives this context as an additional
argument.

struct DistanceContext {
	double ref_x, ref_y;
};

int compare_by_distance(const void *a, const void *b, void *ctx) {
	const struct Point *pa = a;
	const struct Point *pb = b;
	const struct DistanceContext *c = ctx;

	double da = hypot(pa->x - c->ref_x, pa->y - c->ref_y);
	double db = hypot(pb->x - c->ref_x, pb->y - c->ref_y);
	return (da > db) - (da < db);
}

// Usage requires a sorting function that accepts context
struct DistanceContext ctx = { .ref_x = 0.0, .ref_y = 0.0 };
qsort_r(points, n, sizeof(struct Point), compare_by_distance, &ctx);

The qsort_r variant (POSIX, not standard C) threads the context through to the comparator. The pattern is
universal in C callback APIs: a function pointer paired with a void* that the library passes back untouched.

The void* erases type information; nothing prevents us from passing a DistanceContext* to a callback
expecting something else. The compiler cannot verify that the context pointer remains valid when the callback
executes. If the callback outlives the context's stack frame, we have a dangling pointer. The burden falls
entirely on us.

C++ Lambdas
C++11 introduced lambda expressions, syntactic sugar for anonymous function objects. A lambda like

auto ref_x = 0.0, ref_y = 0.0;
auto compare = [ref_x, ref_y](const Point& a, const Point& b) {
	double da = std::hypot(a.x - ref_x, a.y - ref_y);
	double db = std::hypot(b.x - ref_x, b.y - ref_y);
	return da < db;
};

desugars to something equivalent to

struct __lambda_1 {
	double ref_x;
	double ref_y;

	bool operator()(const Point& a, const Point& b) const {
		double da = std::hypot(a.x - ref_x, a.y - ref_y);
		double db = std::hypot(b.x - ref_x, b.y - ref_y);
		return da < db;
	}
};

__lambda_1 compare{ref_x, ref_y};

The capture list specifies what enters the closure and how. [x] captures x by value (copies it into the
struct). [&x] captures by reference (the struct holds a reference). [=] captures everything used by value.
[&] captures everything by reference. [x, &y] mixes modes.

Each lambda has a unique anonymous type that we cannot name. We must use auto for local variables or
templates for function parameters:

template<typename F>
void use_callback(F&& f) {
	f();
}

Alternatively, std::function<R(Args...)> provides type erasure, wrapping any callable with a matching
signature into a uniform type at the cost of heap allocation and virtual dispatch.

The capture mode determines the closure's size. A lambda capturing two doubles by value occupies 16 bytes
(plus alignment). A lambda capturing by reference stores pointers, 8 bytes each on x86-64. A lambda capturing
nothing is stateless; the standard guarantees that captureless lambdas can convert to plain function
pointers:

int (*fp)(int, int) = [](int a, int b) { return a + b; };

C++ lambdas are mutable by default if they capture by value with mutable:

int counter = 0;
auto increment = [counter]() mutable { return ++counter; };
// Each call modifies the lambda's internal copy of counter

Without mutable, the call operator is const and cannot modify captured values. The default reflects the
common case where closures are passed to algorithms and called multiple times; mutating captured state would
be surprising.

Rust Closures

Rust closures follow the same structural principle: a closure is an anonymous struct containing captured
values, with a method implementing the call. But the details differ in ways that matter for safety.

let ref_x = 0.0_f64;
let ref_y = 0.0_f64;
let compare = |a: &Point, b: &Point| {
    let da = ((a.x - ref_x).powi(2) + (a.y - ref_y).powi(2)).sqrt();
    let db = ((b.x - ref_x).powi(2) + (b.y - ref_y).powi(2)).sqrt();
    da.partial_cmp(&db).unwrap()
};

The compiler generates a struct like

struct __closure_1<'a> {
	ref_x: &'a f64,
	ref_y: &'a f64,
}

impl<'a> FnOnce<(&Point, &Point)> for __closure_1<'a> {
	type Output = std::cmp::Ordering;
	extern "rust-call" fn call_once(self, args: (&Point, &Point)) -> Self::Output {
		let (a, b) = args;
		let da = ((a.x - *self.ref_x).powi(2) + (a.y - *self.ref_y).powi(2)).sqrt();
		let db = ((b.x - *self.ref_x).powi(2) + (b.y - *self.ref_y).powi(2)).sqrt();
		da.partial_cmp(&db).unwrap()
	}
}

impl<'a> FnMut<(&Point, &Point)> for __closure_1<'a> {
	extern "rust-call" fn call_mut(&mut self, args: (&Point, &Point)) -> Self::Output {
		self.call_once(args)
	}
}

impl<'a> Fn<(&Point, &Point)> for __closure_1<'a> {
	extern "rust-call" fn call(&self, args: (&Point, &Point)) -> Self::Output {
		self.call_once(args)
	}
}

Unlike C++, Rust does not require explicit capture annotations. The compiler infers the capture mode from how
variables are used in the closure body. If we only read a variable, it captures by shared reference. If we
mutate it, it captures by mutable reference. If we move out of it or if the variable does not implement Copy,
it captures by value.

let s = String::from("hello");

// Captures s by shared reference (only reads)
let c1 = || println!("{}", s);

// Captures s by mutable reference (mutates)
let mut s = String::from("hello");
let c2 = || s.push_str(" world");

// Captures s by value (moves out)
let s = String::from("hello");
let c3 = || drop(s);

The move keyword overrides inference, forcing all captures to be by value:

let s = String::from("hello");
let c = move || println!("{}", s);
// s is no longer accessible here; it was moved into the closure

This is essential when the closure must outlive the scope where it was created, as when spawning a thread:

let data = vec![1, 2, 3];
std::thread::spawn(move || {
	// data is owned by this closure, transferred to the new thread
	println!("{:?}", data);
});

Without move, the closure would capture data by reference. But data lives on the spawning thread's stack,
which may be deallocated before the spawned thread runs. The compiler rejects this:

let data = vec![1, 2, 3];
std::thread::spawn(|| {
	println!("{:?}", data);  // ERROR: closure may outlive borrowed value
});

The move keyword forces ownership transfer, ensuring the closure owns data and can safely take it to another
thread.

The Fn Trait Hierarchy
The three traits Fn, FnMut, and FnOnce form a hierarchy that describes how a closure can be called, not how
it captures.

FnOnce requires ownership of the closure to call it. The method signature is fn call_once(self, args: Args)
-> Output. After calling a FnOnce, the closure is consumed. Every closure implements FnOnce because every
closure can be called at least once.

FnMut requires mutable access to the closure. The signature is fn call_mut(&mut self, args: Args) -> Output.
A closure implements FnMut if calling it does not require consuming any captured values. It may mutate its
captures, but it does not move out of them.

Fn requires only shared access. The signature is fn call(&self, args: Args) -> Output. A closure implements
Fn if calling it neither consumes nor mutates any captured values.

The hierarchy is Fn: FnMut: FnOnce. A closure implementing Fn automatically implements FnMut and FnOnce. The
traits encode what the closure does when called, not how it captured its environment.

This can be tricky if we think in terms of C++ lambdas. For example, a move closure can still implement Fn:

let x = 42;
let c = move || x;  // captures x by value (copies, since i32: Copy)
// c implements Fn because calling it only reads the captured x

Conversely, a closure that captures by reference but mutates the referent implements only FnMut:

let mut counter = 0;
let mut increment = || { counter += 1; };
// increment implements FnMut (mutates through captured &mut)
// does NOT implement Fn

And a closure that moves out of a captured value implements only FnOnce:

let s = String::from("hello");
let consume = move || drop(s);
// consume implements FnOnce only (consumes captured s)
// does NOT implement FnMut or Fn

Functions accepting closures declare which trait they require. Iterator::for_each takes FnMut because it
calls the closure multiple times but does not need concurrent shared access. Iterator::map takes FnMut for
the same reason. thread::spawn takes FnOnce because the closure runs exactly once in the spawned thread.

fn call_twice<F: FnMut()>(mut f: F) {
	f();
	f();
}

fn call_once<F: FnOnce()>(f: F) {
	f();
}

Closure Traits and Send/Sync
Closures inherit Send and Sync from their captures. If all captured values are Send, the closure is Send. If
all values captured by shared reference are Sync, and all values captured by mutable reference, copy, or move
are Send, then the closure is Send. These rules match normal struct composition.

use std::rc::Rc;

let rc = Rc::new(42);
let closure = move || println!("{}", rc);
// closure is NOT Send because Rc is not Send
// std::thread::spawn(closure);  // ERROR

use std::sync::Arc;

let arc = Arc::new(42);
let closure = move || println!("{}", arc);
// closure IS Send because Arc is Send
std::thread::spawn(closure);  // OK

A closure is Clone or Copy if it does not capture by mutable reference and all its captures are Clone or
Copy:

let x = 42;
let closure = move || x;
// closure is Copy because it captures only Copy types by value

let y = closure;  // copy
let z = closure;  // still valid

These automatic trait derivations mean closures integrate with Rust's concurrency primitives without special
handling. A move closure capturing Arc<Mutex<T>> is Send, can be shipped to another thread, and provides safe
interior mutability through the mutex. The type system composes.


---
