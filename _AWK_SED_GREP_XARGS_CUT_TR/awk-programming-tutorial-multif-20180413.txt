filename: awk_programming-tutorial-multif_20180413.txt
http://www.yourownlinux.com/2018/03/awk-programming-tutorial-introduction-to-awk.html

AWK Programming Tutorial - Introduction to awk
24 March 2018

   In some of our articles we have learned about one of the most important and useful utilities in Linux
   for text processing - sed. Using sed, we can:
     * Edit () one or more files in place
     * Simplify and automate file edits on one or more files at a time without using vi
     * Write scripts to process and convert text

   In this article, we are introducing you to another immensely powerful text processing utilities in
   Bash - awk. awk programming is very useful while manipulating structured data, especially while
   creating a summerized reports out of some data that has some structure (like a table, which has
   columns and rows). Below are some of the common tasks which we can perform using awk:
     * Interpret a text file as a SQL database and process fields and records.
     * Perform arithmetic and logical operations
     * Perform string operations
     * Create and use conditionals and loops
     * Define and use functions
     * Execute a shell command and process its output
     * Extract, analyze and create reports from the data

   With the power and range that awk offers us, we can replace an entire shell script with an awk single
   liner.
   How sed and awk operate
     * awk and sed, both of them work in a similar way.
     * Read an input line from a file
     * Store it in a buffer (or create a copy of it, whatever is easier to understand)
     * Run instructions from the script on the buffered (or copied) input line. This won't actually
       change the original input line.
     * Replace the original data with processed data (in case of sed).

   sed and awk Instructions
     * A sed and an awk instruction consist of two components - a pattern and a process
     * The pattern part is a regular expression
     * The process part is the action we wish to perform
     * It reads first line from the input file and first instruction from the script
     * It then matches the pattern against the line.
     * If there is no match, current instruction is skipped and next one is picked up.
     * If a match is found, instruction is executed on the line.
     * Once all the instructions are executed on the current line, the cycle repeats for the all other
       lines from the input file.
     * As soon as all instructions are executed on the current line, sed prints the output. However,
       awk's behaviour is a bit different, as the subsequent instructions in the script control further
       processing on the line.

   How to use awk?
   Till now, it is clear that both sed and awk run instructions on a single line from input file at a
   time. We can provide these instructions either on command line itself or we can store those in a
   file. Depending on the scenario, we have two syntaxes to use awk command:
   When using command line:
$ awk '<instructions>' <input_file/s>

   When using a script file:
$ awk -f <script_file> <input_file/s>

   The awk instructions in the script file have the same components we discussed earlier - pattern and
   process. The later component is a bit more complex than that in sed, as it will have variables,
   conditionals, loops, functions etc.
   awk works better on a structured data, so it considers each line from input file(s) as a record.
   Being a structured data, each line will have strings/words delimited or separated by spaces or tabs
   or some character (comma in case of CSV files). awk interprets each of those strings/words as a
   field. We can reference these fields using their column numbers as $1, $2, $3 and so on, where $1
   represents first field from the record, $2 is second field and so on. $0 is used to reference the
   whole line or record. Lets understand this from below example.
   Consider that, we have an input file with contents as below (snipped):
$ head result.txt
Student Subject Marks
James Biology 31
Velma Biology 43
Kibo Biology 81
Louis Biology 11
Phyllis Biology 18
Zenaida Biology 55
Gillian Biology 38
Constance Biology 16
Giselle Biology 73

   In order to print first field of every line from the input file, we use the reference variable $1 as:
$ awk '{ print $1 }' result.txt
Student
James
Velma
Kibo
Louis
Phyllis
Zenaida
Gillian
Constance
Giselle

   Note that, we have not used any pattern here. So the instructions will simply be executed on each and
   every line from the input file. In the next example, we take a look at using regular expression to
   process only selected lines (that matches the pattern) from the input file. Consider that, we just
   need to find the lines that matches the pattern /Ki/ (Remember to include any pattern inside forward
   slashes).
$ awk '/Ki/' result.txt
Kibo Biology 81
Kirsten Biology 16
Kieran Biology 45
Kitra Chemistry 47

   In above example, we have not mentioned any instructions. So, the default instruction is to print
   every line that matches the pattern.
   In the next example, we will use both pattern and the instruction. Lets's say, we want to extract the
   marks from those records that match the pattern /Ki/. Remember, marks is the 3rd field, so we should
   reference it accordingly.
$ awk '/Ki/ { print $3 }' result.txt
81
16
45
47

   So far, we have not used delimiters in our examples. But, just for your information, awk interprets
   whitespace as a delimiter by default. In order to specify a custom delimiter, say a comma for a CSV
   file, we can use option -F (and not -f which we use to provide script file as an input) as below:
$ awk -F, '/some_pattern/ {print $2}' input_csv_file

   Or we can use multiple instructions in the awk command. These instructions need to be separated by a
   semicolon as shown below:
awk '/Ki/ { print $3; print $2; print $1 }' result.txt
81
Biology
Kibo
16
Biology
Kirsten
45
Biology
Kieran
47
Chemistry
Kitra

   Awesome! Don't worry if its too much to hog. We have many articles coming up covering each topic in
   brief. Just give it a try with simple awk commands. You may face some errors while using awk. You
   just need to correct your syntax and you should be good. Normally, the cause of errors are:
     * Not opening/closing the braces ({ })
     * Not opening/closing the single quotes (' ')
     * Not opening or closing the slashes for patterns (/ /)

---
http://www.yourownlinux.com/2018/03/awk-programming-tutorial-hello-world-with-awk.html

AWK Programming Tutorial - Hello world with awk
27 March 2018

   Hello world with awk: This is the second article from the tutorial series on awk and in this article,
   we will be learning to some different ways to print Hello world with awk. In fact, there in no need
   to have a separate article for this, but usually we do start with a Hello world program while
   learning any programming language. Never mind, we start.
   [18][hello-world-with-awk.png]
   In the first article in this tutorial series on awk, Introduction to awk, we learned the basic syntax
   to use awk on command line and it is as below:
$ awk '<instructions>' <input_file/s>

   Alternately, we can have these instructions stored in a script file, then the syntax becomes:
$ awk -f <script_file> <input_file/s>

   So, in order to print Hello world, we need a print statement in the instruction and a file. Lets
   create a file myFile with any random contents.
$ cat myFile
This is some random content, awk will ignore it

   Now, we write our awk command with the print "Hello world" as instruction and myFile as the input
   file.
$ awk ' {print "Hello world"} ' myFile
Hello world

   In above example, awk will execute the print statement for every single line from the input file. As
   our input file has only one line, awk reads it and prints Hello world.

   Another way, we can have the string Hello world written in myFile and use print $0 statement in the
   instruction. $0 will print entire line read from input file.
$ cat myFile
Hello world

$ awk ' {print $0} ' myFile
Hello world

   From above two examples, it is pretty much clear that, we need an input file to feed awk command, or
   it will read input from stdin. You can check this by simply not mentioning the input file name in awk
   from first example and hitting ENTER key a few times.
$ awk ' {print "Hello world"} '
[ENTER]
Hello world
[ENTER]
Hello world
[ENTER]
Hello world
[Ctrl + C]

   Yet another way, we can use BEGIN block in our instruction. BEGIN block is executed before any input
   lines are read. Similarly, we have END block which gets executed once all of the lines from input
   file are read. Both BEGIN and END blocks are optional.
$ awk ' BEGIN { print "Hello world" } '
Hello world

   We will cover both BEGIN and END blocks in a separate article. 

---
http://www.yourownlinux.com/2018/03/awk-field-separator-delimiter-reference.html

AWK Programming Tutorial - Field separator and Field references
30 March 2018

   Awk Field separator and field references: This is the third article from our tutorial series on awk.
   In first article, we had an [18]introduction with awk and in second one, we created [19]Hello world
   program in awk. In this article, we will be learning about separating fields and referencing them
   using awk.

   Referencing Fields and Records
   In the first article from this tutorial series, [21]Introduction to awk, we covered following points:
     * awk presumes that the input is a structured type of data
     * It interprets each line from input file(s) as a Record
     * Each line will have strings/words separated (or delimited) by whitespaces or some character.
       These separators are referred to as delimiters.
     * Each of those strings/words separated by delimiter is called as a Field.

   Lets consider a familiar example to know about records, fields and delimiters, /etc/passwd file:
messagebus:x:107:111::/var/run/dbus:/bin/false
uuidd:x:108:112::/run/uuidd:/bin/false
sshd:x:110:65534::/var/run/sshd:/usr/sbin/nologin
foouser:x:1001:1001:,,,:/home/foouser:/bin/bash

   In above file, each of the line is interpreted as a record. As each word/string is separated by a
   colon ( : ), it becomes a delimiter and each word separated by the delimiter i.e. foouser, 1001,
   /bin/bash, etc. are the fields.

   In awk, we reference each field using $ operator, followed by a number or an awk variable. We learn
   more about awk variables in later articles to keep things simple here. Thus, we can reference first
   field from the record using $1, second field with $2, third field with $3 and so on. $0 is used to
   reference the record (or the input line).
   Lets take a look at following example. We have an input file result.txt with contents as below
   [snipped]:
Student Subject Marks
James Biology 31
Velma Biology 43
Kibo Biology 81
Louis Biology 11
Phyllis Biology 18
Zenaida Biology 55
Gillian Biology 38
Constance Biology 16
Giselle Biology 73

   We can see that there are 10 records and each record has 3 fields. Now we refer to each record and
   every field with their respective identifiers.
# Referencing first field
$ awk '{ print $1 }' result.txt
Student
James
Velma
Kibo
...
...

# Referencing second field
$ awk '{ print $2 }' result.txt
Subject
Biology
Biology
Biology
...
...

# Referencing third field
$ awk '{ print $3 }' result.txt
Marks
31
43
81
...
...

# Referencing all fields
$ awk '{ print $3, $1, $2 }' result.txt
Marks Student Subject
31 James Biology
43 Velma Biology
81 Kibo Biology
...
...

# Referencing a record
$ awk '{ print $0 }' result.txt
Student Subject Marks
James Biology 31
Velma Biology 43
Kibo Biology 81
...
...

   Field Separator
   In above example, we have not used any field separator or delimiter anywhere in the awk command. So,
   it can be concluded that, awk considers whitespace as a default field separator. awk allows us to set
   a field separator of our own choice with -F option followed by the delimiter. Lets check this with
   /etc/passwd file, that has fields delimited by a colon.
# /etc/passwd file contents (snipped)
$ cat /etc/passwd
...
messagebus:x:107:111::/var/run/dbus:/bin/false
uuidd:x:108:112::/run/uuidd:/bin/false
sshd:x:110:65534::/var/run/sshd:/usr/sbin/nologin
foouser:x:1001:1001:,,,:/home/foouser:/bin/bash
...

$ awk -F ':' '{ print $3, $1, $7 }' /etc/passwd
...
107 messagebus /bin/false
108 uuidd /bin/false
110 sshd /usr/sbin/nologin
1001 foouser /bin/bash
...

   While writing an awk script, we can change the field separator by using awk variable FS. We need to
   instruct awk to consider a custom delimiter before it start reading lines from input file. Here,
   BEGIN block comes handy. BEGIN block is executed before any input lines are read. Similarly, we have
   END block which gets executed once all of the lines from input file are read. Both BEGIN and END
   blocks are optional.
   So, we can write an awk script passwd.awk as:
BEGIN { FS = ":" }
{
    print $3, $1, $7
}

   As covered in our first tutorial ([22]link), we can use the instructions from this script using
   option -f as below:
$ awk -f passwd.awk /etc/passwd
...
107 messagebus /bin/false
108 uuidd /bin/false
110 sshd /usr/sbin/nologin
1001 foouser /bin/bash
...

   To make the output comprehensible, we can introduce a tab ( \t ) character between two output fields.
$ cat passwd.awk
BEGIN { FS = ":" }
{
    print $3 "\t" $1 "\t" $7
}

$ awk -f passwd.awk /etc/passwd
107     messagebus      /bin/false
108     uuidd   /bin/false
110     sshd    /usr/sbin/nologin
1001    foouser /bin/bash

   By default, all the instructions from the script are executed on every single line from the input
   file. To execute these instructions on selected lines, we can also introduce pattern matching by
   enclosing the regular expression within slashes ( /[REGEX]/ ). This will execute the instructions
   from awk script on only those lines matching the regex.
   To verify this, we use our results.txt file again. From the entire list of students and their marks
   in certain subjects, we can filter only those records of students who got exactly 50 marks, whichever
   may be the subject. So, we can use 50 as the pattern to match, as shown below:
awk ' /50/ {print $1"\t"$2"\t"$3} ' result.txt
Ori     Chemistry       50
Hyatt   Mathematics     50

   Or we can filter only those records in which students who have their names starting with string Jo.
   For this, we can use a regex ^Jo with tilde ( ~ ) operator to match against first field ( $1 ) which
   is name of the student.
$ awk ' $1 ~ /^Jo/ { print $1"\t"$2"\t"$3 }' result.txt
John    Biology 55
Jonas   Mathematics     40

   Or we can negate the same using the bang or logical not operator ( ! ) as shown below (result is be
   too long, hence now shown):
$ awk ' $1 !~ /^Jo/ { print $1"\t"$2"\t"$3 }' result.txt

   That's all for the scope of this article.

   
---
http://www.yourownlinux.com/2018/03/awk-programming-tutorial-constants-variables-arithmetic-operators.html

AWK Programming Tutorial - Constants, Variables and Arithmetic Operators
30 March 2018

   Constants, Variables and Operators in Awk : So far in this tutorial series on awk programming, we
   have learned to print stuff, we learned about fields, records and delimiters and how to they are
   referenced. These are very basic operations as we are just extracting data from input lines and
   printing them. Now, we move a step ahead and manipulate the extracted fields by performing some
   common arithmetic operations on them. Before we proceed, I recommend you to please go through the
   third article published on awk - [18]Field separator and Field references.

   Constants
   There are only two types of constants:
   A string constant
     * A string constant is always surrounded within quotes, e.g. "Pineapple", "5_days", etc.
     * String can use escape sequences like \n ( newline ), \t ( horizontal tab ), \b ( backspace ), \v
       ( vertical tab ), \" ( double quotes ), etc.

   A numeric constant
     * A numeric constant is a number without quotes, enough said.
     * A number enclosed within quotes is considered as a string.

   Variables
     * A variable is an identifier that references to the memory location that stores a value.
     * We can initialize a variable by assigning a value to it, using = operator, e.g., age = 20,
       firstName = "Eric", etc. Here, age and firstName are variables.
     * A variable name consist of alphabets, digits and underscores, and it must start with a letter or
       underscore.
     * Variables are case-sensitive. It means, Age, age and AGE are different variables and they can
       store different values in them that won't get overwritten.
     * Variable initialization is optional. If we do not initialize a variable, awk defaults the value
       to numeric 0 or a blank string ( "" ) appropriately.
     * When we assign two or more strings separated by space to a variable, it stores a concatenated
       value
     * We can assign a field value to a variable using field reference variables $1, $2, etc.

   Example:
# Assign a numeric value to variable 'myNum'
myNum = 10

# Assign a string value to variable 'myStr'
myStr = "awk!"

# Space concatenates the strings, so 'myVar' stores the value "AwesomeAwk!"
myVar = "Awesome" "Awk!"

# Assign a field value to a variable using field reference variable
marks = $1

   Arithmetic Operators
   awk supports basic arithmetic operators to be used in expressions, which are listed as below:
   Operator  Description
   +        Addition
   -        Subtraction
   *        Multiplication
   /        Division
   %        Modulus
   ^ or **  Exponentiation
   Below example shows how we can define a variable and perform arithmetic operations on them.
   Example:
# Initialize the variable 'salary'
salary = 300000

# Add 25000 to variable 'salary' and store the result in another variable 'newSalary'
newSalary = salary + 25000

# Print the updated salary
print newSalary

   Alternately, you can directly print the addition of salary and the number 25000 to further shrink the
   code as:
salary = 300000
print salary + 25000

   This way, we will get the similar result from print statement. But, the value stored in variable
   salary remains unchanged. If we were to update the variable salary with the added value, we can use
   assignment operator +=, that combines 2 operations, addition and assignment. So, we have the number
   25000 added to the value stored in salary and the result of addition is again stored in the variable
   salary.
   Below is the list of assignment operators:
   Operator                 Description
   +=       Add and assign
   -=       Subtract and assign
   *=       Multiply and assign
   /        Divide and assign
   %        Perform modulo and assign the result
   ^ or **  Perform exponentiation and assign the result
   To demonstrate this, we can use /etc/passwd and count the number of lines in it. For this, we
   initialize a variable x and increment it after every line is read. After the last line, we print the
   variable, which gives us total number of records read by awk.
# We can also use { x++ } or { x = x + 1 } instead of { x += 1 } in below command
$ awk ' { x += 1 } END { print x } ' /etc/passwd
31

   We can also include a condition here, to print the count of lines those have the string bash inside
   them.
$ awk '/bash/ { x += 1 } END { print x } ' /etc/passwd
3

   Another example. This time, we use a demo file which has 3 fields - Name of the student, Subject name
   and the Marks. Below is the snippet.
Student Subject Marks
James Biology 31
Velma Biology 43
Kibo Biology 81
Louis Biology 11
Phyllis Biology 18
Zenaida Biology 55
Gillian Biology 38
Constance Biology 16
Giselle Biology 73
...
...

   We can calculate the average marks obtained by students in Chemistry as follows:
awk ' /Chemistry/ { total += $3; count += 1 } END { print total/count } ' result.txt
52.4074

   We can also consider a data set of cities and their temperatures as below:
Washington 18 23 21 19 16
London 10 7 13 5 -1
Moscow 2 0 -3 -7 1
Mumbai 24 27 29 29 28

   We can find average temperature for every city as shown below:
$ awk ' {total = $2 + $3 + $4 + $5 + $6; print $1 " : " total/5 }' cities.txt
Washington : 19.4
London : 6.8
Moscow : -1.4
Mumbai : 27.4

   That's all for the scope of this article. We did not cover all of the operators here as there are
   pretty straight forward and most of you may already have an idea about those ones.


---
http://www.yourownlinux.com/2018/04/awk-built-in-variables-fs-ofs-rs-ors-nf-nr.html

AWK Programming Tutorial- Awk built-in variables FS, OFS, RS, ORS, NF, NR
10 April 2018

   Awk built-in variables: This is the fourth article of this tutorial series on awk and in this one, we
   will be learning about built-in variables in awk. In case you have missed any of our previous
   articles, you can find them out [18]here.

   Awk comes up with a number of built-in variables. Of these variables, some have a default value
   associated with them which can be changed e.g. FS ( field separator, with default value of a
   whitespace ) and RS ( record separator, with default value of \n ). While, some variables are quite
   useful while doing analysis or creating reports e.g. NF ( number of fields ) and NR ( number of
   records ). Lets take a look at them one-by-one.

FS (Field Separator) and OFS (Output Field Separator)
     * With FS, we instruct awk that, in a particular input file, fields are separated by some
       character.
     * Default value if this variable is a whitespace, telling awk that fields are separated by one or
       more whitespaces (including tabs).
     * This default value can be overwritten with a character or a regular expression. For example, we
       can use a colon ( : ) to separate fields while working on /etc/passwd file.
     * With OFS, we ask awk to use a particular character to separate the fields in the output.
     * For this variable too, default value is a single whitespace.

   Lets take a look at an example now. For this, we will use demo csv file with contents as shown below:
1,"Eldon Base for stackable storage shelf, platinum",Muhammed MacIntyre,3,-213.25,38.94,35,Nunavut,Storage & Organization,0.8
2,"1.7 Cubic Foot Compact ""Cube"" Office Refrigerators",Barry French,293,457.81,208.16,68.02,Nunavut,Appliances,0.58
3,"Cardinal Slant-DÆ Ring Binder, Heavy Gauge Vinyl",Barry French,293,46.71,8.69,2.99,Nunavut,Binders and Binder Accessories,0.39
4,R380,Clay Rozendal,483,1198.97,195.99,3.99,Nunavut,Telephones and Communication,0.58
5,Holmes HEPA Air Purifier,Carlos Soltero,515,30.94,21.78,5.94,Nunavut,Appliances,0.5
6,G.E. Longer-Life Indoor Recessed Floodlight Bulbs,Carlos Soltero,515,4.43,6.64,4.95,Nunavut,Office Furnishings,0.37
7,"Angle-D Binders with Locking Rings, Label Holders",Carl Jackson,613,-54.04,7.3,7.72,Nunavut,Binders and Binder Accessories,0.38
8,"SAFCO Mobile Desk Side File, Wire Frame",Carl Jackson,613,127.70,42.76,6.22,Nunavut,Storage & Organization,
9,"SAFCO Commercial Wire Shelving, Black",Monica Federle,643,-695.26,138.14,35,Nunavut,Storage & Organization,
10,Xerox 198,Dorothy Badders,678,-226.36,4.98,8.33,Nunavut,Paper,0.38

   By default FS will use whitespace as a default value. Lets check extracting 1st and 3rd column
   without default value of FS.
$ awk '{ print $1, $3 }' input.csv
1,"Eldon for
2,"1.7 Foot
3,"Cardinal Ring
4,R380,Clay and
5,Holmes Air
6,G.E. Indoor
7,"Angle-D with
8,"SAFCO Desk
9,"SAFCO Wire
10,Xerox Badders,678,-226.36,4.98,8.33,Nunavut,Paper,0.38

   And now, using comma ( , ) as the field separator value.
$ awk 'BEGIN { FS = ","; } { print $1, $3 }' input.csv
1  platinum"
2 Barry French
3  Heavy Gauge Vinyl"
4 Clay Rozendal
5 Carlos Soltero
6 Carlos Soltero
7  Label Holders"
8  Wire Frame"
9  Black"
10 Dorothy Badders

   As we can see in above outputs, awk uses the default value of OFS which is a single whitespace. We
   can overwrite this value, to say a pipe ( | ) as shown in below example:
$ awk 'BEGIN { FS = ","; OFS = "|" } { print $1, $3 }' input.csv
1| platinum"
2|Barry French
3| Heavy Gauge Vinyl"
4|Clay Rozendal
5|Carlos Soltero
6|Carlos Soltero
7| Label Holders"
8| Wire Frame"
9| Black"
10|Dorothy Badders

RS (Record Separator) and ORS (Output Record Separator)
     * RS and ORS are useful while dealing with multi-line records. In this case, each field is on a new
       line.
     * Default value of both these variables is a newline character ( \n ).
     * With ORS value overwritten, we can tell awk to separate records with some other character then
       the newline.

   Lets take a look at our demo file wherein each record is separated by dual newlines ( \n\n ) and each
   field in the record is separated using single newline character ( \n ).
$ cat address.txt
Cecilia Chapman
711-2880 Nulla St.
Mankato Mississippi 96522
(257) 563-7401

Iris Watson
P.O. Box 283 8562 Fusce Rd.
Frederick Nebraska 20620
(372) 587-2335

Celeste Slater
606-3727 Ullamcorper. Street
Roseville NH 11523
(786) 713-8616

Theodore Lowe
Ap #867-859 Sit Rd.
Azusa New York 39531
(793) 151-6230

   Now, to display a person's name ( $1 ) and his/her phone number ( $4 ) on a separate line ( ORS will
   be \n, while RS is \n\n ), we can use below command:
$ awk ' BEGIN { FS = "\n"; RS = "\n\n"; ORS = "\n" } { print $1, $4 } ' address.txt
Cecilia Chapman (257) 563-7401
Iris Watson (372) 587-2335
Celeste Slater (786) 713-8616
Theodore Lowe (793) 151-6230

NF (Number of Fields) and NR (Number of Record)
     * Awk variable NF defines the number of fields if the current record ( $0 ).
     * If we try to increase the value of NF, awk adds additional fields separated by the delimiter
       value in OFS.
     * Whereas, when we decrease the value of NF, all the fields with identifiers greater than the value
       are ignored.
     * NR is the variable that stores the current record number being processed by awk.
     * There is another variable, FNR, which is useful while dealing with multiple files. It stores the
       position of the record relative to the current file only.

   Lets take a look at below demo file to illustrate this example. If you observe, it has different
   number of fields on each record.
$ cat cities.txt
Washington 18 23 21 19
London 10 7 13 5 -1
Moscow 2 0 -3
Mumbai 24 27

   Now, we print number of fields a record has before printing the record itself, using below command:
$ awk '{print NF, $0}' cities.txt
5 Washington 18 23 21 19
6 London 10 7 13 5 -1
4 Moscow 2 0 -3
3 Mumbai 24 27

   To illustrate the use of NR, we use the same file again. Its pretty straight forward.
$ awk '{print NR, $0}' cities.txt
1 Washington 18 23 21 19
2 London 10 7 13 5 -1
3 Moscow 2 0 -3
4 Mumbai 24 27

   In case there are multiple files, we can print the record number relative to the current input file
   being processed using the variable FNR.
$ awk '{print FNR, $0}' cities.txt address.txt
1 Washington 18 23 21 19
2 London 10 7 13 5 -1
3 Moscow 2 0 -3
4 Mumbai 24 27
1 Cecilia Chapman
2 711-2880 Nulla St.
3 Mankato Mississippi 96522
4 (257) 563-7401
5
6 Iris Watson
7 P.O. Box 283 8562 Fusce Rd.
8 Frederick Nebraska 20620
9 (372) 587-2335
10
11 Celeste Slater
12 606-3727 Ullamcorper. Street
13 Roseville NH 11523
14 (786) 713-8616
15
16 Theodore Lowe
17 Ap #867-859 Sit Rd.
18 Azusa New York 39531
19 (793) 151-6230

   Observe the line after line #4. Awk has numbered it #1, just because we have used FNR. Had we used NR
   here, it would have been numbered #5. You can check this out, I will leave this for you.
   That's it for the scope of this article.

---
   