filename: sshfs-vs-nfs_20140328.txt
http://www.linuxquestions.org/questions/linux-networking-3/performance-difference-between-sharing-disk-space-via-nfs-or-sshfs-4175448688/


   Performance difference between sharing disk space via NFS or SSHFS
 
   I have a "poor man's" cluster, i.e. 10 boxes connected via 1Gb/s switch and am trying to
   use for simple MPI calculations. So far I have used sshfs to get a shared disk space on the
   machines but it seems that the calculations scale worse than I expected. Is there a huge
   penalty with using sshfs rather than sshfs?


   plesset
   [43]View Public Profile
   [44]View LQ Blog
   [45]View Review Entries
   [46]View HCL Entries
   [47]Find More Posts by plesset

                                  Old 02-05-2013, 10:29 AM                                    #[48]2

   em31amit

   Member


   Registered: Apr 2012
   Location: /root
   Distribution: Ubuntu, Redhat, Fedora, CentOS
   Posts: 190
   Rep: Reputation: 55

   @plesset, Please correct your question, It is confusing. Performance is Degraded in which ?
   nfs or sshfs.
   sshfs is actually using ssh protocol so all the traffic is moving in encryption and
   decryption cycle so there must be a penalty on performance.
   whereas nfs send data in cleartext so performance always better in compare of sshfs.


   em31amit
   [49]View Public Profile
   [50]View LQ Blog
   [51]View Review Entries
   [52]View HCL Entries
   [53]Find More Posts by em31amit

                                  Old 02-05-2013, 11:16 AM                                   #[54]3

   plesset

   LQ Newbie


   Registered: Oct 2008
   Location: Reykjavik, Iceland
   Distribution: openSUSE 12-2, Debian-Wheezy, Windows 7
   Posts: 17
   Original Poster
   Rep: Reputation: 0

   Sorry for the confusion. I'm using sshfs (since it is easy and I know how to) but I find
   the performance to be disappointing. So I think I should be using NFS but I'm not sure how
   to configure it, especially since all the machines are directly linked to the internet.
   Will using hosts.allow & hosts.deny be enough? I could also ask the network admin to block
   all access to all but one. Now there is only ssh access to the machines.


   plesset
   [55]View Public Profile
   [56]View LQ Blog
   [57]View Review Entries
   [58]View HCL Entries
   [59]Find More Posts by plesset

                               Old 02-05-2013, 02:16 PM                              #[60]4

   suicidaleggroll

   Senior Member


   Registered: Nov 2010
   Location: Colorado
   Distribution: OpenSUSE, CentOS
   Posts: 2,336
   Rep: Reputation: 857 Reputation: 857 Reputation: 857 Reputation: 857 Reputation: 857
   Reputation: 857 Reputation: 857

   You can specify which IP address(es) you want to allow NFS connections from in
   /etc/exports. Having the machine on the internet shouldn't be a problem as long as you
   don't set up /etc/exports to allow NFS connections from anybody.


   suicidaleggroll
   [61]View Public Profile
   [62]View LQ Blog
   [63]View Review Entries
   [64]View HCL Entries
   [65]Find More Posts by suicidaleggroll

                                Old 02-06-2013, 02:22 AM    #[66]5

   bigearsbilly

   Senior Member


   Registered: Mar 2004
   Location: england
   Distribution: FreeBSD, Debian, Mint, Puppy
   Posts: 3,257
   Rep: Reputation: 165 Reputation: 165

   NFS is dead easy to set up.
   It's very reliable and efficient.
   You have a private sub net connecting the machines just specify
   connection only on that. i would assume you will have a central data bank
   and connect your calculators to that.


   bigearsbilly
   [67]View Public Profile
   [68]View LQ Blog
   [69]View Review Entries
   [70]View HCL Entries
   [71]Find More Posts by bigearsbilly

                                Old 02-06-2013, 01:28 PM       #[72]6

   lupe

   Member


   Registered: Dec 2008
   Distribution: Slackware
   Posts: 33
   Rep: Reputation: 2

   Have you read about [73]GlusterFS
   For what seems to be your purpose, it seems like a good alternative do nfs and sshfs.


   lupe
   [74]View Public Profile
   [75]View LQ Blog
   [76]View Review Entries
   [77]View HCL Entries
   [78]Find More Posts by lupe

                                Old 02-08-2013, 07:50 AM            #[79]7

   plesset

   LQ Newbie


   Registered: Oct 2008
   Location: Reykjavik, Iceland
   Distribution: openSUSE 12-2, Debian-Wheezy, Windows 7
   Posts: 17
   Original Poster
   Rep: Reputation: 0

   I installed NFS and map the shared space as
   Code:
toppond:/home/xxxx/fds /home/xxxx/fds nfs rw,rsize=16384,wsize=16384,hard,intr,async,nodev,nosuid 0
0

   in fstab.
   When I test it I actually find it to be slower than SSHFS (especially when writing)...
   Code:
> time dd if=/dev/zero of=/home/xxxx/fds/test bs=16k count=16k
16384+0 records in
16384+0 records out
268435456 bytes (268 MB) copied, 4.91929 s, 54.6 MB/s

real    0m5.015s
user    0m0.000s
sys     0m0.160s

time dd if=/home/xxxx/fds/test of=/dev/null bs=16k
16384+0 records in
16384+0 records out
268435456 bytes (268 MB) copied, 2.31482 s, 116 MB/s

real    0m2.317s
user    0m0.008s
sys     0m0.120s

   Code:
> sudo umount fds
[sudo] password for xxxx:

> sshfs toppond:/home/xxxx/fds fds

> time dd if=/dev/zero of=/home/xxxx/fds/test bs=16k count=16k
16384+0 records in
16384+0 records out
268435456 bytes (268 MB) copied, 3.42332 s, 78.4 MB/s

real    0m3.457s
user    0m0.000s
sys     0m0.244s

> time dd if=/home/xxxx/fds/test of=/dev/null bs=16k
16384+0 records in
16384+0 records out
268435456 bytes (268 MB) copied, 3.53909 s, 75.8 MB/s

real    0m3.560s
user    0m0.004s
sys     0m0.072s

   Where have I gone wrong?


   plesset
   [80]View Public Profile
   [81]View LQ Blog
   [82]View Review Entries
   [83]View HCL Entries
   [84]Find More Posts by plesset

                                Old 02-08-2013, 08:44 AM   #[85]8

   bigearsbilly

   Senior Member


   Registered: Mar 2004
   Location: england
   Distribution: FreeBSD, Debian, Mint, Puppy
   Posts: 3,257
   Rep: Reputation: 165 Reputation: 165

   well there you go.
   seems quite fast to me.
   why do you think it's wrong? it is what it is.


   bigearsbilly
   [86]View Public Profile
   [87]View LQ Blog
   [88]View Review Entries
   [89]View HCL Entries
   [90]Find More Posts by bigearsbilly

                                  Old 02-08-2013, 09:11 AM                                  #[91]9

   schneidz

   Senior Member


   Registered: May 2005
   Location: boston, usa
   Distribution: fc-15/ fc-19-live-usb/ aix
   Posts: 3,442
   Rep: Reputation: 488 Reputation: 488 Reputation: 488 Reputation: 488 Reputation: 488

   i prefer sshfs because it is almost as fast as nfs and easier to use (it can also mount
   directories outside of your network).
   maybe the pc's (updates/load) or routers (firmware) or nic's (drivers) are just slow (bad
   cables) ?
   this thread seems to be slow/ bad hard drives which is hampering network performance:
   [92]http://www.linuxquestions.org/questi...ts-4175449064/
     ______________________________________________________________________________________

   Last edited by schneidz; 02-08-2013 at 09:53 AM.


   schneidz
   [93]View Public Profile
   [94]View LQ Blog
   [95]View Review Entries
   [96]View HCL Entries
   [97]Find More Posts by schneidz

                                 Old 02-08-2013, 10:07 AM                                    #[98]10

   plesset

   LQ Newbie


   Registered: Oct 2008
   Location: Reykjavik, Iceland
   Distribution: openSUSE 12-2, Debian-Wheezy, Windows 7
   Posts: 17
   Original Poster
   Rep: Reputation: 0

   Thank you all. At least I have gone through and it is always beneficial to learn something
   new and now I have the option to use both. Given the hassle with setting up/configuring
   NFS, as compared to SSHFS, I assumed the gain would be greater.


   plesset
   [99]View Public Profile
   [100]View LQ Blog
   [101]View Review Entries
   [102]View HCL Entries
   [103]Find More Posts by plesset

                               Old 02-08-2013, 10:10 AM   #[104]11

   suicidaleggroll

   Senior Member


   Registered: Nov 2010
   Location: Colorado
   Distribution: OpenSUSE, CentOS
   Posts: 2,336
   Rep: Reputation: 857 Reputation: 857 Reputation: 857 Reputation: 857 Reputation: 857
   Reputation: 857 Reputation: 857

   I wouldn't call it done right there, from the looks of it you only tried one small transfer
   with one block size. Try a larger file (at least 1GB+) and try different block sizes to see
   how that affects things. Also do each test a few times to make sure the results are
   consistent.
   Also make sure there is no other I/O on either system during the tests if you want the most
   fair comparison.



---
http://arstechnica.com/civis/viewtopic.php?f=16&t=313614

SshFS versus NFS (and others)

   [9]This topic is locked, you cannot edit posts or make further replies.
   7 posts
   [10]drag
   Ars Tribunus Angusticlavius
   Registered: Dec 15, 2004
   Posts: 6849
   [11]Posted: Sun Apr 09, 2006 3:23 pm
   I am using a OpenAFS server at home right now, but I set up a little webserver in a Xen vm
   and I connected it to my desktop machine using the FUSE-based Sshfs file system.
   And basicly I was suprised by how well it performed. So I decided to benchmark it using a
   1022MB file transfers and see how it compared to other file systems.
   Of course this is still fairly useless. OpenAFS gets a big short-end of the stick because
   it uses a combination of simple encryption with complex caching mechanisms and decent
   authentication (kerberos 4 built in with krb5 compatability with add-on packages) that
   makes it a poor choice for moving large files over a LAN, but great for a distributed
   network.
   Sshfs isn't suitable for large numbers of users due to the high cpu load from ssh and the
   fact that you must give them shell access to the server (although ssh can be run from a
   chroot environment or you can use scponly.. which is compatable with sshfs). Also Sshfs
   can't deal with special file types like named pipes. However, it is very fast and network
   security is much superior to other options.
   But here is a result for my setup. It's on a 100mbit network with a 2ghz althon machine on
   either side. The network is the obvious bottleneck. Sshfs was sitting at about 30-40% cpu
   usage during file transfer.
   The samba and nfs servers are using the Debian provided defaults.
   Copying files up to the server
   Using Sshfs:
   real 1m58.775s
   user 0m0.308s
   sys 0m9.045s
   Using NFS:
   real 2m15.288s
   user 0m0.031s
   sys 0m6.003s
   (this varied +- 20 seconds)
   Using Samba:
   real 3m42.976s
   user 0m0.160s
   sys 0m9.592s
   Using OpenAFS:
   real 4m5.940s
   user 0m0.109s
   sys 1m33.730s
   Copying them back down.
   Using Sshfs:
   real 1m40.327s
   user 0m0.186s
   sys 0m10.111s
   Using NFS:
   real 1m30.874s
   user 0m0.124s
   sys 0m8.348s
   Using Samba:
   real 3m45.561s
   user 0m0.132s
   sys 0m10.123s
   Using OpenAFS (copied it all from local harddrive cache; no network activity):
   real 2m21.708s
   user 0m0.109s
   sys 0m16.104s
   Using OpenAFS (after flushing the file from the cache):
   real 2m17.284s
   user 0m0.087s
   sys 0m13.172s
   With Openafs after flushing the file from cache the performance was about the same, but cpu
   load was near 100%
   For comparision, netcat file transfer speed..
   command used was:
   (on host1)
   nc -l -p 8000 > /tmp/testfile
   (on host2)
   time cat testsubject | netcat -q 0 host1 8000
   netcat:
   real 1m43.786s
   user 0m0.250s
   sys 0m8.583s
   To me this is remarkable. I always thought that FUSE based network file systems were going
   to be slow.. but this is as fast as anything else. At least on a 100mbit network.
   Oh well, I ordered a gigabit switch yesterday to replace the now-borked one I used
   previously. See how it performs with that.
   [12]koala
   Ars Praefectus
   Tribus: Barcelona, Spain
   Registered: Dec 31, 2001
   Posts: 3953
   [13]Posted: Mon Apr 10, 2006 12:00 pm
   Oh, well, but you should try reading just a bit from the end of a file and things like
   that... does sshfs support that kind of stuff?
   [14]drag
   Ars Tribunus Angusticlavius
   Registered: Dec 15, 2004
   Posts: 6849
                                        [15]Posted: Mon Apr 10, 2006 12:59 pm
   You mean something like running tail?
   I had a 1.6gig vmware image and ran tail on that.
   real 0m0.035s
   user 0m0.001s
   sys 0m0.004s
   Also it does some directory caching so that when I run find on the file system a second
   time it goes much quicker, but it's not nearly as nice as AFS's stuff is. Although the
   initial du or find or whatnot goes much faster on sshfs and is about the same as nfs.
   I'll play music and movies over it quite well. The overhead from ssh is unoticable. I've
   even run games and such over it. For ID games it works fine, I'll play Doom3 and
   Wolfenstien fine over it, but Ut2004 and Postal2 chokes on it... They act like there is
   missing files. They work fine when mounted over nfs. So sshfs isn't perfect.
   Haven't figured out what exactly is going on.
   The whole thing looks like it takes up 3000 lines of c code so if I knew C better it
   probably wouldn't be very difficult to identify the bug.
   [16]insta
   Ars Tribunus Militum
   Registered: Sep 1, 2002
   Posts: 2946
            [17]Posted: Tue Apr 11, 2006 6:51 am

   quote:

   The whole thing looks like it takes up 3000 lines of c code so if I knew C better it
   probably wouldn't be very difficult to identify the bug.

   Hah ... just like heart problems are easy to fix because the thing's the size of a
   grapefruit Smile . Congrats on your 1k.
   I've used SSHFS with mixed success to a remote server that I didn't control. The servers I
   do control use NFS, and I use SSHFS for the remaining one. I wish SSHFS was a little more
   integrated with my client machine (ability to use /etc/fstab, for instance), but I haven't
   hit an impractacility with it yet. It's quite fast once it connects ... if it connects.
   [18]drag
   Ars Tribunus Angusticlavius
   Registered: Dec 15, 2004
   Posts: 6849
           [19]Posted: Tue Apr 11, 2006 7:26 pm

   quote:

   Originally posted by insta:

   quote:

   The whole thing looks like it takes up 3000 lines of c code so if I knew C better it
   probably wouldn't be very difficult to identify the bug.

   Hah ... just like heart problems are easy to fix because the thing's the size of a
   grapefruit Smile . Congrats on your 1k.
   I've used SSHFS with mixed success to a remote server that I didn't control. The servers I
   do control use NFS, and I use SSHFS for the remaining one. I wish SSHFS was a little more
   integrated with my client machine (ability to use /etc/fstab, for instance), but I haven't
   hit an impractacility with it yet. It's quite fast once it connects ... if it connects.

   Hrm. Haven't had any issues with it connecting or not. Maybe it has to do with the fact
   that I am using kerberos or whatnot. (although I've noticed that if you use stuff like ~/
   in the path statement it will bork, you have to specify the full path.)
   The way I figured it is that I'd use nfs at home, then while I am out and about I'd use
   Sshfs to connect my laptop. That way I get fast secure access and all my paths stay the
   same.
   [20]Bluebottle
   Ars Scholae Palatinae
   Tribus: NZ
   Registered: Apr 21, 1999
   Posts: 1251
                 [21]Posted: Mon Apr 17, 2006 12:19 am

   quote:

   Originally posted by koala:
   Oh, well, but you should try reading just a bit from the end of a file and things like
   that... does sshfs support that kind of stuff?

   This kind of thing is my interest too... when I tried sshfs I couldn't do any streaming of
   video/audio, so I crawled back to NFS. "streaming" consisted of copying the entire file
   locally first. Am I wrong/deluded?
   [22]drag
   Ars Tribunus Angusticlavius
   Registered: Dec 15, 2004
   Posts: 6849
   [23]Posted: Mon Apr 17, 2006 4:17 am
   There were a few things called sshfs.. This one is the sshfs using the FUSE stuff, maybe
   that is the issue.
   I am using Debian sid, up to date.
   sshfs --version
   SSHFS version 1.6
   FUSE library version: 2.5.2
   fusermount version: 2.5.2
   using FUSE kernel interface version 7.5
   Got a share mounted from a Debian testing system. Got the setuid whatnot for the group for
   fusermount and /dev/fuse belongs to the 'fuse' group, which my user is a member of.
   Just tried playing through a mpeg file "night of the living dead" (thanks to archive.org)
   of about 535MB. Openned the directory via nautilus, double clicked on the file, openned it
   in totem. It played fine and I was able to yank the progress bar at any random time in the
   stream and it tracked to it instantly, no problem. (yea totem, yea sshfs)
   Same thing with avi and quicktime ( [24]Jelly donut freestyle) files I tried.
   Oh, now I have gigabit network running again.
   Now NFS is faster, but not buy a large margin.
