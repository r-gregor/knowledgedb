filename: personal-programs-setup-multif-20250417.txt
https://utcc.utoronto.ca/~cks/space/blog/sysadmin/MyPersonalProgramsSetup

How I install personal versions of programs (on Unix)
April 11, 2025

   These days, Unixes are quite generous in what they make available through their packaging systems, so
   you can often get everything you want through packages that someone else worries about building,
   updating, managing, and so on. However, not everything is available that way; sometimes I want
   something that isn't packaged, and sometimes (especially on 'long term support' distributions) I want
   something that's more recent that the system provides (for example, Ubuntu 22.04 only has Emacs
   27.1). Over time, I've evolved my own approach for managing my personal versions of such things,
   which is somewhat derived from the traditional approach for multi-architecture Unixes here.

   The starting point is that I have a ~/lib/<architecture> directory tree. When I build something
   personally, I tell it that its install prefix is a per-program directory within this tree, for
   example, '/u/cks/lib/<arch>/emacs-30.1'. These days I only have one active architecture inside ~/lib,
   but old habits die hard, and someday we may start using ARM machines or FreeBSD. If I install a new
   version of the program, it goes in a different (versioned) subdirectory, so I have 'emacs-29.4' and
   'emacs-30.1' directory trees.

   I also have both a general ~/bin directory, for general scripts and other architecture independent
   things, and a ~/bin/bin.<arch> subdirectory, for architecture dependent things. When I install a
   program into ~/lib/<arch>/<whatever> and want to use it, I will make either a symbolic link or a
   cover script in ~/bin/bin.<arch> for it, such as '~/bin/bin.<arch>/emacs'. This symbolic link or
   cover script always points to what I want to use as the current version of the program, and I update
   it when I want to switch.

   (If I'm building and installing something from the latest development tree, I'll often call the
   subdirectory something like 'fvwm3-git' and then rename it to have multiple versions around. This is
   not as good as real versioned subdirectories, but I tend to do this for things that I won't ever run
   two versions of at the same time; at most I'll switch back and forth.)

   Some things I use, such as pipx, normally install programs (or symbolic links to them) into places
   like ~/.local/bin or ~/.cargo/bin. Because it's not worth fighting city hall on this one, I pretty
   much let them do so, but I don't add either directory to my $PATH. If I want to use a specific tool
   that they install and manage, I put in a symbolic link or a cover script in my ~/bin/bin.<arch>. The
   one exception to this is Go, where I do have ~/go/bin in my $PATH because I use enough Go based
   programs that it's the path of least resistance.

   This setup isn't perfect, because right now I don't have a good general approach for things that
   depend on the Ubuntu version (where an Emacs 30.1 built on 22.04 doesn't run on 24.04). If I ran into
   this a lot I'd probably make an addition ~/bin/bin.<something> directory for the Ubuntu version and
   then put version specific things there. And in general, Go and Cargo are not ready for my home
   directory to be shared between different binary architectures. For Go, I would probably wind up
   setting $GOPATH to something like ~/lib/<arch>/go. Cargo has a similar system for deciding where it
   puts stuff but I haven't looked into it in detail.

   (From a quick skim of 'cargo help install' and my ~/.cargo, I suspect that I'd point
   $CARGO_INSTALL_ROOT into my ~/lib/<arch> but leave $CARGO_HOME unset, so that various bits of Cargo's
   own data remain shared between architectures.)

   PS: In theory I have [**1]a system for keeping track of the command lines used to build things
   ([**2]also, which I'd forgotten when I wrote the more recent entry on this system). In practice I've
   fallen out of the habit of using it when I build things for my ~/lib, although I should probably get
   back into it. For GNU Emacs, I put the ./configure command line into a file in ~/lib/<arch>, since I
   expected to build enough versions of Emacs over time.


---
[**1]
https://utcc.utoronto.ca/~cks/space/blog/sysadmin/HowIHandleSoftwareBuildConfigs

How I (used to) handle keeping track of how I configured software
April 28, 2024

   Once upon a time, back a reasonable while ago, I used to routinely configure (in the './configure'
   sense) and build a fair amount of software myself, software that periodically got updates and so
   needed me to rebuild it. If you've ever done this, you know that one of the annoying things about
   this process is keeping track of just what configuration options you built the software with, so you
   can re-run the configuration process as necessary (which may be on new releases of the software, but
   also when you do things like upgrade your system to a new version of your OS). Since I'm that kind of
   person, I naturally built a system to handle this for me.

   How the system worked was that the actual configuration for each program or package was done by a
   little shell script snippet that I stored in a directory under '$HOME/lib'. Generally the file name
   of the snippet was the base name of the source directory I would be building in, so for example
   'fvwm-cvs'. Also in this directory was a 'MAPPINGS' file that mapped from full or partial paths of
   the source directory to the snippet to use for that particular thing. To actually configure a
   program, I ran a script, inventively called 'doconfig'. Doconfig searched the MAPPINGS file for,
   well, let me just quote from comments in the script:

     Algorithm: we have a file called MAPPINGS.
     We search for first the full path of the current directory and then it with successive things sawn
     off the front; if we get a match, we use the filename named.
     Otherwise, we try to use the basename of the directory as a file. Otherwise we error out.

   There's nothing particularly special about my script and my system for keeping track of how I built
   software. There probably are tons of versions and variations of it that people have created for
   themselves over the years. This is just the sort of thing you want to do when you get tired of trying
   to re-read 'config.log' files or whatever, and realize that you forgot how you built the software the
   last time around, and so on.

   (Having written this up I've realized that I should still be using it, because these days I'm
   building or re-building a number of things and I've slide back to the old silly ways of trying to do
   it all by hand.)

   PS: At work we don't have any particular system for keeping track of software build instructions.
   Generally, if we have to build something from source, we put the relevant command lines and other
   information in [**3]our build instructions.


---
[**2]
https://utcc.utoronto.ca/~cks/space/blog/sysadmin/MyConfigureSolution

How I solve the configure memory problem
July 18, 2010

   For my sins I sometimes build programs from source that use standard autoconf-created configure
   scripts. The whole autoconf system has a number of problems, which you can read people rant about at
   length, but my problem is that I almost invariably build things using various different non-default
   arguments. Which leads to what I call the 'configure memory problem': when it comes time to rebuild
   the package for whatever reason, how do I remember which configure arguments I used and recreate
   them? Especially, how do I have this sitting around in a convenient form that requires as little hand
   work as possible?

   (Yes, configure will write all of this information in various magic files. Which it puts in the
   source directory, and which get deleted if you do a forceful enough 'make clean', and so on.)

   I've gone through a number of evolutionary steps on this problem; not worrying about it at all and
   then discovering that I'd forgotten the necessary magic arguments and had to recreate them, putting a
   little script file that runs configure with the right arguments in the source directory where it will
   get deleted when I clean up the source directory, and putting the little script file in the parent
   directory where I will lose track of it. None of them were ultimately satisfactory, so my current
   solution is a master script that I named doconfig.

   My version has a bunch of complications, but at its heart doconfig can be written as the following
   simple script:

#!/bin/sh
CONFDIR=$HOME/lib/$arch/src-configs

dname=$(basename $(pwd))
if [ -x $CONFDIR/$dname ]; then
	exec $CONFDIR/$dname
else
	echo $0: cannot configure $dname
fi

   (The complications exist to deal with all of the cases where the directory you need to run configure
   in is not a handily named top level directory. My version has an overly complicated file that maps
   some amount of trailing directory components to a script name, so that I can say that
   X11/fvwm-cvs/fvwm maps to the fvwm-cvs script.)

   As you might expect, this turns out to be a handy way of capturing my knowledge of how to configure
   specific packages on my systems. Its simplicity means that it's easy and fast to take advantage of,
   and that I write scripts means that it's easy enough to augment them for various situations (such as
   handling both 32-bit and 64-bit builds, where they need different configure arguments).


---
[**3]
https://utcc.utoronto.ca/~cks/space/blog/sysadmin/OurBuildProcedures

How we do and document machine builds
February 24, 2015

   I've written before about our general Ubuntu install system and I've mentioned before that we have
   documented build procedures but we don't really automate them. But I've never discussed how we do
   reproducible builds and so on. Basically we do them by hand, but we do them systematically.

   Our Ubuntu login and compute servers are essentially entirely built through our standard install
   system. For everything else, the first step is a base install with the same system. As part of this
   base install we make some initial choices, like what sort of NFS mounts this machine will have (all
   of them, only our central administrative filesystem, etc).

   After the base install we have a set of documented additional steps; almost all of these steps are
   either installing additional packages or copying configuration files from that central filesystem. We
   try to make these steps basically cut and paste, often with the literal commands to run interlaced
   with an explanation of what they do. An example is:

* install our Dovecot config files:
     cd /etc/dovecot/conf.d/
     rsync -a /cs/site/machines/aviary/etc/dovecot/conf.d/*.conf .

   Typically we do all of this over a SSH connection, so we are literally cutting and pasting from the
   setup documentation to the machine.

   (In theory we have a system for automatically installing additional Ubuntu packages only on specific
   systems. In practice there are all sorts of reasons that this has wound up relatively disused; for
   example it's tied to the hostname of what's being installed and we often install new versions of a
   machine under a different hostname. Since machines rarely have that many additional packages
   installed, we've moved away from preconfigured packages in favour of explicitly saying 'install these
   packages'.)

   We aren't neurotic about doing everything with cut and paste; sometimes it's easier to describe an
   edit to do to a configuration file in prose rather than to try to write commands to do it
   automatically (especially since those are usually not simple). There can also be steps like 'recover
   the DHCP files from backups or copy them from the machine you're migrating from', which require a bit
   of hand attention and decisions based on the specific situation you're in.

   (This setup documentation is also a good place to discuss general issues with the machine, even
   if it's not strictly build instructions.)

   When we build non-Ubuntu machines the build instructions usually follow a very similar form: we start
   with 'do a standard base install of <OS>' and then we document the specific customizations for the
   machine or type of machine; this is what we do for our OpenBSD firewalls and our CentOS based
   iSCSI backends. Setup of our OmniOS fileservers is sufficiently complicated and picky that a
   bunch of it is delegated to a couple of scripts. There's still a fair number of by-hand commands,
   though.

   In theory we could turn any continuous run of cut and paste commands into a shell script; for most
   machines this would probably cover at least 90% of the install. Despite what I've written in the
   past, doing so would have various modest advantages; for example, it would make sure that we would
   never skip a step by accident. I don't have a simple reason for why we don't do it except 'it's never
   seemed like that much of an issue', given that we build and rebuild this sort of machine very
   infrequently (generally we build them once every Ubuntu version or every other Ubuntu version, as our
   servers generally haven't failed).

   (I think part of the issue is that it would be a lot of work to get a completely hands-off install
   for a number of machines, per my old entry on this. Many machines have one or two little bits
   that aren't just running cut & paste commands, which means that a simple script can't cover all of
   the install.)


---

