filename: wget-curl-howto-download-ioutube-videos-multif_20141027.txt
http://stackoverflow.com/questions/14368823/download-youtube-videos-with-wget

Download YouTube videos with wget

   I usually use keepvid.com or a Firefox download helper plugin for dowloading YouTube
   videos. Is it possible to download them with the wget command available in Linux?

   Also, I have a VPS server. I want to download videos on my VPS server (cPanel).

   If is possible and if so how?

   Thanks.

***

<code>
#!/usr/bin/perl

use strict;
use warnings;
## Two arguments
##      $1 YouTube URL from the browser
##      $2 Prefix to the file name of the video (optional)
#

## Collect the URL from the command line argument
my $url = $ARGV[0] or die "\nError: You need to specify a YouTube URL\n\n";

## Declare the user defined file name prefix
my $prefix = defined($ARGV[1]) ? $ARGV[1] : "";

## Download the HTML code from the YouTube page
my $html = `wget -Ncq -e "convert-links=off" --keep-session-cookies --save-cookies /dev/null --no-ch
eck-certificate "$url" -O-`  or die  "\nThere was a problem downloading the HTML file.\n\n";

## Collect the title of the page to use as the file name
my ($title) = $html =~ m/<title>(.+)<\/title>/si;
$title =~ s/[^\w\d]+/_/g;
$title =~ s/_youtube//ig;
$title =~ s/^_//ig;
$title = lc ($title);

## Collect the URL of the video
my ($download) = $html =~ /"url_encoded_fmt_stream_map"([\s\S]+?)\,/ig;

## Clean up the URL by translating Unicode and removing unwanted strings
$download =~ s/\:\ \"//;
$download =~ s/%3A/:/g;
$download =~ s/%2F/\//g;
$download =~ s/%3F/\?/g;
$download =~ s/%3D/\=/g;
$download =~ s/%252C/%2C/g;
$download =~ s/%26/\&/g;
$download =~ s/sig=/signature=/g;
$download =~ s/\\u0026/\&/g;
$download =~ s/(type=[^&]+)//g;
$download =~ s/(fallback_host=[^&]+)//g;
$download =~ s/(quality=[^&]+)//g;

## Collect the URL and signature since the HTML page randomizes the order
my ($signature) = $download =~ /(signature=[^&]+)/;
my ($youtubeurl) = $download =~ /(http.+)/;
$youtubeurl =~ s/&signature.+$//;

## Combine the URL and signature in order to use in Wget
$download = "$youtubeurl\&$signature";

## A bit more cleanup
$download =~ s/&+/&/g;
$download =~ s/&itag=\d+&signature=/&signature=/g;

## Print the file name of the video collected from the web page title for us to see on the CLI
print "\n Download: $prefix$title.webm\n\n";

## Download the file using Wget and background the Wget process
system("wget -Ncq -e \"convert-links=off\" --load-cookies /dev/null --tries=50 --timeout=45 --no-che
ck-certificate \"$download\" -O $prefix$title.webm &");

#### EOF #####

   </code>

   This is a Perl script that I have been using for a while. I am not sure where it's from
   though... It works great.



---
http://go2linux.garron.me/wget-to-download-youtube-videos

Download Youtube videos with wget
2007-12-06

   Here there is an easy way to download [13]Youtube videos using wget.

   First thing we need is the video ID which we are going to get using also wget together with
   grep.

   First locate the page of the video you want to download, I will use for this example one
   showing compizfusion in action.
$> wget -O /tmp/page.tmp http://www.youtube.com/watch?v=UHvAXLBMWGI

   Then using cat and grep we will get the video ID
$> cat /tmp/page.tmp | grep video_id=

   This will be the output
var fullscreenUrl = '/watch_fullscreen?video_id=UHvAXLBMWGI&l=51&t=OEgsToPDskKBFLkte03dOnia39Rzd63j&
sk=7Ayx8rGROMLJrkh6LkXj7AC&fs=1&title=CompizFusion';
var relatedVideoGridUrl = '/related_ajax?video_id=UHvAXLBMWGI&view_type=G&watch3=1&search=compiz%20l
inux%20ubuntu';
var relatedVideoListUrl = '/related_ajax?video_id=UHvAXLBMWGI&view_type=L&watch3=1&search=compiz%20l
inux%20ubuntu';

   Check that you have more than one Video_id, but only the first one is the one we want,
   check that the others says "relatedVideo", we do not want those ones.

   Now download the video.

   wget -O /tmp/video.flv
   'http://www.youtube.com/get_video?video_id=UHvAXLBMWGI&l=51&t=OEgsToPDskKBFLkte03dOnia39Rzd
   63j&sk=7Ayx8rGROMLJrkh6LkXj7AC&fs=1&title=CompizFusion'

   check out that I am sending the output to /tmp/video.flv you of course can use any other
   file and directory you may want, after this you can play the video with any player you
   usually use.



---
http://www.libregeek.org/2014/03/09/using-wget-or-curl-to-download-files-on-linux/

Using Wget or Curl To Download Files On Linux

   Once you dig a little bit deeper into Linux and start compiling programs from source,
   running option flags, and maybe even making your own package, you'll soon discover Wget or
   curl. Both programs have the capability to download files, with curl having much farther
   reaching applications. For now, I wanted to go over what I use either for everyday,
   downloading files.

What don't you wget, hombre?
   Probably the most easiest utility to use for this purpsose is Wget. Part of the GNU tool
   set, Wget is a free utility for non-interactive download of files from the Web. Wget
   supports HTTP, HTTPS, and FTP protocols, as well as retrieval through HTTP proxies. Wget
   can work in the background without bother you, if you wish it do to so. It also can create
   "snapshots" of remote web sites, duplicating the wed directory structure, unless robots.txt
   is in effet.

   The most common use of wget, is using it without complicated parameters:

   Syntax:
   wget [option]... [URL]...

   Example commands:
   If the URL or target you have is "absolute," meaning without special symbols like # or %,
   the basic command should suffice:
$> wget https://bitcoin.org/bitcoin.pdf

   However, if the URL contains jsp (java server protocol), ajax, or some other server/file
   handling protocol, the downloaded file could be obfuscated. Sometimes, using a few flags
   can help, but is not guaranteed. Some of those are noted below:

   Flags:
   -base=URL
   This option will resolve links using the URL as the de facto point of reference.  As
   example,  if you specify http://foo/bar/a.html for URL, and Wget reads ../baz/b.html from
   the input file, it would be resolved to http://foo/baz/b.html. Think of it as a
   "breadcrumbs" option. "-B" is the shorthand.

$> wget -B wget http://foo/bar/a.html

   Result:
   http://foo/baz/b.html

   -O filename.type
   This flag will allow the target to be concatenated to a file name, rather than a garbled
   and unrecognisable string. This generally does not "override" a file string to be
   "compliant" as a tar.gz file or whichever you specified.  Instead, it is meant mainly for
   outputting a readable file name. This option merely will try to download the file and store
   in a different name than the remote server. This is helpful when the remote URL
   doesnaEUR(TM)t contain the file name in the URL.

$> wget http://www.vim.org/scripts/download_script.php?src_id=7701

   -spider
   Spider is a neat option, which can probe the page for links that can help you determine
   which is the proper for you to download.  Be sure to try the "-r" recursive option after
   the Wget command. The recursive option should save the site to a folder with the top
   domain, such as www.foo.com. Spider is a bit limited at the moment, but can aid in finding
   certain resources on a website domain. I gave this a try when searching for a permalink
   toCitrix Receiver (which is a PIA, by the way).

$> wget -r --spider http://www.foo.com

   Long story shot, inline javascript is evil. The Wget [24]documentation page on this has
   more info that may serve you well on understanding the issues surrounding js code.

   -r-A
   This flag option will download all files of a certain file type. I've used this is the past
   to grab a batch of files off a webpage, but again, like noted above with -spider, it will
   not handle obfuscated redirects using javascript or other methods.

$> wget -r -A.pdf http://website.example.com/

   You can use this under following situations:
	 * Download all images from a website
	 * Download all videos from a website
	 * Download all PDF files from a website

   Directory options:
   Some options for directory parsing can be useful, such as:
	 * -nd for no directories
	 * -x for force directories
	 * -nH no host diretories

   There is much more to Wget than meets the eye (cue classic Transformers music), so be sure
   to also check out the [25]man page for the utility.

I've done 2000 curls baby, you impressed?
   Curl is a whole other beast entirely. It's features and capabilities allow it to interface
   with common stream editors and parsers such as sed and awk. The combination of curl, awk,
   and cut, actually allowed me to find a way to download Citrix Receiver from the CLI. I will
   go through just the basic commands, and then show you the special command I used for
   Citrix, just to give you an idea.

   Curl works with a massive amount of protocols including FTP, HTTP, HTTPS, IMAP, LDAP, RTSP,
   and more. Sounds confusing with all those acronyms, right? I have yet to see  a utility
   personally, that can do as much, and interface as much with other commands as much as curl.
   Since this article is only concerned around downloading files from websites, that is really
   all that I will cover.

$> curl -O URL.file

   Download a text file using its original filename:
$> curl -o - http://test.com/file.txt

   Useful Options/commands:

   -LOC
   follow all redirects, and continue where you left off if the download previously failed.

   curl -LO http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2

   -I
   Check the page header to investigate link redirects:

   curl -I http://somapage.com

   FTP Downloads:
$> curl -u ftpuser:ftppass -O ftp://ftp_server/public_html/xss.php

Summary
   Downloading files, is always typically done in the file browser for general users. If
   you're dealing with source code and revision management, wget and curl are very valuable
   tools. If you have any questions, please let them below in the comments section.


---

