filename: creating-loopback-filesystem-image-on-linux_20140320.txt
http://wiki.osdev.org/Loopback_Device

Loopback Device

   A Loopback Device is a mechanism used to interpret files as real devices. The main
   advantage of this method is that all tools used on real disks can be used with a loopback
   device.

   Note: This article only covers UNIX environments (including [5]Cygwin). For information on
   how to use loopback devices on Windows, see [6]diskpart.

Contents

     * [7]1 Loopback Device under Linux
          + [8]1.1 Floppy Disk Images With GRUB and EXT2
          + [9]1.2 Floppy Disk Images With FAT16
          + [10]1.3 Hard Disk Images
               o [11]1.3.1 Creating an image
               o [12]1.3.2 Partitioning
               o [13]1.3.3 Mounting
               o [14]1.3.4 Formatting the partition
               o [15]1.3.5 Mount Partition
               o [16]1.3.6 Unmount, Detach
               o [17]1.3.7 Making it Easier
               o [18]1.3.8 The End
     * [19]2 Loopback Device under FreeBSD
          + [20]2.1 FreeBSD 4.x
          + [21]2.2 FreeBSD 5.x
     * [22]3 Loopback Device under OpenBSD

Loopback Device under Linux

   The linux loopback device can be used by root only, and needs to be enabled in the kernel
   before use.

Floppy Disk Images With GRUB and EXT2

   First, lets create a empty image.
dd if=/dev/zero of=floppy.img bs=512 count=2880

   Now, lets set it up for mounting.
losetup /dev/loop0 floppy.img

   Now lets make it EXT2 formatted.
mkfs -t ext2 /dev/loop0

   Mount!
mount -t ext2 /dev/loop0 /mnt/myfloppy

   Create GRUB directory.
cd /mnt/myfloppy
mkdir grub

   Copy GRUB's second stage files. (GRUB stage[12] could also be located in /usr/lib/grub/)
cp /lib/grub/i386-pc/stage[12] /mnt/myfloppy/grub

   Create a device mapping for the GRUB installation. You need quotations around the first
   part.
echo "(fd0) /dev/loop0" > /mnt/myfloppy/grub/device.map

   Start GRUB console for installation into the boot record.
grub --device-map=/mnt/myfloppy/grub/device.map /dev/loop0

   In the GRUB console:
root (fd0)
setup (fd0)

   NOTE: You must unmount /mnt/myfloppy before using a emulator to directly read /dev/loop0,
   such as:
qemu -fda /dev/loop0

   NOTE: When deleting the loop device, the original floppy.img file will be saved with the
   modified contents.

Floppy Disk Images With FAT16

   Create an empty image.
dd if=/dev/zero of=floppy.img bs=512 count=2880

   Set it up for mounting.
losetup /dev/loop0 floppy.img

   Make it MSDOS formatted.
mkdosfs /dev/loop0

   Mount!
mount -t msdos /dev/loop0 /mnt/myfloppy

Hard Disk Images

   A hard disk image contains an MBR, then a number of partitions, but the 'mount' instruction
   in Linux works with disk partitions, not full disks. To mount a partition contained in our
   disk image, we need to make sure the 'mount' command only sees our partition, not the whole
   disk.

Creating an image

   First create the empty file that we will use for our disk image. We will assume a disk
   geometry of #cylinders, 16 heads, 63 sectors/track, 512 bytes/sector, which means that each
   cylinder contains 516096 bytes (16*63*512). Decide how large you want your disk image to
   be, and choose an appropriate number of cylinders (I’ll be using #cylinders throughout).

   Example: If I want a 500Mb disk, I would choose 1000 cylinders (approximation of
   (500*1000*1024)/516096).

   Write the disk image (I'll assume the filename c.img throughout):
dd if=/dev/zero of=/path/to/c.img bs=516096c count=#cylinders

   Explanation:
   dd Linux command for copy and convert a file
   if=/dev/zero Source file is /dev/zero which is...*drumroll*...an infinite source of zeros
   of=/path/to/c.img Destination file is our disk image (dd will create the file if it doesn't
   exist)
   bs=516096c Means read and write 516096 bytes at a time (This is just here to keep things
   simple)
   count=#cylinders Copy this number of blocks. Since we have set bs to 516096 bytes each
   block is one cylinder long

   That leaves us with a nice sized file full of zeros that we'll use for our disk image.

Partitioning

   Now to create the MBR and partition table on the disk image (Usually you need to be root).
fdisk -u -C#cylinders -S63 -H16 /path/to/c.img

   Explanation:
   fdisk          Linux DOS partition maintenance program.
   -u             Display units in sectors not cylinders (We will need this).
   -C#cylinders   Set the cylinders of disk to our value.
   -S63           Set the sectors/track to 63.
   -H16           Set the heads/track to 16.
   /path/to/c.img fdisk is capable of partitioning image files directly.

   Within fdisk use the following commands:
o - Create a new empty DOS partition table.
n - Create a new partition (For simplicity just make 1 primary partition covering the whole disk).
a - Toggle the bootable flag (Optional).
p - Print the partition table.

   You should end up with a screen that looks something like this:
Disk /path/to/c.img: 516 MB, 516096000 bytes
16 heads, 63 sectors/track, 1000 cylinders, total 1008000 sectors
Units = sectors of 1 * 512 = 512 bytes

      Device    Boot      Start         End      Blocks   Id  System
/path/to/c.img1   *          63     1007999      503968+  83  Linux

   Obviously the cylinder count, partition end and blocks will be different depending on the
   size of your image.

   Make a note of the start sector (63 here) and the block count (503968 here).

   Note: If you are intending to format the partition to something other than ext2fs then
   change the partition id here using the t command. I should also point out that disk
   manufacturers and programmers don't agree on how many bytes are in a megabyte.
w - Write partition table to our 'disk' and exit.

   Ignore any errors about rereading the partition table. Since it's not a physical device we
   really don't care.

   We now have a partition table on our disk image.

   Unfortunately this also means that from here on out we have to account for the fact that
   our partition does not start at byte 0 of the image.

Mounting

   Ok, now we attach the file to the loopback device, in such a way that we skip everything
   before the start of our partition.
losetup -o32256 /dev/loop0 /path/to/c.img

   Explanation
   -o32256 Move the start of data 32256 bytes into the file

   The reason we move 32256 bytes into the file is this is where the partition starts.
   Remember I said to note the start sector of the partition (63 is usual)? Well, since each
   sector is 512 bytes long we therefore know the starting byte of the partition is 32256
   (63*512) bytes into the file. The reason behind this gap is that most (there is no real
   standard) fdisk programs don't use the first track for anything but the MBR. That space
   isn't always wasted though, some bootloaders (Eg GRUB) use it to store parts of their
   program.

   Note: If you aren't using the suggested geometry then you'll have to calculate this for
   yourself.

   We now have a device (/dev/loop0) which we can use in a similar fashion to a normal one for
   a partition (eg /dev/hda1).

Formatting the partition

   For ext2fs, use:
mke2fs -b1024 /dev/loop0 #blocks

   Explanation:
   mke2fs     Create an ext2 filesytem
   -b1024     Use block size of 1024
   /dev/loop0 Device to make the filesystem on (Here /dev/loop0 is our 'partition')
   #blocks    Remember I said to note the number of blocks from the fdisk section? This is why.

   This gives us a clean ext2 formatted partition.

   Note: mke2fs is smart enough to figure out block size and #blocks for itself, but if you
   ever want to use multiple partitions you'll need to know how to use those values.

   For FAT32, use:
mkdosfs -F32 /dev/loop0 #blocks

   Explanation:
   mkdosfs Create a DOS filesystem (This may be absent on some Linux systems, search for the
   dosfstools package if it is)
   -F32 FAT 32 allocation tables (It should be obvious how to use FAT12/FAT16)
   /dev/loop0 Same as for the ext2fs version
   #blocks Same as for the ext2fs version

   This gives us a clean FAT32 formatted partition (Ignore the floppy warning).

   Note: The reason for #blocks is the same as for ext2fs, ie possible multiple partitions.

Mount Partition

   You should now be able to mount the partition (Because it is still setup on the loopback
   device).

   Command:
mount -text2 /dev/loop0 /mnt/wherever

   or:
mount -tvfat /dev/loop0 /mnt/wherever

   Explanation:
   mount           Linux command to mount a filesystem
   -text2 / -tvfat Filesystem being used, Linux can usually figure this out on its own.
   /dev/loop0      The device representing our partition
   /mnt/wherever   A directory to mount the partition on.

   This should leave you with a nicely mounted partition. If you run df -Th you should end up
   with a line similar to:
Filesystem    Type    Size  Used Avail Use% Mounted on
/dev/loop0    vfat    492M  4.0K  492M   1% /mnt/wherever

   ...or for ext2fs:
Filesystem    Type    Size  Used Avail Use% Mounted on
/dev/loop0    ext2    477M   13K  452M   1% /mnt/wherever

   (Yup, these are for the same disk image. By default ext2fs reserves/uses quite a bit of
   space even empty.)

Unmount, Detach

   Ok, unmount the partition and detach the loopback device.

   Command:
umount /dev/loop0
losetup -d /dev/loop0

   Explanation:
   umount     Linux command to unmount a filesystem.
   /dev/loop0 The device that was mounted

Making it Easier

   One final thing to do, which is to simplify mounting and unmounting that partition.

   Mounting:
mount -text2 -oloop=/dev/loop0,offset=32256 /path/to/c.img /mnt/wherever

   Unmounting:
umount /path/to/c.img

   This is essentially a combination of the losetup and mount commands we used previously when
   formatting the partition. If used it also means we lose access to the raw 'disk' or
   'partition' through /dev/loop0.

   See also [23]http://www.pixelbeat.org/scripts/lomount.sh

   Finally, if you have to mount and umount that image very frequently and you're too lazy to
   type the sudo password each time, just add to /etc/fstab:
/path/to/c.img    /mnt/wherever    ext2    user,loop    0 0

   now you can just call:
mount /mnt/wherever
umount /mnt/wherever

The End

   That's it, you now know how to handle hard disk images under Linux. Whilst mounted you can
   use it in exactly the same way you use a normal disk partition. Multiple partitions are an
   extension of this, just change the offset of the losetup command according to the partition
   you want to use (And format using the correct number of blocks).

   Things to remember:
     * losetup type command will give you the equivalent of a raw disk device (Eg /dev/hda)
     * losetup -o type command will give you the equivalent of a raw partition device (Eg
       /dev/hda1)

   Don't forget to flush the filesystem buffers when manipulating with files on mounted disk
   image. On a Unix-like system, this can be simply done by executing the sync program in your
   shell.

Loopback Device under FreeBSD

   FreeBSD 4.x uses vnconfig FreeBSD 5.x uses mdconfig

   First, use DD to create an empty floppy image (1.44mb in size)

FreeBSD 4.x

dd if=/dev/zero of=floppy.img bs=512 count=2880
vnconfig vn0 floppy.img
newfs_msdos -f 1440 /dev/vn0
mount -t msdosfs /dev/vn0 /mnt/myfloppy

   To shut and image down, unmount and unconfigure it.
umount /mnt/myfloppy
vnconfig -c /dev/vn0


FreeBSD 5.x

   Memdisks are allocated dynamically, and the name is displayed after the mdconfig command.
   This assumes that "md0" is printed.)

   To mount:
dd if=/dev/zero of=floppy.img bs=512 count=2880
mdconfig -a -t vnode -f floppy.img
newfs_msdos -f 1440 /dev/md0
mount -t msdosfs /dev/md0 /mnt/myfloppy

   To unmount:
umount /mnt/myfloppy
mdconfig -d -u md0

Loopback Device under OpenBSD

   OpenBSD has used [24]vnconfig(8) since version 2.2 (perhaps earlier..).

   As root or using su/sudo, Here is an example scenario for configuring a vnode pseudo disk
   device.

   Creating the floppy.img file using dd:
dd if=/dev/zero of=/path/to/floppy.img bs=512 count=2880

   Configuring the vnd0 device:
vnconfig vnd0 /path/to/floppy.img

   Listing configured devices:
vnconfig -l

Output:
   vnd0: covering floppy.img on wd0a, inode 270473
   vnd1: not in use
   vnd2: not in use
   vnd3: not in use

   Creating a FAT12 file system and then mounting the device:
newfs_msdos -F 12 -f 1440 /dev/rvnd0c
mount -t msdos /dev/vnd0i /mnt/floppy

   Removing the device mount and uninstalling the vnd0 device:
umount /mnt/floppy
vnconfig -u vnd0



---
http://www.walkernews.net/2007/07/01/create-linux-loopback-file-system-on-disk-file/

[10]Create Linux Loopback File System On Disk File

   Copyright © [11]Walker 01 Jul 2007 00:19
   Powerful, free-of-charge, open-source Linux operating system. Can you create a [12]Linux
   file system without using partition DIRECTLY – sort of file system within file system
   management?
   The answer is certainly YES!
   With a loopback device in Linux (a feature that’s not natively available in [13]Windows
   Vista and its predecessors) one can easily create a Linux loopback file system on a regular
   disk file, and not directly using a disk partition!
   How to create a Linux loopback file system with a regular disk file?
   To complete this Linux tricks, you need to login with a root user ID for all the steps
   given below
    1. Type dd if=/dev/zero of=/virtualfs bs=1024 count=30720 to create a 30MB disk file
       (zero-filled) called virtualfs in the root (/) directory

    2. Type losetup /dev/loop0 to confirm that the current system is not using any loopback
       devices. Replace /dev/loop0 with /dev/loop1, /dev/loop2, etc, until a free Linux
       loopback device is found. In this case, let’s assume that /dev/loop0 is free for usage

    3. Tpye losetup /dev/loop0 /virtualfs to attach the first Linux loopback device
       (/dev/loop0) with regular disk file (/virtualfs) created in step 1

    4. Type echo $? to confirm the previous step is completed successfully without error – a
       zero will be returned to indicate success. Alternative, type losetup /dev/loop0 to
       confirm

    5. Type mkfs -t ext3 -m 1 -v /dev/loop0 to create a Linux EXT3 file system with 1%
       [14]reserved block count on the loopback device that’s currently associated with a
       regular disk file. Hence, we are creating a file system within file system, or creating
       a file system (mkfs) without using a disk partition directly

    6. Type mkdir /mnt/vfs to create a directory (as mount point) in /mnt

    7. Type mount -t ext3 /dev/loop0 /mnt/vfs to mount the loopback device (regular disk file)
       to /mnt/vfs as a “regular” Linux EXT3 file system! Now, all the Linux file
       system-related commands can be act on this unusual Linux file system. For example, you
       can type df -h to confirm its “disk usage”, type tune2fs -l /dev/loop0 to print its
       file system settings, create / remove files or directories, etc.

    8. To un-mount the loopback file system, type umount /mnt/vfs follow with losetup -d
       /dev/loop0 to effectively remove the loopback file system and release loopback device
       subsequently.

 
    1. Guillaume 18-01-08@17:14
       That’s a good tip !
       It is really helpful but I notice that there is a fastest way to find the first unused
       Linux loopback device :
       losetup -f
    2. Walker 19-01-08@12:28
       Hi Guillaume, thank you for reminding me this command option.
       Good work!
    3. The Bomb Of Adsense Junk Makes Blog Suck – Walker News 23-01-08@23:57
       [...] useful; some readers also really kind to share more info on the topic that I’ve
       made – e.g. Guillaume, Johnny, Brian, CityShaman, Mroids…(Thank you!) However, the
       no-so-good WalkerNews.net [...]
    4. How To Read ISO Image File In Linux – Walker News 06-07-08@15:11
       [...] build in loopback device that allows user to easily mount and access CD image
       files. No complicated installation or [...]
    5. Mastaus 23-10-08@23:48
       Is it possible to do the same in Windows XP or Vista ? and how ?
    6. Scooter 06-11-08@22:38
       Anyone know why sometimes, even after a umount of the mounted file, losetup -d
       /dev/loop returns “LOOP_CLR_FD: Device or resource busy” ?
    7. Scooter 06-11-08@22:46
       Just to avoid any confusion, my loop device is 2, I just typo’d above. /dev/loop2.
    8. palak 10-02-09@05:19
       how can i retain the data in my loopback files even after reboot?
    9. Walker 10-02-09@22:01
       I don’t think the data in loopback file will lost after reboot, unless you’ve recreated
       the loopback file (especial step 1 to step 6) after a reboot.
   10. palak 17-02-09@22:12
       where is the data of loopback files stored after reboot..??
       is it in some image file?
   11. Michael Hampton 19-07-09@20:26
       All that losetup junk is wholly unnecessary.
       Just do:
       mount -o loop myimage.img /mnt/whatever
       The loopback device is automagically handled.
   12. tubs 31-10-09@16:06
       how do you create a disk file.
   13. Andrei 27-02-10@23:54
       Actually i found the losetup junk very useful :)
       I didn’t go too much into the file systems specifics however I needed to mount an ext4
       partition clone (created with partclone) and if the partition was being restored as a
       plain image file on hdd the automagic of the loopback device wouldn’t work on that
       file.
       However, if I would restore the partition clone on a loopback device created as shown
       above by Walker, I could then mount it and restore access to my precious files :)
       Thank you, Walker! This was a really useful tip for me
   14. Jonny Waterlake 09-04-10@22:28
       This is awesome! Thanks!
   15. paul 25-04-10@02:20
       I have trouble visualizing what a loopback actually is and why it’s called that. I’m
       very new to linux and have trouble picturing this idea in my mind.
   16. toro 09-07-10@08:01
       In electronics, the term loopback is generally used to describe methods of routing
       electronic signals or digital data streams, from their originating device quickly back
       to the same source without processing or modification.
       In Unix-like operating systems, a loop device, is a pseudo-device that makes a file
       accessible as a block device. Block devices correspond to devices through which the
       system moves data in the form of blocks. These device nodes often represent addressable
       devices such as CD-ROM drives.
       The two concepts are similar, so it was not a big stretch of the imagination for Linux
       to also use the term loopback originally used in electronics.
   17. David C. 31-07-10@00:09
       That’s a great explanation, toro -
       In order to picture it in your mind, paul, the situation is fairly analogous to owning
       a large, old house.
       You decide that the entire house is too large and, considering modern lifestyles, you
       can reduce the amount of house needed to make a home. (Or for whatever reason.)
       You begin segregating one side of the house from the other (closing off doors,
       rerouting HVAC and electrical, water, sewer, etc.) until you have created a duplex.
       Each side of the house has its own utilities, entrances, parking, etc. allowing the
       former entire house to now function as two complete houses.
       Using a loopback filesystem, you segregate off a portion of your original filesystem so
       that you wind up with two (or more) filesystems available on your one physical hard
       drive. No extra room is created – the original hard drive still has the same overall
       capacity – but the essential functionality of a file storage system is now doubled.
       I hope this helps!
       /David C.
   18. David C. 31-07-10@00:13
       Scooter wrote:
       “Anyone know why sometimes, even after a umount of the mounted file, losetup -d
       /dev/loop returns “LOOP_CLR_FD: Device or resource busy” ?”
       Someone or some process has a file open on the device . . .
   19. David C. 31-07-10@00:23
       Mastaus wrote:
       “Is it possible to do the same in Windows XP or Vista ? and how ?”
       Yes it is possible. Right-click your My Computer (or simply Computer in Vista/W7) icon
       and choose Manage from the pop-up menu.
       Go to Storage -> Disk Management where you can do all kinds of scary things to your
       disks, including creating a new partition. Right-click the (C:) portion of Disk 0 and
       choose to Shrink Volume. Once that process has completed (first you have to make some
       decisions, etc.), right-click the newly vacated space and choose to create a new
       partition. The system will assign a drive letter and you can now use it by accessing
       that drive letter in Windows Explorer. Otherwise, it’s just like using a loop-back file
       system on Linux.
       I hope this helps -
       /David C.
   20. Bluechip 26-09-10@09:03
       If you want the virtual-fs remounted automatically at boot time …add this to the botto
       of /etc/fstab :
       /virtualfs /mnt/vfs ext3 loop,auto 0 0
       …obviously change the fliename[1], mountpoint[6] and filesystem[5] as you require :)
       You can test this by un-mounting the current system :
       umount /mnt/vfs
       …and remounting it with the “mount all” command (as used during boot):
       mount -a
       …Thnaks for the great tutorial :)
   21. Padam 26-09-10@23:17
       I have created a ntfs image file in Linux . I am able to mount it successfully in linux
       and add and delete data. But i dont know how can i use this image file in wondows ?
       Thanks for help
   22. Klepto 11-11-10@02:01
       @Dave C
       What you are describing is creating another partition on a disk, this is NOT the same
       as a loopback filesystem.
   23. CocoChristopher 14-07-11@09:19
       I have found the free IMDISK driver will allow using a filesystem within a host file on
       Windows.
       It can be found here:
       [17]http://www.ltr-data.se/opencode.html/#ImDisk
   24. Archie 19-01-12@19:48
       I get the concept of loopback file system now after reading everyones comments. The one
       question I have is whether this can be achieved on a file system where there is limited
       disk space, e.g. an embedded system with limited resources. From what I understand
       here, you have a file system, you then create a file and turn it into another file
       system. I then need to copy the original file system into this pseudo file system
       structure in order to trick some application software into thinking its running on a
       different file system. Do I need twice the disk space of the first file system?



---
http://www.root9.net/2012/07/creating-loopback-file-system-image.html

Creating a loopback file-system image with partitions

   With building the OpenELEC images for Raspberry Pi - I ran into an odd problem with
   creating system images for people to download.
   The first image I made was DD'd from an 8GB SD card that I had already created and tested.
   I formatted the SD card, tested in the Pi and then took an image with DD. Then, rewrote it
   to the card and tested again. All good. Then a few people commented that the image was
   slightly too large to fit on other 8GB cards which is when I checked and my card it was
   indeed a few sectors large than some others. So how do you create images which are useful
   for more people without going out and buying an army of various sized SD cards? The answer
   is creating loopback devices which are basically file-systems in a single file (which is
   what you create when you DD a working image from an SD card, for example).
   1. Decide what size you would like your image to be. For this example and the OpenELEC
   image I posted previously, I used 1GB.
   2. Create your empty container file using DD. You can replace the expression after seek
   with 1024000000 if you prefer - this is the size of your image in bytes.
 dd if=/dev/zero of=~/1gb_file_image.img bs=1024 count=0 seek=$[1000*1000]

   3. Mount this device as a loopback device
 sudo losetup /dev/loop0 ~/1gb_file_image.img

   4. This will give you a loopback block device under /dev/loop0. You can now create your
   partitions using fdisk / parted. I've used the examples from the [5]OpenELEC wiki here.
 sudo parted -s /dev/loop0 mklabel msdos
 sudo parted -s /dev/loop0 unit cyl mkpart primary fat32 -- 0 16
 sudo parted -s /dev/loop0 set 1 boot on
 sudo parted -s /dev/loop0 unit cyl mkpart primary ext2 -- 16 -2
 sudo mkfs.vfat -n System /dev/loop0p1
 sudo mkfs.ext4 -L Storage /dev/loop0p2

   5. Now you have a filesystem in a file with two partitions. Things can get a little tricky
   here when you need to mount them individually. You need to umount the current loopback
   device so you can remount it with some sector offsets and size limits. If you search and go
   by the scraps of mailing lists that mention this, it can be a little daunting but it's not
   as bad as it seems. Lets look at an fdisk output of our new filesystem in a file.
 sudo fdisk -l ~/1gb_file_image
 Disk 1gb_file_image.img: 1024 MB, 1024000000 bytes
 255 heads, 63 sectors/track, 124 cylinders, total 2000000 sectors
 Units = sectors of 1 * 512 = 512 bytes
 Sector size (logical/physical): 512 bytes / 512 bytes
 I/O size (minimum/optimal): 512 bytes / 512 bytes
 Disk identifier: 0xd00251a6
 Device              Boot Start    End     Blocks Id  System
 1gb_file_image.img1 *    2048     264191  131072 c   W95 FAT32 (LBA)
 1gb_file_image.img2      264192   1999999 867904 83  Linux

   What we need to note here are the start and end sectors as we will use these to calculate
   the offset and sizelimit parameters to pass to losetup when mounting the partitions in the
   file. Taking the first partition as an example (shown as an actual file with a different
   number at the end, that's just how fdisk rolls I guess). We have a starting sector of 2048
   and and end sector of 264191. We need to multiply each of these by 512 to get the total
   amount of bytes instead of sectors (bytes per sector is in the output - line 4 above). So,
   our starting offset becomes 1048576, and the size being  (end - start, multiplied by 512
   again) 134217216.
   We'll add these to our next command.
   6. Mount the loopback image file again, specifying the offset and sizelimit for the first
   partition.
 sudo losetup /dev/loop0 ~/1gb_image_file.img -o 1048576 --sizelimit 134217216

   7. Now, you can mount this loopback device as a block device with the standard mount
   command
 sudo mount /dev/loop0 /mnt/my_mount_point

   This should give you the first FAT partition (if you used the OpenELEC example anyway)
   mounted under /mnt/my_mount_point (make sure it exists first!). Then you can copy files
   into your loopback device to populate your image. To mount the second partition, follow the
   same procedure as in steps 5-7 but use a new loopback device (/dev/loop1 is a good starting
   point) and calculate the new offsets and sizelimits based on the start and end sectors of
   the second partition. When you're finished, make sure to unmount the loop devices, then
   detach the loopback devices with
 sudo losetup -d /dev/loop0
 sudo losetup -d /dev/loop1

   It can be a finicky procedure, but once you've got it nailed it's pretty handy. Let me know
   if any parts don't work as advertised (especially seeing as it took several attempts to get
   mine right too!).

8 comments:

    1. [openid36-rounded.png]
       [21]blog[22]1 September 2012 12:40
       Following your guidance here I got a modified version of the create_sdcard script
       working with the loop0 device. Having moved it to another machine I now find that
       although fdisk shows /dev/loop0p1 and /dev/loop0p2 after partitioning they're not
       appearing in /dev (even after doing partprobe /dev/loop0). Do you have any ideas why
       this might be happening?
       I think I can fix things by using offsets in losetup and then formatting the whole
       device (rather than just a partition), but that seems messy.
       [23]Reply[24]Delete
       [25]Replies
         1. [openid36-rounded.png]
            [26]blog[27]1 September 2012 14:54
            I'm starting to suspect that this is down to the device handling on the VM service
            provider I'm using, and I've worked around this by creating an empty partitioned
            and formatted image on a machine that does work properly then using that for the
            later stages of the image creation process.
            [28]Delete
         2. [rinzler.png]
            [29]Stuart[30]2 September 2012 15:34
            Hey - I had a few problems with partitions not appearing too and I think it was
            mostly down to certain tools not recognising the partitions on a loopback device
            (ie ,they only read the first 512bytes). I gave in and used the offsets which is a
            bit of a faff but seems to be more robust.
            [31]Delete
         3. [Snapshot%2Bof%2Bme%2B3.jpg]
            [32]Agócs[33]26 August 2013 17:23
            Hello folks,
            I'm facing the same problem, missing the partition devices :-/ So I can't make an
            ext4 to my loop partitiions, because mkfs can't find the dev. Can you describe me
            how you managed this issue using the offsets? Thanks in advance!
            [34]Delete
         4. [Snapshot%2Bof%2Bme%2B3.jpg]
            [35]Agócs[36]26 August 2013 21:10
            Ok, I found the solution: 'kpartx' does the mapping for the partitions to
            /dev/mapped/loop0pX devices.
            I learned something new today :)
            [37]Delete
         5. [rinzler.png]
            [38]Stuart[39]31 August 2013 19:49
            Sounds interesting, will have to check that out. Thanks for sharing!
            [40]Delete
            [41]Reply
    2. [b36-rounded.png]
       [42]Jim Moyle[43]27 June 2013 23:24
       Unfortunately when using sudo parted -s /dev/loop0 unit cyl mkpart primary fat32 -- 0
       16 I get the following error:
       Error: Error informing the kernel about modifications to partition /dev/loop0 --
       Invalid argument. This means Linux won't know about any changes you made to /dev/loop0
       until you reboot -- so you shouldn't mount it or use it in any way before rebooting.
       I get a similar result with fdisk, is this what you mean by partitions not appearing?
       [44]Reply[45]Delete
    3. [rinzler.png]
       [46]Stuart[47]28 June 2013 19:28
       It sounds familiar yes, have you tried running 'sudo partprobe /dev/loop0' to update
       manually?
       [48]Reply[49]Delete



---
http://www.tldp.org/HOWTO/archived/Loopback-Root-FS/Loopback-Root-FS-1.html

1. Introduction

1.1 Copyright

   The Loopback Root Filesystem HOWTO Copyright (C) 1998,99 Andrew M. Bishop
   (amb@gedanken.demon.co.uk).

   This documentation is free documentation; you can redistribute it and/or modify it under
   the terms of the GNU General Public License as published by the Free Software Foundation;
   either version 2 of the License, or (at your option) any later version.

   This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
   without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
   See the GNU General Public License for more details.

   The GNU General Public License is available from [5]http://www.fsf.org/ or, write to the
   Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111 USA

1.2 Revision History

   Version 1.0.0
          Initial Version (June 1998)

   Version 1.0.1-1.0.3
          Slight Modifications, kernel version changes, typos etc. (1998 - July 1999)

   Version 1.1
          Added Copyright Information and Re-Submitted (September 1999)

References

   1. http://www.tldp.org/HOWTO/archived/Loopback-Root-FS/Loopback-Root-FS-2.html
   2. http://www.tldp.org/HOWTO/archived/Loopback-Root-FS/Loopback-Root-FS.html#toc1
   3. http://www.tldp.org/HOWTO/archived/Loopback-Root-FS/Loopback-Root-FS-2.html
   4. http://www.tldp.org/HOWTO/archived/Loopback-Root-FS/Loopback-Root-FS.html#toc1
   5. http://www.fsf.org/
   6. http://www.tldp.org/HOWTO/archived/Loopback-Root-FS/Loopback-Root-FS-2.html
   7. http://www.tldp.org/HOWTO/archived/Loopback-Root-FS/Loopback-Root-FS.html#toc1


---
http://www.tldp.org/HOWTO/archived/Loopback-Root-FS/Loopback-Root-FS-2.html

2. Principles of Loopback Devices and Ramdisks

   First I will describe some of the general principles that are used in the setting up of a
   loopback filesystem as the root device.

2.1 Loopback Devices

   A loopback device in Linux is a virtual device that can be used like any other media
   device.

   Examples of normal media devices are hard disk partitions like /dev/hda1, /dev/hda2,
   /dev/sda1, or entire disks like the floppy disk /dev/fd0 etc. They are all devices that can
   be used to hold a files and directory structures. They can be formatted with the filesystem
   that is required (ext2fs, msdos, ntfs etc.) and then mounted.

   The loopback filesystem associates a file on another filesystem as a complete device. This
   can then be formatted and mounted just like any of the other devices listed above. To do
   this the device called /dev/loop0 or /dev/loop1 etc is associated with the file and then
   this new virtual device is mounted.

2.2 Ramdisk Devices

   In Linux it is also possible to have another type of virtual device mounted as a
   filesystem, this is the ramdisk device.

   In this case the device does not refer to any physical hardware, but to a portion of memory
   that is set aside for the purpose. The memory that is allocated is never swapped out to
   disk, but remains in the disk cache.

   A ramdisk can be created at any time by writing to the ramdisk device /dev/ram0 or
   /dev/ram1 etc. This can then be formatted and mounted in the same way that the loopback
   device is.

   When a ramdisk is used to boot from (as is often done on Linux installation disks or rescue
   disks) then the disk image (the entire contents of the disk as a single file) can be stored
   on the boot floppy in a compressed form. This is automatically recognised by the kernel
   when it boots and is uncompressed into the ramdisk before it is mounted.

2.3 The Initial Ramdisk Device

   The initial ramdisk device in Linux is another important mechanism that we need to be able
   to use a loopback device as a the root filesystem.

   When the initial ramdisk is used the filesystem image is copied into memory and mounted so
   that the files on it can be accessed. A program on this ramdisk (called /linuxrc) is run
   and when it is finished a different device is mounted as the root filesystem. The old
   ramdisk is still present though and is mounted on the directory /initrd if present or
   available through the device /dev/initrd.

   This is unusual behaviour since the normal boot sequence boots from the designated root
   partition and keeps on running. With the initial ramdisk option the root partition is
   allowed to change before the main boot sequence is started.

2.4 The Root Filesystem

   The root filesystem is the device that is mounted first so that it appears as the directory
   called / after booting.

   There are a number of complications about the root filesystem that are due to the fact that
   it contains all files. When booting the rc scripts are run, these are either the files in
   /etc/rc.d or /etc/rc?.d depending on the version of the /etc/init program.

   When the system has booted it is not possible to unmount the root partition or change it
   since all programs will be using it to some extent. This is why the initial ramdisk is so
   useful because it can be used so that the final root partition is not the same as the one
   that is loaded at boot time.

2.5 The Linux Boot Sequence

   To show how the initial ramdisk operates in the boot sequence, the order of events is
   listed below.

    1. The kernel is loaded into memory, this is performed by LILO or LOADLIN. You can see the
       Loading... message as this happens.
    2. The ramdisk image is loaded into memory, again this is performed by LILO or LOADLIN.
       You can see the Loading... message again as this happens.
    3. The kernel is initialised, including parsing the command line options and setting of
       the ramdisk as the root device.
    4. The program /linuxrc is run on the initial ramdisk.
    5. The root device is changed to that specified in the kernel parameter.
    6. The init program /etc/init is run which will perform the user configurable boot
       sequence.

   This is just a simplified version of what happens, but is sufficient to explain how the
   kernel starts up and where the initial ramdisk is used.



---
http://www.tldp.org/HOWTO/archived/Loopback-Root-FS/Loopback-Root-FS-3.html

3. How To Create a Loopback Root Device

   Now that the general principles are explained the method of creating the loopback device
   can be explained.

3.1 Requirements

   To create the loopback root device will require a number of things.

     * A working Linux system.
     * A way to copy large files onto the target DOS partition.

   Most important is access to an installed Linux system. This is because the loop device can
   only be created under Linux. This will mean that it is not possible to bootstrap a working
   system from nothing. The requirements of the Linux system that you use is that you can
   compile a kernel on it.

   Once the loopback device is created it will be a large file. I have used an 80 MB files,
   but while this was sufficient for an X terminal it may not be enough if you want to use it
   for much else. This file must be copied onto the DOS partition, so either a network or a
   lot of floppy disks must be used.

   The software that you will require includes

     * LOADLIN version 1.6 or above
     * A version of mount that supports loopback devices
     * A version of the kernel that supports the required options.

   All of these should be standard for recent Linux installations.

3.2 Creating the Linux Kernel

   I created the loopback device using Linux kernel version 2.0.31, other versions should also
   work, but they must have at least the options listed below.

   The kernel options that you will need to enable are the following:
     * RAM disk support (CONFIG_BLK_DEV_RAM).
     * Initial RAM disk (initrd) support (CONFIG_BLK_DEV_INITRD).
     * Loop device support (CONFIG_BLK_DEV_LOOP).
     * fat fs support (CONFIG_FAT_FS).
     * msdos fs support (CONFIG_MSDOS_FS).

   The first two are for the RAM disk device itself and the initial ram disk device. The next
   one is the loop back filesystem option. The last two are the msdos filesystem support which
   is required to mount the DOS partitition.

   Compiling a kernel without modules is the easiest option, although if you do want modules
   then it should be possible although I have not tried it. If modules are used then you
   should make sure that you have the options above compiled in and not as modules themselves.

   Depending on the kernel version that you have you may need to apply a kernel patch. It is a
   very simple one that allows the loopback device to be used as the root filesystem.
     * Kernel versions before 2.0.0; I have no information about these.
     * Kernel version 2.0.0 to 2.0.34; you need to apply the kernel patch for 2.0.x kernels as
       shown below.
     * Kernel version 2.0.35 to 2.0.x; no kernel patch is required.
     * Kernel version 2.1.x; you need to apply the kernel patch for 2.0.x or 2.2.x kernels as
       shown below, depending on the exact 2.1.x version.
     * Kernel version 2.2.0 to 2.2.10; you need to apply the kernel patch for 2.2.x kernels as
       shown below.
     * Kernel version 2.3.x; you need to apply the kernel patch for 2.2.x kernels as shown
       below.

   For 2.0.x kernels the file /init/main.c needs to have a single line added to it as shown by
   the modified version below. The line that says "loop", 0x0700 is the one that was added.

static void parse_root_dev(char * line)
{
        int base = 0;
        static struct dev_name_struct {
                const char *name;
                const int num;
        } devices[] = {
                { "nfs",     0x00ff },
                { "loop",    0x0700 },
                { "hda",     0x0300 },

...

                { "sonycd",  0x1800 },
                { NULL, 0 }
        };

...

}

   For 2.2.x kernels the file /init/main.c needs to have three lines added to it as shown by
   the modified version below. The line that says "loop", 0x0700 and the ones either side of
   it are the ones that were added.

static struct dev_name_struct {
        const char *name;
        const int num;
} root_dev_names[] __initdata = {
#ifdef CONFIG_ROOT_NFS
        { "nfs",     0x00ff },
#endif
#ifdef CONFIG_BLK_DEV_LOOP
        { "loop",    0x0700 },
#endif
#ifdef CONFIG_BLK_DEV_IDE
        { "hda",     0x0300 },

...

        { "ddv", DDV_MAJOR << 8},
#endif
        { NULL, 0 }
};

   Once the kernel is configured it should be compiled to produce a zImage file (make zImage).
   This file will be arch/i386/boot/zImage when compiled.

3.3 Creating the Initial Ramdisk Device

   The initial ramdisk is most easily created as a loopback device from the start. You will
   need to do this as root, the commands that you need to execute are listed below, they are
   assumed to be run from root's home directory (/root).

mkdir /root/initrd
dd if=/dev/zero of=initrd.img bs=1k count=1024
mke2fs -i 1024 -b 1024 -m 5 -F -v initrd.img
mount initrd.img /root/initrd -t ext2 -o loop
cd initrd
[create the files]
cd ..
umount /root/initrd
gzip -c -9 initrd.img > initrdgz.img

   There are a number of steps to this, but they can be described as follows.
    1. Create a mount point for the initial ramdisk (an empty directory).
    2. Create an empty file of the size required. Here I have used 1024kB, you may need less
       or more depending on the contents, (the size is the last parameter).
    3. Make an ext2 filesystem on the empty file.
    4. Mount the file onto the mount point, this uses the loopback device.
    5. Change to the mounted loopback device.
    6. Create the files that are required (see below for details).
    7. Move out of the mounted loopback device.
    8. Unmount the device.
    9. Create a compressed version for use later.

   Contents Of The Initial Ramdisk

   The files that you will need on the ramdisk are the minimum requirements to be able to
   execute any commands.

     * /linuxrc The script that is run to mount the msdos file system (see below).
     * /lib/* The dynamic linker and the libraries that the programs need.
     * /etc/* The cache used by the dynamic linker (not strictly needed, but does stop it
       complaining).
     * /bin/* A shell interpreter (ash because it is smaller than bash. The mount and losetup
       programs for handling the DOS disk and setting up the loopback devices.
     * /dev/* The devices that will be used. You need /dev/zero for ld-linux.so, /dev/hda* to
       mount the msdos disk and /dev/loop* for the lopback device.
     * /mnt An empty directory to mount the msdos disk on.

   The initial ramdisk that I used is listed below, the contents come to about 800kB when the
   overhead of the filesystem are taken into account.

total 18
drwxr-xr-x   2 root     root         1024 Jun  2 13:57 bin
drwxr-xr-x   2 root     root         1024 Jun  2 13:47 dev
drwxr-xr-x   2 root     root         1024 May 20 07:43 etc
drwxr-xr-x   2 root     root         1024 May 27 07:57 lib
-rwxr-xr-x   1 root     root          964 Jun  3 08:47 linuxrc
drwxr-xr-x   2 root     root        12288 May 27 08:08 lost+found
drwxr-xr-x   2 root     root         1024 Jun  2 14:16 mnt

./bin:
total 168
-rwxr-xr-x   1 root     root        60880 May 27 07:56 ash
-rwxr-xr-x   1 root     root         5484 May 27 07:56 losetup
-rwsr-xr-x   1 root     root        28216 May 27 07:56 mount
lrwxrwxrwx   1 root     root            3 May 27 08:08 sh -> ash

./dev:
total 0
brw-r--r--   1 root     root       3,   0 May 20 07:43 hda
brw-r--r--   1 root     root       3,   1 May 20 07:43 hda1
brw-r--r--   1 root     root       3,   2 Jun  2 13:46 hda2
brw-r--r--   1 root     root       3,   3 Jun  2 13:46 hda3
brw-r--r--   1 root     root       7,   0 May 20 07:43 loop0
brw-r--r--   1 root     root       7,   1 Jun  2 13:47 loop1
crw-r--r--   1 root     root       1,   3 May 20 07:42 null
crw-r--r--   1 root     root       5,   0 May 20 07:43 tty
crw-r--r--   1 root     root       4,   1 May 20 07:43 tty1
crw-r--r--   1 root     root       1,   5 May 20 07:42 zero

./etc:
total 3
-rw-r--r--   1 root     root         2539 May 20 07:43 ld.so.cache

./lib:
total 649
lrwxrwxrwx   1 root     root           18 May 27 08:08 ld-linux.so.1 -> ld-linux.so.1.7.14
-rwxr-xr-x   1 root     root        21367 May 20 07:44 ld-linux.so.1.7.14
lrwxrwxrwx   1 root     root           14 May 27 08:08 libc.so.5 -> libc.so.5.3.12
-rwxr-xr-x   1 root     root       583795 May 20 07:44 libc.so.5.3.12

./lost+found:
total 0

./mnt:
total 0

   The only complex steps about this are the devices in dev. Use the mknod program to create
   them, use the existing devices in /dev as a template to get the required parameters.

   The /linuxrc file

   The /linuxrc file on the initial ramdisk is required to do all of the preparations so that
   the loopback device can be used for the root partition when it exits.

   The example below tries to mount /dev/hda1 as an msdos partition and if it succeeds then
   sets up the files /linux/linuxdsk.img as /dev/loop0 and /linux/linuxswp.img as /dev/loop1.

#!/bin/sh

echo INITRD: Trying to mount /dev/hda1 as msdos

if /bin/mount -n -t msdos /dev/hda1 /mnt; then

   echo INITRD: Mounted OK
   /bin/losetup /dev/loop0 /mnt/linux/linuxdsk.img
   /bin/losetup /dev/loop1 /mnt/linux/linuxswp.img
   exit 0

else

   echo INITRD: Mount failed
   exit 1

fi

   The first device /dev/loop0 will become the root device and the second one /dev/loop1 will
   become the swap space.

   If you want to be able to write to the DOS partition as a non-root user when you have
   finished then you should use mount -n -t msdos /dev/hda1 /mnt -o
   uid=0,gid=0,umask=000,quiet instead. This will map all accesses to the DOS partition to
   root and set the permissions appropriately.

3.4 Creating The Root Device

   The root device that you will be using is the file linuxdsk.img. You will need to create
   this in the same way that the initial ramdisk was created, but bigger. You can install any
   Linux installation that you like onto this disk.

   The easiest way might be to copy an existing Linux installation into it. An alternative is
   to install a new Linux installation onto it.

   Assuming that you have done this, there are some minor changes that you must make.

   The /etc/fstab file must reference the root partition and the swap using the two loopback
   devices that are setup on the initial ramdisk.

/dev/loop0     /      ext2   defaults 1 1
/dev/loop1     swap   swap   defaults 1 1

   This will ensure that when the real root device is used the kernel will not be confused
   about where the root device is. It will also allow the swap space to be added in the same
   way a swap partition is normally used. You should remove any other reference to a root disk
   device or swap partition.

   If you want to be able to read the DOS partition after Linux has started then you will need
   to make a number of extra small changes.

   Create a directory called /initrd, this is where the initial ramdisk will be mounted once
   the loopback root filesystem is mounted.

   Create a symbolic link called /DOS that points to /initrd/mnt where the real DOS parition
   will be mounted.

   Add a line into the rc file that mounts the disks. This should run the command mount -f -t
   msdos /dev/hda1 /initrd/mnt, this will create a 'fake' mount of the DOS partition so that
   all programs (like df) will know that the DOS partition is mounted and where to find it. If
   you used different options in the /linuxrc file that obviously you should use them here
   also.

   There is no need to have a Linux kernel on this root device since that is already loaded
   earlier. If you are using modules however then you should include them on this device as
   normal.

3.5 Creating the Swap Device

   The root device that you will be using is the file linuxswap.img. The swap device is very
   simple to create. Create an empty file as was done for the initial ramdisk and then run
   mkswap linuxswap.img to intialise it.

   The size of the swap space will depend on what you plan to do with the installed system,
   but I would recommend between 8MB and the amount of RAM that you have.

3.6 Creating the MSDOS Directory

   The files that are going to be used need to be moved onto the DOS partition.

   The files that are required in the DOS directory called C:\LINUX are the following:

     * LINUXDSK.IMG The disk image that will become the root device.
     * LINUXSWP.IMG The swap space.

3.7 Creating the Boot Floppy

   The boot floppy that is used is just a normal DOS format bootable floppy.

   This is created using format a: /s from DOS.

   Onto this disk you will need to create an AUTOEXEC.BAT file (as below) and copy the kernel,
   compressed initial ramdisk and LOADLIN executable.

     * AUTOEXEC.BAT The DOS automatically executed batch file.
     * LOADLIN.EXE The LOADLIN program executable.
     * ZIMAGE The Linux kernel.
     * INITRDGZ.IMG The compressed initial ramdisk image.

   The AUTOEXEC.BAT file should contain just one line as below.

\loadlin \zImage initrd=\initrdgz.img root=/dev/loop0 ro

   This specifies the kernel image to use, the initial ramdisk image, the root device after
   the initial ramdisk has finished and that the root partition is to be mounted read-only.
 


---
http://www.tldp.org/HOWTO/archived/Loopback-Root-FS/Loopback-Root-FS-4.html

4. Booting the System

   To boot from this new root device all that is required is that the floppy disk prepared as
   described above is inserted for the PC to boot from.

   You will see the following sequence of events.
    1. DOS boots
    2. AUTOEXEC.BAT starts
    3. LOADLIN is run
    4. The Linux kernel is copied into memory
    5. The initial ramdisk is copied into memory
    6. The Linux kernel is started running
    7. The /linuxrc file on the initial ramdisk is run
    8. The DOS partition is mounted and the root and swap devices set up
    9. The boot sequence continues from the loopback device

   When this is complete you can remove the boot floppy and use the Linux system.

4.1 Possible Problems With Solutions

   There are a number of stages where this process could fail, I will try to explain what they
   are and what to check.

   DOS booting is easy to recognise by the message that it prints MS-DOS Starting ... on the
   screen. If this is not seen then the floopy disk is either not-bootable or the PC is not
   bootable from the floppy disk drive.

   When the AUTOEXEC.BAT file is run the commands in it should be echoed to the screen by
   default. In this case there is just the single line in the file that starts LOADLIN.

   When LOADLIN executes it will do two very visible things, firstly it will load the kernel
   into memory, secondly it will copy the ramdisk into memory. Both of these are indicated by
   a Loading... message.

   The kernel starts by uncompressing itself, this can give crc errors if the kernel image is
   corrupted. Then it will start running the initialisation sequence which is very verbose
   with diagnostic messages. Loading of the initial ramdisk device is also visible during this
   phase.

   When the /linuxrc file is run there is no diagnostic messages, but you can add these
   yourself as an aid to debugging. If this stage fails to set up the loopback device as the
   root device then you may see a message that there is no root device and the kernel aborts.

   The normal boot sequence of the new root device will now continue and this is quite
   verbose. There may be problems about the root device being mounted read-write, but the
   LOADLIN command line option 'ro' should stop that. Other problems that can occur are that
   the boot sequence is confused about where the root device is, this is probably due to a
   problem with /etc/fstab.

   When the boot sequence has completed, the remaining problem is that programs are confused
   about whether the DOS partition is mounted or not. This is why it is a good idea to use the
   fake mount command described earlier. This makes life a lot easier if you want to access
   the files on the DOS device.

4.2 Reference Documents

   The documents that I used to create my first loopback root filesystem were:

     * The Linux kernel source, in particular init/main.c
     * The Linux kernel documentation, in particular Documentation/initrd.txt and
       Documentation/ramdisk.txt.
     * The LILO documentation.
     * The LOADLIN documentation
 


---
http://www.tldp.org/HOWTO/archived/Loopback-Root-FS/Loopback-Root-FS-5.html

5. Other Loopback Root Device Possibilities

   Once the principle of booting a filesystem in a file on a DOS partition has been
   established there are many other things that you can now do.

5.1 DOS Hard-disk Only Installation

   If it is possible to boot Linux from a file on a DOS harddisk by using a boot floppy then
   it is obviously also possible to do it using the harddisk itself.

   A configuration boot menu can be used to give the option of running LOADLIN from within the
   AUTOEXEC.BAT. This will give a much faster boot sequence, but is otherwise identical.

5.2 LILO Booted Installation

   Using LOADLIN is only one option for booting a Linux kernel. There is also LILO that does
   much the same but without needing DOS.

   In this case the DOS format floppy disk can be replaced by an ext2fs format one. Otherwise
   the details are very similar, with the kernel and the initial ramdisk being files on that
   disk.

   The reason that I chose the LOADLIN method is that the arguments that need to be given to
   LILO are slightly more complex. Also it is more obvious to a casual observer what the
   floppy disk is since it can be read under DOS.

5.3 VFAT / NTFS Installation

   I have tried the NTFS method, and have had no problems with it. The NTFS filesystem driver
   is not a standard kernel option in version 2.0.x, but is available as a patch from
   [5]http://www.informatik.hu-berlin.de/~loewis/ntfs/. In version 2.2.x the NTFS driver is
   included as standard in the kernel.

   The only changes for the VFAT or NTFS options are in the initial ramdisk, the file /linuxrc
   needs to mount a file system of type vfat or ntfs rather that msdos.

   I know of no reason why this should not also work on a VFAT partition.

5.4 Installing Linux without Re-partitioning

   The process of installing Linux on a PC from a standard distribution requires booting from
   a floppy disk and re-partitioning the disk. This stage could instead be accomplished by a
   boot floppy that creates an empty loopback device and swap file. This would allow the
   installation to proceed as normal, but it would install into the loopback device rather
   than a partition.

   This could be used as an alternative to a UMSDOS installation, it would be more efficient
   in disk usage since the minimum allocation unit in the ext2 filesystem is 1kB instead of up
   to 32kB on a DOS partition. It can also be used on VFAT and NTFS formatted disks which are
   otherwise a problem.

5.5 Booting From a Non-bootable device

   This method can also be used to boot a Linux system from a device that is not normally
   bootable.

     * CD-Rom
     * Zip Disks
     * Parallel port disk drives

   Obviously there are many other devices that could be used, NFS root filesystems are already
   included in the kernel as an option, but the method described here might also be used
   instead.
 filename: cool_apps-and-hacks_20150612.txt
﻿--------------------------------------------------------------------------------------------------------------
from http://varunbpatil.github.io/2012/09/19/linux-tricks/#.VXp7dvntlBc

vimdiff		... colorful differencies display
split & zip	... split and compress files
host [URL]	... find public IP

***
How to completely paralyze any Linux system which is using the bash shell: $ :(){ :|:& };:

--------------------------------------------------------------------------------------------------------------
http://www.tuxradar.com/content/command-line-tricks-smart-geeks

--------------------------------------------------------------------------------------------------------------
from http://www.commandlinefu.com/commands/browse/sort-by-votes

***
Runs previous command replacing foo by bar every time that foo appears:
$> !!:gs/foo/bar

***
Mount a temporary ram partition
Makes a partition in ram which is useful if you need a temporary working space as read/write access is fast.
Be aware that anything saved in this partition will be gone after your computer is turned off.

$> mount -t tmpfs tmpfs /mnt -o size=1024m

--------------------------------------------------------------------------------------------------------------
http://www.efytimes.com/e1/creativenews.asp?edid=113667

--------------------------------------------------------------------------------------------------------------
from http://mylinuxbook.com/20-interesting-and-extremely-helpful-linux-command-line-tricks/

***
how to delete all the files except .c and .py files.
$> rm !(*.c|*.py)

***
How to search man pages for a particular string?
$> man -k login
	access.conf (5)      - the login access control table file
	add-shell (8)        - add shells to the list of valid login shells
	chsh (1)             - change login shell
	faillog (5)          - login failure logging file
	...
	
***
How to get rid of that unknown process that forbids you to delete a file?

There are situations where you want to delete a file but you get an error like ‘file is already in use’.
You try to find which process in using this file but all your effort goes in vain. What would you do in
this case?

Well, you can use the ‘fuser’ command. It tell you the process that is using a particular file. You can use
‘fuser -k [filename]’ command to kill that process.

--------------------------------------------------------------------------------------------------------------
from https://www.linux.com/learn/tutorials/790121-assorted-fun-linux-command-line-hacks-

***
Rainbows In Your Shell
$> yes "$(seq 231 -1 16)" | while read i; do printf "\x1b[48;5;${i}m\n"; sleep .02; done

--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------


filename: dd-tutorial_RWRP.txt
http://www.softpanorama.org/Tools/dd.shtml

DD Command

News	Recommended Links	Recommended articles	Reference
Examples	 Recovery of lost files using DD	 Working with DD Images
Loopback filesystem	dd and netcat	 Mount a partition from dd disk image
DD Rescue	Acronis True Image	Ghost	Alternatives to Norton Ghost
Filesystems	 Working with ISO Images	 Remote Backup	 Disk
Repartitioning	  Sysadmin Horror Stories Humor   Etc


The dd command has been around since the 1970s, ported to many systems,
rewritten many times, and proved to be an indispensable Unix tool. The name
is an allusion to IBM/360 mainframe OS JCL DD statement. It is jokingly said
that dd stands for "destroy disk" or "delete data", since, being used for
low-level operations on hard disks, a small mistake, such as reversing the if
and of parameters, may accidentally render the entire disk unusable. In modern
Linuxes you can mount dd image using loopback interface.  Recently capability
to mount dd images was added to Linux and other Unixes.



It served as an inspiration to the most important recent class backup programs
called ghosters in memory of the original for Windows Ghost. The key idea
is to get an image of the partition in the form of the file. DD uses older
approach and includes into image parts of the disk that are not parts of
the filesystem. That feature can be used to recovery of deleted files and
in computer forensics when the contents of a disk need to be preserved as a
byte-exact copy. In the latter case using cp command would not be sufficient
because data from deleted files still physically present on a disk but are
not visible in the file system naming space.

In Windows dd command can do backups and restores to the same disk, but there
are some problems with cloning Windows systems. Ghost and Acronis True Image
are better tools for that. As a backup tool dd is still useful, but Ghost
and its alternatives are much faster because they understand what part of
the disk belongs to the filesystem and which don't. DD copies the whole
partition or the whole disk indiscriminately which make the image larger,
although it contains deleted files and can be used for recovery of those
files instead of the disk

But this weakness at the same time represents dd advantages over Ghost and
similar, more modern, tools. In case of damaged filesystem dd is much more
useful. You just need to create a DD-image and then search it for files using
grep or similar tool. In most cases you can recover files pretty reliably
using this method. I one manage to recover a script that was deleted several
hours ago before it was discovered and server continued working all this
time. That was the only copy and the user who did it has no backup. See
Recovery of lost files using DD

On Linux Partimage is especially useful alternative to dd. Partition Image
is a Linux utility which saves partitions for the most common filesystems
formats (Ext3, NTFS, FAT32, etc) to a Ghost-style image file. The image
file can be compressed in the GZIP/BZIP2 formats to save disk space, and
split into multiple files. Partitions can be saved across the network since
version 0.6.0.When using Partimage, the partitions must be unmounted.

The GNU clone of dd is part of fileutils package and was written by Paul
Rubin, David MacKenzie, and Stuart Kemp. dd is also available for Windows as
part of Microsoft Unix toolkit (SFU 3.5). It is also part of Cygwin. There
is also native Windows Win32 port of dd (see Native Win32 ports of some GNU
utilities and dd for windows)

Unlike most Unix commands, dd uses a keyword=value format for its
parameters. This was modeled after IBM System/360 JCL, which had an elaborate
DD 'Dataset Definition' specification for I/O devices in JCL language.

The following exit values are returned:

0 The input file was copied successfully.
>0 An error occurred.
After completion, dd reports the number of whole and partial input and
output blocks

A complete listing of all keywords is available via dd --help. It depends
on flavor of dd used: GNU dd is different from, say, Solaris dd. See Reference

Basic dd Options

The basic syntax of dd is as follows:

# dd if= device of= device  bs= blocksize
The preceding options are used almost every time you run dd:

if= file Specifies the input path. Standard input is the default. The if=
argument specifies the input file or the file from which it is going to copy
the data. This is the file or raw partition that you are going to back up
(e.g., dd if=/dev/dsk/c0t0d0s0 or dd if=/home/file). If you want dd to look
at stdin for its data, you don't need this argument.
of= file Specifies the output path. Standard output is the default. If
the seek=expr conversion is not also specified, the output file will be
truncated before the copy begins, unless conv=notrunc is specified. If
seek=expr is specified, but conv=notrunc is not, the effect of the copy
will be to preserve the blocks in the output file over which dd seeks,
but no other portion of the output file will be preserved. (If the size
of the seek plus the size of the input file is less than the previous size
of the output file, the output file is shortened by the copy.) The output
file The of= argument specifies the output file or the file to which you
are sending the data. This could be a file on disk or an optical platter,
another raw partition (e.g., dd of=/backup/file, dd of=/dev/rmt/0n). If you
are sending to stdout, you don't need this argument. the block size
The bs= argument specifies the block size, or the amount of data that is
to be transferred in one I/O operation. This value normally is expressed
in bytes, but in most versions dd also can be specified in kilobytes by
adding a k at the end of the number (e.g., 10 K). (This is different from
a blocking factor, like dump and tar use, which is multiplied by a fixed
value known as the minimum block size. A blocking factor of 20 with a
minimum block size of 512 would give you an actual block size of 10,240,
or 10 K.) It should be noted that when reading from or writing to a pipe,
dd defaults to a block size of 1. Generally on modern hardware bs=4096 is
a reasonable minimum value. One cylinder (255 heards * 63 sectors * 512 =
16065. So bs=16065 also is attractive. Two cylinders fro multiplate disks
(bs=32130) also might make sense.

Changing block size does not affect how the data is physically written to a
disk device, such as a file on disk or optical platter. Using a large block
size just makes the data transfer more efficient. When writing to a tape
device, however, each block becomes a record, and each record is separated by
an interrecord gap. Once a tape is written with a certain block size, it must
be read with that block size or a multiple of that block size. (For example,
if a tape were written with a block size of 1024, you must use the block size
of 1024 when reading it, or you may use 2048 or 10,240, which are multiples
of 1024.) Again, this applies only to tape devices, not disk-like devices.

bs= n Sets both input and output block sizes to n bytes, superseding ibs=
and obs=. If no conversion other than sync, noerror, and notrunc is specified,
each input block is copied to the output as a single block without aggregating
short blocks. This option is used only if ASCII or EBCDIC conversion is
specified. For the ascii and asciib operands, the input is handled as described
for the unblock operand except that characters are converted to ASCII before
the trailing SPACE characters are deleted. For the ebcdic, ebcdicb, ibm,
and ibmb operands, the input is handled as described for the block operand
except that the characters are converted to EBCDIC or IBM EBCDIC after the
trailing SPACE characters are added. Specifying the input and output block
sizes separately When specifying block size with the option bs=, you are
specifying both the incoming and outgoing block size. There are situations
in which you may need a different block size on each. This is done with the
ibs= and obs= options.
the number of records to read The count=n option tells dd how many records
(blocks) to read. You can use this to read the first few blocks of a file
or tape to see what kind of data it is, for example (see the following
section). You can also use it to have dd tell you what block size a tape
was written in (see below).
The dd utility copies the specified input file to the specified output with
possible conversions. If you reverse the source and target, you can wipe
out you file, partitions or even the whole disk. This feature has inspired
the nickname "dd" Data Destroyer.

The standard input and output are used by default. The input and output
block sizes may be specified for tape or just to increase efficiency (large
blocks are generally transferred faster). Sizes are specified in bytes; a
number may end with k, b, or w to specify multiplication by 1024, 512, or 2,
respectively. Numbers may also be separated by x to indicate multiplication.

The dd utility reads the input one block at a time, using the specified input
block size. dd then processes the block of data actually returned, which
could be smaller than the requested block size. dd applies any conversions
that have been specified and writes the resulting data to the output in
blocks of the specified output block size.

Additional options

cbs= n
Specifies the conversion block size for block and unblock in bytes by n
(default is 0). If cbs= is omitted or given a value of 0, using block or
unblock produces unspecified results. bs is used only if ascii, asciib,
unblock, ebcdic, ebcdicb, ibm, ibmb, or block conversion is specified. In
the first two cases, cbs characters are copied into the conversion buffer,
any specified character mapping is done, trailing blanks are trimmed, and a
NEWLINE is added before sending the line to output. In the last three cases,
characters up to NEWLINE are read into the conversion buffer and blanks are
added to make up an output record of size cbs. ASCII files are presumed to
contain NEWLINE characters. If cbs is unspecified or 0, the ascii, asciib,
ebcdic, ebcdicb, ibm, and ibmb options convert the character set without
changing the input file's block structure. The unblock and block options
become a simple file copy.
files= n
Copies and concatenates n input files before terminating (makes sense only
where input is a magnetic tape or similar device).
skip= n
Skips n input blocks (using the specified input block size) before starting
to copy. On seekable files, the implementation reads the blocks or seeks past
them. On non-seekable files, the blocks are read and the data is discarded.
iseek= n
Seeks n blocks from beginning of input file before copying (appropriate for
disk files, where skip can be incredibly slow).
oseek= n
Seeks n blocks from beginning of output file before copying.
seek= n
Skips n blocks (using the specified output block size) from beginning of
output file before copying. On non-seekable files, existing blocks are read
and space from the current end-of-file to the specified offset, if any,
is filled with null bytes. On seekable files, the implementation seeks to
the specified offset or reads the blocks as described for non-seekable files.
count= n
Copies only n input blocks.
conv= value[,value. . . ]
Where values are comma-separated symbols from the following list:
ascii
Converts EBCDIC to ASCII.
asciib
Converts EBCDIC to ASCII using BSD-compatible character translations.
ebcdic
Converts ASCII to EBCDIC. If converting fixed-length ASCII records without
NEWLINEs, sets up a pipeline with dd conv=unblock beforehand.
ebcdicb
Converts ASCII to EBCDIC using BSD-compatible character translations. If
converting fixed-length ASCII records without NEWLINEs, sets up a pipeline
with dd conv=unblock beforehand.
ibm
Slightly different map of ASCII to EBCDIC. If converting fixed-length ASCII
records without NEWLINEs, sets up a pipeline with dd conv=unblock beforehand.
ibmb
Slightly different map of ASCII to EBCDIC using BSD-compatible character
translations. If converting fixed-length ASCII records without NEWLINEs,
sets up a pipeline with dd conv=unblock beforehand.
The ascii (or asciib), ebcdic (or ebcdicb), and ibm (or ibmb) values are
mutually exclusive.

block
Treats the input as a sequence of NEWLINE-terminated or EOF-terminated
variable-length records independent of the input block boundaries. Each record
is converted to a record with a fixed length specified by the conversion
block size. Any NEWLINE character is removed from the input line. SPACE
characters are appended to lines that are shorter than their conversion block
size to fill the block. Lines that are longer than the conversion block size
are truncated to the largest number of characters that will fit into that
size. The number of truncated lines is reported.
unblock
Converts fixed-length records to variable length. Reads a number of bytes
equal to the conversion block size (or the number of bytes remaining in the
input, if less than the conversion block size), delete all trailing SPACE
characters, and append a NEWLINE character.
The block and unblock values are mutually exclusive.

lcase
Maps upper-case characters specified by the LC_CTYPE keyword tolower to
the corresponding lower-case character. Characters for which no mapping is
specified are not modified by this conversion.
ucase
Maps lower-case characters specified by the LC_CTYPE keyword toupper to
the corresponding upper-case character. Characters for which no mapping is
specified are not modified by this conversion.
The lcase and ucase symbols are mutually exclusive.

swab
Swaps every pair of input bytes. If the current input record is an odd number
of bytes, the last byte in the input record is ignored.
noerror
Does not stop processing on an input error. When an input error occurs, a
diagnostic message is written on standard error, followed by the current input
and output block counts in the same format as used at completion. If the sync
conversion is specified, the missing input is replaced with null bytes and
processed normally. Otherwise, the input block will be omitted from the output.
notrunc
Does not truncate the output file. Preserves blocks in the output file
not explicitly written by this invocation of dd. (See also the preceding
of=file operand.)
sync
Pads every input block to the size of the ibs= buffer, appending null
bytes. (If either block or unblock is also specified, appends SPACE characters,
rather than null bytes.)
If operands other than conv= are specified more than once, the last specified
operand=value is used.

For the bs=, cbs=, ibs=, and obs= operands, the application must supply an
expression specifying a size in bytes. The expression, expr, can be:

a positive decimal number
a positive decimal number followed by k, specifying multiplication by 1024
a positive decimal number followed by b, specifying multiplication by 512
two or more positive decimal numbers (with or without k or b) separated by x,
specifying the product of the indicated values.
All of the operands will be processed before any input is read.

Although you may think of dd as a bit copier, it also can manipulate the
format of the data, such as converting between different character sets,
upper- and lowercase, and fixed-length and variable-length records.

conv=ascii
Converts EBCDIC to ASCII
conv=ebcdic
Converts ASCII to EBCDIC
conv=ibm
Converts ASCII to EBCDIC using the IBM conversion table
conv=lcase
Maps US ASCII alphabetic characters to their lowercase counterparts
conv=ucase
Maps US ASCII alphabetic characters to their uppercase counterparts
conv=swab
Swaps every pair of bytes; can be used to read a volume written in different
byte order
conv=noerror
Does not stop processing on an error
conv=sync
Pads every input block to input block size (ibs)
conv=notrunc
Does not truncate existing file on output
conv=block
Converts input record to a fixed length specified by cbs
conv=unblock
Converts fixed-length records to variable length
conv=..., ...
Uses multiple conversion methods separated by commas
Examples

To make ISO from DVD :

dd if=/dev/dvd of=dvd.iso # for dvd
Backing up your Master Boot Record (MBR).

You should do this before you edit your partition table so that you can put
it back if you mess things up.

# dd if=/dev/hda of=/root/hda.boot.mbr bs=512 count=1
If things mess up, you can boot with Knoppix, mount the partition containing
/root (hda1 in this example) and put back the MBR with the command:

# dd if=/mnt/hda1/root/hda.boot.mbr of=/dev/hda bs=512 count=1
Note: You can backup only the MBR and exclude the partition table with
the command:

 # dd if=/dev/hda of=/root/hda.mbr.noparttab bs=446 count=1
Creating a hard drive backup directly to another hard drive (Dd - LQWiki)
# dd if=/dev/hda of=/dev/sda conv=noerror,sync bs=4k
This command is used often to create a backup of a drive (/dev/hda) directly
to another hard drive (/dev/sda). (The device name /dev/hda is typical of
an IDE hard drive, the device /dev/sda is typical of a USB disk.) This works
only if the hard drive has enough storage to accommodate the source drive's
filesystem. The advantage of this is that you do not have to mount the hard
drive to make a backup and the only reference to hda is in /dev and in the
command which is usually in a script in cron.

The option "bs=4k" is used to specify the block size used in the copy. The
default for the dd command is 512 bytes: use of this small block size can
result in significantly slower copying. However, the tradeoff with larger
block sizes is that when an error is encountered, the remainder of the block
is filled with zero-bytes. So if you increase your block size when copying a
failing device, you'll lose more data but also spend less time trying to read
broken sectors. Tools like dd_rescue and dd_rhelp can provide a more flexible
solution in such cases, combining the speed of a large block size for the
regions without errors with finer-grained block-copies for regions with errors.

Creating a compressed hard drive backup image
# dd if=/dev/hda | gzip > /mnt/hdb1/system_drive_backup.img.gz
Here dd is making an image of the first harddrive, and piping it through the
gzip compression program. The compressed image is then placed in a file on
a seperate drive. To reverse the process:

# gzip -dc /mnt/hdb1/system_drive_backup.img.gz | dd of=/dev/hda
Here, gzip is decompressing (the -d switch) the file, sending the results
to stdout (the -c switch), which are piped to dd, and then written to /dev/hda.
You can create floppy images using DD, althouth they are almost extinct:
dd if=<image file> of=/dev/fd0
. For example:
dd if=/dev/fd0 of=floppy.img bs=18k
where /dev/fd0 should be the device for your raw floppy drive (_not_
/dev/floppy) and floppy.img the file you want to save the info to. You can
then copy that file to somewhere you can read it with DOS, or maybe even
zip it so it will fit onto a floppy ;). You should see something like the
following to indicate that the image transfer was successful:

2880+0 records in
2880+0 records out
If you see a smaller block count, your image did not transfer correctly. If
this is the case, it will usually be accompanied by a disk error. After you
make a disk, make sure to label it according to its contents.

Getting around file size limitations using split
When making images, it's quite easy to run up against various file size
limitations. One way to work around a given file size limitation is to use
the split command.

# dd if=/dev/hda1 | gzip -c | split -b 2000m - /mnt/hdc1/backup.img.gz
This example is using dd to take an image of the first partition on the
first harddrive.
The results are passed through to gzip for compression
The -c option switch is used to output the result to stdout.
The compressed image is then piped to the split tool
The -b 2000m switch tells split how big to make the individual files. You
can use k and m to tell switch kilobytes and megabytes (this option uses
bytes by default).
The - option tells split to read from stdin. Otherwise, split would interpret
the /mnt/hdc1... as the file to be split.
The /mnt/hdc1... is the prefix for the created files. Split will create
files named backup.img.gz.aa, backup.img.gz.ab, etc.
To restore the multi-file backup, do the following:

# cat /mnt/hdc1/backup.img.gz.* | gzip -dc | dd of=/dev/hda1
Cat recombines contents of the compressed and split image files to stdout,
in order.
Results are piped through gzip for decompression.
And are then written to the first partition of the hard drive with dd.
Creating empty disk images
To create an empty disk image, to be used as the disk for an emulator for
example, one can get data from /dev/zero. To create a 10mb image:

$ dd if=/dev/zero of=myimage bs=1024 count=10240
A clever alternative is:

$ dd of=myimage bs=1024 count=0 seek=10240
Here we don't write anything, not even zeroes, we just seek 10mb into the
file and close it. The result is a sparse file that is implicitly full of
10mb of zeroes, but that takes no disk space. ls -l will report 10mb, while
du and df will report 0. When the file is written to, either as an emulator
disk or a loopback device, Linux will allocate disk space for the data. ls
will still show 10mb, while du will gradually approach 10mb.

For swap images, where it's more important to reserve the data than to save
disk space, a non-sparse file is better.

Copying from one tape drive to another. The following example copies from tape
drive 0 to tape drive 1, using a common historical device naming convention.\
dd if=/dev/rmt/0h of=/dev/rmt/1h
Stripping the first 10 bytes from standard input. The following example
strips the first 10 bytes from standard input:
dd ibs=10 skip=1
Reading a tape into an ASCII file. This example reads an EBCDIC tape blocked
ten 80-byte EBCDIC card images per block into the ASCII file x:
dd if=/dev/tape of=x ibs=800 cbs=80 conv=ascii,lcase
Using conv=sync to write to tape. The following example uses conv=sync when
writing to a tape:
tar cvf - . | compress | dd obs=1024k of=/dev/rmt/0 conv=sync
A typical task for dd is copying a floppy disk. As the common geometry of a
3.5" floppy is 18 sectors per track, two heads and 80 cylinders, an optimized
dd command to read a floppy is:
dd bs=2x80x18b if=/dev/fd0 of=/tmp/floppy.image
	1+0 records in
	1+0 records out
The 18b specifies 18 sectors of 512 bytes, the 2x multiplies the sector size
by the number of heads, and the 80x is for the cylinders--a total of 1474560
bytes. This issues a single 1474560-byte read request to /dev/fd0 and a single
1474560 write request to /tmp/floppy.image, whereas a corresponding cp command

cp /dev/fd0 /tmp/floppy.image
issues 360 reads and writes of 4096 bytes. While this may seem insignificant
on a 1.44MB file, when larger amounts of data are involved, reducing the
number of system calls and improving performance can be significant.

This example also shows the factor capability in the GNU dd number
specification. This has been around since before the Programmers Work Bench
and, while not documented in the GNU dd man page, is present in the source
and works just fine, thank you.

To finish copying a floppy, the original needs to be ejected, a new diskette
inserted, and another dd command issued to write to the diskette:

Example 1-b : Copying to a 3.5" floppy
	dd bs=2x80x18b < /tmp/floppy.image > /dev/fd0
	1+0 records in
	1+0 records out
Here is shown the stdin/stdout usage, in which respect dd is like most
other utilities.
The original need for dd came with the 1/2" tapes used to exchange data with
other systems and boot and install Unix on the PDP/11. Those days are gone,
but the 9-track format lives. To access the venerable 9-track, 1/2" tape,
dd is superior. With modern SCSI tape devices, blocking and unblocking are
no longer a necessity, as the hardware reads and writes 512-byte data blocks.
However, the 9-track 1/2" tape format allows for variable length blocking
and can be impossible to read with the cp command. The dd command allows for
the exact specification of input and output block sizes, and can even read
variable length block sizes, by specifying an input buffer size larger than
any of the blocks on the tape. Short blocks are read, and dd happily copies
those to the output file without complaint, simply reporting on the number
of complete and short blocks encountered.

Then there are the EBCDIC datasets transferred from such systems as MVS,
which are almost always 80-character blank-padded Hollerith Card Images! No
problem for dd, which will convert these to newline-terminated variable
record length ASCII. Making the format is just as easy and dd again is the
right tool for the job.

dd bs=10240 cbs=80 conv=ascii,unblock if=/dev/st0 of=ascii.out
40+0 records in
38+1 records out
The fixed record length is specified by the cbs=80 parameter, and the
input and output block sizes are set with bs=10240. The EBCDIC-to-ASCII
conversion and fixed-to-variable record length conversion are enabled with
the conv=ascii,noblock parameter.

Notice the output record count is smaller than the input record count. This
is due to the padding spaces eliminated from the output file and replaced
with newline characters.
Sometimes data arrives from sources in unusual formats. For example, every
time I read a tape made on an SGI machine, the bytes are swapped. The dd
command takes this in stride, swapping the bytes as required. The ability
to use dd in a pipe with rsh means that the tape device on any *nix system
is accessible, given the proper rlogin setup.

rsh sgi.with.tape dd bs=256b if=/dev/rmt0 conv=swab | tar xvf -
The dd runs on the SGI and swaps the bytes before writing to the tar command
running on the local host.
Murphy's Law was postulated long before digital computers, but it seems it
was specifically targeted for them. When you need to read a floppy or tape,
it is the only copy in the universe and you have a deadline past due, that is
when you will have a bad spot on the magnetic media, and your data will be
unreadable. To the rescue comes dd, which can read all the good data around
the bad spot and continue after the error is encountered. Sometimes this is
all that is needed to recover the important data.
dd bs=265b conv=noerror if=/dev/st0 of=/tmp/bad.tape.image
The Linux kernel Makefiles use dd to build the boot image. In the Alpha
Makefile /usr/src/linux/arch/alpha/boot/Makefile, the srmboot target issues
the command:
dd if=bootimage of=$(BOOTDEV) bs=512 seek=1 skip=1
This skips the first 512 bytes of the input bootimage file (skip=1) and writes
starting at the second sector of the $(BOOTDEV) device (seek=1). A typical
use of dd is to skip executable headers and begin writing in the middle of
a device, skipping volume and partition data. As this can cause your disk
to lose file system data, please test and use these applications with care.
Really nerdy stuff:
view filesystems dd if=/proc/filesystems | hexdump -C | less
view all loaded modules dd if=/proc/kallsyms | hexdump -C | less
view interrupt table dd if=/proc/interrupts | hexdump -C | less
view system uptime (in seconds) dd if=/proc/uptime | hexdump -C | less
view partitions and sizes in kb dd if=/proc/partitions | hexdump -C | less
view mem stats dd if=/proc/meminfo | hexdump -C | less
Post Learn the DD command by AwesomeMachine (Jan 2005) contains wealth of
interesting examples:
Linux DD
The basic command structure is as follows:

dd if=<source> of=<target> bs=<byte size>
bs "USUALLY" is some power of 2, and usually not less than 512 bytes (ie,
512, 1024, 2048, 4096, 8192, 16384, but can be any reasonable whole integer
value. skip= seek= conv=<conversion>
Source is the data being read. Target is where the data gets written.

Warning!! If you reverse the source and target, you can wipe out
a lot of data. This feature has inspired the nickname "dd" Data
Destroyer. Warning!! Caution should be observed when using dd to duplicate
encrypted partitions.

Examples:

duplicate one hard disk partition to another hard disk partition: Sda2 and
sdb2 are partitions. You want to duplicate sda2 to sdb2.

dd if=/dev/sda2 of=/dev/sdb2 bs=4096 conv=notrunc,noerror
If sdb2 doesn't exist, dd will start at the beginning of the disk, and create
it. Be careful with order of if and of. You can write a blank disk to a good
disk if you get confused. If you duplicate a smaller partition to a larger one,
using dd, the larger one will now be formatted the same as the smaller one. And
there will be no space left on the drive. The way around this is to use
rsync
, as described below.

To make an iso image of a CD: This duplicates sector for sector. MyCD.iso
will be a hard disk image file of the CD.
dd if=/dev/hdc of=/home/sam/myCD.iso bs=2048 conv=sync,notrunc
You can mount the image like this:
mkdir /mnt/myCD
mount -o loop /home/sam/myCD.iso /mnt/myCD
This will make the CD root directory the working directory, and display the
CD root directory.
cd /mnt/myCD
This will duplicate a floppy disk to hard drive image file:
dd if=/dev/fd0 of=/home/sam/floppy.image
If you're concerned about spies taking the platters out of your hard drive,
and scanning them using superconducting quantum-interference detectors,
you can always add a "for" loop for US Government DoD approved secure hard
disk erasure. Copy and paste the following two lines into a text editor.
#!/bin/bash
for n in `seq 7`; do dd if=/dev/urandom of=/dev/sda bs=8b conv=notrunc; done
Save the file as anti_scqid.
chmod +x anti_swqid
Don't run the program until you want to wipe the drive.

Best Laptop Backup: Purchase a laptop drive and an USB 2.0 drive enclosure
(Total cost $100.00USD). Assemble the lappy drive into the external
enclosure. Plug the external drive into the lappy USB port, and boot with The
Knoppix live CD. Launch a terminal. This command will backup the existing
drive:
dd if=/dev/hda of=/dev/sda bs=64k conv=notrunc,noerror
This command will restore from the USB drive to the existing drive:
dd if=/dev/sda of=/dev/hda bs=64k conv=notrunc,noerror
If the existing disk fails, you can boot from the external drive backup and
have your system back instantaneously.

This series will make a DVD backup of hard drive partition:
dd if=/dev/hda3 of=/home/sam/backup_set_1.img bs=1M count=4430
dd if=/dev/hda3 skip=4430 of=/home/sam/backup_set_2.img bs=1M count=4430
dd if=/dev/hda3 skip=8860 of=/home/sam/backup_set_3.img bs=1M count=4430
And so on. This series will burn the images to DVD+/-R/RW:
wodim -dev=/dev/hdc --driveropts=burnfree /home/sam/backup_set_1.img
and so forth. To restore the from the backup, load the DVDs in order, and
use commands like these:
 dd if=/media/dvd/backup_set_1.img of=/dev/hda3 bs=1M conv=sync,noerror
Load another DVD
dd if=/media/dvd/backup_set_2.img of=/dev/hda3 seek=4430 bs=1M
conv=sync,noerror
Load another DVD
dd if=/media/dvd/backup_set_3.img of=/dev/hda3 seek=8860 bs=1M
conv=sync,noerror
and so forth.

If you wrote chat messages and emails to another girl, on your girlfriend's
computer, you can't be sure the files you deleted are unrecoverable. But
you can make sure if anyone were to recover them, that you wouldn't get busted.
dd if=/dev/sda | sed 's/Wendy/Janet/g' | dd of=/dev/sda
Where every instance of Wendy is replaced by Janet, over every millimeter
of disk. I picked names with the same number of characters, but you can pad
a smaller name with blanks.

This command will overwrite the drive with zeroes
dd if=/dev/zero of=/dev/sda bs=4k conv=notrunc
I just want to make sure my drive is really zeroed out!!
dd if=/dev/sda | hexdump -C | grep [^00]
... will return output of every nonzero byte on the drive. Play around with
it. Sometimes drives don't completely zero out on the first try.

The following method of ouputting statistics applies to any dd command
invocation. This is an example dd command so you can try it.
/bin/dd if=/dev/zero of=/dev/null count=100MB
When you want to know how far dd has gotten throwing 100MB of 512 byte blocks
of zeroes into digital hell, open another terminal and do:
ps aux | awk '/bin\/dd/ && !/awk/ {print $2}' | xargs kill -s USR1 $1
In the terminal running the dd command you will find something like this:
33706002+0 records in
33706002+0 records out
17257473024 bytes (17 GB) copied, 34.791 s, 496 MB/s
If you enter the command again, you see more statistics:
58596452+0 records in
58596452+0 records out
30001383424 bytes (30 GB) copied, 60.664 s, 495 MB/s
Again
74473760+0 records in
74473760+0 records out
38130565120 bytes (38 GB) copied, 77.3053 s, 493 MB/s
and so on ... Until the command completes
100000000+0 records in
100000000+0 records out
51200000000 bytes (51 GB) copied, 104.193 s, 491 MB/s
How To Scan a dd Bitstream for Viruses and Malware:
 dd if=/home/sam/file.file | clamscan -
Windows users will find help in the second post, way at the bottom

FYI: duplicating smaller partition or drive to larger partition or drive;
or vice versa:
rsync -avH --exclude=/other_mount_point/ /mount_point/* /other_mount_point/
You want to duplicate the root directory tree to another drive, but the other
drive is larger. If you use dd, you will get a file system that is smaller then
the larger destination drive. To duplicate files, not the file system: Format
and mount the destination drive. Rsync will duplicate the files as files:
 rsync -avH --exclude=/mnt/destination_drive/  /* /mnt/destination_drive/
You need to run:
grub-install
update-grub
from a the rescue menu of an installation CD/DVD for the target to become
bootable. If target was previously bootable, it remains bootable.

Making a NTFS partition, is not easy without using Windows based tools. I
was formatting an external drive for my brother, who uses MS Windows XP. I
wasn't going to admit Linux couldn't make a NTFS partition.

Make an ext3 partition on the drive. Open a hex editor and make a file
containing
07
Save the file as file.bin. Change the ext3 partition to NTFS:
dd if=/home/sam/file.bin of=/dev/sdb bs=1 seek=450 count=1
Will change the partition type byte at offset
0x1c2
from Linux type:
0x83
, to NTFS type:
0x07
Please use a drive without important data on it. And, If you use a text
editor to make the binary 07 file, you will ruin the existing partition table,
because ascii 07 is two hexadecimal bytes (0x3037).

The four primary partition type byte offsets are:
0x1c2=450
0x1d2=466
0x1e2=482
0x1f2=498
If the dd seek= parameter is changed from 450 to a one of the other values,
it will change partition (hd0,1), (hd0,2), or (hd0,3) to NTFS type, rather
than partition (hd0,0).

To be revised at a later date:
To make a bootable flash drive: Download 50 MB Debian based distro here:
http://sourceforge.net/projects/insert/

Plug in the thumb drive into a USB port. Do:
dmesg | tail
Look where the new drive is, sdb1, or something similar. Do:
dd if=/home/sam/insert.iso of=/dev/sdb ibs=4b obs=1b conv=notrunc,noerror
Set the BIOS to USB boot, and boot.
End to be revised

This command will duplicate the MBR and boot sector of a floppy disk to hard
drive image:
dd if=/dev/fd0 of=/home/sam/MBRboot.image bs=512 count=2
To clone an entire hard disk. /dev/sda is the source. /dev/sdb is the target:
dd if=/dev/sda of=/dev/sdb bs=4096 conv=notrunc,noerror
Do not reverse the intended source and target. It happens once in a while,
especially to the inexperienced user. Notrunc means 'do not truncate the
output file'. Noerror means to keep going if there is an error. Dd normally
terminates on any I/O error.

Duplicate MBR, but not partition table. This will duplicate the first 446
bytes of the hard drive to a file:
dd if=/dev/sda of=/home/sam/MBR.image bs=446 count=1
If you haven't already guessed, reversing the objects of if and of, on the
dd command line, reverses the direction of the write.

To wipe a hard drive: (Boot from a live CD distro to do this.)
dd if=/dev/zero of=/dev/sda conv=notrunc
This is useful for making the drive like new. Most drives have 0x00h written
to every byte, from the factory.

To overwrite all the free disk space on a partition (deleted files you don't
want recovered):
dd if=/dev/urandom of=/home/sam/bigfile.file
When dd ouputs
no room left on device
all the free space has been overwritten with random characters. Delete the
big file with
rm bigfile.file
Sometimes one wants to look inside a binary file, looking only for clues. The
output of the command line:
less /home/sam/file.bin
is cryptic, because it's binary. For human readable output:
dd if=/home/sam/file.bin | hexdump -C | less
You may also use:
 dd if=/home/sam/file.file | strings -n 8 -t d | less
Recover deleted JPEG files. Look at the header bytes of any JPEG.
dd if=/home/sam/JPEG.jpg bs=1w count=2 | hexdump -C
The last two bytes are the footer.
dd if=JPEG.jpg | hexdump -C
Using the JPEG header and footer bytes, search the drive. Command returns
the offsets of the beginning and end of each deleted JPEG.
dd if=/dev/sda3 | hexdump -C | "grep 'ff d8 ff e0' | 'ff d9'"
If
grep
returned JPEG header bytes at offset:
0xba0002f
and footer bytes at offset:
0xbaff02a
Convert the hex offsets to decimal offsets, using one of the many logic capable
calculators for Linux. Decimal offsets corresponding to the beginning and end
of the JPEG are 195 035 183 and 196 079 658. (196 079 658) - (195 035 183)
= rough idea of proper bs= and count= parameters. To find the proper count=
figure: (<decinal offset of footer bytes> - <decimal offset of header bytes>)
/ <block size> = <number of blocks in the deleted JPEG file>. (195 035 183
 196 079 658) = (1 044 475) / (bs=4096) = (254.998). That's really close to
255. If we could land exactly at the header bytes using bs=4096, we could use
count=255. But I'm going to use count=257, because random chance dictates
the probability of landing dead on the header bytes, using 2^x block size
is remote. So we start reading before the header bytes.

We need to use skip= parameter to skip to our start point: 195 035 183 /
bs=4096 = 47 616.011. We always round down, so dd will start reading before the
beginning of the file. In this case we round down to skip=47615. The following
writes a file containing the JPEG with some unwanted bytes before and after.
dd if=/dev/sda3 skip=47615 of=/home/sam/work_file.bin count=257 bs=4096
This sequence yields the desired JPEG.
hexdump -C work_file.bin | "grep 'ff d8 ff e0' | 'ff d9'"
dd if=work_file.bin skip=<offset_of_first_header_byte_in_decimal_format>
count=<offset_of_last_footer_byte_in_decimal_format +1> -
<offset_of_first_header_byte_in_decimal_format> bs=1c of=JPG.jpg
That's the way to get your hands dirty deep in digital data. But this process
it automated in the file carving program, foremost.

The principle of file carving negates the need for Linux undelete programs. So
if your from a MS Windows world, don't google for linux undelete, but rather,
foremost NEXT ...

I put two identical drives in every one of my machines. Before I do anything
that most probably spells disaster, like an untested command line in a root
shell, that contains
find / -regex ?*.???* -type f | xargs rm -f "$1"
, I do:
dcfldd if=/dev/sda of=/dev/sdb bs=4096 conv=notrunc,noerror
and duplicate my present working /dev/sda drive system to the /dev/sdb
drive. If I wreck the installation on sda, I boot from a live CD distro,
and do:
dd if=/dev/sdb of=/dev/sda bs=4096 conv=notrunc,noerror
And I get everything back exactly the same it was before whatever daring
maneuver I was trying didn't work. You can really, really learn Linux this way,
because you can't wreck what you have an exact duplicate of. You also might
consider making the root partition separate from /home, and make /home big
enough to hold the root partition, plus more. Then, To make a backup of root:
dd if=/dev/sda2 (root) of=/home/sam/root.img bs=4096 conv=notrunc,noerror
To write the image of root back to the root partition, if you messed up and
can't launch the X server, or edited /etc/fstab, and can't figure out what
you did wrong. It only takes a few minutes to restore a 15 GB root partition
from an image file:
dd if /home/sam/root.img of=/dev/sda2 (root) bs=4096 conv=notrunc,noerror


How to make a swap file, or another swapfile on a running system:
dd if=/dev/zero of=/swapspace bs=4k count=250000
mkswap /swapspace
swapon /swapspace
This can solve out of memory issues due to memory leaks on servers that
cannot easily be rebooted.

How to pick proper block size:
dd if=/dev/zero bs=1024 count=1000000 of=/home/sam/1Gb.file
dd if=/dev/zero bs=2048 count=500000 of=/home/sam/1Gb.file
dd if=/dev/zero bs=4096 count=250000 of=/home/sam/1Gb.file
dd if=/dev/zero bs=8192 count=125000 of=/home/sam/1Gb.file
This method can also be used as a drive benchmark, to find strengths and
weaknesses in hard drives:
Read:
dd if=/home/sam/1Gb.file bs=64k | dd of=/dev/null
Write:
dd if=/dev/zero bs=1024 count=1000000 of=/home/sam/1Gb.file
When dd finishes it outputs (total size)/(total time). You get the idea.
Play with 'bs=' and 'count=', always having them multiply out to the same
toal size. You can calculate bytes/second like this: 1Gb/total seconds =
Gb/s. You can get more realistic results using a 3Gb file.

Rejuvenate a hard drive
To cure input/output errors experienced when using dd. Over time the data
on a drive, especially a drive that hasn't been used for a year or two,
grows into larger magnetic flux points than were originally recorded. It
becomes more difficult for the drive heads to decipher these magnetic flux
points. This results in I/O errors. Sometimes sector 1 goes bad, resulting
in a useless drive. Try:
dd if=/dev/sda of=/dev/sda
to rejuvenate the drive. Rewrites all the data on the drive in nice tight
magnetic patterns that can then be read properly. The procedure is safe
and economical.
Make a file of 100 random bytes:

dd if=/dev/urandom of=/home/sam/myrandom bs=100 count=1
/dev/random produces only as many random bits as the entropy pool
contains. This yields quality randomness for cryptographic keys. If more
random bytes are required, the process stops until the entropy pool is refilled
(waggling your mouse helps). /dev/urandom does not have this restriction. If
the user demands more bits than are currently in the entropy pool, it produces
them using a pseudo random number generator. Here, /dev/urandom is the Linux
random byte device. Myrandom is a file.

Randomize data over a file before deleting it:
ls -l
to find filesize.
In this case it is 3769
ls -l afile -rw------- ... 3769 Nov 2 13:41 <filename>
dd if=/dev/urandom of=afile bs=3769 count=1 conv=notrunc
duplicate a disk partition to a file on a different partition.

Warning!! Do not write a partition image file to the same partition.
dd if=/dev/sdb2 of=/home/sam/partition.image bs=4096 conv=notrunc,noerror
This will make a file that is an exact duplicate of the sdb2 partition. You
can substitue hdb, sda, hda, etc ... OR
dd if=/dev/sdb2 ibs=4096 | gzip > partition.image.gz conv=noerror
Makes a gzipped archive of the entire partition. To restore use:
 dd if=partition.image.gz | gunzip | dd of=/dev/sdb2
For bzip2 (slower,smaller), substitute bzip2 and bunzip2, and name the file
< filename >.bz2
.Restore a disk partition from an image file.
dd if=/home/sam/partition.image of=/dev/sdb2 bs=4096 conv=notrunc,noerror
Convert a file to uppercase:
dd if=filename of=filename conv=ucase
Make a ramdrive:
The Linux kernel makes a number a ramdisks you can make into ramdrives. You
have to populate the drive with zeroes like so:
dd if=/dev/zero of=/dev/ram7 bs=1k count=16384
Populates a 16 MB ramdisk.
mke2fs -m0 /dev/ram7 4096
puts a file system on the ramdisk, turning it into a ramdrive. Watch this
puppy smoke.
debian:/home/sam # hdparm -t /dev/ram7
/dev/ram7:
Timing buffered disk reads: 16 MB in  0.02 seconds = 913.92 MB/sec
You only need to do the timing once, because it's cool. Make the drive again,
because hdparm is a little hard on ramdrives. You can mount the ramdrive with:
mkdir /mnt/mem
mount /dev/ram7 /mnt/mem
Now you can use the drive like a hard drive. This is particularly superb
for working on large documents or programming. You can duplicate the large
file or programming project to the ramdrive, which on my machine is at least
27 times as fast as /dev/sda, and every time you save the huge document, or
need to do a compile, it's like your machine is running on nitromethane. The
only drawback is data security. The ramdrive is volatile. If you lose power,
or lock up, the data on the ramdrive is lost. Use a reliable machine during
clear skies if you use a ramdrive.

Duplicate ram memory to a file:
dd if=/dev/mem of=/home/sam/mem.bin bs=1024
The device
/dev/mem
is your system memory. You can actually duplicate any block or character
device to a file using dd. Memory capture on a fast system, with bs=1024
takes about 60 seconds, a 120 GB HDD about an hour, a CD to hard drive
about 10 minutes, a floppy to a hard drive about 2 minutes. With dd, your
floppy drive images will not change. If you have a bootable DOS diskette,
and you save it to your HDD as an image file, when you restore that image
to another floppy it will be bootable.

Dd will print to the terminal window if you omit the
of=/dev/output
part.
dd if=/home/sam/myfile
will print the file myfile to the terminal window.

To search the system memory:
dd if=/dev/mem | strings | grep
'some-string-of-words-in-the-file-you-forgot-to-save-before-the-power-failed'
If you need to cover your tracks quickly, put the following commands in a
script to overwrite system ram with zeroes. Don't try this for fun.
mkdir /mnt/mem
mount -t ramfs /dev/mem /mnt/mem
dd if=/dev/zero > /mnt/mem/bigfile.file
This will overwrite all unprotected memory structures with zeroes, and freeze
the machine so you have to reboot (Caution, this also prevents committment
of the file system journal, and could trash the file system).

You can get arrested in 17 states for doing this next thing. Make an AES
encrypted loop device:
dd if=/dev/urandom of=/home/sam/aes-drv bs=16065b count=100
modprobe loop
modprobe cryptoloop
modprobe aes
losetup -e aes /dev/loop1 ./aes-drv
password:
mkreiserfs /dev/loop1
mkdir /aes
mount -o loop,encryption=aes,acl ./aes-drv /aes
password:
mv /home/sam/porno /aes
to get the porno on the aes drive image.
umount /aes
losetup -d /dev/loop1
rmmod aes
rmmod cryptoloop
rmmod loop
to make 'aes-drv' look like a 400 MB file of random bytes. Every time the
lo interface is configured using losetup, according to the above, and the
file 'aes-drv' is mounted, as above, the porno stash will be accessible in
/aes/porno. You don't need to repeat the dd command, OR, the format with
reiserfs, OR, the mv command. You only do those steps once. If you forget the
password, there is no way to recover it besides guessing. Once the password
is set, it can't be changed. To change the password, make a new file with the
desired password, and move everything from the old file to the new file. Acl
is a good mount option, because it allows use of acls. Otherwise your stuck
with u,g,o and rwx.

If you are curious about what might be on you disk drive, or what an MBR
looks like, or maybe what is at the very end of your disk:
dd if=/dev/sda count=1 | hexdump -C
Will show you sector 1, or the MBR. The bootstrap code and partition table
are in the MBR.
To see the end of the disk you have to know the total number of sectors,
and the MAS must be set equal to the MNA. The helix CD has a utility to set
this correctly. In the dd command, your skip value will be one less than
MNA of the disk. For a 120 GB Seagate SATA drives
dd if=/dev/sda of=home/sam/myfile skip=234441646 bs=512
,
So this reads sector for sector, and writes the last sector to myfile. Even
with LBA addressing, disks still secretly are read in sectors, cylinders,
and heads.
There are 63 sectors per track, and 255
heads per cylinder. There is a total cylinder count.
512_bytes/sector*63_sectors/track*255heads=16065*512bytes/cylinder=8,225,280_bytes/cylinder.
63_sectors/track*255_heads=sectors/cylinder. With 234441647 total sectors,
and 16065 sectors per cylinder, you get some trailing sectors which do not
make up an entire cylinder: 14593.317584812_cylinders/drive. This leaves
5102 sectors which cannot be partitioned, because to be in a partition
you have to be a whole cylinder. It's like having part of a person. That
doesn't really count as a person. These become surplus sectors after the
last partition. You can't ordinarily read past the last partition. But dd
can. It's a good idea to check for anything writing to surplus sectors. For
our Seagate 120 GB drive, 234,441,647_sectors/drive - 5102_surplus_sectors =
234,436,545 partitionable sectors.
dd if=/dev/sda of=/home/sam/myfile skip=234436545
writes the last 5102 sectors to myfile. Launch midnight commander (mc)
to view the file. If there is something in there, you do not need it for
anything. In this case you would write over it with random characters:
dd if=/dev/urandom of=/dev/sda bs=512 seek=234436545
Will overwrite the 5102 surplus sectors on our 120 GB Seagate drive.

Block size:
One cylinder in LBA mode =
255_heads*63_sectors/track=16065_sectors=16065*512_bytes=8,225,280_bytes. The
b means '* 512'. 32130b represents a two cylinder block size. Cylinder block
size always works to cover every sector in a partition, because partitions
are made of a whole number of cylinders. One cylinder is 8,225,280 bytes. If
you want to check out some random area of the disk:
dd if=/dev/sda of=/home/sam/myfile bs=4096 skip=2000 count=1000
Will give you 8,000 sectors in myfile, after the first 16,000 sectors. You
can open that file with a hex editor, edit some of it, and write the edited
part back to disk:
dd if=/home/sam/myfile of=/dev/sda bs=4096 seek=2000 count=1000
Image a partition to another machine:
On source machine:
dd if=/dev/hda bs=16065b | netcat < targethost-IP > 1234
On target machine:
netcat -l -p 1234 | dd of=/dev/hdc bs=16065b
Variations on target machine:
netcat -l -p 1234 | bzip2 > partition.img
makes a compressed image file using bzip2 compression.
netcat -l -p 1234 | gzip > partition.img
makes a compressed image file using gzip compression. I back up a 100 GB lappy
disk on a desktop drive, over a lan connection, and the 100 GB compresses to
about 4.0 GB. Most of the drive is empty, so it's mostly zeroes. Repetitive
zeroes compress well.
Alert!! Don't hit enter yet. Hit enter on the target machine. THEN hit enter
on the source machine.

Netcat is a program, available by default, on most linux installations. It's
a networking swiss army knife. In the preceding example, netcat and dd
are piped to one another. One of the functions of the linux kernel is
to make pipes. The pipe character looks like two little lines on top of
one another, both vertical. Here is how this command behaves: This byte
size is a cylinder. bs=16065b equals one cylinder on an LBA drive. The dd
command is piped to netcat, which takes as its arguments the IP address of
the target(like 192.168.0.1, or any IP address with an open port) and what
port you want to use (1234).

You can also use ssh.
dd if=/dev/sdb2 | ssh sam@192.168.0.121 "sudo dd of=/home/sam/sdb2.img"
NOTES
Do not use dd to copy files between file systems having different block sizes.

Using a blocked device to copy a file will result in extra nulls being added
to the file to pad the final block to the block boundary.

When dd reads from a pipe, using the ibs=X and obs=Y operands, the output
will always be blocked in chunks of size Y. When bs=Z is used, the output
blocks will be whatever was available to be read from the pipe at the time.

When using dd to copy files to a tape device, the file size must be a multiple
of the device sector size (for example, 512 Kbyte). To copy files of arbitrary
size to a tape device, use tar(1) or cpio(1).

For SIGINT, dd writes status information to standard error before exiting. It
takes the standard action for all other signals.

Top updates

Bulletin	 Latest  Past week	 Past month
Google Search

Old News ;-)

[Jan 05, 2012] Scalpel: A Frugal, High Performance File Carver

digitalforensicssolutions

Scalpel is a fast file carver that reads a database of header and footer
definitions and extracts matching files or data fragments from a set of
image files or raw device files. Scalpel is filesystem-independent and will
carve files from FATx, NTFS, ext2/3, HFS+, or raw partitions. It is useful
for both digital forensics investigation and file recovery.

Notes on Platforms

Linux

The preferred platform for using Scalpel is Linux.

Windows

Scalpel will also compile under Windows (32 or 64-bit) using mingw. If you'd
like to try Scalpel on Windows without the bother of compiling it yourself,
an executable and appropriate libraries are included in the distribution--just
untar and go. Note that under Windows, the pthreads DLL must be present in
the same directory as the Scalpel executable. Carving physical and logical
devices directly under Windows (e.g., using \\.\physicaldrive0 as a target)
is not supported in the current release.

Mac OS X

As of v1.53, Scalpel is supported on Mac OS X.

All platforms

As of v1.54, Scalpel supports carving files larger than 4GB on all platforms.

As of v1.60, Scalpel supports preview carving and other new carving modes. See
the distribution for details.

As for v2.0, Scalpel supports regular expressions for headers and footers,
minimum carve sizes, multithreading and asynchronous I/O, and beta-level
support for GPU-accelerated file carving.

[Jan 19, 2011] Image software

LinkedIn

Leon Waldman

I always bet on dd + netcat to do imaging over network.

On the system where you will store the image (192.168.0.10):
netcat l p 7000 > file.iso

On the system to be cloned:
dd if=/dev/sda | netcat 192.168.0.10 7000 -q
where: dd if=volume_to_be_cloned| netcat ip_of_destination_ip port -q

You can even do it to full clone the system instead store it.

Give a look here:
http://digiassn.blogspot.com/2006/01/dd-over-netcat-for-cheap-ghost.html

[Mar 9, 2010] Copying Windows to a new drive, using linux - How-to-Guide by
Ed Anderson

20 Mar 2005 | nilbus.com

Updated 18 May 2009 (NTFS Updates; device names)

This guide will show you how to copy an existing installation of Windows
(or any other OS) from one drive to another - as long as the destination
drive is the same size or larger.

This is a free and relatively easy method that will create a clone of your
current hard disk, without having to buy any third party software.

Gathering tools
Physical Installation
Preparing new partition table
Copy the MBR
Copy the Partition
Resizing the Partition
FAQ
[Feb 11, 2009] Create a file with given size - Linux dd command

Feb 11, 2009 |unstableme.blogspot.com

"The command is:

dd if=/dev/zero of=testfile_10MB bs=10485760 count=1

	"1+0 records in
	1+0 records out

	10485760 bytes (10 MB) copied, 0.312 s, 33.6 MB/s"
[Oct 3, 2008] Red Hat Magazine This isnt your grandpappys dd command

Looks like dd was slightly faster with 128K blocks. Python program might be
reused for other purposes

Oct 02, 2008 | redhatmagazine.com

Block Size: 128 Throughput: 62.8 MB/s

Block Size: 256 Throughput: 61.8 MB/s

Block Size: 512 Throughput: 57.1 MB/s

Block Size: 1024 Throughput: 56.5 MB/s

We benchmarked the throughput of the disk by running the dd command with
various block sizes from 128 KB to 1 MB. (Note: If you want to run the script
on your own machine, make sure that the volume you use doesnt contain any
valuable data, because the data will be erased by the dd command. Remember,
data loss makes grandpappy mad!)
For the benchmark, we wrote a Python script that uses the commands module
to run and capture the output of the dd command. The script also uses the
csv module to generate a comma-separated values file so that we can graph
the results later. For this example, we chose to graph the results using
the Google Chart API.

dd_chart on Flickr - Photo Sharing!

[Sep 12, 2008] Tips For Linux - How and when to use the dd command

[Oct 1, 2004 ] POWER TOOLS Performing Data Surgery

Data Dumping with dd

October 1, 2004 | Linux Magazine

dd  does low-level data transfer, byte-by-byte or block-by-block, with
adjustable block sizes. It can also skip specified numbers of blocks in the
input and/or output files, as well as converting data formats. All of those
are handy for working with magnetic tape and disks. But it's also useful
for many types of data transfers.

By default, dd reads the standard input and writes to the standard
output. Input and output filenames, and other options too, are given in an
unusual syntax without leading dash ( - ) characters.

For instance, to read a floppy disk and write its image to a file, you
could type:

$ dd if=/dev/fd0 of=dosboot.img
2880+0 records in
2880+0 records out
$ ls -l dosboot.img
-rw-rw-r- ... 1474560 Nov 2 12:59 dosboot.img
The dd command line says, "Reading from the input file /dev/fd0, write
all of the data to the file dosboot.img." dd doesn't try to find lines of
data or individual files on the disk; it does a binary copy of the bytes
from first to last. dd always tells you (on the standard error) how many
times it read and wrote data. Above, it read 2,880 512-byte blocks. If
you don't want to see this information -- or any error messages, either --
you can redirect dd's standard error to the Linux "bit bucket," /dev/null,
by adding the Bourne shell operator 2>/dev/ null to the command line.

It's more efficient to specify a larger block size so the device drivers
do a single read and write. There are lots of other options, and many of
them start with conv= , like conv=unblock to replace trailing spaces in a
block with a newline, and conv= swap to swap pairs of input bytes (which
is needed with some tapes written on other types of hardware). But we'll
leave that sort of optimization to you and the dd man page. Let's look at
some less-obvious uses of this handy utility.

Stupid dd Tricks

Need a file with 100 arbitrary bytes -- for testing, for instance? The
Linux device /dev/urandom (available since Linux 1.3.30) can supply as many
pseudo-random bytes as you can read from it. To get just 100 bytes, set a
block size of 1 byte with bs=1 and tell dd to stop after copying 100 "blocks"
(here, that's 100 bytes):

$ dd if=/dev/urandom of=myrand bs=1 count=100
What's in that myrand file? The od utility can show you. (See the sidebar
"What's In That File?")

If you need more-random data, try /dev/random instead. Reading data
from /dev/random can take some time, though, as the random(4) man page
explains. When you read from /dev /random, set a block size of 1.

Another use for dd is for "wiping" a text file before you delete it. Simply
removing a Linux file (with rm, for instance) only deletes the inode that
points to the data. A cracker with root access might read the raw disk (with
dd!) and find the "deleted" file. We can use dd to write random data over
the file before deleting it. Normally dd truncates a file before writing,
so use conv=notrunc to make it write over the existing data. Set bs to the
file size and count to 1 . For example:

% ls -l afile
-rw------- ... 3769 Nov  2 13:41 afile
% dd if=/dev/urandom of=afile \
  bs=3769 count=1 conv=notrunc
1+0 records in
1+0 records out
% rm afile
If you want to, you can repeat the "wiping" command several times with the
C shell repeat command, the Z shell repeat loop, or simply use the history
operator !!

[Oct 1, 2004 ] Moving your data to a backup device

faqs.org

The dd command can be used to put data on a disk, or get it off again,
depending on the given input and output devices. An example:

gaby:~>dd if=images-without-dir.tar.gz of=/dev/fd0H1440
98+1 records in
98+1 records out

gaby~>dd if=/dev/fd0H1440 of=/var/tmp/images.tar.gz
2880+0 records in
2880+0 records out

gaby:~>ls /var/tmp/images*
/var/tmp/images.tar.gz
Note that the dumping is done on an unmounted device. Floppies created using
this method will not be mountable in the file system, but it is of course
the way to go for creating boot or rescue disks. For more information on
the possibilities of dd, read the man pages.

This tool is part of the GNU fileutils package.

[Feb 13, 1998] SUMMARY How to build a Solaris bootable CDROM by James
Hutchinson

sunmanagers.org

One would think this would be a simple task. Half the commerical packages
proclaim that they can do it. But alas, for me, none of them worked.

The task was to create a system installer that would boot a sun, load
up the appropriate system images and tools required for a server in our
environment. Yes, this is a perfect job for jumpstart, but the systems would
be distributed across the state and not connected via a fast enough pipe to
use jumpstart effectivly. The other option was to build external disks with
the proper images on them and boot from there. The cost of producing a few
hundrad of these was prohibitive, thus the CDROM approach was taken.

Of the few responses I received from Sun Managers readers, all of them
basicly said 'Its easy to clone the install disc!' or 'it just cant be done!'

Well. It is possible.

First, a breakdown of the process. When the command > boot cdrom < is givin to
openboot the system actually looks at slice 2 to 5 for its bootblks depending
on the machine type. A sun4c is slice 2 to an sun4u at slice 5. These bootblks
redirect the system to slice 1 to load its kernel. The root partition is
also stored in slice 1 and slice 0 is usr as well as packages. All slices
save 0 are ufs, and 0 is hsfs.

This brings us to a small problem point. First we must boot from a UFS
partition, as the bootblks all require that. Second, a CDROM has no label
by default, thus its kinda hard to make partitions. And third, even if we
do flush a UFS filesystem image off to the cdrom, the geometries will be
all wrong, unless your staging disk just happens to have the same
geometries as the CDROM.

My solution is probally not the best. What would be required is an application
that simply converts the UFS geometries from the staging disk uses to those
used by the cdrom. I didnt really have time to create this so I did it as
follows. Im sure I will get lots of flak for this
solution, but it does work.

1) Use dd to grab the first cylinder off the solaris boot cdrom. This
contains a valid disk label and VTOC for the cdrom. Once this is created,
our limitation is that we must work within the defines of this VTOC. You
should be able to use prtvtoc on the cdrom to get a look at this VTOC,
but this dosnt work if volmgr is running.

dd if=/dev/dsk/c0t6d0s0 of=cdrom.vtoc bs=512 count=1

2) Now use dd to grab the UFS slices from 1 to 5

for slice in 1 2 3 4 5
do
dd if=/dev/dsk/c0t6d0${slice} of=cdrom.s${slice}
done

3) Create a staging area and copy the parts of the usr filesystem (slice 0)
off the cdrom into it. I started by copying cdrom:/export to it and then
trimmed out the parts I didnt need like X and openwindows.

4) Add in all the things you need for your disc. For me, this ment a shell
script that automated the build process, and images of all the data I wanted
to move out. Make sure you donot go beyond te size of the slice 0 on the cdrom
you started with. prtvtoc will should you this, if you cant get prtvtoc to
work on the cdrom (sometimes it does, sometimes it dosnt) then use

> dd if=/dev/dsk/c0t6d0s0 of=/dev/null bs=512 <

Recordthe exact size of the partition in blocks as you will need it later.

5) Patch the slice 1 image (cdrom.s1) to start your custom application rather
then the suninstall. This can be done by finding the break point you wish
to use in the file cdrom:/sbin/sysconfig, selecting a unique set of chars
in this file ( I used the string #***** S30sysid.net ) and then searching
via a hexeditor or emacs in bin mode for that string.

Then find a comment line, change the first # and chars after to point to
your script, then add a # line after. Example, I patched my image so the line

#***** S30sysid.net

became

exec build #ysid.net

I then put a script in my staging area into the /usr/bin dir. (the staging
area will be made into slice 0 which is hsfs)

6) Using mkisofs or the tools that came with your cdrom burning package (HyCD
worked well, as well as Gear) turn your staging area into a hsfs filesystem
image, making sure that symbloic links are unmodified. HyCD required changing
a default option that would have modified all the links.

7) Using dd, throw away the first block of this image.

dd if=image of=image.data bs=512 skip=1

8) Subtract the block count of slice 0 from the solaris cdrom from the image
size above, add one to the answer. Say dd reported for the above step that
your image was 500000 blocks, and your solaris cdrom has a size of 787840
for slice 0

787840 - ( 500000 + 1 ) = 287839

9) Feed this number into dd reading from /dev/zero to build a pad file.

dd if=/dev/zero of=image.pad bs=512 count=287839

10) Cat all of the image files together with the VTOC and the UFS slices.

cat cdrom.vtoc > image
cat image.data >> image
cat image.pad >> image
cat cdrom.s1 >> image
..
cat cdrom.s5 >> image

11) Burn this image to the cdrom drive using cdrecord, HyCD, Gear, etc

12) put it in a machine and test boot it and make sure it does what you need.

Thats about it. We have to go through the gyrations because UFS is geometry
sensitive. We cannot take a image of a hard disk built UFS as all the cylinder
groups will be off. The RIGHT way of doing this would be to build a tool
that did the conversion for you and built an image up for burning. But that
requires a bit more work, this gets the job done with a
minimal amount of strain. I am working on the above mentioned tool, but so
far my progress has been to create lots of coasters. Sun has a tool called
MakeDisc that does this job, or something similar, but I do not have a copy
of it, so had to develop a method, while under a big gun, to do it in a very
compressed amount of time. The only real limitation this
has is you can only store about 400 megs of information, of which around 70
or so are needed by usr in the hsfs partition. Plus you do not have to go
through the pain of figuring out which parts of the system need to be moved
to the memfs filesystem (cdrom is readonly, so dev, devices, etc need to be
linked to /tmp)

If you wish to flame the procedure and tell me that I did it really stupid,
please correct me! I would willing stop development of my tools to do same
and do it the right way :) But my original question on how to do this went
unanswered.

Have Fun
James
filename: learn-the-dd-command_RWRP.txt
http://www.linuxquestions.org/questions/linux-newbie-8/learn-the-dd-command-362506/

Learn The DD Command Revised

This post contains comprehensive documentation with examples for one of the
most useful Linux/UNIX/Windows commands: dd. Dd is a bit-stream duplicator. If
you have questions, post them. The latest addition, How To Encrypt an 8.0
GB SDHC MicroSD Card was on 06-19-2011.

First Time visitors please reply.

How To Encrypt an 8.0 GB SDHC MicroSD Card
Put the card into an USB adapter. Such devices are not perfect. One might
have to push the MicroSD card into the reader as far as it will go, and others
might have to pull it back a millimeter or two. If the kernel does not detect
a partition on a new card, it's detecting the USB adapter only. Adjust the card
slighty, and replace the adapter if necessary. Should show some new device(s):
Code:
ls /dev/sd*
/dev/sdb /dev/sdb1
Write random data to the drive:
Code:
dd if=/dev/urandom of=/dev/sdb bs=4k
/dev/sdb is only an example.
Code:
apt-get install cryptsetup
Learn to use parted, or I quit! Partition the card:
Code:
parted
Encrypt the partition with a good passphrase, one that's easy to remember,
but hard to guess. DO NOT use the standard example, because everyone knows
it: Deep inside, she knows she cannot attain masculinity, but she can attain
masculinity deep inside!
Code:
cryptsetup --verbose --verify-passphrase luksFormat /dev/sdb1
Open the encrypted device:
Code:
cryptsetup luksOpen /dev/sdb1 vol_1
Create a filesystem:
Code:
mkfs.xfs -imaxpct=3 /dev/mapper/vol_1
Mount:
Code:
mkdir /AES_Drive
&& mount /dev/mapper/vol_1 /AES_Drive
Umount:
Code:
umount /AES_Drive && cryptsetup luksClose /dev/mapper/vol_1
Just a footnote, Laptops that went to sleep with the encrypted volume open,
may wake up with it open!

Linux DD
The basic command structure is as follows:
Code:
dd if=<source> of=<target> bs=<byte size> ("USUALLY" some power of 2, and
usually not less than 512 bytes (ie, 512, 1024, 2048, 4096, 8192, 16384,
but can be any reasonable whole integer value.) skip= seek= conv=<conversion>
Source is the data being read. Target is where the data gets written.

Warning!! If you reverse the source and target, you can wipe out
a lot of data. This feature has inspired the nickname "dd" Data
Destroyer. Warning!! Caution should be observed when using dd to duplicate
encrypted partitions.

Examples: duplicate one hard disk partition to another hard disk partition:
Sda2 and sdb2 are partitions. You want to duplicate sda2 to sdb2.
Code:
dd if=/dev/sda2 of=/dev/sdb2 bs=4096 conv=notrunc,noerror
If sdb2 doesn't exist, dd will start at the beginning of the disk, and create
it. Be careful with order of if and of. You can write a blank disk to a good
disk if you get confused. If you duplicate a smaller partition to a larger one,
using dd, the larger one will now be formatted the same as the smaller one. And
there will be no space left on the drive. The way around this is to use
Code:
rsync
, as described below.

To make an iso image of a CD: This duplicates sector for sector. MyCD.iso
will be a hard disk image file of the CD.
Code:
dd if=/dev/hdc of=/home/sam/myCD.iso bs=2048 conv=sync,notrunc
You can mount the image like this:
Code:
mkdir /mnt/myCD
mount -o loop /home/sam/myCD.iso /mnt/myCD
This will make the CD root directory the working directory, and display the
CD root directory.
Code:
cd /mnt/myCD
This will duplicate a floppy disk to hard drive image file:
Code:
dd if=/dev/fd0 of=/home/sam/floppy.imageIf you're concerned about spies
taking the platters out of your hard drive, and scanning them using
superconducting quantum-interference detectors, you can always add a "for"
loop for US Government DoD approved secure hard disk erasure. Copy and paste
the following two lines into a text editor.
Code:
#!/bin/bash
for n in `seq 7`; do dd if=/dev/urandom of=/dev/sda bs=8b conv=notrunc; done
Save the file as anti_scqid.
Code:
chmod +x anti_swqid
Don't run the program until you want to wipe the drive.

Best Laptop Backup: Purchase a laptop drive and an USB 2.0 drive enclosure
(Total cost $100.00USD). Assemble the lappy drive into the external
enclosure. Plug the external drive into the lappy USB port, and boot with The
Knoppix live CD. Launch a terminal. This command will backup the existing
drive:
Code:
dd if=/dev/hda of=/dev/sda bs=64k conv=notrunc,noerror
This command will restore from the USB drive to the existing drive:
Code:
dd if=/dev/sda of=/dev/hda bs=64k conv=notrunc,noerror
If the existing disk fails, you can boot from the external drive backup and
have your system back instantaneously.

This series will make a DVD backup of hard drive partition:
Code:
dd if=/dev/hda3 of=/home/sam/backup_set_1.img bs=1M count=4430
dd if=/dev/hda3 skip=4430 of=/home/sam/backup_set_2.img bs=1M count=4430
dd if=/dev/hda3 skip=8860 of=/home/sam/backup_set_3.img bs=1M count=4430
And so on. This series will burn the images to DVD+/-R/RW:
Code:
wodim -dev=/dev/hdc --driveropts=burnfree /home/sam/backup_set_1.img
and so forth. To restore the from the backup, load the DVDs in order, and
use commands like these:
Code:
 dd if=/media/dvd/backup_set_1.img of=/dev/hda3 bs=1M conv=sync,noerror
Load another DVD
Code:
dd if=/media/dvd/backup_set_2.img of=/dev/hda3 seek=4430 bs=1M
conv=sync,noerror
Load another DVD
Code:
dd if=/media/dvd/backup_set_3.img of=/dev/hda3 seek=8860 bs=1M
conv=sync,noerror
and so forth.

If you wrote chat messages and emails to another girl, on your girlfriend's
computer, you can't be sure the files you deleted are unrecoverable. But
you can make sure if anyone were to recover them, that you wouldn't get busted.
Code:
dd if=/dev/sda | sed 's/Wendy/Janet/g' | dd of=/dev/sda
Where every instance of Wendy is replaced by Janet, over every millimeter
of disk. I picked names with the same number of characters, but you can pad
a smaller name with blanks.

This command will overwrite the drive with zeroes
Code:
dd if=/dev/zero of=/dev/sda bs=4k conv=notrunc
I just want to make sure my drive is really zeroed out!!
Code:
dd if=/dev/sda | hexdump -C | grep [^00]
... will return output of every nonzero byte on the drive. Play around with
it. Sometimes drives don't completely zero out on the first try.

The following method of ouputting statistics applies to any dd command
invocation. This is an example dd command so you can try it.
Code:
/bin/dd if=/dev/zero of=/dev/null count=100MB
When you want to know how far dd has gotten throwing 100MB of 512 byte blocks
of zeroes into digital hell, open another terminal and do:
Code:
ps aux | awk '/bin\/dd/ && !/awk/ {print $2}' | xargs kill -s USR1 $1
In the terminal running the dd command you will find something like this:
Code:
33706002+0 records in
33706002+0 records out
17257473024 bytes (17 GB) copied, 34.791 s, 496 MB/s
If you enter the command again, you see more statistics:
Code:
58596452+0 records in
58596452+0 records out
30001383424 bytes (30 GB) copied, 60.664 s, 495 MB/s
Again
Code:
74473760+0 records in
74473760+0 records out
38130565120 bytes (38 GB) copied, 77.3053 s, 493 MB/s
and so on ... Until the command completes
Code:
100000000+0 records in
100000000+0 records out
51200000000 bytes (51 GB) copied, 104.193 s, 491 MB/s
How To Scan a dd Bitstream for Viruses and Malware:
Code:
 dd if=/home/sam/file.file | clamscan -
Windows users will find help in the second post, way at the bottom

FYI: duplicating smaller partition or drive to larger partition or drive;
or vice versa:
Code:
rsync -avH --exclude=/other_mount_point/ /mount_point/* /other_mount_point/
You want to duplicate the root directory tree to another drive, but the other
drive is larger. If you use dd, you will get a file system that is smaller then
the larger destination drive. To duplicate files, not the file system: Format
and mount the destination drive. Rsync will duplicate the files as files:
Code:
 rsync -avH --exclude=/mnt/destination_drive/  /* /mnt/destination_drive/
You need to run:
Code:
grub-install
update-grub
from a the rescue menu of an installation CD/DVD for the target to become
bootable. If target was previously bootable, it remains bootable.

Making a NTFS partition, is not easy without using Windows based tools. I
was formatting an external drive for my brother, who uses MS Windows XP. I
wasn't going to admit Linux couldn't make a NTFS partition.

Make an ext3 partition on the drive. Open a hex editor and make a file
containing
Code:
07
Save the file as file.bin. Change the ext3 partition to NTFS:
Code:
dd if=/home/sam/file.bin of=/dev/sdb bs=1 seek=450 count=1
Will change the partition type byte at offset
Code:
0x1c2
from Linux type:
Code:
0x83
, to NTFS type:
Code:
0x07
Please use a drive without important data on it. And, If you use a text
editor to make the binary 07 file, you will ruin the existing partition table,
because ascii 07 is two hexadecimal bytes (0x3037).

The four primary partition type byte offsets are:
Code:
0x1c2=450
0x1d2=466
0x1e2=482
0x1f2=498
If the dd seek= parameter is changed from 450 to a one of the other values,
it will change partition (hd0,1), (hd0,2), or (hd0,3) to NTFS type, rather
than partition (hd0,0).

To be revised at a later date:
To make a bootable flash drive: Download 50 MB Debian based distro here:
http://sourceforge.net/projects/insert/

Plug in the thumb drive into a USB port. Do:
Code:
dmesg | tail
Look where the new drive is, sdb1, or something similar. Do:
Code:
dd if=/home/sam/insert.iso of=/dev/sdb ibs=4b obs=1b conv=notrunc,noerror
Set the BIOS to USB boot, and boot.
End to be revised

This command will duplicate the MBR and boot sector of a floppy disk to hard
drive image:
Code:
dd if=/dev/fd0 of=/home/sam/MBRboot.image bs=512 count=2
To clone an entire hard disk. /dev/sda is the source. /dev/sdb is the target:
Code:
dd if=/dev/sda of=/dev/sdb bs=4096 conv=notrunc,noerror
Do not reverse the intended source and target. It happens once in a while,
especially to the inexperienced user. Notrunc means 'do not truncate the
output file'. Noerror means to keep going if there is an error. Dd normally
terminates on any I/O error.

Duplicate MBR, but not partition table. This will duplicate the first 446
bytes of the hard drive to a file:
Code:
dd if=/dev/sda of=/home/sam/MBR.image bs=446 count=1
If you haven't already guessed, reversing the objects of if and of, on the
dd command line, reverses the direction of the write.

To wipe a hard drive: (Boot from a live CD distro to do this.)
Code:
dd if=/dev/zero of=/dev/sda conv=notrunc
This is useful for making the drive like new. Most drives have 0x00h written
to every byte, from the factory.

To overwrite all the free disk space on a partition (deleted files you don't
want recovered):
Code:
dd if=/dev/urandom of=/home/sam/bigfile.file When dd ouputs
Code:
no room left on device
all the free space has been overwritten with random characters. Delete the
big file with
Code:
rm bigfile.file.

Sometimes one wants to look inside a binary file, looking only for clues. The
output of the command line:
Code:
less /home/sam/file.bin is cryptic, because it's binary. For human readable
output:
Code:
dd if=/home/sam/file.bin | hexdump -C | less
You may also use:
Code:
 dd if=/home/sam/file.file | strings -n 8 -t d | less
Recover deleted JPEG files. Look at the header bytes of any JPEG.
Code:
dd if=/home/sam/JPEG.jpg bs=1w count=2 | hexdump -C
The last two bytes are the footer.
Code:
dd if=JPEG.jpg | hexdump -C
Using the JPEG header and footer bytes, search the drive. Command returns
the offsets of the beginning and end of each deleted JPEG.
Code:
dd if=/dev/sda3 | hexdump -C | "grep 'ff d8 ff e0' | 'ff d9'"
If
Code:
grep
returned JPEG header bytes at offset:
Code:
0xba0002f
and footer bytes at offset:
Code:
0xbaff02a
Convert the hex offsets to decimal offsets, using one of the many logic capable
calculators for Linux. Decimal offsets corresponding to the beginning and end
of the JPEG are 195 035 183 and 196 079 658. (196 079 658) - (195 035 183)
= rough idea of proper bs= and count= parameters. To find the proper count=
figure: (<decinal offset of footer bytes> - <decimal offset of header bytes>)
/ <block size> = <number of blocks in the deleted JPEG file>. (195 035 183
 196 079 658) = (1 044 475) / (bs=4096) = (254.998). That's really close to
255. If we could land exactly at the header bytes using bs=4096, we could use
count=255. But I'm going to use count=257, because random chance dictates
the probability of landing dead on the header bytes, using 2^x block size
is remote. So we start reading before the header bytes.

We need to use skip= parameter to skip to our start point: 195 035 183 /
bs=4096 = 47 616.011. We always round down, so dd will start reading before the
beginning of the file. In this case we round down to skip=47615. The following
writes a file containing the JPEG with some unwanted bytes before and after.
Code:
dd if=/dev/sda3 skip=47615 of=/home/sam/work_file.bin count=257 bs=4096
This sequence yields the desired JPEG.
Code:
hexdump -C work_file.bin | "grep 'ff d8 ff e0' | 'ff d9'"
dd if=work_file.bin skip=<offset_of_first_header_byte_in_decimal_format>
count=<offset_of_last_footer_byte_in_decimal_format +1> -
<offset_of_first_header_byte_in_decimal_format> bs=1c of=JPG.jpgThat's
the way to get your hands dirty deep in digital data. But this process it
automated in the file carving program, foremost.

The principle of file carving negates the need for Linux undelete programs. So
if your from a MS Windows world, don't google for linux undelete, but rather,
foremost NEXT ...

I put two identical drives in every one of my machines. Before I do anything
that most probably spells disaster, like an untested command line in a root
shell, that contains
Code:
find / -regex ?*.???* -type f | xargs rm -f "$1"
, I do:
Code:
dcfldd if=/dev/sda of=/dev/sdb bs=4096 conv=notrunc,noerror
and duplicate my present working /dev/sda drive system to the /dev/sdb
drive. If I wreck the installation on sda, I boot from a live CD distro,
and do:
Code:
dd if=/dev/sdb of=/dev/sda bs=4096 conv=notrunc,noerror
And I get everything back exactly the same it was before whatever daring
maneuver I was trying didn't work. You can really, really learn Linux this way,
because you can't wreck what you have an exact duplicate of. You also might
consider making the root partition separate from /home, and make /home big
enough to hold the root partition, plus more. Then, To make a backup of root:
Code:
dd if=/dev/sda2 (root) of=/home/sam/root.img bs=4096 conv=notrunc,noerror
To write the image of root back to the root partition, if you messed up and
can't launch the X server, or edited /etc/fstab, and can't figure out what
you did wrong. It only takes a few minutes to restore a 15 GB root partition
from an image file:
Code:
dd if /home/sam/root.img of=/dev/sda2 (root) bs=4096 conv=notrunc,noerror


How to make a swap file, or another swapfile on a running system:
Code:
dd if=/dev/zero of=/swapspace bs=4k count=250000
mkswap /swapspace
swapon /swapspace
This can solve out of memory issues due to memory leaks on servers that
cannot easily be rebooted.

How to pick proper block size:

Code:
dd if=/dev/zero bs=1024 count=1000000 of=/home/sam/1Gb.file
dd if=/dev/zero bs=2048 count=500000 of=/home/sam/1Gb.file
dd if=/dev/zero bs=4096 count=250000 of=/home/sam/1Gb.file
dd if=/dev/zero bs=8192 count=125000 of=/home/sam/1Gb.file
This method can also be used as a drive benchmark, to find strengths and
weaknesses in hard drives:
Read:
Code:
dd if=/home/sam/1Gb.file bs=64k | dd of=/dev/null
Write:
Code:
dd if=/dev/zero bs=1024 count=1000000 of=/home/sam/1Gb.file
When dd finishes it outputs (total size)/(total time). You get the idea.
Play with 'bs=' and 'count=', always having them multiply out to the same
toal size. You can calculate bytes/second like this: 1Gb/total seconds =
Gb/s. You can get more realistic results using a 3Gb file.

Rejuvenate a hard drive
To cure input/output errors experienced when using dd. Over time the data
on a drive, especially a drive that hasn't been used for a year or two,
grows into larger magnetic flux points than were originally recorded. It
becomes more difficult for the drive heads to decipher these magnetic flux
points. This results in I/O errors. Sometimes sector 1 goes bad, resulting
in a useless drive. Try:
Code:
dd if=/dev/sda of=/dev/sda
to rejuvenate the drive. Rewrites all the data on the drive in nice tight
magnetic patterns that can then be read properly. The procedure is safe
and economical.

Make a file of 100 random bytes:
Code:
dd if=/dev/urandom of=/home/sam/myrandom bs=100 count=1
/dev/random produces only as many random bits as the entropy pool
contains. This yields quality randomness for cryptographic keys. If more
random bytes are required, the process stops until the entropy pool is refilled
(waggling your mouse helps). /dev/urandom does not have this restriction. If
the user demands more bits than are currently in the entropy pool, it produces
them using a pseudo random number generator. Here, /dev/urandom is the Linux
random byte device. Myrandom is a file.

Randomize data over a file before deleting it:
Code:
ls -l
to find filesize.
In this case it is 3769
Code:
ls -l afile -rw------- ... 3769 Nov 2 13:41 <filename>
Code:
dd if=/dev/urandom of=afile bs=3769 count=1 conv=notrunc
duplicate a disk partition to a file on a different partition.

Warning!! Do not write a partition image file to the same partition.
Code:
dd if=/dev/sdb2 of=/home/sam/partition.image bs=4096 conv=notrunc,noerror
This will make a file that is an exact duplicate of the sdb2 partition. You
can substitue hdb, sda, hda, etc ... OR
Code:
dd if=/dev/sdb2 ibs=4096 | gzip > partition.image.gz conv=noerror
Makes a gzipped archive of the entire partition. To restore use:
Code:
 dd if=partition.image.gz | gunzip | dd of=/dev/sdb2
For bzip2 (slower,smaller), substitute bzip2 and bunzip2, and name the file
Code:
< filename >.bz2
.Restore a disk partition from an image file.
Code:
dd if=/home/sam/partition.image of=/dev/sdb2 bs=4096 conv=notrunc,noerror
Convert a file to uppercase:
Code:
dd if=filename of=filename conv=ucase
Make a ramdrive:
The Linux kernel makes a number a ramdisks you can make into ramdrives. You
have to populate the drive with zeroes like so:
Code:
dd if=/dev/zero of=/dev/ram7 bs=1k count=16384
Populates a 16 MB ramdisk.
Code:
mke2fs -m0 /dev/ram7 4096
puts a file system on the ramdisk, turning it into a ramdrive. Watch this
puppy smoke.
Code:
debian:/home/sam # hdparm -t /dev/ram7
/dev/ram7:
Timing buffered disk reads:   16 MB in  0.02 seconds = 913.92 MB/sec
You only need to do the timing once, because it's cool. Make the drive again,
because hdparm is a little hard on ramdrives. You can mount the ramdrive with:
Code:
mkdir /mnt/mem
mount /dev/ram7 /mnt/mem
Now you can use the drive like a hard drive. This is particularly superb
for working on large documents or programming. You can duplicate the large
file or programming project to the ramdrive, which on my machine is at least
27 times as fast as /dev/sda, and every time you save the huge document, or
need to do a compile, it's like your machine is running on nitromethane. The
only drawback is data security. The ramdrive is volatile. If you lose power,
or lock up, the data on the ramdrive is lost. Use a reliable machine during
clear skies if you use a ramdrive.

Duplicate ram memory to a file:
Code:
dd if=/dev/mem of=/home/sam/mem.bin bs=1024
The device
Code:
/dev/mem
is your system memory. You can actually duplicate any block or character
device to a file using dd. Memory capture on a fast system, with bs=1024
takes about 60 seconds, a 120 GB HDD about an hour, a CD to hard drive
about 10 minutes, a floppy to a hard drive about 2 minutes. With dd, your
floppy drive images will not change. If you have a bootable DOS diskette,
and you save it to your HDD as an image file, when you restore that image
to another floppy it will be bootable.

Dd will print to the terminal window if you omit the
Code:
of=/dev/output
part.
Code:
dd if=/home/sam/myfile
will print the file myfile to the terminal window.

To search the system memory:
Code:
dd if=/dev/mem | strings | grep
'some-string-of-words-in-the-file-you-forgot-to-save-before-the-power-failed'
If you need to cover your tracks quickly, put the following commands in a
script to overwrite system ram with zeroes. Don't try this for fun.
Code:
mkdir /mnt/mem
mount -t ramfs /dev/mem /mnt/mem
dd if=/dev/zero > /mnt/mem/bigfile.fileThis will overwrite all unprotected
memory structures with zeroes, and freeze the machine so you have to reboot
(Caution, this also prevents committment of the file system journal, and
could trash the file system).

You can get arrested in 17 states for doing this next thing. Make an AES
encrypted loop device:
Code:
dd if=/dev/urandom of=/home/sam/aes-drv bs=16065b count=100
modprobe loop
modprobe cryptoloop
modprobe aes
losetup -e aes /dev/loop1 ./aes-drv
password:
mkreiserfs /dev/loop1
mkdir /aes
mount -o loop,encryption=aes,acl ./aes-drv /aes
password:
mv /home/sam/porno /aes
to get the porno on the aes drive image.
Code:
umount /aes
losetup -d /dev/loop1
rmmod aes
rmmod cryptoloop
rmmod loop
to make 'aes-drv' look like a 400 MB file of random bytes. Every time the
lo interface is configured using losetup, according to the above, and the
file 'aes-drv' is mounted, as above, the porno stash will be accessible in
/aes/porno. You don't need to repeat the dd command, OR, the format with
reiserfs, OR, the mv command. You only do those steps once. If you forget the
password, there is no way to recover it besides guessing. Once the password
is set, it can't be changed. To change the password, make a new file with the
desired password, and move everything from the old file to the new file. Acl
is a good mount option, because it allows use of acls. Otherwise your stuck
with u,g,o and rwx.

If you are curious about what might be on you disk drive, or what an MBR
looks like, or maybe what is at the very end of your disk:
Code:
dd if=/dev/sda count=1 | hexdump -C
Will show you sector 1, or the MBR. The bootstrap code and partition table
are in the MBR.
To see the end of the disk you have to know the total number of sectors,
and the MAS must be set equal to the MNA. The helix CD has a utility to set
this correctly. In the dd command, your skip value will be one less than
MNA of the disk. For a 120 GB Seagate SATA drives
Code:
dd if=/dev/sda of=home/sam/myfile skip=234441646 bs=512
,
So this reads sector for sector, and writes the last sector to myfile. Even
with LBA addressing, disks still secretly are read in sectors, cylinders,
and heads.
There are 63 sectors per track, and 255
heads per cylinder. There is a total cylinder count.
512_bytes/sector*63_sectors/track*255heads=16065*512bytes/cylinder=8,225,280_bytes/cylinder.
63_sectors/track*255_heads=sectors/cylinder. With 234441647 total sectors,
and 16065 sectors per cylinder, you get some trailing sectors which do not
make up an entire cylinder: 14593.317584812_cylinders/drive. This leaves
5102 sectors which cannot be partitioned, because to be in a partition
you have to be a whole cylinder. It's like having part of a person. That
doesn't really count as a person. These become surplus sectors after the
last partition. You can't ordinarily read past the last partition. But dd
can. It's a good idea to check for anything writing to surplus sectors. For
our Seagate 120 GB drive, 234,441,647_sectors/drive - 5102_surplus_sectors =
234,436,545 partitionable sectors.
Code:
dd if=/dev/sda of=/home/sam/myfile skip=234436545
writes the last 5102 sectors to myfile. Launch midnight commander (mc)
to view the file. If there is something in there, you do not need it for
anything. In this case you would write over it with random characters:
Code:
dd if=/dev/urandom of=/dev/sda bs=512 seek=234436545
Will overwrite the 5102 surplus sectors on our 120 GB Seagate drive.

Block size:
One cylinder in LBA mode =
255_heads*63_sectors/track=16065_sectors=16065*512_bytes=8,225,280_bytes. The
b means '* 512'. 32130b represents a two cylinder block size. Cylinder block
size always works to cover every sector in a partition, because partitions
are made of a whole number of cylinders. One cylinder is 8,225,280 bytes. If
you want to check out some random area of the disk:
Code:
dd if=/dev/sda of=/home/sam/myfile bs=4096 skip=2000 count=1000
Will give you 8,000 sectors in myfile, after the first 16,000 sectors. You
can open that file with a hex editor, edit some of it, and write the edited
part back to disk:
Code:
dd if=/home/sam/myfile of=/dev/sda bs=4096 seek=2000 count=1000
Image a partition to another machine:
On source machine:
Code:
dd if=/dev/hda bs=16065b | netcat < targethost-IP > 1234
On target machine:
Code:
netcat -l -p 1234 | dd of=/dev/hdc bs=16065b
Variations on target machine:
Code:
netcat -l -p 1234 | bzip2 > partition.img makes a compressed image file
using bzip2 compression.
Code:
netcat -l -p 1234 | gzip > partition.img makes a compressed image file
using gzip compression. I back up a 100 GB lappy disk on a desktop drive,
over a lan connection, and the 100 GB compresses to about 4.0 GB. Most of
the drive is empty, so it's mostly zeroes. Repetitive zeroes compress well.
Alert!! Don't hit enter yet. Hit enter on the target machine. THEN hit enter
on the source machine.

Netcat is a program, available by default, on most linux installations. It's
a networking swiss army knife. In the preceding example, netcat and dd
are piped to one another. One of the functions of the linux kernel is
to make pipes. The pipe character looks like two little lines on top of
one another, both vertical. Here is how this command behaves: This byte
size is a cylinder. bs=16065b equals one cylinder on an LBA drive. The dd
command is piped to netcat, which takes as its arguments the IP address of
the target(like 192.168.0.1, or any IP address with an open port) and what
port you want to use (1234).

You can also use ssh.
Code:
dd if=/dev/sdb2 | ssh sam@192.168.0.121 "sudo dd of=/home/sam/sdb2.img"

Dd is like Symantec Norton Ghost, Acronis True Image, Symantec Drive Image. You
can perform disk drive backup, restore, imaging, disk image, cloning, clone,
drive cloning, transfer image, transfer data, clone to another drive or clone
to another machine, move Windows XP to a new hard drive, clone Windows XP,
clone Windows, transfer Windows, hard drive upgrade, duplicate a boot drive,
duplicate a bootable drive, upgrade your operating system hard drive, Tired
of reinstalling WinXP Windows XP?

How to rescue a CD or DVD, HDD, flash drive or photos:
Code:
 ddrescue -b 2048 -d -e 20 -r 300 -v /dev/hdc /home/sam/dvd.iso log.txt If
 /home/sam/dvd.iso won't mount, carve it:
Code:
foremost -t all -k 256 -v -b 2048 -i ~/dvd.iso -o ~/dvd/
Your files will be in directory: /home/sam/dvd.

MS Windows Section

Use dd for drive cloning, backup, drive upgrading, and restore tasks. Boot
a Windows XP machine with a Knoppix Linux live CD. Download Knoppix, burn
the iso image file to a CD, boot with it, and clone drives. Drives are
described to the dd command using device files. Boot into Knoppix, and open
a root shell. It's in the penguin menu. (80 conductor grey ribbon cable) =
/dev/hda for master; /dev/hdb for slave. SATA are /dev/sda and /dev/sdb.

The partitions on the first drive:
Code:
fdisk -l /dev/hda
The manual page for fdisk:
Code:
man fdisk
Use parted to make partitions:
Code:
man parted
If one has trouble, leave a reply. Knoppix is slow, because it runs on a CD
drive (1/1000 the speed of a HDD).

Many games written for Windows are virtually impossible back-up. This leaves
only the original media. If it gets damaged, the user SOL. But there is a way
to make back-ups using Linux. Download Adrienne Knoppix, and choose the option
to copy it to a flash drive. Boot the flash-drive. If there are two optical
drives available, load the CD/DVD source disk in one drive, and the blank disk
in the other. Open a terminal from the menu, and get the root account by typing
Code:
su
Now type
Code:
ls /dev/hd*
If /dev/hdb, /dev/hdc and possibly /dev/hdd come up, these are your optical
drives, unless you have (2) ide hard drives. In that instance /dev/hdb would
be a hard drive. If nothing comes up, type
Code:
ls /dev/sr*
The possibilities are /dev/sr0 and sr1. If nothing comes up, your optical
drive are /dev/scd0 and scd1. After the disks are loaded, type
Code:
wodim dev=/dev/hdc fs=16m speed=8 -raw96r driveropts=noburnfree,hidecdr
-dummy -overburn -copy /dev/hdc/
If there are no errors, hit the up arrow, remove '-dummy' and run the command.

Feel free to read the entire posts.
Dd is not presently able to clone Microsoft Windows Vista OEM Partitions.

How to make a bootable CD from a bootable floppy
Put the floppy in the floppy drive, and don't mount it, YET!

Code:
dd if=/dev/fd0 of=/home/sam/floppy.img Mount /home/sam/floppy.img:
Code:
mkdir /mnt/floppy.img
mount -o loop /home/sam/floppy.img /mnt/floppy.img Customize the image:
Code:
cd /mnt/floppy.img/
rm <files you don't need>
cp /home/sam/bios.bin or any other files you want on the CD. But don't exceed
the 1.44 MB size of a floppy. Check space left in the mounted image
Code:
df -h
Unmount the floppy image
Code:
cd ..
umount /mnt/floppy.img Make the .iso CD image file:
Code:
mkisofs -o /home/sam/floppy.img.iso -b /home/sam/floppy.img
/home/sam/floppy.img Burn the iso file to a CD:
Code:
wodim dev=/dev/hdc -sao driveropts=burnfree -dummy /home/sam/floppy.img.iso
This is a dummy burn, with the drive laser off. After you check the dummy
run for errors, by looking at the program output, hit the up arrow,
delete '-dummy', Enter. If you need a DOS boot floppy image file:
[URL]http://www.freedos.org/[URL]

You want to find out if your girlfriend is cheating on you, having cyber
whoopie, or your a control freak. Even if the computer is secured with a
password, you can boot with the: Knoppix Live CD and search the entire drive
partition for text strings:
Code:
dd if=/dev/sda2 bs=16065 | strings -n 10 -t d | grep -i -B 20 -A 20 'luv U'
will search the drive partition specified in the dd command, for the text
string specified between the single quotes in the grep command. Searching
an entire disk partition several times can be time consuming. You might
gift her a day at the spa. Females love that. That would keep her occupied
while you invade her privacy. I highly recommend googling: "How can I tell
if he's married?". This will produce a list of hits for sites females use
to check whether or not their dates are married. You must click the link,
go to the site, get the name of the organization running the site, google
"<name of organization>", until you find the correct mailing address. Send
a letter by certified mail instructing the organization to hereafter refrain
from using a certain name for profit, because it doesn't belong to them, and
they aren't entitled to profit by using it in a query or search result. State
the specific name without a middle initial, and send a copy to any judge,
with a cover letter, of course, and the enclosure marked COPY.

This particular command string prints the search results, preceded by the
decimal offset of its location on the drive, to the screen. If the offfset
is: 34,409,872, we want manageable numbers, custom designed for speed and
ease of use. The decimal disk offset is roughly 34 million, so the data we
want to view is 34 MB into the partition. We divide 34,409,872 by some power
of 2. Experience says 2^13 is about what we want, to get a quotient in the
thousands. 34,409,872/8192=~4200. The data we want is 8,192 4,200 byte blocks,
OR, 4,200 8,192 byte blocks, into the partition. We check: 4200*8192=34406400;
34,409,872-34406400=3472. This means the following command line will start
reading 3,472 bytes before the string location.
Code:
dd if=/dev/sda2 bs=4200 skip=8192 count=2 | strings > file.txt
... and finish reading approximately 4,200 bytes after the string. This will
net you 3.4k of disk contents before the search string, and 4.2k after. That's
a 7.6k chunk of disk, plenty for what we're doing. With this method you
search all the deleted files, any chat activity, Internet temporary files,
and emails. It works regardless of what security has been deployed on the
machine. It works with NTFS, ext2, ext3, reiserfs, swap, UFS, iso9660,
and FAT partitions. But, it is illegal to use this method on a computer you
aren't authorized to search. People can be sued, or imprisoned for performing
unauthorized searches.

[color="red"]This next thing does not work anymore, because the Linux kernel
wised up a bit. I'm working on an alternate method.[color] You can search
system memory with this method, by substituting
Code:
/dev/mem
for
Code:
/dev/sda2
.
Write system memory to a CD. This is useful for documenting memory contents
without contaminating the HDD. I recommend using a CD-RW so you can practice
a little. This doesn't involve dd, but it's cool.
Code:
wodim /dev=/dev/scd0 -raw driveropts=burnfree /dev/mem
to find the cdwriter:
Code:
wodim --devices
This method records raw, so you have to do a:
Code:
dd if=/dev/hdd | hexdump -C | less
to view the recorded memory. You can also employ the string search method
above, substituting
Code:
/dev/hdd
for
Code:
/dev/sda2
.
Code:
dd if=/dev/hdd | strings -n 6 -t d | grep 'string'
string is any ascii sequence, hex sequence (must be separated with a space:
'55<space>aa<space>09' searches for the hex string '55aa09'),
list:
Code:
'[[:alnum:]]' any alphanumeric characters
'[[:alpha:]]' any alpha character
'[[:digit:]]' any numeric character
'[[:blank:]]' tabs and spaces
'[[:lower:]]' any lower case alpha characters
'[[:upper:]]' any uppercase alpha character
'[[:cntrl:]]' ASCII characters 000 thru 037, and 177 octal
'[[:graph:]]' [:alnum:] and [:punct:]
'[[:punct:]]' any punctuation character ` ! ' # $ % ' ( ) * + - . / : ; <
= > ? @ [ \ ] ^ _ { | } ~
'[[:space:]]' tab, newline, vertical tab, form feed, carriage return,
and space '[[:xdigit:]]' any hex digit ranges('[a-d]' = any, or all abcd,
'[0-9]' = any, or all 0123456789)
Code:
dd if=/dev/sda | hexdump -C | grep '[:punct:]' | less
... will return every line from the
Code:
hexdump -C
output that contains any punctuation characters specified above. It will
not gather only punctuation characters.
Back up your MBR:
Code:
dd if=/dev/sda of=mbr.bin count=1
Put this on a floppy you make with:
Code:
dd if=boot.img of=/dev/fd0
I back up floppies to a HDD. Floppies don't last forever, so I do:
Code:
dd if=/dev/fd0 of=/home/sam/floppies/backup.bin conv=notrunc
If my floppy fails, I can make unlimited copies:
Code:
dd if=/home/sam/floppies/backup.bin of=/dev/fd0 conv=notrunc
Here is a command line to read your BIOS, and interfaces:
Code:
dd if=/dev/mem bs=1k skip=768 count=256 2>/dev/null | strings -n 8
dd will not duplicate or erase an HPA, OR, host protected area. Dd will
erase a disk completely, but not as well as using the hardware secure erase,
security erase unit command
Dd need not be black boxed like other inexpensive forensic software:
http://www.cftt.nist.gov/
For a low cost bootable CD based professional ghosting solution, that supports
all operating systems and file systems:
http://www.feyrer.de/g4u/
Variation of dd for data rescue off defective media:
http://www.garloff.de/kurt/linux/ddrescue/
Department of Defense implementation of dd:
http://dcfldd.sourceforge.net/
Sdd is useful when input block size is different than output block size,
and will succeed in some instances where dd fails:
http://linux.maruhn.com/sec/sdd.html
This is one of the best links I haven't written about dd:
http://www.softpanorama.org/Tools/dd.shtml

Copyright 2008 by AwesomeMachine.
All Rights Reserved.


Public Domain Copyright Material Begins Here:

Note that sending a SIGUSR1 signal to a running 'dd' process makes it
print to standard error the number of records read and written so far,
then to resume copying.

Code:
$ dd if=/dev/zero of=/dev/null& pid=$!
	      $ kill -USR1 $pid; sleep 1; kill $pid

	      10899206+0 records in 10899206+0 records out
BLOCKS and BYTES may be followed by the following multiplicative suffixes: c
1, w 2, b 512, kB 1000, K 1024, MB 1000*1000, M 1024*1024, GB 1000*1000*1000,
G 1024*1024*1024
So,
Code:
dd if=/dev/sda of=/dev/sdb bs=1GB
Will use one gigabyte block sizes.
bs=4b would give dd a block size of 4 disk sectors. 1 sector=512 bytes.
bs=4k would indicate dd use a 4 kilobyte block size. I have found bs=4k to
be the fastest for copying disk drives on a modern machine.

OPERANDS The following operands are supported:
Code:
if=file
Specifies the input path. Standard input is the default.
Code:
of=file
Specifies the output path.
Standard output is the default.
seek=blocks Skip this many blocks in the output file.
Code:
ibs=n
Specifies the input block size in n bytes (default is 512).
Code:
obs=n
Specifies the output block size in n bytes (default is 512).
If no conversion other than
Code:
sync, noerror, and, notrunc
is specified, each input block is copied to the output as a single block
without aggregating short blocks.
Code:
cbs=n
Specifies the conversion block size for block and unblock in bytes by n
(default is 0). If cbs= is omitted or given a value of 0, using
Code:
block or unblock
produces unspecified results. This option is used only if ASCII or EBCDIC
conversion is specified.
Code:
ascii and asciib
operands, the input is handled as described for the unblock operand except
that characters are converted to ASCII before the trailing SPACE characters
are deleted.
Code:
ebcdic, ebcdicb, ibm, and ibmb
operands, the input is handled as described for the block operand except
that the characters are converted to EBCDIC or IBM EBCDIC after the trailing
SPACE characters are added.
Code:
files=n
Copies and concatenates n input files before terminating (makes sense only
where input is a magnetic tape or similar device).
Code:
skip=n
Skips n input blocks (using the specified input block size) before starting
to copy. On seekable files, the implementation reads the blocks or seeks past
them. On non-seekable files, the blocks are read and the data is discarded.
Code:
iseek=n
Seeks n blocks from beginning of input file before copying (appropriate for
disk files, where skip can be incredibly slow).
Code:
oseek=n
Seeks n blocks from beginning of output file before copying.
Code:
seek=n
Skips n blocks (using the specified output block size) from beginning of
output file before copying. On non-seekable files, existing blocks are read
and space from the current end-of-file to the specified offset, if any,
is filled with null bytes. On seekable files, the implementation seeks to
the specified offset or reads the blocks as described for non-seekable files.
Code:
count=n
Copies only n input blocks.
Code:
conv=value
[,value. . . ] Where values are comma-separated symbols from the following
list:
Code:
conv=notrunc
Do not truncate the output file.
Code:
ascii
Converts EBCDIC to ASCII.
Code:
asciib
Converts EBCDIC to ASCII using BSD-compatible character translations.
Code:
ebcdic
Converts ASCII to EBCDIC. If converting fixed-length ASCII records without
NEWLINEs, sets up a pipeline with
Code:
dd conv=unblock
beforehand.
Code:
ebcdicb
Converts ASCII to EBCDIC using BSD-compatible character translations. If
converting fixed-length ASCII records without NEWLINEs, sets up a pipeline with
Code:
dd conv=unblock
beforehand.
Code:
ibm
Slightly different map of ASCII to EBCDIC. If converting fixed-length ASCII
records without NEWLINEs, sets up a pipeline with dd
Code:
dd conv=unblock
beforehand.
Code:
ibmb
Slightly different map of ASCII to EBCDIC using BSD-compatible character
translations. If converting fixed-length ASCII records without NEWLINEs,
sets up a pipeline with
Code:
dd conv=unblock
beforehand. The
Code:
ascii (or asciib), ebcdic (or ebcdicb), and ibm (or ibmb)
values are mutually exclusive. block Treats the input as a sequence of
NEWLINE-terminated or EOF-terminated variable-length records independent
of the input block boundaries. Each record is converted to a record with a
fixed length specified by the conversion block size. Any NEWLINE character
is removed from the input line. SPACE characters are appended to lines that
are shorter than their conversion block size to fill the block. Lines that
are longer than the conversion block size are truncated to the largest number
of characters that will fit into that size. The number of truncated lines is
reported. unblock Converts fixed-length records to variable length. Reads a
number of bytes equal to the conversion block size (or the number of bytes
remaining in the input, if less than the conversion block size), delete all
trailing SPACE characters, and append a NEWLINE character. The block and
unblock values are mutually exclusive.
Code:
lcase
Maps upper-case characters specified by the LC_CTYPE keyword tolower to
the corresponding lower-case character. Characters for which no mapping is
specified are not modified by this conversion.
Code:
ucase
Maps lower-case characters specified by the LC_CTYPE keyword toupper to
the corresponding upper-case character. Characters for which no mapping is
specified are not modified by this conversion. The lcase and ucase symbols
are mutually exclusive.
Code:
swab
Swaps every pair of input bytes. If the current input record is an odd number
of bytes, the last byte in the input record is ignored.
Code:
noerror
Does not stop processing on an input error. When an input error occurs,
a diagnostic message is written on standard error, followed by the current
input and output block counts in the same format as used at completion. If the
Code:
sync
conversion is specified, the missing input is replaced with null bytes
and processed normally. Otherwise, the input block will be omitted from
the output. notrunc Does not truncate the output file. Preserves blocks in
the output file not explicitly written by this invocation of dd. (See also
the preceding
Code:
of=file
operand.)
Code:
sync
Pads every input block to the size of the ibs= buffer, appending null
bytes. (If either block or unblock is also specified, appends SPACE characters,
rather than null bytes.)

ENVIRONMENT VARIABLES

The following environment variables affect the messages and errors messages
of dd:

Code:
LANG
Provide a default value for the internationalisation variables that are
unset or null. If

Code:
LANG
is unset or null, the corresponding value from the implementation-dependent
default locale will be used. If any of the internationalisation variables
contains an invalid setting, the utility will behave as if none of the
variables had been defined.

Code:
LC_ALL
If set to a non-empty string value, override the values of all the other
internationalisation variables.

Code:
LC_CTYPE
Determine the locale for the interpretation of sequences of bytes of text
data as characters (for example, single- as opposed to multi-byte characters
in arguments and input files), the classification of characters as upper-
or lower-case, and the mapping of characters from one case to the other.

Code:
LC_MESSAGES
Determine the locale that should be used to affect the format and contents
of diagnostic messages written to standard error and informative messages
written to standard output.

Code:
NLSPATH
Determine the location of message catalogues for the processing of LC_MESSAGES.

---

Rep:
True. Also, what is the difference between those:

dd if=/dev/urandom bs=1 count=100
dd if=/dev/urandom bs=100 count=1

Rep:
dd if=/dev/urandom bs=1 count=100
reads/writes 100 times 1 byte, whereas
dd if=/dev/urandom bs=100 count=1
reads/writes 1 time 100 bytes; the result is the same (but the latter
is faster).
I've also experimented with blocksizes of below L1/L2 cache size but it
wasn't faster, probably because the hard disk was the bottle neck, not the RAM

---

dd if=/dev/urandom bs=100 count=1

Is faster than

dd if=/dev/urandom bs=1 count=100

With urandom it is possible to read a byte size of 100. With /dev/random
you need to do bs=1. Urandom does not have this restriction.

conv=notrunc is to prevent the output file from being truncated.

If you just want to write 1024 bytes to the beginning of
other_existing__file.file, but leave the remainder of the file intact,
this command will not work:
Code:
dd if=/home/sam/existing_file.file bs=1k count=1
of=/home/sam/other_existing_file.file The output file will end up 1024 bytes
long, and the rest will be gone. Adding conv=notrunc tells dd to leave the
remainder of the file in place.

﻿filename: howto-speed-up-things-by-using-tmpfs_20151223.txt
http://docs.gz.ro/node/212

How to speed up things by using tmpfs
Submitted by root on Mon, 06/09/2014 - 23:50

You may know (or not) that /dev/shm/ is a direct read/write access to your memory (RAM). So everything you
copy to that place is in fact stored in your RAM, hence, it is turbocharged fast!

By default, the size of /dev/shm/ is half the memory size so if you have 8G of ram, this filesystem will
have 4G:

core:shm# df -h .
Filesystem            Size  Used Avail Use% Mounted on
tmpfs                 4.0G     0  4.0G   0% /dev/shm

core:shm# cat /proc/meminfo | grep MemTotal
MemTotal:        8190552 kB

If you want to make this fs bigger (do it at your own risk - remember, this is in your RAM):
core:shm# mount -o remount,size=6G /dev/shm

Heck, you can also create a different volume group like the default one:
core:shm# mkdir -p /my/superfast/application/ramstorage
core:shm# mount -t tmpfs -o size=500M,mode=0744 tmpfs /my/superfast/application/ramstorage

If you want this after reboot, do not forget to add it to /etc/fstab:
tmpfs /my/superfast/application/ramstorage tmpfs size=500M,mode=0777 0 0

Play as much as you want but remember something very important: tmpfs is volatile memory so after reboot
everything will disappear from there (will be gone forever). You may have to tweak things a bit like for
example copy the stuff to hard disk on shutdown and put them back on startup. Of course, if you need that
temporary file up2date, to back it up from time to time.filename: howto_create-linux-ram-disk-filesystem_multif_20151202.txt
http://www.cyberciti.biz/faq/howto-create-linux-ram-disk-filesystem/

Software RAM disks use the normal RAM in main memory as if it were a partition on a hard drive rather than
actually accessing the data bus normally used for secondary storage such as hard disk. How do I create
and store a web cache on a RAM disk to improve the speed of loading pages under Linux operating systems?

You can create the ram disk as follows (8192 = 8M, no need to format the ramdisk as a journaling file
system) :
# mkfs -q /dev/ram1 8192
# mkdir -p /ramcache
# mount /dev/ram1 /ramcache
# df -H | grep ramcache

Sample outputs:
/dev/ram1              8.2M   1.1M   6.7M  15% /ramcache
Next you copy images or caching objects to /ramcache
# cp /var/www/html/images/*.jpg /ramcache

Now you can edit Apache or squid reverse proxy to use /ramcache to map to images.example.com:

<VirtualHost 1.2.3.4:80>
     ServerAdmin admin@example.com
     ServerName images.example.com
     DocumentRoot /ramcache
     #ErrorLog /var/logs/httpd/images.example.com_error.log
     #CustomLog /var/logs/httpd/images.example.com_access.log combined
</VirtualHost>

Reload httpd:
# service httpd reload

Now all hits to images.example.com will be served from the ram. This can improve the speed of loading
pages or images. However, if server rebooted all data will be lost. So you may want to write /etc/init.d/
script to copy back files to /ramcache. Create a script called initramcache.sh:

#!/bin/sh
mkfs -t ext2 -q /dev/ram1 8192
[ ! -d /ramcache ] && mkdir -p /ramcache
mount /dev/ram1 /ramcache
/bin/cp /var/www/html/images/*.jpg /ramcache

Call it from /etc/rc.local or create softlink in /etc/rc3.d/
# chmod +x /path/to/initramcache.sh
# echo '/path/to/initramcache.sh' >> /etc/rc.local

A Note About tmpfs
tmpfs is supported by the Linux kernel from version 2.4+. tmpfs (also known as shmfs) is a little different
from the Linux ramdisk. It allocate memory dynamically and by allowing less-used pages to be moved onto
swap space. ramfs, in contrast, does not make use of swap which can be an advantage or disadvantage in
many cases. See how to use tmpfs under Linux.



---
http://www.cyberciti.biz/tips/what-is-devshm-and-its-practical-usage.html

/dev/shm is nothing but implementation of traditional shared memory concept. It is an efficient means
of passing data between programs. One program will create a memory portion, which other processes (if
permitted) can access. This will result into speeding up things on Linux.

shm / shmfs is also known as tmpfs, which is a common name for a temporary file storage facility on
many Unix-like operating systems. It is intended to appear as a mounted file system, but one which uses
virtual memory instead of a persistent storage device.

If you type the mount command you will see /dev/shm as a tempfs file system. Therefore, it is a file
system, which keeps all files in virtual memory. Everything in tmpfs is temporary in the sense that no
files will be created on your hard drive. If you unmount a tmpfs instance, everything stored therein is
lost. By default almost all Linux distros configured to use /dev/shm:
$ df -h

Sample outputs:

Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/wks01-root
                      444G   70G  351G  17% /
tmpfs                 3.9G     0  3.9G   0% /lib/init/rw
udev                  3.9G  332K  3.9G   1% /dev
tmpfs                 3.9G  168K  3.9G   1% /dev/shm
/dev/sda1             228M   32M  184M  15% /boot
Nevertheless, where can I use /dev/shm?

You can use /dev/shm to improve the performance of application software such as Oracle or overall
Linux system performance. On heavily loaded system, it can make tons of difference. For example VMware
workstation/server can be optimized to improve your Linux host's performance (i.e. improve the performance
of your virtual machines).

In this example, remount /dev/shm with 8G size as follows:
# mount -o remount,size=8G /dev/shm

To be frank, if you have more than 2GB RAM + multiple Virtual machines, this hack always improves
performance. In this example, you will give you tmpfs instance on /disk2/tmpfs which can allocate 5GB
RAM/SWAP in 5K inodes and it is only accessible by root:
# mount -t tmpfs -o size=5G,nr_inodes=5k,mode=700 tmpfs /disk2/tmpfs

Where,
-o opt1,opt2 : Pass various options with a -o flag followed by a comma separated string of options. In
this examples, I used the following options:
remount : Attempt to remount an already-mounted filesystem. In this example, remount the system and
increase its size.
size=8G or size=5G : Override default maximum size of the /dev/shm filesystem. he size is given in bytes,
and rounded up to entire pages. The default is half of the memory. The size parameter also accepts a
suffix % to limit this tmpfs instance to that percentage of your pysical RAM: the default, when neither
size nor nr_blocks is specified, is size=50%. In this example it is set to 8GiB or 5GiB. The tmpfs
mount options for sizing ( size, nr_blocks, and nr_inodes) accept a suffix k, m or g for Ki, Mi, Gi
(binary kilo, mega and giga) and can be changed on remount.
nr_inodes=5k : The maximum number of inodes for this instance. The default is half of the number of your
physical RAM pages, or (on a machine with highmem) the number of lowmem RAM pages, whichever is the lower.
mode=700 : Set initial permissions of the root directory.
tmpfs : Tmpfs is a file system which keeps all files in virtual memory.

How do I restrict or modify size of /dev/shm permanently?
You need to add or modify entry in /etc/fstab file so that system can read it after the reboot. Edit,
/etc/fstab as a root user, enter:
# vi /etc/fstab

Append or modify /dev/shm entry as follows to set size to 8G

none      /dev/shm        tmpfs   defaults,size=8G        0 0
Save and close the file. For the changes to take effect immediately remount /dev/shm:
# mount -o remount /dev/shm

Verify the same:
# df -h

Recommend readings:
See man pages of mount regarding tmpfs options.
Details regarding tmpfs is available in /usr/share/doc/kernel-doc-/Documentation/filesystems/tmpfs.txt file.


---
http://www.vanemery.com/Linux/Ramdisk/ramdisk.html

Linux Ramdisk mini-HOWTO

RAM Disk

Introduction
What is a RAM disk? A RAM disk is a portion of RAM which is being used as if it were a disk drive. RAM
disks have fixed sizes, and act like regular disk partitions. Access time is much faster for a RAM disk
than for a real, physical disk. However, any data stored on a RAM disk is lost when the system is shut
down or powered off. RAM disks can be a great place to store temporary data.

The Linux kernel version 2.4 has built-in support for ramdisks. Ramdisks are useful for a number of
things, including:

Working with the unencrypted data from encrypted documents
Serving certain types of web content
Mounting Loopback file systems (such as run-from-floppy/CD distributions)
Why did I write this document? Because I needed to setup a 16 MB ramdisk for viewing and creating
encrypted documents. I did not want the unencrypted documents to be written to any physical media on my
workstation. I also found it amazing that I could easily create a "virtual disk" in RAM that is larger
than my first hard drive, a 20 MB Winchester disk. At the time, that disk was so large that I never even
considered filling it up, and I never did!

This document should take you step-by-step through the process of creating and using RAM disks.

Assumptions/Setup
I was using Red Hat 9 for this test, but it should work with other GNU/Linux distributions running 2.4.x
kernels. I am also assuming that the distribution you are using already has ramdisk support compiled
into the kernel. My test machine was a Pentium 4 and had 256 MB of RAM. The exact version of the kernel
that I used was: 2.4.20-20.9

Step 1: Take a look at what has already been created by your system
Red Hat creates 16 ramdisks by default, although they are not "active" or using any RAM. It lists
devices ram0 - ram 19, but only ram0 - ram15 are usable by default. To check these block devices out,
use the following command:

[root]# ls -l /dev/ram*
lrwxrwxrwx    1 root     root            4 Jun 12 00:31 /dev/ram -> ram1
brw-rw----    1 root     disk       1,   0 Jan 30  2003 /dev/ram0
brw-rw----    1 root     disk       1,   1 Jan 30  2003 /dev/ram1
brw-rw----    1 root     disk       1,  10 Jan 30  2003 /dev/ram10
brw-rw----    1 root     disk       1,  11 Jan 30  2003 /dev/ram11
brw-rw----    1 root     disk       1,  12 Jan 30  2003 /dev/ram12
brw-rw----    1 root     disk       1,  13 Jan 30  2003 /dev/ram13
brw-rw----    1 root     disk       1,  14 Jan 30  2003 /dev/ram14
brw-rw----    1 root     disk       1,  15 Jan 30  2003 /dev/ram15
brw-rw----    1 root     disk       1,  16 Jan 30  2003 /dev/ram16
brw-rw----    1 root     disk       1,  17 Jan 30  2003 /dev/ram17
brw-rw----    1 root     disk       1,  18 Jan 30  2003 /dev/ram18
brw-rw----    1 root     disk       1,  19 Jan 30  2003 /dev/ram19
brw-rw----    1 root     disk       1,   2 Jan 30  2003 /dev/ram2
brw-rw----    1 root     disk       1,   3 Jan 30  2003 /dev/ram3
brw-rw----    1 root     disk       1,   4 Jan 30  2003 /dev/ram4
brw-rw----    1 root     disk       1,   5 Jan 30  2003 /dev/ram5
brw-rw----    1 root     disk       1,   6 Jan 30  2003 /dev/ram6
brw-rw----    1 root     disk       1,   7 Jan 30  2003 /dev/ram7
brw-rw----    1 root     disk       1,   8 Jan 30  2003 /dev/ram8
brw-rw----    1 root     disk       1,   9 Jan 30  2003 /dev/ram9
lrwxrwxrwx    1 root     root            4 Jun 12 00:31 /dev/ramdisk -> ram0

Now, grep through dmesg output to find out what size the ramdisks are:

[root]# dmesg | grep RAMDISK
RAMDISK driver initialized: 16 RAM disks of 4096K size 1024 blocksize
RAMDISK: Compressed image found at block 0

As you can see, the default ramdisk size is 4 MB. I want a 16 MB ramdisk, so the next step will be to
configure Linux to use a larger ramdisk size during boot.

Step 2: Increase ramdisk size
Ramdisk size is controlled by a command-line option that is passed to the kernel during boot. Since GRUB
is the default bootloader for Red Hat 9, I will modify /etc/grub.conf with the new kernel option. The
kernel option for ramdisk size is:  ramdisk_size=xxxxx, where xxxxx is the size expressed in 1024-byte
blocks. Here is what I will add to /etc/grub.conf to configure 16 MB ramdisks:

# grub.conf generated by anaconda
#
# Note that you do not have to rerun grub after making changes to this file
# NOTICE:  You have a /boot partition.  This means that
#          all kernel and initrd paths are relative to /boot/, eg.
#          root (hd0,0)
#          kernel /vmlinuz-version ro root=/dev/hda5
#          initrd /initrd-version.img
#boot=/dev/hda
default=0
timeout=10
splashimage=(hd0,0)/grub/splash.xpm.gz
title Red Hat Linux (2.4.20-20.9)
        root (hd0,0)
        kernel /vmlinuz-2.4.20-20.9 ro root=LABEL=/ hdc=ide-scsi ramdisk_size=16000
        initrd /initrd-2.4.20-20.9.img
	
Once you save the file, you will need to reboot your system. After the reboot, a look at the dmesg output
should confirm the change has taken effect:

[root]# dmesg | grep RAMDISK
RAMDISK driver initialized: 16 RAM disks of 16000K size 1024 blocksize
RAMDISK: Compressed image found at block 0

Step 3: Format the ramdisk
There is no need to format the ramdisk as a journaling file system, so we will simply use the ubiquitous
ext2 file system. I only want to use one ramdisk, so I will only format /dev/ram0:

[root]# mke2fs -m 0 /dev/ram0
mke2fs 1.32 (09-Nov-2002)
Filesystem label=
OS type: Linux
Block size=1024 (log=0)
Fragment size=1024 (log=0)
4000 inodes, 16000 blocks
0 blocks (0.00%) reserved for the super user
First data block=1
2 block groups
8192 blocks per group, 8192 fragments per group
2000 inodes per group
Superblock backups stored on blocks:
        8193

Writing inode tables: done
Writing superblocks and filesystem accounting information: done

This filesystem will be automatically checked every 22 mounts or
180 days, whichever comes first.  Use tune2fs -c or -i to override.

The -m 0 option keeps mke2fs from reserving any space on the file system for the root user, which is
the default behavior. I want all of the ramdisk space available to a regular user for working with
encrypted files.

Step 4: Create a mount point and mount the ramdisk
Now that you have formatted the ramdisk, you must create a mount point for it. Then you can mount your
ramdisk and use it. We will use the directory /mnt/rd for this operation.

[root]# mkdir /mnt/rd
[root]# mount /dev/ram0 /mnt/rd
Now verify the new ramdisk mount:

[root]# mount | grep ram0
/dev/ram0 on /mnt/rd type ext2 (rw)
[root]# df -h | grep ram0
/dev/ram0              16M   13K   16M   1% /mnt/rd

You can even take a detailed look at the new ramdisk with the tune2fs command:

[root]# tune2fs -l /dev/ram0
tune2fs 1.32 (09-Nov-2002)
Filesystem volume name:   none
Last mounted on:          not available
Filesystem UUID:          fbb80e9a-8e7c-4bd4-b3d9-37c29813a5f5
Filesystem magic number:  0xEF53
Filesystem revision #:    1 (dynamic)
Filesystem features:      filetype sparse_super
Default mount options:    (none)
Filesystem state:         not clean
Errors behavior:          Continue
Filesystem OS type:       Linux
Inode count:              4000
Block count:              16000
Reserved block count:     0
Free blocks:              15478
Free inodes:              3989
First block:              1
Block size:               1024
Fragment size:            1024
Blocks per group:         8192
Fragments per group:      8192
Inodes per group:         2000
Inode blocks per group:   250
Filesystem created:       Mon Dec  8 14:33:57 2003
Last mount time:          Mon Dec  8 14:35:39 2003
Last write time:          Mon Dec  8 14:35:39 2003
Mount count:              1
Maximum mount count:      22
Last checked:             Mon Dec  8 14:33:57 2003
Check interval:           15552000 (6 months)
Next check after:         Sat Jun  5 14:33:57 2004
Reserved blocks uid:      0 (user root)
Reserved blocks gid:      0 (group root)
First inode:              11
Inode size:               128

In my case, I need the user "van" to be able to read and write to the ramdisk, so I must change the
ownership and permissions of the /mnt/rd directory:

[root]# chown van:root /mnt/rd
[root]# chmod 0770 /mnt/rd
[root]# ls -ald /mnt/rd
drwxrwx---    2 van     root         4096 Dec  8 11:09 /mnt/rd

The ownership and permissions on the ramdisk filesystem/directory should be tailored to your particular
needs.

Step 5: Use the ramdisk
Now that it has been created, you can copy, move, delete, edit, and list files on the ramdisk exactly as
if they were on a physical disk partiton. This is a great place to view decrypted GPG or OpenSSL files,
as well as a good place to create files that will be encrypted. After your host is powered down, all
traces of files created on the ramdisk are gone.

To unmount the ramdisk, simply enter the following:

[root]# umount -v /mnt/rd
/dev/ram0 umounted

Note:  If you remount the ramdisk, your data will still be there. Once memory has been allocated to the
ramdisk, it is flagged so that the kernel will not try to reuse the memory later. Therefore, you cannot
"reclaim" the RAM after you are done with using the ramdisk. For this reason, you will want to be careful
not to allocate more memory to the ramdisk than is absolutely necessary. In my case, I am allocating <
10% of the physical RAM. You will have to tailor the ramdisk size to your needs. Of course, you can
always free up the space with a reboot!

Automating Ramdisk Creation
If you need to create and mount a ramdisk every time your system boots, you can automate the process by
adding some commands to your /etc/rc.local init script. Here are the lines that I added:

# Formats, mounts, and sets permissions on my 16MB ramdisk
/sbin/mke2fs -q -m 0 /dev/ram0
/bin/mount /dev/ram0 /mnt/rd
/bin/chown van:root /mnt/rd
/bin/chmod 0750 /mnt/rd


Conclusion
You have now seen how to setup and use a ramdisk on your GNU/Linux system. Hopefully, you will find this
information to be interesting and useful!



---
http://www.linuxfocus.org/English/November1999/article124.html

Introduction to RamDisk
How to use RamDisk
Changing the size of the ramdisks
Example of how to use a RamDisk for a webserver.
Comments
How to use a Ramdisk for Linux

Introduction to RamDisk
This is a brief article about how to setup a RamDisk on a RedHat 6.0 system. It should be very similar
for other Linux distributions.
What is a RamDisk? A RamDisk is a portion of memory that you allocate to use as a partition. Or, in other
words, you are taking memory, pretending to treat it as a hard drive, and you are saving your files to
it. Why would you want to use a RamDisk? Well, if you know that certain files you have are constantly
going to be used, putting the files into memory will increase the performance of your computer since
your memory is faster than your hard drive. Things like web servers with lots of data can be sped up
this way. Or, if you are insane, and you have a PII 550 Mhz computer with 1 gig of memory and an old
500 meg hard drive, you can use it just to increase your hard drive space. Then again, if you want an
almost diskless machine, it might not be that crazy afterall.

Here are some more resources to help you.
http://metalab.unc.edu/LDP/HOWTO/Kernel-HOWTO.html
http://metalab.unc.edu/LDP/HOWTO/mini/LILO.html
/usr/src/linux/Documentation/ramdisk.txt

How to use RamDisk
Well, it is very easy to use a ramdisk. First of all, the default installation of RedHat 6.0 comes with
ramdisk support. All you have to do is format a ramdisk and then mount it to a directory. To find out all
the ramdisks you have available, do a "ls -al /dev/ram*". This gives you the preset ramdisks available
to your liking. These ramdisks don't actually grab memory until you use them somehow (like formatting
them). Here is a very simple example of how to use a ramdisk.
# create a mount point:
mkdir /tmp/ramdisk0
# create a filesystem:
mke2fs /dev/ram0
# mount the ramdisk:
mount /dev/ram0 /tmp/ramdisk0

Those three commands will make a directory for the ramdisk , format the ramdisk (create a filesystem),
and mount the ramdisk to the directory "/tmp/ramdisk0". Now you can treat that directory as a pretend
partition! Go ahead and use it like any other directory or as any other partition.
If the formatting of the ramdisk faild then you might have no support for ramdisk compiled into the
Kernel. The Kernel configuration option for ramdisk is CONFIG_BLK_DEV_RAM .
The default size of the ramdisk is 4Mb=4096 blocks. You saw what ramdisk size you got while you were
running mke2fs. mke2fs /dev/ram0 should have produced a message like this:

mke2fs 1.14, 9-Jan-1999 for EXT2 FS 0.5b, 95/08/09
Linux ext2 filesystem format
Filesystem label=
1024 inodes, 4096 blocks
204 blocks (4.98%) reserved for the super user
First data block=1
Block size=1024 (log=0)
Fragment size=1024 (log=0)
1 block group
8192 blocks per group, 8192 fragments per group
1024 inodes per group

Running df -k /dev/ram0 tells you how much of that you can really use (The filesystem takes also some space):
>df -k /dev/ram0
Filesystem  1k-blocks  Used Available Use% Mounted on
/dev/ram0        3963    13      3746   0% /tmp/ramdisk0

What are some catches? Well, when the computer reboots, it gets wiped. Don't put any data there that
isn't copied somewhere else. If you make changes to that directory, and you need to keep the changes,
figure out some way to back them up.

Changing the size of the ramdisks
To use a ram disk you either need to have ramdisk support compiled into the Kernel or you need to compile
it as loadable module. The Kernel configuration option is CONFIG_BLK_DEV_RAM . Compiling the ramdisk a
loadable module has the advantage that you can decide at load time what the size of your ramdisks should be.
Okay, first the hard way. Add this line to your lilo.conf file:
   ramdisk_size=10000 (or ramdisk=10000 for old kernels)
   
and it will make the default ramdisks 10 megs after you type the "lilo" command and reboot the computer. Here
is an example of my /etc/lilo.conf file.

boot=/dev/hda
map=/boot/map
install=/boot/boot.b
prompt
timeout=50
image=/boot/vmlinuz
	label=linux
	root=/dev/hda2
	read-only
	ramdisk_size=10000
	
Actually, I got a little over 9 megs of usable space as the filesystem takes also a little space.
When you compile ramdisk support as loadable module then you can decide at load time what the size should
be. This is done either with an option line in the /etc/conf.modules file:

options rd rd_size=10000
or as a command line parameter to ismod:
insmod rd rd_size=10000
Here is an example which shows how to use the module:
Unmount the ramdisk mounted in the previous chapter, umount /tmp/ramdisk0 .
Unload the module (it was automatically loaded in the previous chapter), rmmod rd
Load the ramdisk module and set the size to 20Mb, insmod rd rd_size=20000
create a file system, mke2fs /dev/ram0
mount the ramdisk, mount /dev/ram0 /tmp/ramdisk0

Example of how to use a RamDisk for a webserver.
Okay, here is an example of how to use 3 ramdisks for a webserver. Let us say you are 99% confident that
your default installation of Apache for RedHat 6.0 won't use more than 9 megs for its cgi-scripts, html,
and icons. Here is how to install one.
First, issue this command to move the real copy of the document root directory of your webserver to a
different place. Also, make the directories to mount the ramdisks .
mv /home/httpd/ /home/httpd_real
mkdir /home/httpd
mkdir /home/httpd/cgi-bin
mkdir /home/httpd/html
mkdir /home/httpd/icons

Then, add these commands to the start procedure in your /etc/rc.d/init.d/httpd.init (or where ever the
httpd gets started on your system):

	### Make the ramdisk partitions
/sbin/mkfs -t ext2 /dev/ram0
/sbin/mkfs -t ext2 /dev/ram1
/sbin/mkfs -t ext2 /dev/ram2

	### Mount the ramdisks to their appropriate places

mount /dev/ram0 /home/httpd/cgi-bin
mount /dev/ram1 /home/httpd/icons
mount /dev/ram2 /home/httpd/html

	### Copying real directory to ramdisks (the
  ### data on the ramdisks is lost after a reboot)
tar -C /home/httpd_real -c . | tar -C /home/httpd -x

  ### After this you can start the web-server.

Comments
Please remember one thing, BACKUP YOUR DATA if you change it and you need it. When the computer reboots,
any changes are lost.

A cron job should do it. Have it check every 10 minutes and see if any files have changed and backup
any changes. Another thing you could do is make your changes to the real directory, and then copy over
the changes to the ramdisks. That is much safer.
A cool use of this would be to have a computer with 1 gig of memory and then use 256 megs for "/tmp". If
you have lots of processes that use "/tmp", it should help speed up your system. Also, anything in /tmp
would get lost when the computer reboots, which can be a good thing.
Linux uses all the memory that is not in use by programs as a unified disk-cache but my experience is
that ramdisks give you despite that still some speed increase.
Webpages maintained by the LinuxFocus Editor team
© Mark Nielsen
LinuxFocus 1999



---
http://www.jamescoyle.net/knowledge/951-the-difference-between-a-tmpfs-and-ramfs-ram-disk

THE DIFFERENCE BETWEEN A TMPFS AND RAMFS RAM DISK

Linux penguinThere are two file system types built into most modern Linux distributions which allow you
to create a RAM based storage area which can be mounted and used link a normal folder.

Before using this type of file system you must understand the benefits and problems of memory file system
in general, as well as the two different types. The two types of RAM disk file systems are tmpfs and
ramfs and each type has it’s own strengths and weaknesses.

See my other post for details on how to create a RAM disk in Linux.

What is a memory based file system (RAM disk)?
A memory based file system is something which creates a storage area directly in a computers RAM as if
it were a partition on a disk drive. As RAM is a volatile type of memory which means when the system is
restarted or crashes the file system is lost along with all it’s data.

The major benefit to memory based file systems is that they are very fast – 10s of times faster than
modern SSDs. Read and write performance is massively increased for all workload types. These types of fast
storage areas are ideally suited for applications which need repetitively small data areas for caching or
using as temporary space. As the data is lost when the machine reboots the data must not be  precious as
even scheduling backups cannot guarantee that all the data will be replicated in the even of a system crash.

tmpfs vs. ramfs
The two main RAM based file system types in Linux are tmpfs and ramfs. ramfs is the older file system
type and is largely replaced in most scenarios by tmpfs.

ramfs
ramfs creates an in memory file system which uses the same mechanism and storage space as Linux file system
cache. Running the command free in Linux will show you the amount of RAM you have on your system, including
the amount of file system cache in use. The below is an example of a 31GB of ram in a production server.

free -g
       total used free shared buffers cached
Mem:   31    29   2    0      0       8
-/+ buffers/cache: 20 11
Swap:  13    6	  7

Currently 8GB of file system cache is in use on the system. This memory is generally used by Linux to
cache recently accessed files so that the next time they are requested then can be fetched from RAM very
quickly. ramfs uses this same memory and exactly the same mechanism which causes Linux to cache files
with the exception that it is not removed when the memory used exceeds threshold set by the system.

ramfs file systems cannot be limited in size like a disk base file system which is limited by it’s
capacity. ramfs will continue using memory storage until the system runs out of RAM and likely crashes or
becomes unresponsive. This is a problem if the application writing to the file system cannot be limited
in total size. Another issue is you cannot see the size of the file system in df and it can only be
estimated by looking at the cached entry in free.

tmpfs
tmpfs is a more recent RAM file system which overcomes many of the drawbacks with ramfs. You can specify
a size limit in tmpfs which will give a ‘disk full’ error when the limit is reached. This behaviour
is exactly the same as a partition of a physical disk.

The size and used amount of space on  a tmpfs partition is also displayed in df. The below example shows
an empty 512MB RAM disk.

df -h /mnt/ramdisk
Filesystem Size Used Avail Use% Mounted on
tmpfs	   512M 0    512M  0%	/mnt/ramdisk

These two differences between ramfs and tmpfs make tmpfs much more manageable  however this is one
major drawback; tmpfs may use SWAP space. If your system runs out of physical RAM, files in your tmpfs
partitions may be written to disk based SWAP partitions and will have to be read from disk when the file
is next accessed. In some environments this can be seen as a benefit as you are less likely to get out
of memory exceptions as you could with ramfs because more ‘memory’ is available to use.

See my other post for details on how to create a RAM disk in Linux.



---
http://www.jamescoyle.net/how-to/943-create-a-ram-disk-in-linux

CREATE A RAM DISK IN LINUX

Linux penguinThere are many reasons for creating a memory based file system in Linux, not least of which
is to provide a near zero latency and extremely fast area to story files. A prime use of a RAM disk is
for application caching directories or work areas.

There are two main types of RAM disk which can be used in Linux and each have their own benefits and
weaknesses:

ramfs
tmpfs

See my other post for the differences between ramfs and tmpfs.

Check the amount of free RAM you have left on your machine before creating a RAM disk. Use the Linux
command free to see the unused RAM. The below is an example of a 31GB of ram in a production server.

free -g
       total used free shared buffers cached
Mem:   31    29   2    0      0       8
-/+ buffers/cache: 20 11
Swap:  13    6	  7

The free command shows the amount of RAM availale on your system in addition to the amount of memory used,
free and used for caching. SWAP space is also displayed and shows if your system is writing memory to disk.

Create a folder to use as a mount point for your RAM disk.

mkdir /mnt/ramdisk

Then use the mount command to create a RAM disk.

mount -t [TYPE] -o size=[SIZE] [FSTYPE] [MOUNTPOINT]

Substitute the following attirbutes for your own values:
[TYPE] is the type of RAM disk to use; either tmpfs or ramfs.
[SIZE] is the size to use for the file system. Remember that ramfs does not have a physical limit and
is specified as a starting size.
[FSTYPE] is the type of RAM disk to use; either tmpfs, ramfs, ext4, etc.

Example:
mount -t tmpfs -o size=512m tmpfs /mnt/ramdisk

You can add the mount entry into /etc/fstab to make the RAM disk persist over reboots. Remember however,
that the data will disappear each time the machine is restarted.

vi /etc/fstab

tmpfs	    /mnt/ramdisk tmpfs	 nodev,nosuid,noexec,nodiratime,size=1024M   0 0
filename: howto_ramdisk-usage-linux_multif_20151203.txt
http://minecraft.gamepedia.com/Tutorials/Ramdisk_enabled_server

Tutorials/Ramdisk enabled server

This tutorial is intended to give you a basic understanding of what a ramdisk is, what use it is for
Minecraft and how to make a Minecraft server use a ramdisk.

Contents  [hide]
1 Ramdisk Introduction
2 Advantages and Disadvantages
2.1 Advantages
2.2 Disadvantages
3 Why it makes sense for Minecraft servers
4 Basic Minecraft and ramdisk setup
4.1 GNU/Linux (Easy Way)
4.2 GNU/Linux (alternative)

Ramdisk Introduction
Conventionally, files and directories are stored on hard disk drives which, by today's standards, offer
a lot of space at mediocre data transfer rates (between 80MB/s and 200MB/s). Ramdisks are virtual file
systems (unlike HDDs which are hardware) that live completely inside the computer's RAM. They offer
significantly higher data transfer rates (between 3,000MB/s and 15,000MB/s) at the cost of volatility
(data will be lost after restarting the computer) and space (limited by the amount of RAM installed on
the system, including swap space). Many utilities however make it possible to backup Ramdisk data at
set intervals, and before the system is shut down, then load the last data when the system starts up.

Advantages and Disadvantages
Advantages
Very high transfer speed (data to application)
Very low seek time (searching between and in files)

Disadvantages
Ramdisks will be cleared when a system restarts
Unfeasible if the world size exceeds the available RAM
Why it makes sense for Minecraft servers

In a Minecraft server, one of the strongest bottlenecks are disk I/O related operations (e.g. chunk
management). By moving the data into the RAM, access times will be near instant and data transfer rates will
be significantly faster, making chunk loading and saving much faster operations. Since a Minecraft world
currently consists of very many chunk files, seek time is equally, if not more, important for overall speed.

Basic Minecraft and ramdisk setup
Make sure to back up your files before starting!

GNU/Linux (Easy Way)
A simple way to load a minecraft server into a ramdisk was posted on the Aimless Bits blog [1] on March
12, 2011. It involves modifying the server startup script available on the wiki and making some minor
changes to fstab. This guide fleshes out the process and makes some minor changes to Aimless Bits' script.

This quick guide assumes you have a user for loading minecraft, a minecraft directory and a server
running. It also helps to be familiar with the /etc/init.d/minecraft startup script.

Firstly, start by creating a directory for the ramdisk in your home directory,
i.e. "/home/username/minecraft_ramdisk".
To mount it as a ramdisk, simply edit your /etc/fstab/ file:
sudo nano /etc/fstab

Then add this line, making sure that the path is correct (username, dir name etc.)
tmpfs  /home/username/minecraft_ramdisk tmpfs  defaults,size=512m      0       0

The size of the ramdisk MUST be larger than the minecraft directory world. Make sure that you give
yourself some overhead.

Restart your computer. The ramdisk will now be loaded every time you restart. If you wish to load
immediately, type
mount -t tmpfs none /home/username/minecraft_ramdisk -o size=512m

It's now a matter of simply running a modified script that loads the files on the drive onto the server,
copies them back on a timely basis to prevent data loss, and does backups. Again, this is a modified
version of the script found at Aimless Bits.

If you have /etc/init.d/minecraft, delete it or overwrite it with this script. If you don't, make a new
text file, call it minecraft, and copy this script into it.

#!/bin/bash
# /etc/init.d/minecraft
# version 0.6 2012-02-25 (YYYY-MM-DD)

### BEGIN INIT INFO
# Provides:   minecraft
# Required-Start: $local_fs $remote_fs
# Required-Stop:  $local_fs $remote_fs
# Should-Start:   $network
# Should-Stop:    $network
# Default-Start:  2 3 4 5
# Default-Stop:   0 1 6
# Short-Description:    Minecraft server
# Description:    Starts the minecraft server
### END INIT INFO

#Settings
JARFILE='craftbukkit-beta_1.4.6-R0.3.jar'
USERNAME="minecraft"
MCSTORE="/home/$USERNAME/minecraft"
MCPATH="/home/$USERNAME/minecraft_ramdisk"
CPU_COUNT=1
INVOCATION="java -Xmx2048M -Xms2048M -server -jar $JARFILE -o false"
BACKUPPATH="/home/$USERNAME/minecraft_backups/"
WORLD=Asgarde


as_user() {
  if [ "`whoami`" == "$USERNAME" ] ; then
    bash -c "$1"
  else
    su - $USERNAME -c "$1"
  fi
}

mc_status() {
  ps aux |grep -F -v grep|grep -F -v SCREEN|grep -F --quiet $JARFILE
  return $?
}

mc_start() {
  if mc_status; then
    echo "Tried to start but $JARFILE was already running!"
  else
    echo "$JARFILE was not running... starting."
    if [ -d $MCSTORE/$WORLD.bak ]; then
      echo "last $WORLD.bak still exist, crashed warning! manual check required!!!"
      exit 1
    fi
    cd $MCPATH
    if [ ! -f "$MCPATH/$JARFILE" ]; then
      echo "Ram drive empty...  prepping."
      as_user "cp -R $MCSTORE/* $MCPATH/"
    fi
    as_user "cd $MCPATH && screen -dmS minecraft $INVOCATION"
    sleep 7
    if mc_status; then
      echo "$JARFILE is now running."
    else
      echo "Could not start $JARFILE."
    fi
  fi
}

mc_saveoff() {
  if mc_status; then
    echo "$JARFILE is running... suspending saves"
    TO_SCREEN="screen -p 0 -S minecraft -X eval 'stuff "
    as_user "$TO_SCREEN \"say SERVER BACKUP STARTING. Server going readonly...\"\015'"
    as_user "$TO_SCREEN \"save-off\"\015'"
    as_user "$TO_SCREEN \"save-all\"\015'"
    sync
    sleep 10
  else
    echo "$JARFILE was not running. Not suspending saves."
  fi
}

mc_saveon() {
  if mc_status; then
    echo "$JARFILE is running... re-enabling saves"
    TO_SCREEN="screen -p 0 -S minecraft -X eval 'stuff "
    as_user "$TO_SCREEN \"save-on\"\015'"
    as_user "$TO_SCREEN \"say SERVER BACKUP ENDED. Server going read-write...\"\015'"
  else
    echo "$JARFILE was not running. Not resuming saves."
  fi
}

mc_stop() {
  if mc_status; then
    echo "$JARFILE is running... stopping."
    TO_SCREEN="screen -p 0 -S minecraft -X eval 'stuff "
    as_user "$TO_SCREEN \"say SERVER SHUTTING DOWN IN 5 SECONDS. Saving map...\"\015'"
    as_user "$TO_SCREEN \"save-all\"\015'"
    sleep 5
    as_user "$TO_SCREEN \"stop\"\015'"
    sleep 5
  else
    echo "$JARFILE was not running."
  fi

  if mc_status; then
    echo "$JARFILE could not be shut down... still running."
  else
    echo "$JARFILE is shut down."
  fi
}


mc_update() {
  if mc_status; then
    echo "$JARFILE is running! Will not start update."
  else
    MC_SERVER_URL=http://minecraft.net/`wget -q -O - http://www.minecraft.net/download.jsp | grep
    minecraft_server.jar\</a\> | cut -d \" -f 2`
    as_user "cd $MCPATH && wget -q -O $MCPATH/minecraft_server.jar.update $MC_SERVER_URL"
    if [ -f $MCPATH/minecraft_server.jar.update ]; then
      if `diff $MCPATH/$JARFILE $MCPATH/minecraft_server.jar.update >/dev/null`
     then
       echo "You are already running the latest version of $JARFILE."
     else
       as_user "mv $MCPATH/minecraft_server.jar.update $MCPATH/$JARFILE"
       echo "Minecraft successfully updated."
      fi
    else
      echo "Minecraft update could not be downloaded."
    fi
  fi
}

mc_backup() {
   echo "Backing up minecraft files"
   as_user "tar zcf $BACKUPPATH/MCBKUP_`date "+%Y.%m.%d-%H"`.tar.gz $MCSTORE"
   echo "Backup complete"
}

mc_disksaverun() {
  if mc_status; then
    echo "Saving ramdrive to disk."
    if [ ! -f $MCPATH/$JARFILE ]; then
      echo "Error.. Minecraft not in ram"
    else
      if [ -d $MCSTORE/$WORLD.bak ]; then
        echo "last $WORLD.bak still exist, crashed warning! manual check required!!!"
        exit 1
      fi
      if [ -d $MCSTORE/$WORLD ]; then
        as_user "mv $MCSTORE/$WORLD $MCSTORE/$WORLD.bak"
      fi

      TO_SCREEN="screen -p 0 -S minecraft -X eval 'stuff "
      as_user "$TO_SCREEN \"save-off\"\015'"
      as_user "$TO_SCREEN \"save-all\"\015'"
      as_user "cp -R $MCPATH/* $MCSTORE/"
      as_user "$TO_SCREEN \"save-on\"\015'"

      if [ -d $MCSTORE/$WORLD.bak ]; then
        as_user "rm -r $MCSTORE/$WORLD.bak"
      fi
    fi
  else
    echo "Service is not running"
 fi

}

mc_disksavehalt() {
   echo "Saving ramdrive to disk."
   if [ ! -f $MCPATH/$JARFILE ]; then
     echo "Error.. Minecraft not in ram"
   else
     if [ -d $MCSTORE/$WORLD.bak ]; then
        echo "last $WORLD.bak still exist, crashed warning! manual check required!!!"
        exit 1
     fi
     if [ -d $MCSTORE/$WORLD ]; then
       as_user "mv $MCSTORE/$WORLD $MCSTORE/$WORLD.bak"
     fi

     echo "Saving, screen session closed"
     as_user "cp -R $MCPATH/* $MCSTORE/"

     if [ -d $MCSTORE/$WORLD.bak ]; then
       as_user "rm -r $MCSTORE/$WORLD.bak"
     fi
   fi
}


#Start-Stop here
case "$1" in
  start)
    mc_start
    ;;
  stop)
    mc_stop
    mc_disksavehalt
    ;;
  restart)
    mc_stop
    mc_disksavehalt
    mc_start
    ;;
  update)
    mc_stop
    mc_backup
    mc_update
    mc_start
    ;;
  backup)
    mc_disksaverun
    mc_saveoff
    mc_backup
    mc_saveon
    ;;
  disksavehalt)
    mc_disksavehalt
    ;;
  disksaverun)
    mc_disksaverun
    ;;
  status)
    if mc_status; then
      echo "$JARFILE is running."
    else
      echo "$JARFILE is not running."
    fi
    ;;
  *)
  echo "Usage: /etc/init.d/minecraft {start|stop|update|backup|status|restart|disksaverun}"
  exit 1
  ;;
esac

Move this script into your /etc/init.d/ directory, and make it executable:
mv /directory/wherefileis/filename /etc/init.d/minecraft
chmod a+x /etc/init.d/minecraft

Note: This script misses the command option that the other minecraft init script has on this website,
http://www.minecraftwiki.net/wiki/Server_startup_script Therefor I rewrote the script with the command code
in it, so ramdisk servers can also use th command thing to sync things without having to get another plugin
to schedule things: http://pastebin.com/4ynwL2js Hope someone can use this, if they need the command option.

You're almost done! This script behaves exactly like the standard startup script, only that it takes
care of loading and maintaining the ramdisk. You can also modify the script to use rsync instead of cp

"rsync -r -t $MCSTORE/ $MCPATH/"
in case you want to do other things, such as remote copying, but performance differences are probably
negligible unless you have very big worlds.

DO NOT SKIP THIS STEP. You need to add a crontab entry to save your world. See below for specific reasons,
but you run the risk of losing data if you don't do this. This script has two disk save functions,
disksavehalt and disksaverun. Disksavehalt assumes the screen session is closing or backing up, and thus
does not disable level saving. Do NOT call this function in crontab. Use disksave run instead. To do this
sudo crontab -e
Then add the line:

*/5 * * * * /etc/init.d/minecraft disksaverun
20 */6 * * * /etc/init.d/minecraft backup

The number represents how often in minutes should you save the world. If you feel like you have a robust
setup, power supply backups and the whole shebang, run this less frequently. Otherwise, stick to 5
minutes at the least!

The other line runs minecraft backup every 6 hours, at :20. Don't skimp on backups! You've been warned!

Hope this helps all those would be admins out there. Good luck!

GNU/Linux (alternative)
On most GNU/Linux distributions there is already a ramdisk set up (usually mounted to /dev/shm (shared
memory)) which defaults to using at most half of your total installed RAM. If there is not one already
set up, resources on how to do it are widely available on the Internet.

It is possible to move anything into the ramdisk, but here I will focus on just moving the map into it
and leaving the server files on the conventional drive.

Given the following basic server directory "minecraft_server/", inside a user's home directory, containing
the world "world" and all other required files

~/minecraft_server/
world/
minecraft_server.jar
server.log
server.properties
...

We will want to move "world/" into the shared memory. Because of the volatility of ramdisks, we will also
want to create a new folder into which an automated script will periodically save the current snapshot
of the world, called (for example) "world_storage" by copying the current world to a new name

$ cd ~/minecraft_server/
$ cp -r world/ world_storage/

Now with the old world in a safe location, we can go ahead and move the world into the ram-disk

$ mkdir /dev/shm/minecraft
$ mv world/ /dev/shm/minecraft

By now, the world is loaded into the RAM, but the Minecraft server doesn't see it in its directory anymore,
causing it to recreate it when started. To stop it from doing that, we have to create a symbolic link
to the world in the ramdisk by running

$ ln -s /dev/shm/minecraft/world/ .

This will create a link to "/dev/shm/minecraft/world/" called "world/" in the server's directory, which
the server will use like the actual world folder, but now inside the RAM.

Now we need to take care of the volatility of the ramdisk, by periodically saving the world from the
RAM into "world_storage/". I'm going to use cron for scheduling and rsync for synching here.

First, we need a script that can be called by cron (it doesn't have to be a script, you could call rsync
directly from the cron command line, but this allows for easy customizing later on)

#!/bin/sh

VOLATILE="/home/$USER/minecraft_server/world/"
PERMANENT="/home/$USER/minecraft_server/world_storage/"

#TODO: Check if both directories actually exist, skipped here for clearness
rsync -r -t -v "$VOLATILE" "$PERMANENT"

And then we need to make this script execute every few minutes (I'll use 5 minutes here, you can test
out what works best for you)

$ crontab -e
You will be put into an editor (more precisely: the editor in your "EDITOR" environment variable) for
editing your user cron table. Add the following line:

*/5 * * * * bash /home/<your_username>/minecraft_server/save_world.sh &>/dev/null

Now if your server restarts you will need to recreate the world folder (/dev/shm/minecraft) then
(/dev/shm/minecraft/world) in the shared memory because the /dev/shm/ empties after restart,. You can
do this by making another similar shell script.

So make a shell script file like before:

exec 1>/tmp/backup_world.log 2>&1 #sends the output to this file
#!/bin/sh
#remake the paths
mkdir /dev/shm/minecraft
mkdir /dev/shm/minecraft/world

VOLATILE="/home/$USER/minecraft_server/world/"
PERMANENT="/home/$USER/minecraft_server/world_storage/"

#TODO: Check if both directories actually exist, skipped here for clearness
#reversed the order
rsync -r -t -v "$PERMANENT" "$VOLATILE"

Everytime you restart you need to run this script to remount the Ramdisk. Do not add this to the
crontab. You can add this to the start up if you figure it out.



---
http://superuser.com/questions/870763/mysql-data-on-ramdisk-partition

MySQL data on ramdisk partition
I'm using Docker with Ubuntu for my CI and development environment. I would like to put the database on
a ramdisk partition to speed up the builds, since I have to reload my fixtures a lot, so data persistence
isn't an issue here.

Is that possible? What steps I should add to my Docker file?

***
Here is excerpt from my post in the DBA StackExchange

RAMDISK_SIZE=32g
service mysql stop
mkdir /var/tmpfs
echo "none   /var/tmpfs  tmpfs  defaults,size=${RAMDISK_SIZE} 1 2" >> /etc/fstab
mount -t tmpfs -o size=${RAMDISK_SIZE} none /var/tmpfs
cp -R /var/lib/mysql/* /var/tmpfs
mv /var/lib/mysql /var/lib/mysql_old
ln -s /var/tmpfs /var/lib/mysql
chown -R mysql:mysql /var/tmpfs
chown -R mysql:mysql /var/lib/mysql
service mysql start
I hope you can apply it to Ubuntu

Give it a Try !!!



---
https://wiki.ubuntu.com/BootToRAM

BootToRAM

Booting Ubuntu To RAM
Unnecessary on 11.04 LiveCD. Casper now includes a functioning toram option, which even works with the
iso-scan/filename= option.

Last update for 9.10 compability.

Latest update for 2.6.31-16 upgrade

Preface
This article aims to document the process of creating a customized Ubuntu that loads an image from the
hard disk to RAM, then boots an entire Ubuntu session out of RAM. It is intended for intermediate to
advanced Ubuntu users who are familiar with the shell, and may have limited experience customizing the
livecd (LiveCDCustomization) and shell scripting. We will customize a LiveCD and copy it to the hard
drive, and make a few modifications to bootup scripts so that it copies to RAM via our good friend tmpfs.

WARNING: The author asserts that this procedure works for him, but cannot guarantee that this procedure
works for anyone else. Although this procedure is meant to be 100% safe, it is feasible that there may be
mistakes, or a chance of misunderstanding the instructions in a manner that causes loss of data. Please
make a backup and do not attempt on mission critical systems. Read through this article thoroughly,
and do not attempt if you do not comprehend or feel comfortable about any of the instructions!

CAUTION: I hope this is intuitively obvious, but I'll humor you and state it bluntly: Changes you make
under the live session are NOT saved and WILL BE LOST when you reboot or shut down. Don't save anything
important to the "home directory" and expect it to still be around! If you want to save data permanently,
mount a permanent medium (such as your hard drive), plug in a thumbdrive, or use some network functionality
built into Ubuntu to save your data to a non-volatile destination.

Use Cases
There are many cases where one would like to boot Ubuntu to RAM:

Performance: The desktop performance is dramatically improved. A 400MB squashed filesystem in RAM, that
holds 1200MB of data, is read back on a 1.6GHz Core Duo in about 3 seconds, including decompression time.
Power, Noise, Durability: Although modern hard disks don't use much power compared to other system
components, this may still be important for some. In laptops, hard disks are often the noisiest components,
so this setup can reduce system noise. With the hard disk spun down, a laptop can potentially withstand
greater shocks without damage.

Abrupt poweroff: Since the hard disk is only momentarily used in read-only mode during boot, then never
touched again, there are few or no negative consequences of an abrupt poweroff. If a system is used where
power is inconsistent, or the system is regularly used in a context where fast shutoffs are required,
this is very handy.

Privacy: Anything you do in this session are lost when you reboot or power off. This is great for kiosks
or other systems where permanent modification are not desired. (Note that by default the livecd user has
full sudo access, so potentially a malicious user can still make permanent changes by mounting the hard
drive and following this HOWTO)

Requirements
The most obvious increased requirement is RAM. For best performance, I recommend having 256MB RAM +
enough RAM to hold a customized image. Stripping Openoffice and some fonts and documentation from a stock
Ubuntu LiveCD results in a 400MB compressed image, which fits in RAM comfortably on a system with 1GB RAM.

Tmpfs can fall back on swap (Ubuntu LiveCD scripts will mount any swap it finds), which is excellent
for a bit of overflow, but if you regularly need to fall back on swap, performance will naturally suffer.

I have not investigated CPU requirements, but squashfs is compressed and decompressing takes some CPU
power, so this is probably not a great idea on systems older than the Pentium III era.

As far as setting this system up, the requirements would be:
Having an Ubuntu LiveCD ISO or CD handy. I used a Ubuntu LiveCD, but I see no reason why Xubuntu,
Kubuntu, etc wouldn't work (they don't have differing casper boot scripts, as far as I know). This
procedure should work on all LiveCD's Dapper and later, with appropriate minor adaptations.
About 2-4GB of free space

A combined 1GB or so of RAM and swap available.

The Process

Unpack LiveCD
First, we need to unpack the LiveCD for customization. For this article, I am going to make the following
assumptions about paths. You can of course reject my choices and substitute your own.

The compressed root image is at /casper/filesystem.squashfs
The kernel is at /casper/vmlinuz
The initramfs is at /casper/initrd.gz
We will make a temporary directory /casper/chroot where we edit this root filesystem.
For the first 3 files are located in /casper on the LiveCD. Please copy these files from the LiveCD to
/casper on your hard disk. You may use the GNOME archive manager (file-roller) as root, bind-mounting,
or a physical CD. I will assume you know how to do this.

Customize Live Environment
This procedure is almost identical to customizing a LiveCD (up to the generation of the .squashfs
image). Please see LiveCDCustomization for detailed instructions on customizing the LiveCD. I will only
be providing a basic rundown on the process.

EXTRACT /CASPER/FILESYSTEM.SQUASHFS TO /CASPER/CHROOT/
sudo mount -o loop -t squashfs /casper/filesystem.squashfs /mnt
sudo mkdir /casper/chroot
sudo rsync -ax /mnt/. /casper/chroot/.
sudo umount /mnt

PATCH BOOT SCRIPTS
The stock casper "toram" functionality is broken in Feisty. In addition, even when it worked, it would
completely decompress the filesystem into RAM, which requires 3-4x more RAM, and is hence undesired. As
a result, we will be providing some nasty hacks on casper to make it copy to RAM the way we want it
to. Casper developers, please look away.

gksu gedit /casper/chroot/usr/share/initramfs-tools/scripts/casper

Around line 35, find:

                export SHOWMOUNTS='Yes' ;;
            persistent)
	    
Between them add extra two lines, so it looks like:

                export SHOWMOUNTS='Yes' ;;
            toram)
                export TORAM='Yes' ;;
            persistent)
	    
Around line 573, find:

    if [ "${TORAM}" ]; then
        live_dest="ram"
    elif [ "${TODISK}" ]; then
    
Between them add some extra lines, like this:

    if [ "${TORAM}" ]; then
        #live_dest="ram"
        echo "Copying CD contents to ram"
        mkdir /store
        #TODO: add a test for amount of ram here
        mount -t tmpfs -o size=1G none /store
        mkdir /store/casper
        cp /cdrom/casper/*.squashfs /store/casper/
        echo "CD-Rom has been unmounted, it's safe to eject!"
        umount /cdrom
        mount -o bind /store /cdrom

    elif [ "${TODISK}" ]; then
    
You need to comment or delete live_dest="ram". Replace "-o size=1G" with a larger (or smaller) size if
your customized image is not 1GB (for example, 2G, 1500M, or 350M). Look at the size of your squashfs
image to determine the required size.

Save this file, and quit the editor.

REGENERATE INITRD.GZ
Since we edited bootup scripts, we need to regenerate the file we know as /casper/initrd.gz to incorporate
these changes:

sudo cp -L /etc/resolv.conf /casper/chroot/etc/
sudo mount -t proc none /casper/chroot/proc
sudo mount -o bind /dev /casper/chroot/dev
sudo chroot /casper/chroot /bin/bash

At this point, you are "in" the live environment's filesystem. We will be doing this a few more times
before the day is over. Remember that our Live environment is at /casper/chroot, not at edit/ (adjust
customization commands accordingly).

Optional: Customize Live Environment Further
It would be a great idea to add or remove some packages, or add some default user settings, etc, to make
the live environment friendlier. The previously linked LiveCD customization article provides full details
on how to do a wide variety of customizations. Follow those instructions, up to: "Putting the CD together"
(don't do that step). Instead, replace it with

Ideas for customizations specific to this howto include:
Removing behemoth packages like OpenOffice.
Adding proprietary 3D video drivers by default (TODO: expand on this idea)
Removing shutdown scripts (TODO: expand)
Customizing user default settings in /etc/skel, including importing a firefox profile, etc. The LiveCD
howto roughly states how to do this. (TODO: expand)
Suppress the eject notice at shutdown.

rm /etc/rc?.d/*casper*

Install something like sshfs so you can easily use SSH-able systems as permanent storage (TODO: Expand)
Upgrade the whole system and regenerate initrd.gz

apt-get update
apt-get dist-upgrade
apt-get autoclean
apt-get autoremove
apt-get clean
mkinitramfs -o /new-initrd.gz 2.6.31-16-generic
exit
umount -l /casper/chroot/dev
umount -l /casper/chroot/proc

The mkinitramfs command may take 30 seconds to a few minutes, depending on your CPU speed. The exit
command will take you back to your original shell, that's not within with live environment. Now we will
move this initrd to the right spot:

sudo mv /casper/chroot/new-initrd.gz /casper/initrd.gz
sudo mksquashfs /casper/chroot /casper/filesystem.squashfs -noappend -always-use-fragments

The always-use-fragments argument allows space to be used more efficiently, at the cost of more
seeking. Since our image is to be loaded into RAM, seeking is costless and not a concern as opposed to
on a mechanical medium.

Make a GRUB Entry
Ok, the last thing we need to do is tell GRUB how to boot this system. Read Making a GRUB bootable
CD-ROM first.

Edit /boot/grub/menu.lst to include an entry like this at the bottom:

title Jaunty RAM Session
kernel /casper/vmlinuz boot=casper toram splash
initrd /casper/initrd.gz

Reboot and Enjoy

Now, it's time to reboot and select Jaunty RAM Session, and see if it works out. If the system does not
boot, then double-check you followed all the instructions properly. It took me about 10 or 11 customization
cycles to get my live system JUST the way I like it, so be patient!

Remember, the possibilities are endless! Enjoy, and share any cool things you do with this HOWTO!

Comments
Why don't you work on bug #25496: toRam or copy2Ram (run ubuntu live from ram), to make this feature in
the default Ubuntu livecd? https://bugs.edge.launchpad.net/ubuntu/+source/casper/+bug/25496

Any idea on how to get this to work with "iso-scan/filename=" option? It loads everything to RAM fine,
but won't let me unmount /isodevice (probably because a loopback device needs to be killed). I can't
seem to find which script creates the loopback nor where to destroy it and unmount the /isodevice folder.

Lucid Updates
For trying this out in Lucid, it's easier to just go into edit mode in grub2 having booted the live
cd/usb and change the options to

kernel /casper/vmlinuz boot=casper toram splash
initrd /casper/initrd.gz

Else you'll need to get grub legacy packages and use that to create the grub install cd stage2_eltorito
can be extracted from http://packages.ubuntu.com/lucid/i386/grub/download

When making the iso use -J option to Generate Joliet directory information so that you can use
usb-creator-gtk (Startup disk creator) and not waste CDs!

mkisofs -R -J -b boot/grub/stage2_eltorito -no-emul-boot \
         -boot-load-size 4 -boot-info-table -o grub.iso iso
	 
Pitfalls:
remember to set permissions on all the files before you create the iso as these will be honored when imaged
Make sure your chroot doesn't end up on the iso
Variations:

loading the squashfs from a cheap usb stick or cd can be slow, so if you do have a hard drive use that
instead e.g.

if [ "${TORAM}" ]; then
#live_dest="ram"
      echo "Copying contents to ram.."
      mkdir /store
      mount -t tmpfs -o size=2G none /store
      mkdir /store/casper
      mkdir /mnt
      
# A use for my old windows recovery partition!
      mount /dev/sda3 /mnt/
      cp /mnt/casper/*.squashfs /store/casper/
      echo "Copy done.. safe to eject boot medium"
      umount /cdrom
      umount /mnt
      mount -o bin /store /cdrom

    elif [ "${TODISK}" ]; then
	BootToRAM

(nazadnje spreminjano 2011-07-18 07:35:36, spreminjal ubuntu-launchpad-bobpaul)



---
http://blog.csdn.net/wang_xya/article/details/43410651

Ubuntu using Ramdisk for better performance and fast response
ubunturamdisk
2015-02-02 16:56 217

Ubuntu using Ramdisk for better performance and fast response
I have written a tutorial about speed up Ubuntu response time by optimizing the usage of swap area, and
that’s avoiding swapping processes out of physical memory for as long as possible. Here’s another
way using ramdisk to get better performance and fast response for Ubuntu.

Ramdisk is part of system memory. Ubuntu by default uses a half of physical memory (RAM) as ramdisk,
and it is mounted onto /dev/shm, it can be used just like normal disk space (create files and folders and
manipulate them with better performance rather if they were stored on the hard disk). If ramdisk uses more
than a half of RAM, data will be moved into the swap space. If ramdisk uses less, the remaining can still do
what RAM’s doing.

Set upper limit of ramdisk
As is said above, ramdisk by default can use a half of RAM. If you want to change the upper limit,
follow the steps below:

1. Edit /etc/fstab by your favourate editor:
gksudo gedit /etc/fstab

2. Find this line and change to make it looks like this(add this line if not exist, and change 512M to
what you like.):
tmpfs /dev/shm tmpfs defaults,size=512M 0 0

3. Reboot or re-mount
/dev/shm
.

Mount /tmp onto ramdisk
To make it easy to use, you can mount a directory into
/dev/shm

by following commands:
mkdir /dev/shm/tmp
chmod 1777 /dev/shm/tmp
mount --bind /dev/shm/tmp /tmp


---
filename: ram-cache-and-tmpfs-speddup-linux_multif_20151222.txt

http://fixmynix.com/speed-up-firefox-with-ram-cache-and-tmpfs-linux/

Speed up firefox with RAM cache and tmpfs in Linux
BY ARNAB · JULY 29, 2015

Firefox, the second most popular web browser, the default pre installed browser in many GNU/Linux
distributions. It is really great in terms of it’s customization capability and large collection
of add-ons.

Though many users find firefox frustratingly slow and sluggish, so they switch to other browsers like google
chrome/Chromium etc. There are many reasons for it’s sluggish performance, like storing cache to disk,
using slow sqlite databases, frequent disk access by other processes, no DNS cache and like so many of them.

But there are many of ways to make it faster and improve page load time. In this tutorial we are going
to do this by modifying the firefox about:config configurations, moving all profile data to a tmpfs
partition on RAM and moving the cache to RAM instead of hard disk. This guide will work all GNU/Linux
Distributions like Ubuntu, Fedora, Debian, Arch Linux etc. etc.



optimize the about:config configurations

Open up firefox, type about:config in the URL bar , hit Enter and press the I’ll be careful, I
promise! button, now you can edit the configurations. Use the search bar to find configurations easily.

speed up firefox linux about:config

Search for browser.cache.use_new_backend , if the value is 0 , modify it to 1 , this makes the caching
efficient.
Search for network.http.pipelining.ssl toggle it value to true .
Search for network.http.pipelining , toggle the value to true .
Search for network.http.proxy.pipelining , toggle its value to true .
Search for network.http.pipelining.maxrequests , modify the value , set any thing above 100 , like 120 .
If you are not using IPv6, then search for network.dns.disableIPv6  and toggle its value to true .
Search for browser.safebrowsing.enabled and browser.safebrowsing.malware.enabled , toggle the value to
false, in both of them, though this is a little bit dangerous.


move the cache to RAM

As RAM is much much more faster than hard disk, so moving cache to RAM will increase the user experience
greatly. Recent computers have plenty of RAM, at least 4 GB in most cases, so moving the cache to RAM
will not be a problem. But avoid this if you have less than 1 GB RAM.

Again, open up firefox and type about:config in the URL bar, hit Enter and press the I’ll be careful,
I promise! button. Now edit few thing in this section.
Search for browser.cache.disk.enable  in the filter bar, toggle the value to false , this will disable
cache to disk.
Now it is the time to enable cache to RAM, search for browser.cache.memory.enable , if the
value is true leave it else toggle it to true.Now assign how much memory could be used as
RAM cache, search for browser.cache.memory.capacity , if not found create it.speed up firefox
browser.cache.memory.capacityRight click on blank area, then select “New” > “Integer”  , set the
preference name browser.cache.memory.capacity , enter the integer value in KB, like 204800 for 200 MB ,
and done . Check it with about:cache .
finally move profile data to a tmpfs partition

Finally move all profile data to a tmpfs partition on RAM , tmpfs is a incredibly fast special type of
file system that uses a part of RAM to store data. Firefox stores almost every information in SQLite
databases, but SQLite is inheriently slow and disk I/O dependent. This process will make firefox blazing
fast and disk I/O independent.

1. clear all cache and cookies

perhaps you know how to do that, if not, click Edit > Preferances go to Privacy and clear cookies,
then go to Advanced > Network clear Cached Web Content and done !

2. backup default profile data

Create a backup archive of your current data, before doing this close Firefox and look at the commands
bellow.

cd .mozilla/firefox/

cp -a profile_name.default/ profile.bac

rm -rf profile_name.default/*
must replace profile folder name with your own profile folder name.  This will create a folder named
profile.bac containing all profile data and clear all data inside the default profile folder.

3. mount the tmpfs on profile folder

add the line bellow to the /etc/fstab file,

tmpfs /home/$USER/.mozilla/firefox/profile_name.default tmpfs size=128M,user,exec,uid=1000,gid=1000 0 0
my firefox default profile name was p3iyxcse.default , so the line at /etc/fstab looks like bellow.

tmpfs /home/b00m/.mozilla/firefox/p3iyxcse.default tmpfs size=128M,user,exec,uid=1000,gid=1000 0 0
Make sure that the /home/$USER/.mozilla/firefox/profile_name.default directory exists, otherwise tmpfs
mounting will fail .

Now mount the tmpfs at profile directory , i.e. at /home/$USER/.mozilla/firefox/profile_name.default
and copy all the backup data to the default profile folder.

sudo mount -a

cd .mozilla/firefox/

cp -a profile.bac/* profile_name.default/
Do not forget to replace profile_name.default with your own profile name. And now start firefox, if you
have a slow HDD like me, then you will notice a much faster start up time and no sluggish performance .

4. sync any changes to the profile folder with the backup folder

In order to keep the changes intact after a reboot or shutdown, we need to sync the profile folder with
the backup folder time to time. To do this without any user interaction in later, save the script bellow
as ff_profile_sync and add it to crontab  . Before doing this if you do not have rsync , run sudo apt-get
install rsync for Ubuntu/Debian /Linux Mint etc.the script

#!/bin/sh
cd /home/$USER/.mozilla/firefox/
if test -d *.default;then
if test -d profile.bac;then
rsync -a --delete *.default/ profile.bac/
fi;fi
Do not forget to replace $USER with your username, save this script at /usr/local/bin or wherever you
like, make it executable , and add to crontab with crontab -e , set the cron interval 5-10 minuite or
whatever you like.

crontab -e
put the line bellow for a 5 minuite  update interval .

*/5 * * * * /usr/local/bin/ff_profile_sync > /dev/null 2>&1
Finally add the line bellow to the /etc/rc.local file just before the exit 0 , to restore the previous
changes after a reboot .

cp -a /home/$USER/.mozilla/firefox/profile.bac/* /home/$USER/.mozilla/firefox/profile_name.default/
Again, Do not forget to replace $USER with your username and profile_name with your firefox profile name.

This setup will backup all changed profle data to the profile.bac folder automatically, so forgot about
it , and enjoy super fast firefox.



If you have any question or need further assistance just leave a comment, we’d be happy to assist you.
Feel free to share this tutorial with your friends.



---
https://www.joeyconway.com/blog/2011/09/11/ubuntu-ssd-move-chrome-cache-to-ram/

Ubuntu – Move Google Chrome cache from SSD to RAM
Posted on September 11, 2011 by joey — 2 Comments ↓
Running an SSD for the main boot partition is quite convenient for any OS, including Ubuntu. However
having Google Chrome, or any browser, store its cache on the SSD is not the ideal scenario.
Under Ubuntu Natty 11.04 moving Google Chrome’s cache to RAM is fairly simple and only takes a few
commands. The advantages to storing the web browser cache in RAM are: quicker reads/writes than on disk
drives, no wear and tear on disk drives and the cache will be erased on reboot. The disadvantages are:
the cache will be erased on reboot and will consume RAM, which can be limited on some systems.

1) Decide where to move the Chrome cache location to. I’ve picked the following location: /tmp/chrome.
This directory, /tmp/chrome will need to be created on boot and properly setup.
On Ubuntu 11.04 and probably older versions, this can be simply done in the /etc/rc.local file as follows:
sudo gedit /etc/rc.local
Add the following lines:
mkdir /tmp/chrome
mount -t tmpfs -o size=1024M,mode=0744 tmpfs /tmp/chrome/
chmod 777 /tmp/chrome/ -R

There are two ways to accomplish the next and last step. One is to create a symlink between the default
google cache directory and the new temporary cache directory in RAM. The second is to add a switch to
the google chrome command line telling each instance of the application to use our newly created cache
directory in RAM.
1) rm -rf ~/.cache/google-chrome
ln -s /tmp/chrome/ ~/.cache/google-chrome
OR
2) Change the default here: sudo gedit /usr/local/share/applications/google-chrome.desktop
Replace the line:
#Exec=/opt/google/chrome/google-chrome %U
with
Exec=/opt/google/chrome/google-chrome –disk-cache-dir=”/tmp/chrome/”

The only adjustment some might want to make will be the size of the tmpfs partition created in RAM. I
set the size to 1024MB as I don’t ever want to have to worry about it or adjust it. For systems with
a lot of RAM the above size should not be an issue.

Used the following main sources:
Firefox & Chrome Cache on RAM Drive -Fedora / Ubuntu
How To Change Google Chrome’s Cache Location And Size

‹ Android – Custom Recovery Code ResearchUbuntu – Move /home partition from SSD to RAID ›
Posted in Technology Tagged with: ubuntu, natty, 11.04, 10.10, google chrome, ram, tmpfs, ramdisk, cache,
browser cache, mount, fstab, SSD, solid state disk, partition
2 comments on “Ubuntu – Move Google Chrome cache from SSD to RAM”

mike wilson says:
May 23, 2013 at 12:39 pm
Can you help me change it back to the default? I’m running ubuntu 12.04 on an acer c7.

joey says:
May 24, 2013 at 4:03 pm
Sure. The removal process will essentially be the opposite of the installation process. For installing
there are two main steps and for removing there will also be two main steps:
First, remove the tmpfs partition that is created on each boot by removing the three lines that were
added to the /etc/rc.local file.
Second, remove either the symlink and/or the flag added to the chrome execution command.
Hope that helps!



---
http://www.insanitybit.com/2012/09/25/move-chromes-cache-to-ram-guide-for-linux-users/

Move Chrome’s Cache To RAM (Guide For Linux Users)
September 25, 2012 by insanitybit
Browsers will keep ‘pieces’ of a webpage in what’s called a cache. This cache allows them to
quickly pull files from the disk (which is quite somewhat quick) instead of having to redownload them
(which is slow). Your system’s RAM is even faster than your disk, hundreds of times so, and keeping a
file in RAM means accessing it will be nearly instant. Browsers are going to load up these files to RAM
regardless but we can speed up writes to the cache and improve privacy by having the entire cache kept
in RAM from the beginning. To do so we’ll be creating a RAM disk and then telling Chrome to use it.

Remember, your cache is deleted every time you shut down your computer if you follow this guide. It will
get rebuilt the next session.

First off we’re going to create a directory in /tmp/ , which we’ll call ccache.

mkdir /tmp/ccache

Then we need to open up /etc/rc.local. The commands in /etc/rc.local are run whenever the system starts
up. Enter the following line, which tells the system to mount the RAM disk at /tmp/ccache/. You’ll
see size=700M. Keep in mind that this is in Bytes and it’s how much RAM you’re allocating to the
new filesystem. You can change that size to whatever you want but I don’t think anyone’s going to
be using much more than 300MB, but I keep some extra room in there.

mount -t tmpfs -o size=700M,mode=0744 tmpfs /tmp/ccache/

We now set the permissions on /tmp/ccache (recursively) to 777. For some reason 777 is all that’ll
work for me.

chmod 777 /tmp/ccache/ -R

Now that’s all set up we need to create a Chrome desktop shortcut. However your distro lets you do
that, just drag it wherever. Right click it, properties, and add the following (word press messes up
double -‘s. You’ll have to type those out).

–disk-cache-dir=”/tmp/ccache/” –disk-cache-size=600000000

Now when you launch Chrome from that shortcut it’ll use the disk cache we’ve set up.

Now, in terms of privacy what we’ve done is eliminated the ability for an attacker with access to
your system to view your cache/ try to see what you’ve been up to *except* for that session. Every
time you shut your system down you lose the cache, as such no one can see where you’ve been.

If you were on some dodgy site or doing something sensitive or whatever all you have to do is restart
the system and there will be no trace left (in the cache at least).

It may or may not be worth the trouble to you. Since Chrome’s already mapping this stuff to RAM anyways
you shouldn’t expect any major performance improvements. But those who fear micro-writes to their SSD
can use this to prevent wear/tear.


***
filename: /c/Users/gregor.redelonghi/Dropbox/ODPRTO/_TXT/ramdisk_ramfs-vs-tmpfs_use-case-multif_20161027.txt
https://www.jamescoyle.net/knowledge/951-the-difference-between-a-tmpfs-and-ramfs-ram-disk

The difference between tmpfs and ramfs ramdisk

There are two file system types built into most modern Linux distributions which allow you to
create a RAM based storage area which can be mounted and used link a normal folder.

Before using this type of file system you must understand the benefits and problems of memory file system in
general, as well as the two different types. The two types of RAM disk file systems are tmpfs and ramfs and
each type has it?s own strengths and weaknesses.

See my other post for details on how to create a RAM disk in Linux.

What is a memory based file system (RAM disk)?

A memory based file system is something which creates a storage area directly in a computers RAM as if it
were a partition on a disk drive. As RAM is a volatile type of memory which means when the system is
restarted or crashes the file system is lost along with all it?s data.

The major benefit to memory based file systems is that they are very fast ? 10s of times faster than modern
SSDs. Read and write performance is massively increased for all workload types. These types of fast storage
areas are ideally suited for applications which need repetitively small data areas for caching or using as
temporary space. As the data is lost when the machine reboots the data must not be  precious as even
scheduling backups cannot guarantee that all the data will be replicated in the even of a system crash.

tmpfs vs. ramfs
The two main RAM based file system types in Linux are tmpfs and ramfs. ramfs is the older file system type
and is largely replaced in most scenarios by tmpfs.

ramfs
ramfs creates an in memory file system which uses the same mechanism and storage space as Linux file system
cache. Running the command free in Linux will show you the amount of RAM you have on your system, including
the amount of file system cache in use. The below is an example of a 31GB of ram in a production server.
	1 free -g
	2        total used free shared buffers cached
	3 Mem:   31    29   2    0      0       8
	4 -/+ buffers/cache: 20 11
	5 Swap:  13    6    7

Currently 8GB of file system cache is in use on the system. This memory is generally used by Linux to cache
recently accessed files so that the next time they are requested then can be fetched from RAM very quickly.
ramfs uses this same memory and exactly the same mechanism which causes Linux to cache files with the
exception that it is not removed when the memory used exceeds threshold set by the system.

ramfs file systems cannot be limited in size like a disk base file system which is limited by it?s capacity.
ramfs will continue using memory storage until the system runs out of RAM and likely crashes or becomes
unresponsive. This is a problem if the application writing to the file system cannot be limited in total
size. Another issue is you cannot see the size of the file system in df and it can only be estimated by
looking at the cached entry in free.

tmpfs
tmpfs is a more recent RAM file system which overcomes many of the drawbacks with ramfs. You can specify a
size limit in tmpfs which will give a ?disk full? error when the limit is reached. This behaviour is exactly
the same as a partition of a physical disk.

The size and used amount of space on  a tmpfs partition is also displayed in df. The below example shows an
empty 512MB RAM disk.
	1 df -h /mnt/ramdisk
	2 Filesystem Size Used Avail Use% Mounted on
	3 tmpfs      512M 0    512M  0%   /mnt/ramdisk

These two differences between ramfs and tmpfs make tmpfs much more manageable  however this is one major
drawback; tmpfs may use SWAP space. If your system runs out of physical RAM, files in your tmpfs partitions
may be written to disk based SWAP partitions and will have to be read from disk when the file is next
accessed. In some environments this can be seen as a benefit as you are less likely to get out of memory
exceptions as you could with ramfs because more ?memory? is available to use.

**************************************************************************************************************************************

**************************************************************************************************************************************


---
https://www.jamescoyle.net/how-to/943-create-a-ram-disk-in-linux

Create a RAM disk in Linux

Linux penguinThere are many reasons for creating a memory based file system in Linux, not least of which is
to provide a near zero latency and extremely fast area to story files. A prime use of a RAM disk is for
application caching directories or work areas.

There are two main types of RAM disk which can be used in Linux and each have their own benefits and
weaknesses:
  * ramfs
  * tmpfs

See my other post for the differences between ramfs and tmpfs.

Check the amount of free RAM you have left on your machine before creating a RAM disk. Use the Linux command 
free to see the unused RAM. The below is an example of a 31GB of ram in a production server.
	1 free -g
	2        total used free shared buffers cached
	3 Mem:   31    29   2    0      0       8
	4 -/+ buffers/cache: 20 11
	5 Swap:  13    6    7

The free command shows the amount of RAM availale on your system in addition to the amount of memory used,
free and used for caching. SWAP space is also displayed and shows if your system is writing memory to disk.

Create a folder to use as a mount point for your RAM disk.

1 mkdir /mnt/ramdisk

Then use the mount command to create a RAM disk.
	mount -t [TYPE] -o size=[SIZE] [FSTYPE] [MOUNTPOINT]

Substitute the following attirbutes for your own values:
  * [TYPE] is the type of RAM disk to use; either tmpfs or ramfs.
  * [SIZE] is the size to use for the file system. Remember that ramfs does not have a physical limit and is
    specified as a starting size.
  * [FSTYPE] is the type of RAM disk to use; either tmpfs, ramfs, ext4, etc.

Example:
	mount -t tmpfs -o size=512m tmpfs /mnt/ramdisk

You can add the mount entry into /etc/fstab to make the RAM disk persist over reboots. Remember however, that
the data will disappear each time the machine is restarted.
	vi /etc/fstab
	tmpfs       /mnt/ramdisk tmpfs   nodev,nosuid,noexec,nodiratime,size=1024M   0 0
**************************************************************************************************************************************

**************************************************************************************************************************************
---
http://askubuntu.com/questions/173094/how-can-i-use-ram-storage-for-the-tmp-directory-and-how-to-set-a-maximum-amount

How can I use RAM storage for the /tmp directory and how to set a maximum amount of RAM usage for it?

                After seeing the comment by Anonymous on the question How is the /tmp directory cleaned up?,
                I found that it would be a great idea to implement on my system, since I have 16GB of RAM and
                I never used all of it.

                    My temporary files never get written to the disk. They get written to a RAM disk. I did
                    put tmpfs /tmp tmpfs defaults,noatime,mode=1777 0 0 in /etc/fstab.

                My question is:
                Can I set a maximum value for RAM Usage for /tmp? And in that case, what would happen if the
                maximum amount got exceeded, would it write into the hard-disk drive?

                I have read this solution which states:
			mkdir -p /tmp/ram
			sudo mount -t tmpfs -o size=512M tmpfs /tmp/ram/

		But in my understanding, this won't be a permanent solution. If I need it to be permanent, it
                has to be added to the /etc/fstab configuration file.

                If this is the correct solution, how can I transform that mount command into a line in /etc/
                fstab?


                this is also a good idea if you have and SSD as you will not 'consume' it with not
                useful writings ... ? Postadelmaga Apr 2 '14 at 10:47

                add a comment |   

***
            You are absolutely right. The according fstab entry would look like this:
		tmpfs /tmp tmpfs defaults,noatime,nosuid,nodev,noexec,mode=1777,size=512M 0 0

            Please note:
            As tmpfs gets filled up, it will behave as any physical harddrive by giving an "not enough space"
            error. While rebooting (and thus emptying the cache) will fix this, you may run into trouble when
            a single operation consumes more space to begin with than there's space on tmpfs. In this case
            your computer will start to swap from ram to disk, which will make your system crawl to a halt,
            given you've got a swap partition to begin with, of course.

            Considering this, a size of 512MB might be far too less nowadays, since much more ram is in
            existence in modern machines and it has become much cheaper. Since you've already got 16GB of
            ram, using the default value of half your ram for tmpfs should more than suffice for almost all
            scenarios. To use the default value, simply leave out the size=512M entry in your /etc/fstab
            file.

            Another note:
	    You can quite as easily mount other system folders into ramdisk as well, such as
		    /var/cache
		    /var/games
		    /var/log/apt (use only defaults,noatime without mode= or nosuid)

            But beware: the same rules apply as above, running out of space might cause major trouble. E.g.
            imagine running out of space for /var/log/apt will render you unable to install any programs!
            Furthermore, loading /var/log folders into ramdisk will delete all your log files upon reboot, so
            you won't be able to debug your system if anything unexpected happens. So use these settings at
            your own risk!

            Editorial note: I removed the /run in tmpfs mount option since this folder and its subfolders are
            already mounted in tmpfs by default.

***
            4   If i'm not wrong, /var/tmp/ is for keeping files after reboot. Thats the main difference
                between this and /tmp/ , so YOU should not move /var/tmp to ram. ? user285767 May 27 '14 at
                13:20
            3   If Ubuntu runs out of 4GB tmpfs, will it use my 20GB SWAP partition? ? loostro Jun 24 '14 at
                16:30
            2   Yes, I'll need to add this, too. As soon as tmpfs exeeeds its limits, it'll extend to swap
                partition (give there is one). ? FuzzyQ Jun 24 '14 at 19:10
            2   Does allocating half your RAM to this mean that half your RAM is reserved for the RAMDISK, or
                is it only a cap to what the RAMDISK may consume, and whatever is not in use is free RAM that
                gets assigned to whatever program needs RAM?? ? matt Jan 19 '15 at 15:01
            2   @matt It's only a cap. ? FuzzyQ Jan 19 '15 at 16:04

***
             On systems using systemd, you have the option of using a systemd unit file instead of fstab to
             accomplish the goal of using tmpfs to mount tmp. On my Ubuntu 16.04 system, I ran:
		     sudo cp /usr/share/systemd/tmp.mount /etc/systemd/system/tmp.mount
		     sudo systemctl start tmp.mount
	
             The file /usr/share/systemd/tmp.mount looks like:
		     #  This file is part of systemd.
		     #
		     #  systemd is free software; you can redistribute it and/or modify it
		     #  under the terms of the GNU Lesser General Public License as published by
		     #  the Free Software Foundation; either version 2.1 of the License, or
		     #  (at your option) any later version.
		     [Unit]
		     Description=Temporary Directory
		     Documentation=man:hier(7)
		     Documentation=http://www.freedesktop.org/wiki/Software/systemd/APIFileSystems
		     ConditionPathIsSymbolicLink=!/tmp
		     DefaultDependencies=no
		     Conflicts=umount.target
		     Before=local-fs.target umount.target
		     After=swap.target

		     [Mount]
		     What=tmpfs
		     Where=/tmp
		     Type=tmpfs
		     Options=mode=1777,strictatime

		     [Install]
		     WantedBy=local-fs.target

             Using FuzzyQ's fstab approach, systemd translates your fstab entries into mount units
             dynamically. I don't think either approach is better.

***	     
	     This looks interesting! I will try this when I'm next to my laptop ? Dan Aug 13 at 19:15
             You will need systemctl enable tmp.mount after cp else systemctl will fail with message
             "Failed to start tmp.mount: Unit tmp.mount not found." ? vimdude Oct 8 at 18:17


---
http://askubuntu.com/questions/152868/how-do-i-make-a-ram-disk

How do I make a RAM disk?

           I want to make a partition that is made of ram ...
           Example
           In windows 7 you can make a partition that is made of ram

	   Is there any good Alternative in Ubuntu ?

***	   
                Ubuntu comes with tmpfs. You need not create a RAMDISK. ? mixdev Mar 30 '14 at 4:03

                Depending on your intended use case you may not need a ramdisk for Ubuntu (or most Linux
                distros). The operating system caches reads and write activity to RAM while it is working
                with regular disks. If you read a small file several times, it will only be fetched from disk
                once, then retrieved from the RAM cache on the following times. If you have plenty of RAM,
                everything you do will get cached in this way so you get very little repeat disk activity. If
                you want non-persistent fast memory use instead of files, you need a RAMDISK still. See
                linuxatemyram.com for more details. ? TafT Oct 6 at 14:11

***		
                    This will show you how to make a RAMDISK super fast and easily. With a RAMDISK you can
                    use your memory for temporary space and it?s also a lot quicker than your hard drive.

                    Now lets start by using the next 2 commands to make your RAMDISK.

                    Put whatever you want your RAMDISK to be called where I wrote ?nameme?.
			    mkdir -p /media/nameme
			    mount -t tmpfs -o size=2048M tmpfs /media/nameme/

		    The above commands would use 2GB of my RAM for the RAMDISK. If you don?t have as much ram
                    as I do I would use 512MB or 1GB. So next were going to create a command for Terminal
                    that will automatically create the RAMDISK for you.

***		    
            The tmpfs filesystem is a RAMDISK. The following will create a 2G RAMDISK that will always be
            available.
		    sudo mkdir -p /media/ramdisk
		    sudo mount -t tmpfs -o size=2048M tmpfs /media/ramdisk

            The ramdisk folder is owned by root as it is to be available on reboot. The ramdisk permissions
            should be writeable by everyone. The tmpfs default permissions (chmod 1777) are correct.
		drwxrwxrwt 2 root root 180 Apr 23 07:34 /media/ramdisk

            To make the ramdisk permanently available, add it to /etc/fstab.
		grep /media/ramdisk /etc/mtab | sudo tee -a /etc/fstab

            You will see the line moved from mtab to fstab. It will look something like this.
		tmpfs /media/ramdisk tmpfs rw,size=2048M 0 0

            The RAMDISK won't consume memory until you use it. Double check your memory requirements during
	    maximum system load. If the RAMDISK is too large, your system will consume swap storage to make
	    up the difference.

            To adjust the size of the RAMDISK, edit /etc/fstab and verify by remounting the ramdisk (you will
            lose your current RAMDISK content as you will on reboot). The following will change the size of
            the ramdisk to 512M
		    # Check the existing ramdisk size.
		    df /media/ramdisk
		    # change size=512M for a 512 megabyte ram drive.
		    sudo vi /etc/fstab
		    # Remount the ramdisk, you will lose any existing content.
		    sudo mount -a /media/ramdisk
		    # Verify the new ramdisk size.
		    df /media/ramdisk


---
http://www.vanemery.com/Linux/Ramdisk/ramdisk.html

Linux Ramdisk mini-HOWTO

Introduction
What is a RAM disk? A RAM disk is a portion of RAM which is being used as if it were a disk drive. RAM disks
have fixed sizes, and act like regular disk partitions. Access time is much faster for a RAM disk than for a
real, physical disk. However, any data stored on a RAM disk is lost when the system is shut down or powered
off. RAM disks can be a great place to store temporary data.

The Linux kernel version 2.4 has built-in support for ramdisks. Ramdisks are useful for a number of things,
including:
  * Working with the unencrypted data from encrypted documents
  * Serving certain types of web content
  * Mounting Loopback file systems (such as run-from-floppy/CD distributions)

Why did I write this document? Because I needed to setup a 16 MB ramdisk for viewing and creating encrypted
documents. I did not want the unencrypted documents to be written to any physical media on my workstation. I
also found it amazing that I could easily create a "virtual disk" in RAM that is larger than my first hard
drive, a 20 MB Winchester disk. At the time, that disk was so large that I never even considered filling it
up, and I never did!

This document should take you step-by-step through the process of creating and using RAM disks.

Assumptions/Setup
I was using Red Hat 9 for this test, but it should work with other GNU/Linux distributions running 2.4.x
kernels. I am also assuming that the distribution you are using already has ramdisk support compiled into the
kernel. My test machine was a Pentium 4 and had 256 MB of RAM. The exact version of the kernel that I used
was: 2.4.20-20.9

-------------------------------------------------------------------------------------------------------------

Step 1: Take a look at what has already been created by your system
Red Hat creates 16 ramdisks by default, although they are not "active" or using any RAM. It lists devices
ram0 - ram 19, but only ram0 - ram15 are usable by default. To check these block devices out, use the
following command:
	[root]# ls -l /dev/ram*
	lrwxrwxrwx    1 root     root            4 Jun 12 00:31 /dev/ram -> ram1
	brw-rw----    1 root     disk       1,   0 Jan 30  2003 /dev/ram0
	brw-rw----    1 root     disk       1,   1 Jan 30  2003 /dev/ram1
	brw-rw----    1 root     disk       1,  10 Jan 30  2003 /dev/ram10
	brw-rw----    1 root     disk       1,  11 Jan 30  2003 /dev/ram11
	brw-rw----    1 root     disk       1,  12 Jan 30  2003 /dev/ram12
	brw-rw----    1 root     disk       1,  13 Jan 30  2003 /dev/ram13
	brw-rw----    1 root     disk       1,  14 Jan 30  2003 /dev/ram14
	brw-rw----    1 root     disk       1,  15 Jan 30  2003 /dev/ram15
	brw-rw----    1 root     disk       1,  16 Jan 30  2003 /dev/ram16
	brw-rw----    1 root     disk       1,  17 Jan 30  2003 /dev/ram17
	brw-rw----    1 root     disk       1,  18 Jan 30  2003 /dev/ram18
	brw-rw----    1 root     disk       1,  19 Jan 30  2003 /dev/ram19
	brw-rw----    1 root     disk       1,   2 Jan 30  2003 /dev/ram2
	brw-rw----    1 root     disk       1,   3 Jan 30  2003 /dev/ram3
	brw-rw----    1 root     disk       1,   4 Jan 30  2003 /dev/ram4
	brw-rw----    1 root     disk       1,   5 Jan 30  2003 /dev/ram5
	brw-rw----    1 root     disk       1,   6 Jan 30  2003 /dev/ram6
	brw-rw----    1 root     disk       1,   7 Jan 30  2003 /dev/ram7
	brw-rw----    1 root     disk       1,   8 Jan 30  2003 /dev/ram8
	brw-rw----    1 root     disk       1,   9 Jan 30  2003 /dev/ram9
	lrwxrwxrwx    1 root     root            4 Jun 12 00:31 /dev/ramdisk -> ram0

Now, grep through dmesg output to find out what size the ramdisks are:
	[root]# dmesg | grep RAMDISK
	RAMDISK driver initialized: 16 RAM disks of 4096K size 1024 blocksize
	RAMDISK: Compressed image found at block 0

As you can see, the default ramdisk size is 4 MB. I want a 16 MB ramdisk, so the next step will be to
configure Linux to use a larger ramdisk size during boot.

Step 2: Increase ramdisk size
Ramdisk size is controlled by a command-line option that is passed to the kernel during boot. Since GRUB is
the default bootloader for Red Hat 9, I will modify /etc/grub.conf with the new kernel option. The kernel
option for ramdisk size is:  ramdisk_size=xxxxx, where xxxxx is the size expressed in 1024-byte blocks. Here
is what I will add to /etc/grub.conf to configure 16 MB ramdisks:
	# grub.conf generated by anaconda
	#
	# Note that you do not have to rerun grub after making changes to this file
	# NOTICE:  You have a /boot partition.  This means that
	#          all kernel and initrd paths are relative to /boot/, eg.
	#          root (hd0,0)
	#          kernel /vmlinuz-version ro root=/dev/hda5
	#          initrd /initrd-version.img
	#boot=/dev/hda
	default=0
	timeout=10
	splashimage=(hd0,0)/grub/splash.xpm.gz
	title Red Hat Linux (2.4.20-20.9)
		root (hd0,0)
		kernel /vmlinuz-2.4.20-20.9 ro root=LABEL=/ hdc=ide-scsi ramdisk_size=16000
		initrd /initrd-2.4.20-20.9.img

Once you save the file, you will need to reboot your system. After the reboot, a look at the dmesg output
should confirm the change has taken effect:
	[root]# dmesg | grep RAMDISK
	RAMDISK driver initialized: 16 RAM disks of 16000K size 1024 blocksize
	RAMDISK: Compressed image found at block 0

Step 3: Format the ramdisk
There is no need to format the ramdisk as a journaling file system, so we will simply use the ubiquitous ext2
file system. I only want to use one ramdisk, so I will only format /dev/ram0:
	[root]# mke2fs -m 0 /dev/ram0
	mke2fs 1.32 (09-Nov-2002)
	Filesystem label=
	OS type: Linux
	Block size=1024 (log=0)
	Fragment size=1024 (log=0)
	4000 inodes, 16000 blocks
	0 blocks (0.00%) reserved for the super user
	First data block=1
	2 block groups
	8192 blocks per group, 8192 fragments per group
	2000 inodes per group
	Superblock backups stored on blocks:
		8193

	Writing inode tables: done
	Writing superblocks and filesystem accounting information: done

	This filesystem will be automatically checked every 22 mounts or
	180 days, whichever comes first.  Use tune2fs -c or -i to override.

The -m 0 option keeps mke2fs from reserving any space on the file system for the root user, which is the
default behavior. I want all of the ramdisk space available to a regular user for working with encrypted
files.

Step 4: Create a mount point and mount the ramdisk
Now that you have formatted the ramdisk, you must create a mount point for it. Then you can mount your
ramdisk and use it. We will use the directory /mnt/rd for this operation.
	[root]# mkdir /mnt/rd
	[root]# mount /dev/ram0 /mnt/rd

Now verify the new ramdisk mount:
	[root]# mount | grep ram0
	/dev/ram0 on /mnt/rd type ext2 (rw)
	[root]# df -h | grep ram0
	/dev/ram0              16M   13K   16M   1% /mnt/rd

You can even take a detailed look at the new ramdisk with the tune2fs command:
	[root]# tune2fs -l /dev/ram0
	tune2fs 1.32 (09-Nov-2002)
	Filesystem volume name:   none
	Last mounted on:          not available
	Filesystem UUID:          fbb80e9a-8e7c-4bd4-b3d9-37c29813a5f5
	Filesystem magic number:  0xEF53
	Filesystem revision #:    1 (dynamic)
	Filesystem features:      filetype sparse_super
	Default mount options:    (none)
	Filesystem state:         not clean
	Errors behavior:          Continue
	Filesystem OS type:       Linux
	Inode count:              4000
	Block count:              16000
	Reserved block count:     0
	Free blocks:              15478
	Free inodes:              3989
	First block:              1
	Block size:               1024
	Fragment size:            1024
	Blocks per group:         8192
	Fragments per group:      8192
	Inodes per group:         2000
	Inode blocks per group:   250
	Filesystem created:       Mon Dec  8 14:33:57 2003
	Last mount time:          Mon Dec  8 14:35:39 2003
	Last write time:          Mon Dec  8 14:35:39 2003
	Mount count:              1
	Maximum mount count:      22
	Last checked:             Mon Dec  8 14:33:57 2003
	Check interval:           15552000 (6 months)
	Next check after:         Sat Jun  5 14:33:57 2004
	Reserved blocks uid:      0 (user root)
	Reserved blocks gid:      0 (group root)
	First inode:              11
	Inode size:               128

In my case, I need the user "van" to be able to read and write to the ramdisk, so I must change the ownership
and permissions of the /mnt/rd directory:
	[root]# chown van:root /mnt/rd
	[root]# chmod 0770 /mnt/rd
	[root]# ls -ald /mnt/rd
	drwxrwx---    2 van     root         4096 Dec  8 11:09 /mnt/rd

The ownership and permissions on the ramdisk filesystem/directory should be tailored to your particular
needs.

Step 5: Use the ramdisk
Now that it has been created, you can copy, move, delete, edit, and list files on the ramdisk exactly as if
they were on a physical disk partiton. This is a great place to view decrypted GPG or OpenSSL files, as well
as a good place to create files that will be encrypted. After your host is powered down, all traces of files
created on the ramdisk are gone.

To unmount the ramdisk, simply enter the following:
	[root]# umount -v /mnt/rd
	/dev/ram0 umounted

Note:  If you remount the ramdisk, your data will still be there. Once memory has been allocated to the
ramdisk, it is flagged so that the kernel will not try to reuse the memory later. Therefore, you cannot
"reclaim" the RAM after you are done with using the ramdisk. For this reason, you will want to be careful not
to allocate more memory to the ramdisk than is absolutely necessary. In my case, I am allocating < 10% of the
physical RAM. You will have to tailor the ramdisk size to your needs. Of course, you can always free up the
space with a reboot!

-------------------------------------------------------------------------------------------------------------

Automating Ramdisk Creation
If you need to create and mount a ramdisk every time your system boots, you can automate the process by
adding some commands to your /etc/rc.local init script. Here are the lines that I added:
	# Formats, mounts, and sets permissions on my 16MB ramdisk
	/sbin/mke2fs -q -m 0 /dev/ram0
	/bin/mount /dev/ram0 /mnt/rd
	/bin/chown van:root /mnt/rd
	/bin/chmod 0750 /mnt/rd

-------------------------------------------------------------------------------------------------------------

Conclusion
You have now seen how to setup and use a ramdisk on your GNU/Linux system. Hopefully, you will find this
information to be interesting and useful!

-------------------------------------------------------------------------------------------------------------

Resources

  * /usr/src/linux-2.4/Documentation/ramdisk.txt
  * /usr/src/linux-2.4/drivers/block/rd.c
  * man mke2fs
  * Ramdisk article by Mark Nielsen for Red Hat 6.0

  

---
http://www.thegeekstuff.com/2008/11/overview-of-ramfs-and-tmpfs-on-linux/

Overview of RAMFS and TMPFS on Linux

[Linux Ramfs and Tmpfs]Using ramfs or tmpfs you can allocate part of the physical memory to be used as a
partition. You can mount this partition and start writing and reading files like a hard disk partition. Since
you?ll be reading and writing to the RAM, it will be faster.

When a vital process becomes drastically slow because of disk writes, you can choose either ramfs or tmpfs
file systems for writing files to the RAM.

Both tmpfs and ramfs mount will give you the power of fast reading and writing files from and to the primary
memory. When you test this on a small file, you may not see a huge difference. You?ll notice the difference
only when you write large amount of data to a file with some other processing overhead such as network.

1. How to mount Tmpfs
	# mkdir -p /mnt/tmp
	# mount -t tmpfs -o size=20m tmpfs /mnt/tmp

The last line in the following df -k shows the above mounted /mnt/tmp tmpfs file system.
	# df -k
	Filesystem      1K-blocks  Used     Available Use%  Mounted on
	/dev/sda2       32705400   5002488  26041576  17%   /
	/dev/sda1       194442     18567    165836    11%   /boot
	tmpfs           517320     0        517320    0%    /dev/shm
	tmpfs           20480      0        20480     0%    /mnt/tmp

2. How to mount Ramfs
	# mkdir -p /mnt/ram
	# mount -t ramfs -o size=20m ramfs /mnt/ram
	
The last line in the following mount command shows the above mounted /mnt/ram ramfs file system.
	# mount
	/dev/sda2 on / type ext3 (rw)
	proc on /proc type proc (rw)
	sysfs on /sys type sysfs (rw)
	devpts on /dev/pts type devpts (rw,gid=5,mode=620)
	/dev/sda1 on /boot type ext3 (rw)
	tmpfs on /dev/shm type tmpfs (rw)
	none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)
	sunrpc on /var/lib/nfs/rpc_pipefs type rpc_pipefs (rw)
	fusectl on /sys/fs/fuse/connections type fusectl (rw)
	tmpfs on /mnt/tmp type tmpfs (rw,size=20m)
	ramfs on /mnt/ram type ramfs (rw,size=20m)

You can mount ramfs and tmpfs during boot time by adding an entry to the /etc/fstab.

3. Ramfs vs Tmpfs
Primarily both ramfs and tmpfs does the same thing with few minor differences.

  * Ramfs will grow dynamically.   So, you need control the process that writes the data to make sure ramfs
    doesn?t go above the available RAM size in the system. Let us say you have 2GB of RAM on your system and
    created a 1 GB ramfs and mounted as /tmp/ram. When the total size of the /tmp/ram crosses 1GB, you can
    still write data to it.   System will not stop you from writing data more than 1GB. However, when it goes
    above total RAM size of 2GB, the system may hang, as there is no place in the RAM to keep the data.
  * Tmpfs will not grow dynamically. It would not allow you to write more than the size you?ve specified
    while mounting the tmpfs. So, you don?t need to worry about controlling the process that writes the data
    to make sure tmpfs doesn?t go above the specified limit. It may give errors similar to ?No space left on
    device?.
  * Tmpfs uses swap.
  * Ramfs does not use swap.

4. Disadvantages of Ramfs and Tmpfs
Since both ramfs and tmpfs is writing to the system RAM, it would get deleted once the system gets rebooted,
or crashed. So, you should write a process to pick up the data from ramfs/tmpfs to disk in periodic
intervals. You can also write a process to write down the data from ramfs/tmpfs to disk while the system is
shutting down. But, this will not help you in the time of system crash.

                       Table: Comparison of ramfs and tmpfs
+--------------------------------------------------------------------------------+
|            Experimentation            |      Tmpfs       |        Ramfs        |
|---------------------------------------+------------------+---------------------|
|Fill maximum space and continue writing|Will display error|Will continue writing|
|---------------------------------------+------------------+---------------------|
|Fixed Size                             |Yes               |No                   |
|---------------------------------------+------------------+---------------------|
|Uses Swap                              |Yes               |No                   |
|---------------------------------------+------------------+---------------------|
|Volatile Storage                       |Yes               |Yes                  |
+--------------------------------------------------------------------------------+

If you want your process to write faster, opting for tmpfs is a better choice with precautions about the
system crash.

This article was written by SathiyaMoorthy. He is working at bksystems, interested in writing articles and  
contribute to open source in his leisure time. The Geek Stuff welcomes your tips and guest articles.



---
http://unix.stackexchange.com/questions/66329/creating-a-ram-disk-on-linux

Creating a ram disk on Linux

               I have a machine with 62GB of RAM, and a trunk that's only 7GB, so I thought I would create a
               RAM disk and compile there. I am not a Linux expert. I found instructions on the internet to
               create the RAM disk:
		mkfs -q /dev/ram1 8192

               but I changed the 8192 to 16777216 in an attempt to allocate 16GB of ram disk.

               I got the following error:
		       mkfs.ext2: Filesystem larger than apparent device size.
		       Proceed anyway? (y,n)

               At which point I got spooked and bailed.
			sudo dmidecode --type 17 | grep Size

	       shows
			8x8192MB + 2048MB = 67584 MB

               but du on /dev gives 804K.
               Is that the problem? Can I overcome that /dev size?

***
               12    Did you try tmpfs? It is a filesystem in RAM, no need for ext2. mount -o size=16G -t
                     tmpfs none /mnt/tmpfs ? t-8ch Feb 27 '13 at 18:17
                     That worked! Thanks! But so far, not much speed-up: I think the tools I'm using to build
                     are still using the regular disk. I'll put more stuff on the ram disk. ? Frank Feb 27 '13
                     at 18:24
               2     Putting the tools themselves on the ramdisk shouldn't make much difference as the kernel
                     will cache them anyways in ram. ? t-8ch Feb 27 '13 at 18:28
               1     @goldilocks It's anecdotal evidence, but when compiling our Java projects with Maven,
                     there is a significant speedup when using a ramdisk. I would guess though that this is
                     more because of seek time than read time. ? SpellingD Feb 27 '13 at 18:54
               1     /dev/shm, actually /run/shm, can be used; it's almost always there. ? Camille Goudeseune 
                     Feb 17 '14 at 22:06

***		     
          The best way to create a ram disk on linux is tmpfs. It's a filesystem living in ram, so there is
          no need for ext2. You can create a tmpfs of 16Gb size with:
		mount -o size=16G -t tmpfs none /mnt/tmpfs

***		
              on my system, with nothing at all in /mnt, it says: ls: cannot access /mnt/tmpfs: No such file
          2   or directory mount: mount point /mnt/tmpfs does not exist. Is that something to worry about? If
              I simply mkdir /mnt/tmpfs, does that defeat the purpose (by creating tmpfs on the regular disk
              - please no flames, I'm a beginner here). ? Frank Feb 27 '13 at 19:23
              You need a mountpoint (directory) as target, so after you created this directory (you can use
          9   any directory, existing contents are shadowed) you can mount it with the command from the
              answer. ? t-8ch Feb 27 '13 at 19:26

***	      
        Linux is very efficient in using RAM. There is little surprise that you see little if any speedup
        with tmpfs. The largest pieces to read into memory (and thus able to slow the process down) are the
        tools (compiler, assembler, linker), and in a longish make they will be loaded into memory at startup
        and never leave it. What is left is reading in source (the writing out of the results won't slow you
        down, unless severely memory constrained). Again, comon header files will stay around, only the
        user's source will require reading. And that is unlikely to be more than a few megabytes. Creating a
	large RAMdisk (or even much use of tmpfs) can very well slow things down (by making the build memory
	constrained, the files on RAMdisk or on tmpfs can not be used directly from there).

***
              What! How can they not be used directly from there? ? Kazark Feb 28 '13 at 16:08

              They are in RAM, but not in a format that is directly usable. ? vonbrand Feb 28 '13 at 16:09

        1     Really! How so? (Pardon my slowness.) ? Kazark Feb 28 '13 at 16:34

              @Kazark, to handle executables in memory special data structures are used. As RAMdisks and tmpfs
        6     aren't in common use to store executables (RAMdisks are a remnant from the good old days of
              excruciatingly slow floppy disks and such, tmpfs is for stricty temporary data), nobody has
              considered it important enough to add the required ugly hacks. ? vonbrand Feb 28 '13 at 16:39
              I have tried running my rails code from a tmpfs (RAM) filesystem, and I did not see any
        5     difference at all. I was really hoping for a noticeable difference but I was disappointed by how
              awesome linux is. ? Khaja Minhajuddin Mar 4 '13 at 19:25

***	      
        The problem is that the maximum size of a ramdisk, more specifically of size of memory that can be
        accessed via the ramdisk driver is configured at compiletime, can be overwritten at boottime, but
        remains fixed once the kernel is loaded into memory. The default value is probably measured in
        Megabytes. If I recall correctly the memory for a ramdisk is reserved right when the driver is
        loaded, all ramdisks are the same size and there is are some 16 ramdisks by default. So not even you
        want a ramdisk size of 16G :-)

	As stated in the other answer, tmpfs is what you want to use. Further, you won't win a lot by having
	your entire OS in a ramdisk/tmpfs. Just copy your builddir to a tmpfs and do your compiling then. You
	may have to ensure that all temporary results are written to a location thats in the tmpfs as well.

***	
              They don't actually use any memory until you write things to them. The boot time limit is just
              the limit. Even after filling one you can free the memory back up with blockdev --flushbufs. ? 
              psusi Feb 28 '13 at 13:57
              @psusi: can you give us more information on that? I can only find statements mentioning that
              once claimed by the ramdisk memory is never reclaimed, e.g. in Documentation/blockdev/
              ramdisk.txt in the kernel sources. And on my answer: that file also says the ramdisk grows as
              memory is consumed so it is not all allocated at once. ? Bananguin Mar 1 '13 at 14:46
              What sort of information? You run the command and it frees the ram, assuming you don't still
              have it mounted anyhow. ? psusi Mar 1 '13 at 15:13
              How do you know that the command does what you say it does? Its man page doesn't confirm that
              and the documentation in the kernel source tree can be understood to contradict your
              information. ? Bananguin Mar 1 '13 at 20:40
        4     I read the source code, and verified it by trying it. ? psusi Mar 1 '13 at 20:50

***	
             To make a large ram disk after boot, with no messing around with kernel parameters, this seems
             to work. Use tmpfs, make a file, mount it via loop, and mount that via a filesystem:
		     mount -t tmpfs -o size=200M tmpfs temp/
		     cd temp/
		     dd if=/dev/zero of=disk.img bs=1M count=199
		     losetup /dev/loop0 disk.img
		     mkfs.ext4 /dev/loop0
		     mount /dev/loop0 temp2/

             Probably a bit of performance penalty going through multiple different layers... but at least it
             works.

***
                 OP the amount of RAM is expressed in MB. So all you need to enter there is 16384. And then
                 voila you'd be in business.


---
http://superuser.com/questions/45342/when-should-i-use-dev-shm-and-when-should-i-use-tmp

When should I use /dev/shm/ and when should I use /tmp?

                        When should I use /dev/shm/ and when should I use /tmp? Can I always rely on them
                        both being there on Unices?

***			
           /dev/shm is a temporary file storage filesystem, i.e., tmpfs, that uses RAM for the backing
           store.  It can function as a shared memory implementation that facilitates IPC.

           From Wikipedia:
               Recent 2.6 Linux kernel builds have started to offer /dev/shm as shared memory in the form of
               a ramdisk, more specifically as a world-writable directory that is stored in memory with a
               defined limit in /etc/default/tmpfs.  /dev/shm support is completely optional within the
               kernel config file.  It is included by default in both Fedora and Ubuntu distributions, where
               it is most extensively used by the Pulseaudio application. ????????????^(Emphasis added.)

           /tmp is the location for temporary files as defined in the Filesystem Hierarchy Standard, which is
           followed by almost all Unix and Linux distributions.

	   Since RAM is significantly faster than disk storage, you can use /dev/shm instead of /tmp for the
	   performance boost, if your process is I/O intensive and extensively uses temporary files.

           To answer your questions: No, you cannot always rely on /dev/shm being present, certainly not on
           machines strapped for memory. You should use /tmp unless you have a very good reason for using /
           dev/shm.

           Remember that /tmp can be part of the / filesystem instead of a separate mount, and hence can grow
           as required. The size of /dev/shm is limited by excess RAM on the system, and hence you're more
           likely to run out of space on this filesystem.

***	   
                 I will be using it to redirect output from a commands' standard error output to a file. Then
                 I will read this file and process it. I will be doing this several thousand times (it's part
                 of the condition of a loop construct). I thought memory would be nice in this case. But I
                 also want it to be portable. I guess I'll check if /dev/shm exists, use it if it does, or
                 fallback to /tmp. Does that sound good? ? Deleted Sep 23 '09 at 16:04
           1     I'd also add a check for the minimum size and current usage level of /dev/shm to guard
                 against inadvertently filling it up. ? nagul Sep 23 '09 at 21:01
                 Under Linux 2.6 and later the /dev/shm is required to be mounted for the POSIX shared memory
           3     system calls like shm_open() to work. In other words some programs will break if its not
                 mounted - so it should be. It is not just a RAM disk. So you should make sure some of /dev/
                 shm is free. ? EdH Nov 30 '12 at 21:07
                 There is no performance boost by using /dev/shm. /dev/shm is memory (tmpfs) backed by the
           2     disk (swap). /var/tmp is memory (disk cache) backed by the disk (on-disk filesystem). In
                 practice, performance is about the same (tmpfs has a slight edge but not enough to matter). /
                 tmp may be tmpfs or not depending on how the administrator configured it. There is no good
                 reason to use /dev/shm in your scripts. ? Gilles Jun 18 '13 at 7:13
                 @GaretClaborn There's plenty of good reasons to use memory backed by swap, but that's called
                 normal process memory. If you're using a file, it's called a filesystem, and all filesystems
                 are memory (cache), which is backed by swap if the filesystem is something like tmpfs.
           2     Allocating disk space between swap and other storage areas is typically in the real of the
                 administrator. If an application wants files that tend to remain in RAM, /tmp is the normal
                 location (with $TMPDIR to override). The choice to make /tmp backed by swap, other disk space
                 or nothing is the administrator's. ? Gilles Jun 5 '14 at 14:09

***		 
        Okay, here's the reality.

        Both tmpfs and a normal filesystem are a memory cache over disk.

        The tmpfs uses memory and swapspace as it's backing store a filesystem uses a specific area of disk,
        neither is limited in the size the filesystem can be, it is quite possible to have a 200GB tmpfs on a
        machine with less than a GB of ram if you have enough swapspace.

        The difference is in when data is written to the disk. For a tmpfs the data is written ONLY when
        memory gets too full or the data unlikely to be used soon. OTOH most normal Linux filesystems are
        designed to always have a more or less consistent set of data on the disk so if the user pulls the
        plug they don't lose everything.

        Personally, I'm used to having operating systems that don't crash and UPS systems (eg: laptop
        batteries) so I think the ext2/3 filesystems are too paranoid with their 5-10 second checkpoint
        interval. The ext4 filesystem is better with a 10 minute checkpoint, except it treats user data as
	second class and doesn't protect it. (ext3 is the same but you don't notice it because of the 5
	second checkpoint)

        This frequent checkpointing means that unnecessary data is being continually written to disk, even
        for /tmp.

        So the result is you need to create swap space as big as you need your /tmp to be (even if you have
        to create a swapfile) and use that space to mount a tmpfs of the required size onto /tmp.

        NEVER use /dev/shm.

        Unless, you're using it for very small (probably mmap'd) IPC files and you are sure that it exists
        (it's not a standard) and the machine has more than enough memory + swap available.

***	
            Agreed, except for the conclusion, "NEVER use /dev/shm". You want to use /dev/shm in cases where
            you don't want a file to be written to the disk at all, and you want to minimize disk i/o. For
            example, I need to download very large zip files from an FTP server, unzip them, and then import
        9   them into a database. I unzip to /dev/shm so that for both the unzip and the import operations
            the HDD only needs to perform half the operation, rather than moving back and forth between
            source and destination. It speeds up the process immensely. That's one example of many, but I
            agree that it's a niche tool. ? Nathan Stretch Dec 11 '14 at 23:13

***	    
       In descending order of tmpfs likelyhood:

       +++++++++++++++++++++++++++++++++++++++++++++
       + /dev/shm  + always tmpfs + Linux specific +
       +++++++++++++++++++++++++++++++++++++++++++++
       + /tmp      + can be tmpfs + FHS 1.0        +
       +++++++++++++++++++++++++++++++++++++++++++++
       + /var/tmp  + never tmpfs  + FHS 1.0        +
       +++++++++++++++++++++++++++++++++++++++++++++

       Since you are asking about a Linux specific tmpfs mountpoint versus a portably defined directory that
       may be tmpfs (depending on your sysadmin and what's default for your distro), your question has two
       aspects, which other answers have emphasized differently:
		1. When to use these directories, based on good practice
		2. When it is appropriate to use tmpfs

       ------------------------------------------------------------------------------------------------------
       
       Good practices
       Conservative edition (mixture of conventions from FHS and common use):
		 * When in doubt, use /tmp.
		 * Use /var/tmp for large data that may not easily fit in ram.
		 * Use /var/tmp for data that is beneficial to keep across reboots (like a cache).
		 * Use /dev/shm as a side-effect of calling shm_open(). The intended audience is data buffers of
		   bounded size that are overwritten frequently and endlessly.
		 * If still in doubt, provide a way for the user to override. For example, the mktemp program 
		   honors the TMPDIR environment variable.

       Pragmatic edition:
       Use /dev/shm when it is important to use tmpfs, /var/tmp when it is important not to, else /tmp.

       Where tmpfs excels
       fsync is a no-op on tmpfs. This syscall is the number one enemy of (IO) performance (and flash
       longevity, if you care about that), though if you find yourself using tmpfs (or eatmydata) just to
       defeat fsync, then you (or some other developer in the chain) are doing something wrong. It means that
       the transactions toward the storage device are unnecessarily fine grained for your purpose ? you are
       clearly willing to skip some savepoints for performance, as you have now gone to the extreme of
       sabotaging them all ? seldom the best compromise. Also, it is here in transaction performance land
       where some of the greatest benefits of having an SSD are ? any decent SSD is going to perform
       out-of-this-world compared to what a spinning disk can possibly take (7200 rpm = 120 Hz, if notihing
       else is accessing it), not to mention flash memory cards, which vary widely on this metric (not least
       because it is a tradeoff with sequential performance, which is what they are rated by, e.g. SD card
       class rating). So beware, developers with blazing fast SSDs, not to force your users into this use
       case!

       Wanna hear a ridiculous story? My first fsync lesson: I had a job that involved routinely "upgrading"
       a bunch of Sqlite databases (kept as testcases) to an ever-changing current format. The "upgrade"
       framework would run a bunch of scripts, making at least one transaction each, to upgrade one database.
       Of course, I upgraded my databases in parallel (8 in parallel, since I was blessed with a mighty 8
       core CPU). But as I found out, there was no parallelization speedup whatsoever (rather a slight hit)
       because the process was entirely IO bound. Hilariously, wrapping the upgrade framework in a script
       that copied each database to /dev/shm, upgraded it there, and copied it back to disk was like 100
       times faster (still with 8 in parallel). As a bonus, the PC was usable too, while upgrading databases.

       Where tmpfs is appropriate
       The appropriate use of tmpfs is to avoid unnecessary writing of volatile data. Effectively disabling
       writeback, like setting /proc/sys/vm/dirty_writeback_centisecs to infinity on a regular filesystem.

       This has very little to do with performance, and failing this is a much smaller concern than abusing
       fsync: The writeback timeout determines how lazily the disk content is updated after the pagecache
       content, and the default of 5 seconds is a long time for a computer ? an application can overwrite a
       file as frequently as it wants, in pagecache, but the content on disk is only updated about once every
       5 seconds. Unless the application forces it through with fsync, that is. Think about how many times an
       application can output a small file in this time, and you see why fsyncing every single one would be a
       much bigger problem.

       What tmpfs can not help you with
         * Read performance. If your data is hot (which it better be if you consider keeping it in tmpfs),
           you will hit the pagecache anyway. The difference is when not hitting the pagecache; if this is
           the case, go to "Where tmpfs sux", below.
         * Short lived files. These can live their entire lives in the pagecache (as dirty pages) before ever
           being written out. Unless you force it with fsync of course.

       Where tmpfs sux
       Keeping cold data. You might be tempted to think that serving files out of swap is just as efficient
       as a normal filesystem, but there are a couple of reasons why it isn't:

         * The simplest reason: There is nothing that contemporary storage devices (be it harddisk or flash
           based) loves more than reading fairly sequential files neatly organized by a proper filesystem;
           swapping in 4KiB blocks is unlikely to improve on that.
         * More significantly, the cost of swapping out: Tmpfs pages are dirty ? they need to be written
           somewhere (to swap) to be evicted from pagecache, as opposed to file backed clean pages that can
           be dropped instantly. This extra write penalty is felt by whoever else needed that memory, which
           is hard to know, but deserves to be part of your performance evaluation.

***	   
        Use /tmp/ for temporary files. Use /dev/shm/ when you want shared memory (ie, interprocess
        communication through files).

        You can rely on /tmp/ being there, but /dev/shm/ is a relatively recent Linux only thing.

***	
             Isn't there a performance aspect as well? As /dev/shm is most often mounted as a tmpfs volume
             and essentially a RAM-disk? ? Deleted Sep 23 '09 at 9:37
             You can also mount /tmp as a tmpfs filesystem, I do so on my netbook to speed some things up by
             reducing writes to the (slow) SSD. There are disadvantages to doing so, of course (mainly the
             RAM use, but my netbook has far more RAM than it generally needs anyway). ? David Spillett Sep
             23 '09 at 10:55
             For my specific case I would use it for a sort of process communication. I capture the output of
             standard error from an application and act on the contents (and I still need the standard output
             untouched, so I can't do any 1>/dev/null 2>&1. I would do this several thousand times so a tmpfs
             would be nice. However if I release the script I can't rely on tmpfs being used for /tmp as I
             think it's not that common. If it's more common for /dev/shm then that's better for me. But I'm
             looking for guidelines regarding portability etc. ? Deleted Sep 23 '09 at 16:00

***	     
           /dev/shm is used for shared virtual memory system specific device drivers and programs.

           If you are creating a program that requires a virtual memory heap that should be mapped to virtual
           memory. This goes double so if you need multiple processes or threads to be able to safely access
           that memory.

           The fact is that just because the driver uses a special version of tmpfs for it, doesn't mean you
	   should use it as a generic tmpfs partition. Instead, you should just create another tmpfs
	   partition if you want one for your temporary directory.

***	   
         In PERL, having 8GB minimum on any machine (all running Linux Mint), I am of what I think is a good
         habit of doing DB_File-based (data structure in a file) complex algorithms with millions of reads
         and writes using /dev/shm

         In other languages, not having gigether everywhere, to avoid the starts and stops in network
         transfer (working locally on a file that is located on a server in a client-server atmosphere),
         using a batch file of some type, I will copy the whole (300-900MB) file at once to /dev/shm, run the
         program with output to /dev/shm, write the results back to the server, and delete from /dev/shm

	 Naturally, if I had less RAM, I would not be doing this. Ordinarily, the in-memory file system of /
	 dev/shm reads as a size being one half of your available RAM. However, ordinary use of RAM is
         constant. So you really couldn't do this on a device with 2GB or less. To turn paraphrase to
         hyperbole, there is often things in RAM that even the system doesn't report well.

***	 
              (I think this is in the spirit of what was originally asked.) What I mean basically is that I'm
              comfortable using /dev/shm as a RAM disk as long as I have sufficient memory. If it is
              inefficient to do so somehow, that should not dissuade you from doing so, but should trigger a
              question like "How can I have a ram disk on linux?". The answer is /dev/shm ? David Grove Sep
              27 '15 at 21:44



---
https://forums.plex.tv/discussion/80100/plex-transcoding-on-ramdisk

Plex Transcoding on RAMDisk

Hey all,
I just recently stumbled on an article that made me feel like I was back in 1994 again and it was about
RAMDisk's and how much faster they are than SSD's.  I decided to try it out and indeed the RAMDisk I made
benchmarked at over 5 GB/s read and 6 GB/s write speeds (my SSD's get 250+ MB/s).  It got me thinking, I have
this home media server with an abundance of extra RAM (32 GB total) and I'd like to find a way to utilize
that.  Then I was wondering what exactly I'd do with a RAMDisk and I was considering if this would be a
viable transcoding drive for Plex.  I've Googled it, but haven't found much information especially as it
pertains to Plex.

Currently, I don't have many issues with Plex and network speed isn't one of them, so I'm not looking to
"speed" things up necessarily.  What I'm more interested in is whether or not this could solve the issue that
no matter what I do, I can't watch videos streamed by Plex and continue downloading and processing data in
the background.  I'm almost positive that the reason the video freezes on me when this happens is because I'm
running SATA drives and if I had SAS drives that would allow simultaneous read/write, I don't think this
would be an issue.  But, instead of purchasing a SAS controller card at a few hundred bucks and then spending
hundreds more on SAS drives, what if I just used all this extra RAM I have and make a RAMDisk?

I'd be very interested in hearing if anybody is currently using this, how well it works, etc.  I'd also like
to know how transcoding with Plex is done, more specifically, does it do a frame at a time and then push it
to the device?  How big would the RAMDisk need to be for it to be efficient?

I appreciate any input in this matter!

***
    It depends what transcoding you're wanting to speed up. Many transcoding operations are very CPU-limited,
    meaning the CPU is the bottleneck, not any other part of the system. Granted, this is more true for video
    than it is for music, but for the most part you could probably speed up your storage system and find it
    wouldn't matter one bit.

    It also might not make any sense to speed up the transcoding, if Plex's on the fly transcoding we're
    talking about. For example, if your server can transcode video at 2x realtime (say), it can already keep
    up with the demands of any single media player. Being able to transcode a single stream at 4x real time
    makes no difference whatsoever.

    Sequential transfer rates sound really impressive for RAM disks, but again they're pointless for this
    sort of application. The bitrate of a raw CD stream is 1440 kbits/sec, or 180KB/sec. Any compressed
    2-channel, 16-bit 44kHz audio will by definition be less than that. Your SSD can stream that data at
    least a million times faster than that; what's the point of a RAM disk at that point?

    Bluray's maximum bitrate is something in the region of 60Mbit/sec, or call it 7.5MB/sec. That's a paltry
    data rate for a modern computer to keep up with, and again by definition any compressed media will have a
    lower rate. A cheap software RAID array of mechanical disks can easily outdo that by a factor of 10 or
    more and your SSD is 33 times faster. For transcoding, there's no point in being able to shovel the data
    at the CPU any faster as it simply can't keep up.

    RAM disks are a pain in the bum, to be honest. They excel as temporary scratch space for working on data
    with next to no access time hit, but they're hopeless otherwise. As soon as you shut the machine down,
    the RAM disk's contents are lost, and have to be repopulated the next time the system starts up.

    IMHO, if you wanted to make use of the extra RAM on your home server, a better approach might be to get
    into virtualisation.

    I have a Vsphere whitebox at home ("whitebox" is the term given to a server that's built from
    comparatively cheap commodity hardware rather than anything that's expensive and on VMware's hardware
    compatibility lists). It's quad-core Ivy bridge (i5-3470) system with 16GB of RAM, a boot/datastore SSD
    and a handful of 3TB drives; it runs the free version of Vsphere 5.1 (register with VMware and they give
    you a key to turn the trial version into something permanent), but it could quite easily be built on top
    of Microsoft's Hyper-V or something open-source like Xen. I chose Vsphere as that's what I wanted to
    learn at the time.

    On there I run an instance of FreeNAS (with the onboard SATA controller handed over to that VM), several
    instances of Ubuntu 12.04 for various purposes (including one that runs Plex server), an instance of
    WinXP for a couple of bits and bobs I'm playing with that are Windows-only and a small cluster of virtual
    machines for work.

    Doing things this way means I've been free to pick and choose between functions. I like ZFS as a
    filesystem, so something BSD-based made a lot of sense and FreeNAS was simple to get up and running.
    However, a lot of other software runs on Linux; if I'd dedicated the entire machine to FreeNAS, I would
    have been out of luck in running any of that.

    Another nice side-effect is the segregation between functions. For example, nothing I do to the VM
    running Plex can affect my FreeNAS install; I can't bork my data just because I much up something in
    Ubuntu. Similarly, I can feel free to fire up new VMs for work or whatever that I can trash if I make a
    complete mess of them, and it affects nothing else.

    The hardware cost no more than a pre-built Atom NAS box, consumes about the same amount of idle power
    (Ivy Bridge idles at single-digit Watts; it's damn impressive) and offers far more flexibility in what I
    can do with the system. Instead of hoping everything plays nicely with one another under a single OS, I
    have over a dozen servers up and running that all do their own thing. The main cost over a NAS is my
    time; I had to spend the time to set everything up and so forth.

    The reason RAM is important is that every instance of a VM incurs some overhead. I'm running many
    instances of different (and sometimes the same) operating system, which isn't free. However, RAM is
    cheap, and ~100MB or so per instance of (say) Ubuntu server is something I'd planned for by building with
    16GB.

    The problem with virtualisation is it really has to be planned from the beginning. It's hard to take a
    machine that already has data on it and turn it into a VM factory (where does the data go in the
    meantime?). It's also hard to take just any old machine and turn it into a Vsphere system; there are
    certain hardware features that are very desirable for running Vsphere, and not all processors, chipsets
    and motherboards support them.

    Like I said, it cost me some time to set up, but it's opened up a world of possibilities that I wouldn't
    otherwise have.

***
    So I read your topic and decided to try it out. Just set the transcoder temporary directory to the
    mounted tmpfs and it's working smoothly.

    If  you're not familiar with ramfs / tmpfs here's a good read:
	http://www.thegeekstuff.com/2008/11/overview-of-ramfs-and-tmpfs-on-linux/#more-251

    I'd recommend a tmpfs.

    The transcoder, it seems, stores many .ts files in different directories for each show being watched. I
    believe it cleans up after itself after the video finishes. I of course made my initial tmpfs 1024m so
    it's taking the better part of a movie to see what happens when it reaches the limit. 

    I read tmpfs will pour over into swap once it's full, I really hope plex will just cleanup watched ts
    files out as the tmpfs fills up.

    @Smegheid - your post was an interesting read, the reason I'd continue to use a ram disk for transcoding
    is to remove unnecessary read and writes on my SSD.

    EDIT:
    well finally played to the 1GB mark, plex clears the tmp dir and my 1GB tmpfs is empty again, the
    transcoder kicks off and hangs as the player was still buffering & w/ cpu utilization but nothing written
    to the tmp dir.

    Some purge logic needs to be introduced to manage the transcoder's temp directory if we want to have a
    limited directory size.

    TBH it would be nice to not have to write the transcoded data back to disk if we don't have to. For
    multi-user setups that come under concurrent load this could prove to be quite useful.

***
    Thanks for the responses and very solid info!

    @Smegheid - I really don't have any need to speed things up per se.  I guess what I'm trying to say is my
    primary issue isn't speed at all, but instead it's doing too much disk access at one time or at least I
    think that's the problem :).  Here's my setup:  i7-3770k overclocked to 4.2Ghz, MSI Z77A-GD65 mobo, 32 GB
    of RAM, PowerColor 7870 Myst Edition, Corsair SATA III 128 GB SSD's (for the OS which includes my
    installed programs) and my video drive is a 3 TB USB 3.0 external Seagate 7200 rpm drive.  When I'm
    downloading something and I'm watching a video at the same time, it's fine until the download gets
    processed and is being extracted.  When that happens, my video that I'm transcoding freezes and won't
    recover unless I restart the video from scratch after the entire process is over.  So what I was
    wondering is if I use a RAMdisk as the transcoding destination, would this eliminate the freezing?
     Currently my transcoding drive is setup to transcode to the external 3 TB drive (which again is what I'm
    playing the video from as well as where the downloaded data is being written) although I've also had it
    transcoding on the SSD and there was no difference on the freezing.  I suppose this could also be an
    issue with the processor, but that just seems so unlikely considering I've ran this thing through
    seemingly much higher loads in benchmarking and it hasn't hiccuped or done anything weird.  It just seems
    like a disk access issue and so I was wondering if a RAMdisk would offload that stress on these drives.
     I do have a bunch of other hard drives, none RAIDed, just random old drives that I've thrown in here and
    I haven't tried putting the transcoder on any of those either to see if it would help.  I suppose I could
    do that to test it as well.  I just figured that since I overdid it on the RAM with 32 GB and I've maybe
    used at most, 10 GB of that at any given time that I could utilize that extra RAM with the RAMdisk and
    save some money instead of buying commercial-grade SAS equipment.  I definitely am interested in
    transcoding to multiple devices simultaneously at some point in the future as well.

    @Vulcanworlds - I never really thought about it from the perspective of conserving read/writes on the SSD
    to prolong longevity, but that's a good point. :)  I have a trial edition of Dataram's RAMdisk and it's a
    great program, but only allows for a max of 4 GB to a single disk.  I was wondering whether or not if
    you're transcoding a 13 GB MKV file if that would mean that I'd at least need a 13 GB RAMdisk to
    effectively transcode or if I'd need more RAM allocated for that.  I'm totally willing to purchase the
    software because it would only be $20 and I like the fact that you can have Windows mount the drive on
    startup, yet you can still throw away the data after each reboot and not commit it to the saved image
    file.  When you say some "purge logic needs to be introduced" I believe you're saying that it doesn't
    automatically purge the transcode directory, correct?  I noticed a while back when the transcoding was
    still setup on my SSD that WinDirStat showed a huge amount of data being eaten up by the transcoding
    directory, so I suppose that would be the case.  With this program, it'll theoretically throw away that
    data every reboot, but that doesn't help if the thing never reboots, which is the goal...  Maybe there's
    some Windows scripting that could be done on this to automate clearing the directory out?

    Again, thanks for the helpful info!  Whenever I get a plan figured out and have more time, I'd be happy
    to post my findings on here as well.

***
    RandomTask83RandomTask83 Posts: 81Members, Plex Pass Plex Pass
    September 2013
    I've done it in Archlinux and it doesn't really is worthy because the bottleneck would not be your HD , I
    think SATA III is 3 gbps IIRC so split it in a half, so 1,5 per up and down . Still enough. As plex uses
    ffmpeg for all things, and it relies on your CPU speed , that is the bottleneck, do a test, encode a file
    manually with ffmpeg with x264 and mp3 or AAC and that is exactly what plex does to transcode. What I'm
    frying to argue is that a bluray movie is 40 Mbps < 1,5 gbps so the drive wouldn't be a bottleneck issue
    AFAIK.

    If you want to preserve read/write cycles in SSD to prevent isolation then is a good choice but you need
    at least 16-32 Gb of ram, because tmpfs uses only half of your ram at most, so if plex is transcoding and
    tmpfs wants out of space then your transcoding session aka your movie and your joy will end suddenly.

***    
    @jvillanova - I wouldn't even call this a bottleneck issue really.  I have no speed concerns for wanting
    to do this.  It's a matter of figuring out if my HDD/SSD are under too much stress from read/writes and
    whether or not a RAMdisk will solve that and preserving read/write cycles would be an added benefit.  I
    understand that the CPU is what does the heavy lifting on the transcoding process :)  Also SATA III is 6
    Gbps theoretical.  On my SSD I see real-world speeds around 250 Mb/s (2 Gbps) sequential and 150 Mb/s
    (1.2 Gbps) random.

    Maybe I should just forget about it and build a 21 Tb SuperNAS like Linus did... 

***
    I'd think your bottleneck is the USB 3.0 drive, put one of your spare hdd's in the box and set that as
    the transcoding directory. With it being the only I/O for that disk, you should be better off than having
    the transcoding write to the external hdd. Or, use a spare hdd as the download destination or extract
    destination. It'd be better to have 1 disk be read from while writing to another rather than your current
    setup which is attempting to read 2 different files (the archive and the file to transcode) while
    attempting to write 2 different files (the transcoded data & the extracted data) all over the same USB
    port. Sounds like a good recipe for some I/O blockage.

    It sounds like you're running windows, have you watched your drive I/O at all? Task Manager > Performance
    Tab > Resource Monitor

    In my experience, my 3770k @ 4.5 seems to handle transcoding 2 different 1080p movies simultaneously to
    the same 1 TB wd black sata III drive haven't really tried anything past that, though cpu utilization
    wise it seems like maybe 3 is all I'd manage.

    Technically you're correct about the ramdisk size. I'm thinking plex doesn't clear the transcoding
    directory until the file finishes playing or (a guess) you navigate back to the library. So, if you had
    enough space in your ramdisk for the entire transcoded file it should work without freezing & crashing
    PMS. If you want, try it out with a 20gb ramdisk and leave the other 12gb for windows. As long as you've
    got a single user or multiple users aren't all watching movies you may be okay.

    The purge logic I mentioned is something that the plex developers should / would need to implement such
    that the transcoded directory only holds on to the previous few minutes you just watched rather than from
    the beginning of the file.

***
    @Vulcanworlds - I offloaded the transcoding directory for Plex to a spare drive that I just got.  It's a
    2 TB drive with a bunch of reallocated sectors, so if it fails, no big loss.  I'll have to see how this
    goes, but I'm a bit skeptical of whether or not this will help.  The logistics of how data flows in from
    my downloads and then becomes available in the Plex library might be impossible to solve.  I don't see
    how I can get the data downloaded, uncompressed, moved to the library, but yet play from that same
    library simultaneously without encountering issues.

    For instance, say you're using Sickbeard/Couchpotato/Sabnzbd and you have them all installed on your
    local disk C:.  By default they download to a directory on C:, but you can of course change that download
    destination to a different drive like V:\.  Now on V:\ you want Plex to read that as your library so you
    set it to the directory there.  Now say you have another drive that you set as your transcoding drive T:
    \.  Now you have something downloading and you watch something at the same time so it's downloading to V:
    \ and you're watching a transcoded file, it's still accessing the V:\ drive for your video and then
    trasncoding it to T:\ right?  Eventually the download gets done and it needs to extract and process, so
    it's doubling the reads and writing to V: as well, correct?  Even if you left Sab to download to your C:
    \, you'd still need to eventually move it at some point to the V:\ drive to make it available to the Plex
    library, which would still be causing stress on the disk if you're trying to watch something at the same
    time.  After the download completes, Sab needs to process the file and extract it, potentially repair it
    with parity, etc. or am I missing something?

    It just seems like there's no way to get around the disk that houses the library on it from being
    accessed by multiple sources both reading and writing simultaneously, which is why I was considering
    going with SAS drives for that, but of course those are so crazy expensive.  I thought potentially using
    the RAMdisk would at least eliminate the stress caused to the disk by transcoding, but I think the bigger
    issue is simultaneous read/writes...

    Ugh my head is spinning...

    EDIT:  What about RAID'ing the drive that the Plex library resides on?  If I put those in a RAID that
    would increase the bandwidth and cut the read/writes in half correct?

***
    A couple of things:

    1) Raid'ing your drives won't necessarily increase the available bandwidth to your media. It really
    depends on how it's setup.

    2) Unless I miss understood, it seems like your media and your download path are on your external hard
    drive. Is that correct?

    If so, you mentioned your external media drive is USB3, but is it plugged in to a USB3?

    If it is on an external hard drive, that could be your bottle neck/ From my experience with external
    drives (which is not much), transfers over USB usually deteriorate if multiple read/write are occuring on
    the same drive. 

    What I would suggest is that you download your media to your SSD and then manually transfer them to your
    external later, which is a hassle, but could be done automatically with a script.

***
    Running Plex Media Server from a machine with fairly intensive operations on the filesystem I really like
    running transcoder reads/writes on a tmpfs. I've been doing this for a little while now. It all runs
    smoothly, but I do need to restart playback if the tmpfs fills up. So that's for anything where the
    quality after transcoding is still fairly high and the file is lengthy. (say, a 1080p 3 hour movie gets
    transcoded to a high bitrate 720p)

    I see no reason why Plex would hang on to older parts of the transcoding session. It doesn't seem to
    influence seeking much. Can't really think of another reason they would be kept in the first place. Would
    love to hear more reasons if there are any, though.

    So yes, I think it would be great if space would get purged/recycled. Alternatively it would also be
    great to simply have a configurable space limit per transcoding session. (Or a collective limit for all
    sessions, but that seems more error-prone.)

***
    @thejixn0r - if it's setup in a RAID-0, then it's just striped across all drives, right?  That should
    theoretically increase the bandwidth unless I'm mistaken.  Guess it really doesn't matter since my
    largest media storage drives are external and I'd need to rip them out of the enclosures.  Also, yes it's
    on a 3 TB Seagate 7200 RPM USB 3.0 drive.  The drive typically sees sequential read/writes around 145 MB/
    s when transferring to/from my SSD.  Not sure about how multiple connections degrade it, but it
    definitely is degraded.  As soon as something decompresses in SAB, whatever I'm watching, whether
    transcoding or not freezes up.  I've even tried just watching something in VLC and that freezes as soon
    as something is unpacking in Sab.  After the unpacking is over, the video is still frozen and I have to
    exit out of VLC and restart the video, but then it's fine until it happens again.

    @pzt - it seems as if Plex doesn't remove transcoding data from the transcode directory and if that fills
    up, then I can see how it would cause problems.  It would need more intensive purge logic than just a
    flush though because if you only had the last few minutes as Vulcanworlds suggested, then wouldn't that
    eliminate the ability to effectively search/fast-forward/rewind?  Granted, that feature doesn't seem to
    work too keen for me right now anyways, especially if I'm streaming to a 360, but just throwing out
    questions.

    I haven't had the free time lately that I wish I'd have to really dig into this stuff, test, and post
    results.  I'm also out of money to buy any other equipment to benchmark against.

***
    Vulcanworlds, that's an interesting idea. I hadn't considered that plex would generate intermediate files
    in the process of transcoding; I figured it would just be a plain old stream that didn't actually land
    anywhere.

    In that case, tmpfs is perfect. It's pretty much designed for this sort of thing, for data that is
    short-lived and not permanent.

    One word of warning to anyone else reading that has a passing familiarity with Unix/Linux is that /tmp is
    no longer a RAM disk on a bunch of Linux distributions. Ubuntu (and a bunch of its derivatives) now leave
    /tmp on the root filesystem, which  can be a surprise. Even the server version of Ubuntu (certainly
    12.04) does this these days.

***
    I've been trying to cleanup the transcode dir with a cron task:
find /ramdisk/plex-transcode-* -type f \( -iname 'media-0*.ts' ! -iname "*.tmp" \) -mmin 0.33 -exec rm {} \;
find /ramdisk/plex-transcode-* -type f \( -iname 'chunk-0*' ! -iname "*.tmp" \) -mmin 0.33 -exec rm {} \;
     
    Which seems to delete the old files, but  somehow it's not a 100% stable fix

***    
    1st of all.
    Sorry for necroing this thread. :(

    2nd.
    I can not get Plex Media Server to run on a RamDisk. I am Using 32Gb Corsair Vengeance Pro 2400 DDR3 and
    DimmDrive.

    If I run Plex Media Server, from the RamDisk, the client can not connect to the server despite all manual
    workarounds.

    Running Plex Media Server, off of my HDD and Plex connects to my tablet without a hitch.

    I figured I did not have enough to post to create another thread.



---
http://www.linuxfocus.org/English/November1999/article124.html

[ !!! OLD !!! ]
How to use a Ramdisk for Linux

Introduction to RamDisk
This is a brief article about how to setup a RamDisk on a RedHat 6.0 system. It should be very similar for
other Linux distributions.

What is a RamDisk? A RamDisk is a portion of memory that you allocate to use as a partition. Or, in other
words, you are taking memory, pretending to treat it as a hard drive, and you are saving your files to it.
Why would you want to use a RamDisk? Well, if you know that certain files you have are constantly going to be
used, putting the files into memory will increase the performance of your computer since your memory is
faster than your hard drive. Things like web servers with lots of data can be sped up this way. Or, if you
are insane, and you have a PII 550 Mhz computer with 1 gig of memory and an old 500 meg hard drive, you can
use it just to increase your hard drive space. Then again, if you want an almost diskless machine, it might
not be that crazy afterall.

Here are some more resources to help you.
 1. http://metalab.unc.edu/LDP/HOWTO/Kernel-HOWTO.html
 2. http://metalab.unc.edu/LDP/HOWTO/mini/LILO.html
 3. /usr/src/linux/Documentation/ramdisk.txt

How to use RamDisk
Well, it is very easy to use a ramdisk. First of all, the default installation of RedHat 6.0 comes with
ramdisk support. All you have to do is format a ramdisk and then mount it to a directory. To find out all the
ramdisks you have available, do a "ls -al /dev/ram*". This gives you the preset ramdisks available to your
liking. These ramdisks don't actually grab memory until you use them somehow (like formatting them). Here is
a very simple example of how to use a ramdisk.
	# create a mount point:
	mkdir /tmp/ramdisk0
	# create a filesystem:
	mke2fs /dev/ram0
	# mount the ramdisk:
	mount /dev/ram0 /tmp/ramdisk0

Those three commands will make a directory for the ramdisk , format the ramdisk (create a filesystem), and
mount the ramdisk to the directory "/tmp/ramdisk0". Now you can treat that directory as a pretend partition!
Go ahead and use it like any other directory or as any other partition.
If the formatting of the ramdisk faild then you might have no support for ramdisk compiled into the Kernel.
The Kernel configuration option for ramdisk is CONFIG_BLK_DEV_RAM .

The default size of the ramdisk is 4Mb=4096 blocks. You saw what ramdisk size you got while you were running
mke2fs. mke2fs /dev/ram0 should have produced a message like this:
	mke2fs 1.14, 9-Jan-1999 for EXT2 FS 0.5b, 95/08/09
	Linux ext2 filesystem format
	Filesystem label=
	1024 inodes, 4096 blocks
	204 blocks (4.98%) reserved for the super user
	First data block=1
	Block size=1024 (log=0)
	Fragment size=1024 (log=0)
	1 block group
	8192 blocks per group, 8192 fragments per group
	1024 inodes per group

Running df -k /dev/ram0 tells you how much of that you can really use (The filesystem takes also some space):
	>df -k /dev/ram0
	Filesystem  1k-blocks  Used Available Use% Mounted on
	/dev/ram0        3963    13      3746   0% /tmp/ramdisk0

What are some catches? Well, when the computer reboots, it gets wiped. Don't put any data there that isn't
copied somewhere else. If you make changes to that directory, and you need to keep the changes, figure out
some way to back them up.    

Changing the size of the ramdisks
To use a ram disk you either need to have ramdisk support compiled into the Kernel or you need to compile it
as loadable module. The Kernel configuration option is CONFIG_BLK_DEV_RAM . Compiling the ramdisk a loadable
module has the advantage that you can decide at load time what the size of your ramdisks should be.

Okay, first the hard way. Add this line to your lilo.conf file:
	ramdisk_size=10000 (or ramdisk=10000 for old kernels)
and it will make the default ramdisks 10 megs after you type the "lilo" command and reboot the computer. Here
is an example of my /etc/lilo.conf file.
	boot=/dev/hda
	map=/boot/map
	install=/boot/boot.b
	prompt
	timeout=50
	image=/boot/vmlinuz
		label=linux
		root=/dev/hda2
		read-only
		ramdisk_size=10000

Actually, I got a little over 9 megs of usable space as the filesystem takes also a little space.

When you compile ramdisk support as loadable module then you can decide at load time what the size should be.
This is done either with an option line in the /etc/conf.modules file:
	options rd rd_size=10000

or as a command line parameter to ismod:
	insmod rd rd_size=10000

Here is an example which shows how to use the module:
 1. Unmount the ramdisk mounted in the previous chapter, umount /tmp/ramdisk0 .
 2. Unload the module (it was automatically loaded in the previous chapter), rmmod rd
 3. Load the ramdisk module and set the size to 20Mb, insmod rd rd_size=20000
 4. create a file system, mke2fs /dev/ram0
 5. mount the ramdisk, mount /dev/ram0 /tmp/ramdisk0

Example of how to use a RamDisk for a webserver.
Okay, here is an example of how to use 3 ramdisks for a webserver. Let us say you are 99% confident that your
default installation of Apache for RedHat 6.0 won't use more than 9 megs for its cgi-scripts, html, and
icons. Here is how to install one.
First, issue this command to move the real copy of the document root directory of your webserver to a
different place. Also, make the directories to mount the ramdisks .
	mv /home/httpd/ /home/httpd_real
	mkdir /home/httpd
	mkdir /home/httpd/cgi-bin
	mkdir /home/httpd/html
	mkdir /home/httpd/icons

Then, add these commands to the start procedure in your /etc/rc.d/init.d/httpd.init (or where ever the httpd
gets started on your system):
### Make the ramdisk partitions
	/sbin/mkfs -t ext2 /dev/ram0
	/sbin/mkfs -t ext2 /dev/ram1
	/sbin/mkfs -t ext2 /dev/ram2

### Mount the ramdisks to their appropriate places
	mount /dev/ram0 /home/httpd/cgi-bin
	mount /dev/ram1 /home/httpd/icons
	mount /dev/ram2 /home/httpd/html

### Copying real directory to ramdisks (the
### data on the ramdisks is lost after a reboot)
	tar -C /home/httpd_real -c . | tar -C /home/httpd -x

### After this you can start the web-server.

-------------------------------------------------------------------------------------------------------------
                              Webpages maintained by the LinuxFocus Editor team
                                              (C) Mark Nielsen
                                               LinuxFocus 1999
1999-11-01, generated by lfparser version 0.8
[ !!! OLD !!! ]filename: tmpfs_20150520.txt
https://wiki.archlinux.org/index.php/Tmpfs

tmpfs

   From ArchWiki
   Jump to: [12]navigation, [13]search

   [14]tmpfs is a temporary filesystem that resides in memory and/or your swap partition(s), depending
   on how much you fill it up. Mounting directories as tmpfs can be an effective way of speeding up
   accesses to their files, or to ensure that their contents are automatically cleared upon reboot.
   Note: When using [15]systemd, temporary files in tmpfs directories can be recreated at boot by using
   [16]tmpfiles.d.

Contents

     * [17]1 Usage
     * [18]2 Examples
     * [19]3 Disable automatic mount
     * [20]4 Troubleshooting
          + [21]4.1 Opening symlinks in tmpfs as root fails
     * [22]5 See also

Usage

   Some directories where tmpfs is commonly used are [23]/tmp, [24]/var/lock and [25]/var/run. Do not
   use it on [26]/var/tmp, because that folder is meant for temporary files that are preserved across
   reboots.

   [27]glibc 2.2 and above expects tmpfs to be mounted at /dev/shm for [28]POSIX shared memory. Mounting
   tmpfs at /dev/shm is handled automatically by [29]systemd, so manual configuration in [30]fstab is no
   longer necessary.

   Arch uses a tmpfs /run directory, with /var/run and /var/lock simply existing as symlinks for
   compatibility. It is also used for /tmp by the default systemd setup and does not require an entry in
   [31]fstab unless a specific configuration is needed.

   Generally, I/O intensive tasks and programs that run frequent read/write operations can benefit from
   using a tmpfs folder. Some applications can even receive a substantial gain by offloading some (or
   all) of their data onto the shared memory. For example, [32]relocating the Firefox profile into RAM
   shows a significant improvement in performance.

Examples

   By default, a tmpfs partition has its maximum size set to half your total RAM, but this can be
   customized. Note that the actual memory/swap consumption depends on how much you fill it up, as tmpfs
   partitions do not consume any memory until it is actually needed.

   To explicitly set a maximum size, in this example to override the default /tmp mount, use the size
   mount option:
/etc/fstab
tmpfs   /tmp         tmpfs   nodev,nosuid,size=2G          0  0

   Here is a more advanced example showing how to add tmpfs mounts for users. This is useful for
   websites, mysql tmp files, ~/.vim/, and more. It's important to try and get the ideal mount options
   for what you are trying to accomplish. The goal is to have as secure settings as possible to prevent
   abuse. Limiting the size, and specifying uid and gid + mode is very secure. For more information on
   this subject, follow the links listed in the [33]#See also section.
/etc/fstab
tmpfs   /www/cache    tmpfs  rw,size=1G,nr_inodes=5k,noexec,nodev,nosuid,uid=648,gid=648,mode=1700   0  0

   See the mount command man page for more information. One useful mount option in the man page is the
   default option. At least understand that.

   Reboot for the changes to take effect. Note that although it may be tempting to simply run mount -a
   to make the changes effective immediately, this will make any files currently residing in these
   directories inaccessible (this is especially problematic for running programs with lockfiles, for
   example). However, if all of them are empty, it should be safe to run mount -a instead of rebooting
   (or mount them individually).

   After applying changes, you may want to verify that they took effect by looking at /proc/mounts and
   using findmnt:
$ findmnt --target /tmp
TARGET SOURCE FSTYPE OPTIONS
/tmp   tmpfs  tmpfs  rw,nosuid,nodev,relatime

   The tmpfs can also be temporarily resized without the need to reboot, for example when a large
   compile job needs to run soon. In this case, you can run:
# mount -o remount,size=4G,noatime /tmp

Disable automatic mount

   Under [34]systemd, /tmp may be automatically mounted as a tmpfs even though you have no entry for
   that in your /etc/fstab.

   To disable the automatic mount, run:
# systemctl mask tmp.mount

   Files will no longer be stored in a tmpfs, but your block device instead. The /tmp contents will now
   be preserved between reboots, which you might not want. To regain the previous behavior and clean the
   /tmp folder automatically when restarting your machine, consider using tmpfiles.d(5):
/etc/tmpfiles.d/tmp.conf
# see tmpfiles.d(5)
# always enable /tmp folder cleaning
D! /tmp 1777 root root 0

# remove files in /var/tmp older than 10 days
D /var/tmp 1777 root root 10d

# namespace mountpoints (PrivateTmp=yes) are excluded from removal
x /tmp/systemd-private-*
x /var/tmp/systemd-private-*
X /tmp/systemd-private-*/tmp
X /var/tmp/systemd-private-*/tmp

Troubleshooting

Opening symlinks in tmpfs as root fails

   Considering /tmp is using tmpfs, change the current directory to /tmp, then create a file and create
   a symlink to that file in the same /tmp directory. If you try to open the file you created via the
   symlink, you will get a permission denied error. This is expected as /tmp [35]has the sticky bit set.

   This behaviour can be controlled via /proc/sys/fs/protected_symlinks or simply via sysctl: sysctl -w
   fs.protected_symlinks=0. See [36]Sysctl#Configuration to make this permanent.
   Warning: Changing this behaviour can lead to security issues! Disable it only if you know what you
   are doing.
filename: tmpfs_convert_h-264-AC3_mkv_to_h-264-AAC_mp4_20151222.txt
http://forums.fedoraforum.org/showthread.php?t=290195

[Howtp] Convert h.264/AC3 .mkv to h.264/AAC .mp4

If you are familiar with using BitTorrent to obtain certain ethically grey area high-def video files,
you are probably familiar with the Matroska Media Container, though you may only know it by the extension
.mkv at the end of the file.

Matroska Media Container can contain many different video/audio encodings, in fact the WebM standard
being pushed by Google for HTML5 video is actually a Matroska Media Container using VP8 stream for video
and Vorbis stream for audio.

Anyway, for certain video content commonly shared by BitTorrent, the video stream is h.264 and the audio
stream is Dolby 5.1 AC3.

There is nothing inherently wrong with that, that may be fine for you. Many media players can play
it. However, I like to repackage the .mkv into an MPEG-4 container, keeping the h.264 stream for video
but transcoding the Dolby stream to stereo encoded with AAC.

* My software media player of choice does not decode Dolby 5.1.
* The audio volume level seems to rarely be consistent. Transcoding gives me an opportunity to normalize
the audio so that I don't have to adjust the volume switch between media files.
* I can serve the media file over my LAN using html5 video
* My Bluray player on my TV usually plays the .mkv files just fine but sometimes complains they are
corrupt, my guess is sometimes the .mkv files break the standard in a way the Bluray player doesn't
compensate for. Repackaged as .mp4 it is never an issue.
* I never invested in surround sound, so I don't give a ***** about losing that.
* I'm paranoid that a software update to my bluray player will start checking checksum of media files
with a list known ethically grey area files and refuse to play if there's a match. Repackaging it means
a checksum match is highly unlikely.

The following shell script is what I have been using for years to turn .mkv files into .mp4 files. It
does assume the video stream is h.264 and the audio is Dolby AC3. That's not always the case and I should
fix it, but the torrent tracker I use, that is always the case.

Only "gotcha" is that it requires ffmpeg linked against libfaac and that can be hard to find pre-compiled,
you may have to build it from source. All other binaries used should be readily available for most
Linux distributions.

For Fedora 18, the rpmfusion repos have everything except ffmpeg linked against libfaac. But the src.rpm
will do it if you set a macro telling it to when you rebuild it.

Anyway, if turning .mkv into .mp4 is something that interests you (doesn't interest most), enjoy.

Code:
#!/bin/bash

base=`echo $1 |sed -e s?"\.mkv$"?""?`
CWD=`pwd`
TMP=`mktemp -d --tmpdir=/tmp`

### This is fragile and can be done better ###
n=`mkvinfo ${base}.mkv |grep "Track type" |grep -n "video" |cut -d":" -f1`
vidTrack=`echo "${n} - 1" |bc`
n=`mkvinfo ${base}.mkv |grep "Track type" |grep -n "audio" |cut -d":" -f1`
audTrack=`echo "${n} - 1" |bc`
### End Fragile ###

mkvextract tracks ${base}.mkv ${vidTrack}:${TMP}/${base}.h264
mkvextract tracks ${base}.mkv ${audTrack}:${TMP}/${base}.ac3

pushd ${TMP}
a52dec -o wavdolby ${base}.ac3 > tmp.wav
rm -f ${base}.ac3
# normalize the audio
sox --norm tmp.wav -b 16 ${base}.wav rate 48000 dither
rm -f tmp.wav
# package in mpeg-4
ffmpeg -i ${base}.h264 -i ${base}.wav -map 0:0 -map 1:0 \
  -vcodec copy -acodec libfaac -ab 128k -y -f mp4 ${base}.mp4
# fiv the MOOV atom for streaming and copy to original directory
qt-faststart ${base}.mp4 ${CWD}/${base}.mp4
popd
rm -rf ${TMP}
I call it mkv2mp4.sh and keep it in ~/bin/

Use it like this:

Code:
[user@host ~]$ mkv2mp4.sh Game.of.Thrones.S03E01.REPACK.720p.HDTV.x264-EVOLVE.mkv
End result will be identical filename but ending in .mp4 instead of .mkv

If /tmp is tmpfs - it is really fast.
Reply With Quote
  #2
Old 2nd April 2013, 07:32 PM
Skull One Offline
Registered User

Join Date: Jun 2010
Location: Lost...
Posts: 1,300
linuxredhatmozilla
Re: [Howtp] Convert h.264/AC3 .mkv to h.264/AAC .mp4
WAV files are usually big, and you use two of them.
I guess you can improve your script using pipes.
I do not have a sample to test, but maybe something like this is fine:
Code:
# ! ! ! ! ! ! ! ! !
#  - draft code -
# ! ! ! ! ! ! ! ! !

# use a fifo for ffmpeg input
mkfifo fifo.wav
ffmpeg -i ${base}.h264 -i fifo.wav -map 0:0 -map 1:0 \
  -vcodec copy -acodec libfaac -ab 128k -y -f mp4 ${base}.mp4 &

# do we need a sleep?
# sleep 1
# decode and normalise audio
a52dec -o wavdolby ${base}.ac3 | sox --norm - -b 16 fifo.wav rate 48000 dither

rm -f fifo.wav
__________________
:confused:
Reply With Quote
  #3
Old 2nd April 2013, 08:28 PM
FunkyRes Offline
Registered User

Join Date: Nov 2004
Posts: 238
windows_7chrome
Re: [Howtp] Convert h.264/AC3 .mkv to h.264/AAC .mp4
Wav files are big but I do it in /tmp and delete them when done.
piping the output of a52dec to sox though is a good idea.

I'm not sure why a sleep would be necessary, I run /tmp as tmpfs though so maybe there is a race condition
with one of the commands exiting before a disc arm has finished writing the file? I didn't ever see that
even when I ran /tmp on same platter drive as /
Reply With Quote
  #4
Old 3rd April 2013, 08:01 AM
Skull One Offline
Registered User

Join Date: Jun 2010
Location: Lost...
Posts: 1,300
linuxredhatmozilla
Re: [Howtp] Convert h.264/AC3 .mkv to h.264/AAC .mp4

Quote:
Originally Posted by FunkyRes  View Post
Wav files are big but I do it in /tmp and delete them when done.
piping the output of a52dec to sox though is a good idea.

I'm not sure why a sleep would be necessary, I run /tmp as tmpfs though so maybe there is a race condition
with one of the commands exiting before a disc arm has finished writing the file? I didn't ever see that
even when I ran /tmp on same platter drive as /
The file is a named pipe, so actually nothing is written in /tmp.
Since the first command is run in background, I think that the sleep is here to make sure that ffmpeg
is ready
and listening to the fifo when we start sending data.
__________________
:confused:
filename: tmpfs_moving-transcode-video-to-tmpfs_20151222.txt

https://lime-technology.com/forum/index.php?topic=37553.0

Topic: Plex: Guide to Moving Transcoding to RAM  (Read 9836 times)
Offline jonp
Administrator
Hero Member
*****
Posts: 4435
Lime Technology

Plex: Guide to Moving Transcoding to RAM
« on: January 07, 2015, 10:16:28 AM »
Hey everyone, just thought I'd put this up here after reading a syslog by another forum member and realizing a repeating pattern I've seen here where folks decide to let Plex create temporary files for transcoding on an array or cache device instead of in RAM.

Why should I move transcoding into RAM?  What do I gain?
In short, transcoding is both CPU and IO intensive.  Many write operations occur to the storage medium used for transcoding, and when using an SSD specifically, this can cause unnecessary wear and tear that would lead to SSD burnouts happening more quickly than is necessary.  By moving transcoding to RAM, you alleviate the burden from your non-volatile storage devices.  RAM isn't subject to "burn out" from usage like an SSD would be, and transcoding doesn't need nearly as much space in memory to perform as some would think.

How much RAM do I need for this?
A single stream of video content transcoded to 12mbps on my test system took up 430MB on the root ram filesystem.  The quality of the source content shouldn't matter, only the bitrate to which you are transcoding.  In addition, there are other settings you can tweak to transcoding that would impact this number including how many second of transcoding should occur in advance of being played.  Bottom line:  If you have 4GB or less of total RAM on your system, you may have to tweak settings based on how many different streams you intend on transcoding simultaneously.  If you have 8GB or more, you are probably in the safe zone, but obviously the more RAM you use in general, the less space will be available for transcoding.

How do I do this
There are two tweaks to be made in order to move your transcoding into RAM.  One is to the Docker Container you are running and the other is a setting from within the Plex web client itself.

Step 1:  Changing your Plex Container Properties
From within the webGui, click on "Docker" and click on the name of the PlexMediaServer container.  From here, add a new volume mapping:

/transcode to /tmp

Click "Apply" and the container will be started with the new mapping.

Step 2:  Changing the Plex Media Server to use the new transcode directory
Connect to the Plex web interface from a browser (e.g. http://tower:32400/web).  From there, click the wrench in the top right corner of the interface to get to settings.  Now click the "Server" tab at the top of this page.  On the left, you should see a setting called "Transcoder."  Clicking on that and then clicking the "Show Advanced" button will reveal the magical setting that let's you redirect the transcoding directory.  Type "/transcode" in there and click apply and you're all set.  You can tweak some of the other settings if desired to see if that improves your media streaming experience.

Thanks for reading and enjoy!
 Logged
Twitter: limetechnology
Wiki Documentation

Want to work with me one-on-one?  Schedule a services session today!
 Offline StevenD
Hero Member
*****
Posts: 1112

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #1 on: January 07, 2015, 10:42:23 AM »
Thanks. I will implement this when I get home  today.
 Logged



My 48TB Rig
 Offline clowrym
Full Member
***
 
Posts: 157

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #2 on: January 07, 2015, 11:35:38 AM »
I'll set this on my PLugin as well.....
 Logged
24 Bay Rack Mount AIC Server Case from TAMS
Supermicro - X7DB8-X
Unraid Pro
Currently 2TB parity 13TB Storage, SSD 240GB Cache 
16GB Ram
Powerconnect 6024 24Port 1Gb Switch
Second 10Bay Arca 3100r Testing Server
 Offline jumperalex
Hero Member
*****
Posts: 1749

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #3 on: January 07, 2015, 12:28:07 PM »
hmmm an interesting think piece. I can certainly see the benefits in some cases, but I would disagree that SSD wear is of any real concern. By your own stats you're only looking at writing 430mb to the SSD. I don't know how long that video is, so I started from your 12mbps value (a reasonable, but even high one) and turned that into 129,600 MB/day or 127GB/day if you were to run a single stream for 24 hours non-stop.

Per this article http://techreport.com/review/27062/the-ssd-endurance-experiment-only-two-remain-after-1-5pb it is reasonable to expect ~ 1PB of writes before seeing the start of errors (some drives a bit less, some drives MUCC more). But based on 1PB = 104,8576GB that means we can push a 127GB stream, non-stop, for 8256 days, or 22.6 years.

I mean sure, there are other write operations going on our SSD's but I think it is fair to say even if we doubled, or tripled our daily SSD write throughput we're still safe from hitting the endurance limits.

My real concern with transcoding to RAM is that pushing three or four simultaneous transcoded streams might stretch the memory limits of some people, though it is unlikely for most.

I will add that even in the Plex forums there are discussions about dealing with /tmp (iirc?) writing to ram instead of disc and being a problem. This is just the first link I found in a google search of "plex /tmp ram" https://forums.plex.tv/index.php/topic/119669-issues-with-tmp-on-ram/
 Logged
UnRAID 6.1.6 | FLASH: PNY Classic Attache 8GB | CASE: NZXT Elite 210 | MOBO: Asus M5A97 R2.0 2501 | MEMORY: Kingston 8GB 12800 ECC UDIMM | CPU: AMD FX-8320| Video: ATI Rage XL 8MB PCI | SATA Add-on: HighPoint Rocket 620 2-port SATAIII | PARITY: Hitachi Green 2TB | DATA: Hitachi Green 2TB x2, Seagate Green 2TB x1, WDC Black 640GB | CACHE/Docker: Crucial MX100 512GB | Dockers: Plex, Handbrake

Mount an SSD with Trim [DEPRECATED]
 Offline jonp
Administrator
Hero Member
*****
Posts: 4435
Lime Technology

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #4 on: January 07, 2015, 02:11:12 PM »
Quote from: jumperalex on January 07, 2015, 12:28:07 PM
hmmm an interesting think piece. I can certainly see the benefits in some cases, but I would disagree that SSD wear is of any real concern. By your own stats you're only looking at writing 430mb to the SSD. I don't know how long that video is, so I started from your 12mbps value (a reasonable, but even high one) and turned that into 129,600 MB/day or 127GB/day if you were to run a single stream for 24 hours non-stop.

Per this article http://techreport.com/review/27062/the-ssd-endurance-experiment-only-two-remain-after-1-5pb it is reasonable to expect ~ 1PB of writes before seeing the start of errors (some drives a bit less, some drives MUCC more). But based on 1PB = 104,8576GB that means we can push a 127GB stream, non-stop, for 8256 days, or 22.6 years.

I mean sure, there are other write operations going on our SSD's but I think it is fair to say even if we doubled, or tripled our daily SSD write throughput we're still safe from hitting the endurance limits.

My real concern with transcoding to RAM is that pushing three or four simultaneous transcoded streams might stretch the memory limits of some people, though it is unlikely for most.

I will add that even in the Plex forums there are discussions about dealing with /tmp (iirc?) writing to ram instead of disc and being a problem. This is just the first link I found in a google search of "plex /tmp ram" https://forums.plex.tv/index.php/topic/119669-issues-with-tmp-on-ram/

Good write up.  I'll concede that this may or may not be a big deal, but no one can dispute the fact that cells on flash memory do suffer from endurance wear and will eventually burn out whereas RAM does not.

As far as people having issues, I haven't researched anything on Plex forums or otherwise regarding that.  All I can say is that I personally have yet to have an issue from doing it this way and I've been doing this pretty much since Docker was implemented in beta 6.  If someone has undesirable effects on their system, of course they should change it back.

I just wanted to put this out there because I do it this way, don't have issues, and can see the logic in doing this especially for those that don't have an SSD in their system.

Edit:

Regarding RAM consumption for multiple streams, I think you're just restating what I mentioned in the OP.  If you don't have enough RAM, don't do this, or try tweaking settings.  I don't know how many folks out there ever have 4 concurrent streams in their home.  Guess it depends on the size of your family.  For my household of 3, it's pretty much no more than 1-2 streams at a time.  I also have 16-32GB of RAM in every system I own, so I'm "RAM rich" and don't worry about the # of streams.
« Last Edit: January 07, 2015, 02:12:59 PM by jonp »
 Logged
Twitter: limetechnology
Wiki Documentation

Want to work with me one-on-one?  Schedule a services session today!
 Offline jonp
Administrator
Hero Member
*****
Posts: 4435
Lime Technology

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #5 on: January 07, 2015, 02:16:12 PM »
Oh, and you should probably read through that link you posted.  I don't think that was a statement against using /tmp in general, but just one guys issue.  Also, the recommendation in there about "making it as large as your largest piece of media" is just nonsense.  The transcoder doesn't need that much space as it's default setting is to only transcode 60 second of content at a time.
 Logged
Twitter: limetechnology
Wiki Documentation

Want to work with me one-on-one?  Schedule a services session today!
 Offline StevenD
Hero Member
*****
Posts: 1112

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #6 on: January 07, 2015, 02:20:30 PM »
I got three transcoded streams running successfully.  Of course, I do have 16GB allocated to unRAID right now.
 Logged



My 48TB Rig
 Offline archedraft
Community Developer
Hero Member
*****
 
Posts: 1604

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #7 on: January 07, 2015, 02:25:20 PM »
I suppose this still applies if I have Plex running on an Ubuntu VM?
 Logged
| unRAID: 6.1.6 Plus :: MOBO: ASRock H77 Pro4-M :: CPU: Intel i7-3770s 3.10GHz :: RAM: G.SKILL Ripjaws X Series 16GB :: PSU: CORSAIR TX650 V2 650W |
| Parity: 4TB WD Green :: Cache: 750GB WD Black :: Data [7TB]: 4TB WD Green + 3TB WD Red |
| VMdisk: 250GB Samsung SSD 840 EVO [KVM] :: Windows 10 (Nvidia GeForce GTX 750 Ti & USB) :: Ubuntu Server 14.04 |
| VMdisk-2: 2TB Toshiba DT01ACA200 [KVM] :: OS X El Capitan :: OS X Yosemite (Radeon HD 6450 & USB) :: Windows 7 (Nvidia GeForce GTX 750 Ti & USB) |
 Offline jumperalex
Hero Member
*****
Posts: 1749

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #8 on: January 07, 2015, 02:57:52 PM »
Quote from: jonp on January 07, 2015, 02:16:12 PM
Oh, and you should probably read through that link you posted.  I don't think that was a statement against using /tmp in general, but just one guys issue.  Also, the recommendation in there about "making it as large as your largest piece of media" is just nonsense.  The transcoder doesn't need that much space as it's default setting is to only transcode 60 second of content at a time.

Yeah that was just one link from that search query. Others, like yours, had success though they generally admitted to not finding much benefit (usually related to stream initiation speed which was their goal). 

Its not a horrible idea, and you are right flash does eventually wear out. I just don't know that it is enough to matter or to drive change absent other benefits. Change for change sake and all that [shrug]. 

For sure, I'm glad to see you posting the method, regardless of use case, because it means Limetech is "on it" with regards to plex :)

archdraft: if you look at the link / search term I posted you'll see that it does indeed apply as an alternative method for general linux install.  You'll also see discussions about using ramfs vs tmpfs (iirc) and the possibility of rollover into swap if you run out of ram etc etc. That means you need to have swap of course; I do not but I also am not planning to change transcoding to ram right now as can probably tell.

StevenD: can you characterize the memory usage with those three streams? Keep in mind you need to let them run to completion to get the full story. Plex does not discard .ts files until the stream is stopped. So usage will grow and grow until that point.
 Logged
UnRAID 6.1.6 | FLASH: PNY Classic Attache 8GB | CASE: NZXT Elite 210 | MOBO: Asus M5A97 R2.0 2501 | MEMORY: Kingston 8GB 12800 ECC UDIMM | CPU: AMD FX-8320| Video: ATI Rage XL 8MB PCI | SATA Add-on: HighPoint Rocket 620 2-port SATAIII | PARITY: Hitachi Green 2TB | DATA: Hitachi Green 2TB x2, Seagate Green 2TB x1, WDC Black 640GB | CACHE/Docker: Crucial MX100 512GB | Dockers: Plex, Handbrake

Mount an SSD with Trim [DEPRECATED]
 Offline Kryspy
Hero Member
*****
 
Posts: 508
Assume I know Next to Nothing

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #9 on: January 07, 2015, 03:02:47 PM »
Been running fine for me since you suggested it a while back jonp :)  16GB of DDR3 1600 here.

Kryspy
 Logged
Case: Lian-Li PC Q-25
Motherboard: ASUS H87I-PLUS
CPU: Intel Core i5-4670 CPU @ 3.40GHz
Ram: 16 GB DDR3 RAM
Hard Drives: 3TB WD Red parity 2TB WD Red 1TB WD Green 1.5 WD Green 500 GB WD Green cache drive.
unRAID Server PLUS: 6.1.6
Media Players: Apple TV 4, Roku Streaming Stick
 Offline Pducharme
Community Developer
Sr. Member
*****
Posts: 480

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #10 on: January 07, 2015, 07:17:53 PM »
Implemented :)

Thanks for the tip.  I have 32GB ECC, so i guess i'm all good :)
 Logged
unRAID Server Pro :  V6 (6.1.4) Pro
10 x WD Red 3TB, 1 x WD Green 3TB (All in array (XFS) | 1 x WD Red 3TB (Parity)  |  1 x Samsung 840 Evo 512GB SSD (Cache Drive, BTRFS) | SanDisk Cruzer Fit 16GB USB 2.0 (Flash)

CASE: Lian-Li PC-A77F & 4 x iStarUSA BPN-DE350SS-RED 5-in-3 Module | MB: Supermicro  S10SLL-F-O | Memory: Samsung ECC (32GB) | CPU: Intel Xeon E3-1240v3 3.40GHz
PCI-e card:  2 x LSI MegaRAID 9240-8i Flashed | PSU: SeaSonic X Series X-850 GOLD| UPS: CyberPower 1350VA Pure Sine LCD UPS.
 Offline jonp
Administrator
Hero Member
*****
Posts: 4435
Lime Technology

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #11 on: January 07, 2015, 08:27:00 PM »
Quote from: archedraft on January 07, 2015, 02:25:20 PM
I suppose this still applies if I have Plex running on an Ubuntu VM?
Yes, but in that instance, you have a little more work to do.

option a:  use virtfs and mount /tmp to a path somewhere in the VM.  Then use that as your transcode directory following the guide I outline in step 2 in the original post.  Not sure how well this method would work exactly.

Option b: manually create a virtual disk under /tmp and mount that to the VM, then specify a path to that mount for transcoding.  Downside with this is you'd have to recreate the vdisk on every reboot (or script it in through the go file).

Option C:  get on the docker bangwagon and ditch that old VM to the curb!
 Logged
Twitter: limetechnology
Wiki Documentation

Want to work with me one-on-one?  Schedule a services session today!
 Offline ClunkClunk
Full Member
***
Posts: 231

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #12 on: January 07, 2015, 08:59:55 PM »
Quote from: jumperalex on January 07, 2015, 12:28:07 PM
I mean sure, there are other write operations going on our SSD's but I think it is fair to say even if we doubled, or tripled our daily SSD write throughput we're still safe from hitting the endurance limits.

Thank you. There's so much FUD about SSD wear that I see all sorts of goofy workarounds in lots of places. Your write up was quite succinct.
 Logged
29TB unRAID - 3.0GHz Core 2 Quad - Gigabyte GA-EP45-UD3P - 4GB RAM
 Offline extremeaudio
Sr. Member
****
Posts: 264

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #13 on: January 08, 2015, 02:26:12 AM »
I have 8GB of RAM. I would never do more than 4 simultaneous streams. Am I safe enough to just map the transcode directory to RAM and get going? I had instances with very old unRAID where my server has crashed many times and this was attributed by some members to suspected unusually high RAM usage by Plex, hence the skepticism.
 Logged
unRAID 6.0 beta6: Intel Quad Core Processor, Intel DG41 MoBo, 8GB RAM. Drives: 4 x WD 3TB for Data, 1x WD 3TB Parity. 500GB Cache Drive
 Offline jonp
Administrator
Hero Member
*****
Posts: 4435
Lime Technology

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #14 on: January 08, 2015, 04:51:21 AM »
Quote from: extremeaudio on January 08, 2015, 02:26:12 AM
I have 8GB of RAM. I would never do more than 4 simultaneous streams. Am I safe enough to just map the transcode directory to RAM and get going? I had instances with very old unRAID where my server has crashed many times and this was attributed by some members to suspected unusually high RAM usage by Plex, hence the skepticism.
I would think so but what other apps are you running and are you running VMs at all?


---
https://forums.plex.tv/discussion/80100/plex-transcoding-on-ramdisk

Plex Transcoding on RAMDisk
 praeixpraeix Posts: 5 September 2013 in General Discussions (Public)
Hey all,
 
I just recently stumbled on an article that made me feel like I was back in 1994 again and it was about RAMDisk's and how much faster they are than SSD's.  I decided to try it out and indeed the RAMDisk I made benchmarked at over 5 GB/s read and 6 GB/s write speeds (my SSD's get 250+ MB/s).  It got me thinking, I have this home media server with an abundance of extra RAM (32 GB total) and I'd like to find a way to utilize that.  Then I was wondering what exactly I'd do with a RAMDisk and I was considering if this would be a viable transcoding drive for Plex.  I've Googled it, but haven't found much information especially as it pertains to Plex.
 
Currently, I don't have many issues with Plex and network speed isn't one of them, so I'm not looking to "speed" things up necessarily.  What I'm more interested in is whether or not this could solve the issue that no matter what I do, I can't watch videos streamed by Plex and continue downloading and processing data in the background.  I'm almost positive that the reason the video freezes on me when this happens is because I'm running SATA drives and if I had SAS drives that would allow simultaneous read/write, I don't think this would be an issue.  But, instead of purchasing a SAS controller card at a few hundred bucks and then spending hundreds more on SAS drives, what if I just used all this extra RAM I have and make a RAMDisk?
 
I'd be very interested in hearing if anybody is currently using this, how well it works, etc.  I'd also like to know how transcoding with Plex is done, more specifically, does it do a frame at a time and then push it to the device?  How big would the RAMDisk need to be for it to be efficient?
 
I appreciate any input in this matter!
 
Tagged: transcoding
 SmegheidSmegheid Posts: 5 September 2013
It depends what transcoding you're wanting to speed up. Many transcoding operations are very CPU-limited, meaning the CPU is the bottleneck, not any other part of the system. Granted, this is more true for video than it is for music, but for the most part you could probably speed up your storage system and find it wouldn't matter one bit.
It also might not make any sense to speed up the transcoding, if Plex's on the fly transcoding we're talking about. For example, if your server can transcode video at 2x realtime (say), it can already keep up with the demands of any single media player. Being able to transcode a single stream at 4x real time makes no difference whatsoever.
Sequential transfer rates sound really impressive for RAM disks, but again they're pointless for this sort of application. The bitrate of a raw CD stream is 1440 kbits/sec, or 180KB/sec. Any compressed 2-channel, 16-bit 44kHz audio will by definition be less than that. Your SSD can stream that data at least a million times faster than that; what's the point of a RAM disk at that point?
Bluray's maximum bitrate is something in the region of 60Mbit/sec, or call it 7.5MB/sec. That's a paltry data rate for a modern computer to keep up with, and again by definition any compressed media will have a lower rate. A cheap software RAID array of mechanical disks can easily outdo that by a factor of 10 or more and your SSD is 33 times faster. For transcoding, there's no point in being able to shovel the data at the CPU any faster as it simply can't keep up.
RAM disks are a pain in the bum, to be honest. They excel as temporary scratch space for working on data with next to no access time hit, but they're hopeless otherwise. As soon as you shut the machine down, the RAM disk's contents are lost, and have to be repopulated the next time the system starts up.
IMHO, if you wanted to make use of the extra RAM on your home server, a better approach might be to get into virtualisation.
I have a Vsphere whitebox at home ("whitebox" is the term given to a server that's built from comparatively cheap commodity hardware rather than anything that's expensive and on VMware's hardware compatibility lists). It's quad-core Ivy bridge (i5-3470) system with 16GB of RAM, a boot/datastore SSD and a handful of 3TB drives; it runs the free version of Vsphere 5.1 (register with VMware and they give you a key to turn the trial version into something permanent), but it could quite easily be built on top of Microsoft's Hyper-V or something open-source like Xen. I chose Vsphere as that's what I wanted to learn at the time.
On there I run an instance of FreeNAS (with the onboard SATA controller handed over to that VM), several instances of Ubuntu 12.04 for various purposes (including one that runs Plex server), an instance of WinXP for a couple of bits and bobs I'm playing with that are Windows-only and a small cluster of virtual machines for work.
Doing things this way means I've been free to pick and choose between functions. I like ZFS as a filesystem, so something BSD-based made a lot of sense and FreeNAS was simple to get up and running. However, a lot of other software runs on Linux; if I'd dedicated the entire machine to FreeNAS, I would have been out of luck in running any of that.
Another nice side-effect is the segregation between functions. For example, nothing I do to the VM running Plex can affect my FreeNAS install; I can't bork my data just because I much up something in Ubuntu. Similarly, I can feel free to fire up new VMs for work or whatever that I can trash if I make a complete mess of them, and it affects nothing else.
The hardware cost no more than a pre-built Atom NAS box, consumes about the same amount of idle power (Ivy Bridge idles at single-digit Watts; it's damn impressive) and offers far more flexibility in what I can do with the system. Instead of hoping everything plays nicely with one another under a single OS, I have over a dozen servers up and running that all do their own thing. The main cost over a NAS is my time; I had to spend the time to set everything up and so forth.
The reason RAM is important is that every instance of a VM incurs some overhead. I'm running many instances of different (and sometimes the same) operating system, which isn't free. However, RAM is cheap, and ~100MB or so per instance of (say) Ubuntu server is something I'd planned for by building with 16GB.
The problem with virtualisation is it really has to be planned from the beginning. It's hard to take a machine that already has data on it and turn it into a VM factory (where does the data go in the meantime?). It's also hard to take just any old machine and turn it into a Vsphere system; there are certain hardware features that are very desirable for running Vsphere, and not all processors, chipsets and motherboards support them.
Like I said, it cost me some time to set up, but it's opened up a world of possibilities that I wouldn't otherwise have.
 VulcanworldsVulcanworlds Posts: 3Members, Plex Pass Plex Pass September 2013 edited September 2013
So I read your topic and decided to try it out. Just set the transcoder temporary directory to the mounted tmpfs and it's working smoothly.
If  you're not familiar with ramfs / tmpfs here's a good read:
http://www.thegeekstuff.com/2008/11/overview-of-ramfs-and-tmpfs-on-linux/#more-251
I'd recommend a tmpfs.
The transcoder, it seems, stores many .ts files in different directories for each show being watched. I believe it cleans up after itself after the video finishes. I of course made my initial tmpfs 1024m so it's taking the better part of a movie to see what happens when it reaches the limit. 
I read tmpfs will pour over into swap once it's full, I really hope plex will just cleanup watched ts files out as the tmpfs fills up.
@Smegheid - your post was an interesting read, the reason I'd continue to use a ram disk for transcoding is to remove unnecessary read and writes on my SSD.
EDIT:
well finally played to the 1GB mark, plex clears the tmp dir and my 1GB tmpfs is empty again, the transcoder kicks off and hangs as the player was still buffering & w/ cpu utilization but nothing written to the tmp dir.
Some purge logic needs to be introduced to manage the transcoder's temp directory if we want to have a limited directory size.
TBH it would be nice to not have to write the transcoded data back to disk if we don't have to. For multi-user setups that come under concurrent load this could prove to be quite useful.
 praeixpraeix Posts: 5 September 2013
Thanks for the responses and very solid info!
@Smegheid - I really don't have any need to speed things up per se.  I guess what I'm trying to say is my primary issue isn't speed at all, but instead it's doing too much disk access at one time or at least I think that's the problem .  Here's my setup:  i7-3770k overclocked to 4.2Ghz, MSI Z77A-GD65 mobo, 32 GB of RAM, PowerColor 7870 Myst Edition, Corsair SATA III 128 GB SSD's (for the OS which includes my installed programs) and my video drive is a 3 TB USB 3.0 external Seagate 7200 rpm drive.  When I'm downloading something and I'm watching a video at the same time, it's fine until the download gets processed and is being extracted.  When that happens, my video that I'm transcoding freezes and won't recover unless I restart the video from scratch after the entire process is over.  So what I was wondering is if I use a RAMdisk as the transcoding destination, would this eliminate the freezing?  Currently my transcoding drive is setup to transcode to the external 3 TB drive (which again is what I'm playing the video from as well as where the downloaded data is being written) although I've also had it transcoding on the SSD and there was no difference on the freezing.  I suppose this could also be an issue with the processor, but that just seems so unlikely considering I've ran this thing through seemingly much higher loads in benchmarking and it hasn't hiccuped or done anything weird.  It just seems like a disk access issue and so I was wondering if a RAMdisk would offload that stress on these drives.  I do have a bunch of other hard drives, none RAIDed, just random old drives that I've thrown in here and I haven't tried putting the transcoder on any of those either to see if it would help.  I suppose I could do that to test it as well.  I just figured that since I overdid it on the RAM with 32 GB and I've maybe used at most, 10 GB of that at any given time that I could utilize that extra RAM with the RAMdisk and save some money instead of buying commercial-grade SAS equipment.  I definitely am interested in transcoding to multiple devices simultaneously at some point in the future as well.
@Vulcanworlds - I never really thought about it from the perspective of conserving read/writes on the SSD to prolong longevity, but that's a good point.   I have a trial edition of Dataram's RAMdisk and it's a great program, but only allows for a max of 4 GB to a single disk.  I was wondering whether or not if you're transcoding a 13 GB MKV file if that would mean that I'd at least need a 13 GB RAMdisk to effectively transcode or if I'd need more RAM allocated for that.  I'm totally willing to purchase the software because it would only be $20 and I like the fact that you can have Windows mount the drive on startup, yet you can still throw away the data after each reboot and not commit it to the saved image file.  When you say some "purge logic needs to be introduced" I believe you're saying that it doesn't automatically purge the transcode directory, correct?  I noticed a while back when the transcoding was still setup on my SSD that WinDirStat showed a huge amount of data being eaten up by the transcoding directory, so I suppose that would be the case.  With this program, it'll theoretically throw away that data every reboot, but that doesn't help if the thing never reboots, which is the goal...  Maybe there's some Windows scripting that could be done on this to automate clearing the directory out?
Again, thanks for the helpful info!  Whenever I get a plan figured out and have more time, I'd be happy to post my findings on here as well.
 jqvillanovajqvillanova Posts: 81Members, Plex Pass Plex Pass September 2013
I've done it in Archlinux and it doesn't really is worthy because the bottleneck would not be your HD , I think SATA III is 3 gbps IIRC so split it in a half, so 1,5 per up and down . Still enough. As plex uses ffmpeg for all things, and it relies on your CPU speed , that is the bottleneck, do a test, encode a file manually with ffmpeg with x264 and mp3 or AAC and that is exactly what plex does to transcode. What I'm frying to argue is that a bluray movie is 40 Mbps < 1,5 gbps so the drive wouldn't be a bottleneck issue AFAIK. 

If you want to preserve read/write cycles in SSD to prevent isolation then is a good choice but you need at least 16-32 Gb of ram, because tmpfs uses only half of your ram at most, so if plex is transcoding and tmpfs wants out of space then your transcoding session aka your movie and your joy will end suddenly.

Cheers.
 praeixpraeix Posts: 5 September 2013
@jvillanova - I wouldn't even call this a bottleneck issue really.  I have no speed concerns for wanting to do this.  It's a matter of figuring out if my HDD/SSD are under too much stress from read/writes and whether or not a RAMdisk will solve that and preserving read/write cycles would be an added benefit.  I understand that the CPU is what does the heavy lifting on the transcoding process   Also SATA III is 6 Gbps theoretical.  On my SSD I see real-world speeds around 250 Mb/s (2 Gbps) sequential and 150 Mb/s (1.2 Gbps) random.
Maybe I should just forget about it and build a 21 Tb SuperNAS like Linus did... 
 VulcanworldsVulcanworlds Posts: 3Members, Plex Pass Plex Pass September 2013
praeix wrote on September 10 2013, 4:20 AM: »
Thanks for the responses and very solid info!
@Smegheid - I really don't have any need to speed things up per se.  I guess what I'm trying to say is my primary issue isn't speed at all, but instead it's doing too much disk access at one time or at least I think that's the problem .  Here's my setup:  i7-3770k overclocked to 4.2Ghz, MSI Z77A-GD65 mobo, 32 GB of RAM, PowerColor 7870 Myst Edition, Corsair SATA III 128 GB SSD's (for the OS which includes my installed programs) and my video drive is a 3 TB USB 3.0 external Seagate 7200 rpm drive.  When I'm downloading something and I'm watching a video at the same time, it's fine until the download gets processed and is being extracted.  When that happens, my video that I'm transcoding freezes and won't recover unless I restart the video from scratch after the entire process is over.  So what I was wondering is if I use a RAMdisk as the transcoding destination, would this eliminate the freezing?  Currently my transcoding drive is setup to transcode to the external 3 TB drive (which again is what I'm playing the video from as well as where the downloaded data is being written) although I've also had it transcoding on the SSD and there was no difference on the freezing.  I suppose this could also be an issue with the processor, but that just seems so unlikely considering I've ran this thing through seemingly much higher loads in benchmarking and it hasn't hiccuped or done anything weird.  It just seems like a disk access issue and so I was wondering if a RAMdisk would offload that stress on these drives.  I do have a bunch of other hard drives, none RAIDed, just random old drives that I've thrown in here and I haven't tried putting the transcoder on any of those either to see if it would help.  I suppose I could do that to test it as well.  I just figured that since I overdid it on the RAM with 32 GB and I've maybe used at most, 10 GB of that at any given time that I could utilize that extra RAM with the RAMdisk and save some money instead of buying commercial-grade SAS equipment.  I definitely am interested in transcoding to multiple devices simultaneously at some point in the future as well.
@Vulcanworlds - I never really thought about it from the perspective of conserving read/writes on the SSD to prolong longevity, but that's a good point.   I have a trial edition of Dataram's RAMdisk and it's a great program, but only allows for a max of 4 GB to a single disk.  I was wondering whether or not if you're transcoding a 13 GB MKV file if that would mean that I'd at least need a 13 GB RAMdisk to effectively transcode or if I'd need more RAM allocated for that.  I'm totally willing to purchase the software because it would only be $20 and I like the fact that you can have Windows mount the drive on startup, yet you can still throw away the data after each reboot and not commit it to the saved image file.  When you say some "purge logic needs to be introduced" I believe you're saying that it doesn't automatically purge the transcode directory, correct?  I noticed a while back when the transcoding was still setup on my SSD that WinDirStat showed a huge amount of data being eaten up by the transcoding directory, so I suppose that would be the case.  With this program, it'll theoretically throw away that data every reboot, but that doesn't help if the thing never reboots, which is the goal...  Maybe there's some Windows scripting that could be done on this to automate clearing the directory out?
Again, thanks for the helpful info!  Whenever I get a plan figured out and have more time, I'd be happy to post my findings on here as well.
I'd think your bottleneck is the USB 3.0 drive, put one of your spare hdd's in the box and set that as the transcoding directory. With it being the only I/O for that disk, you should be better off than having the transcoding write to the external hdd. Or, use a spare hdd as the download destination or extract destination. It'd be better to have 1 disk be read from while writing to another rather than your current setup which is attempting to read 2 different files (the archive and the file to transcode) while attempting to write 2 different files (the transcoded data & the extracted data) all over the same USB port. Sounds like a good recipe for some I/O blockage.
It sounds like you're running windows, have you watched your drive I/O at all? Task Manager > Performance Tab > Resource Monitor
In my experience, my 3770k @ 4.5 seems to handle transcoding 2 different 1080p movies simultaneously to the same 1 TB wd black sata III drive haven't really tried anything past that, though cpu utilization wise it seems like maybe 3 is all I'd manage.
Technically you're correct about the ramdisk size. I'm thinking plex doesn't clear the transcoding directory until the file finishes playing or (a guess) you navigate back to the library. So, if you had enough space in your ramdisk for the entire transcoded file it should work without freezing & crashing PMS. If you want, try it out with a 20gb ramdisk and leave the other 12gb for windows. As long as you've got a single user or multiple users aren't all watching movies you may be okay.
The purge logic I mentioned is something that the plex developers should / would need to implement such that the transcoded directory only holds on to the previous few minutes you just watched rather than from the beginning of the file.
 praeixpraeix Posts: 5 September 2013 edited September 2013
@Vulcanworlds - I offloaded the transcoding directory for Plex to a spare drive that I just got.  It's a 2 TB drive with a bunch of reallocated sectors, so if it fails, no big loss.  I'll have to see how this goes, but I'm a bit skeptical of whether or not this will help.  The logistics of how data flows in from my downloads and then becomes available in the Plex library might be impossible to solve.  I don't see how I can get the data downloaded, uncompressed, moved to the library, but yet play from that same library simultaneously without encountering issues.
For instance, say you're using Sickbeard/Couchpotato/Sabnzbd and you have them all installed on your local disk C:.  By default they download to a directory on C:, but you can of course change that download destination to a different drive like V:\.  Now on V:\ you want Plex to read that as your library so you set it to the directory there.  Now say you have another drive that you set as your transcoding drive T:\.  Now you have something downloading and you watch something at the same time so it's downloading to V:\ and you're watching a transcoded file, it's still accessing the V:\ drive for your video and then trasncoding it to T:\ right?  Eventually the download gets done and it needs to extract and process, so it's doubling the reads and writing to V: as well, correct?  Even if you left Sab to download to your C:\, you'd still need to eventually move it at some point to the V:\ drive to make it available to the Plex library, which would still be causing stress on the disk if you're trying to watch something at the same time.  After the download completes, Sab needs to process the file and extract it, potentially repair it with parity, etc. or am I missing something?
It just seems like there's no way to get around the disk that houses the library on it from being accessed by multiple sources both reading and writing simultaneously, which is why I was considering going with SAS drives for that, but of course those are so crazy expensive.  I thought potentially using the RAMdisk would at least eliminate the stress caused to the disk by transcoding, but I think the bigger issue is simultaneous read/writes...
Ugh my head is spinning...
EDIT:  What about RAID'ing the drive that the Plex library resides on?  If I put those in a RAID that would increase the bandwidth and cut the read/writes in half correct?
 thejinx0rthejinx0r Posts: 193Members, Plex Pass Plex Pass September 2013
praeix wrote on September 15 2013, 9:31 PM: »
@Vulcanworlds - I offloaded the transcoding directory for Plex to a spare drive that I just got.  It's a 2 TB drive with a bunch of reallocated sectors, so if it fails, no big loss.  I'll have to see how this goes, but I'm a bit skeptical of whether or not this will help.  The logistics of how data flows in from my downloads and then becomes available in the Plex library might be impossible to solve.  I don't see how I can get the data downloaded, uncompressed, moved to the library, but yet play from that same library simultaneously without encountering issues.
For instance, say you're using Sickbeard/Couchpotato/Sabnzbd and you have them all installed on your local disk C:.  By default they download to a directory on C:, but you can of course change that download destination to a different drive like V:\.  Now on V:\ you want Plex to read that as your library so you set it to the directory there.  Now say you have another drive that you set as your transcoding drive T:\.  Now you have something downloading and you watch something at the same time so it's downloading to V:\ and you're watching a transcoded file, it's still accessing the V:\ drive for your video and then trasncoding it to T:\ right?  Eventually the download gets done and it needs to extract and process, so it's doubling the reads and writing to V: as well, correct?  Even if you left Sab to download to your C:\, you'd still need to eventually move it at some point to the V:\ drive to make it available to the Plex library, which would still be causing stress on the disk if you're trying to watch something at the same time.  After the download completes, Sab needs to process the file and extract it, potentially repair it with parity, etc. or am I missing something?
It just seems like there's no way to get around the disk that houses the library on it from being accessed by multiple sources both reading and writing simultaneously, which is why I was considering going with SAS drives for that, but of course those are so crazy expensive.  I thought potentially using the RAMdisk would at least eliminate the stress caused to the disk by transcoding, but I think the bigger issue is simultaneous read/writes...
Ugh my head is spinning...
EDIT:  What about RAID'ing the drive that the Plex library resides on?  If I put those in a RAID that would increase the bandwidth and cut the read/writes in half correct?
A couple of things:
1) Raid'ing your drives won't necessarily increase the available bandwidth to your media. It really depends on how it's setup.
2) Unless I miss understood, it seems like your media and your download path are on your external hard drive. Is that correct?
If so, you mentioned your external media drive is USB3, but is it plugged in to a USB3?
If it is on an external hard drive, that could be your bottle neck/ From my experience with external drives (which is not much), transfers over USB usually deteriorate if multiple read/write are occuring on the same drive. 
What I would suggest is that you download your media to your SSD and then manually transfer them to your external later, which is a hassle, but could be done automatically with a script.
 pztpzt Posts: 1Members, Plex Pass September 2013
Vulcanworlds wrote on September 9 2013, 3:21 AM: »
Some purge logic needs to be introduced to manage the transcoder's temp directory if we want to have a limited directory size.
TBH it would be nice to not have to write the transcoded data back to disk if we don't have to. For multi-user setups that come under concurrent load this could prove to be quite useful.
Running Plex Media Server from a machine with fairly intensive operations on the filesystem I really like running transcoder reads/writes on a tmpfs. I've been doing this for a little while now. It all runs smoothly, but I do need to restart playback if the tmpfs fills up. So that's for anything where the quality after transcoding is still fairly high and the file is lengthy. (say, a 1080p 3 hour movie gets transcoded to a high bitrate 720p)
I see no reason why Plex would hang on to older parts of the transcoding session. It doesn't seem to influence seeking much. Can't really think of another reason they would be kept in the first place. Would love to hear more reasons if there are any, though.
So yes, I think it would be great if space would get purged/recycled. Alternatively it would also be great to simply have a configurable space limit per transcoding session. (Or a collective limit for all sessions, but that seems more error-prone.)
 praeixpraeix Posts: 5 September 2013
@thejixn0r - if it's setup in a RAID-0, then it's just striped across all drives, right?  That should theoretically increase the bandwidth unless I'm mistaken.  Guess it really doesn't matter since my largest media storage drives are external and I'd need to rip them out of the enclosures.  Also, yes it's on a 3 TB Seagate 7200 RPM USB 3.0 drive.  The drive typically sees sequential read/writes around 145 MB/s when transferring to/from my SSD.  Not sure about how multiple connections degrade it, but it definitely is degraded.  As soon as something decompresses in SAB, whatever I'm watching, whether transcoding or not freezes up.  I've even tried just watching something in VLC and that freezes as soon as something is unpacking in Sab.  After the unpacking is over, the video is still frozen and I have to exit out of VLC and restart the video, but then it's fine until it happens again.
@pzt - it seems as if Plex doesn't remove transcoding data from the transcode directory and if that fills up, then I can see how it would cause problems.  It would need more intensive purge logic than just a flush though because if you only had the last few minutes as Vulcanworlds suggested, then wouldn't that eliminate the ability to effectively search/fast-forward/rewind?  Granted, that feature doesn't seem to work too keen for me right now anyways, especially if I'm streaming to a 360, but just throwing out questions.
I haven't had the free time lately that I wish I'd have to really dig into this stuff, test, and post results.  I'm also out of money to buy any other equipment to benchmark against.
 SmegheidSmegheid Posts: 5 September 2013
Vulcanworlds, that's an interesting idea. I hadn't considered that plex would generate intermediate files in the process of transcoding; I figured it would just be a plain old stream that didn't actually land anywhere.
In that case, tmpfs is perfect. It's pretty much designed for this sort of thing, for data that is short-lived and not permanent.
One word of warning to anyone else reading that has a passing familiarity with Unix/Linux is that /tmp is no longer a RAM disk on a bunch of Linux distributions. Ubuntu (and a bunch of its derivatives) now leave /tmp on the root filesystem, which  can be a surprise. Even the server version of Ubuntu (certainly 12.04) does this these days.
 MirabisMirabis Posts: 67Members, Plex Pass Plex Pass May 24
I've been trying to cleanup the transcode dir with a cron task:
 
find /ramdisk/plex-transcode-* -type f \( -iname 'media-0*.ts' ! -iname "*.tmp" \) -mmin 0.33 -exec rm  {} \;
find /ramdisk/plex-transcode-* -type f \( -iname 'chunk-0*' ! -iname "*.tmp" \) -mmin 0.33 -exec rm {} \;
 
Which seems to delete the old files, but  somehow it's not a 100% stable fix



---
https://www.mythtv.org/wiki/Optimizing_Performance

Optimizing Performance
This HOWTO aims to collect the multitude of tips regarding optimizing performance of your system for use with MythTV.

Contents [hide] 
1 File Systems
1.1 Local File Systems
1.1.1 Put the database on a separate spindle
1.1.2 General Tips For Any File System
1.1.2.1 Combat Fragmentation
1.1.2.2 Disabling File Access Time Logging
1.1.2.3 Using "relatime" Mount Option
1.1.3 XFS-Specific Tips
1.1.3.1 Combat Fragmentation
1.1.3.2 Optimizing XFS on RAID Arrays
1.1.3.2.1 Examples
1.1.3.3 Further Information
1.2 Network File Systems
1.2.1 Disable NFS file attribute caching
1.2.2 NFS servers
1.3 RemoteFS
2 Devices
2.1 Capture Cards
2.2 Video Cards & Hardware Accelerated Video
2.2.1 VDPAU
2.2.2 VAAPI
2.2.3 CrystalHD
2.3 CPU / Processor
2.3.1 Clock Speed Throttling
3 Networks
3.1 Wireless Networks
3.2 Ethernet Full-Duplex Mode
4 Operating System
4.1 Kernel Configuration
4.1.1 Processor Family
4.1.2 IDE Controller
4.1.3 Preemptible Kernel
4.1.4 Timer Frequency
4.2 Realtime Threads
4.3 PCI Latency
4.4 RTC Maximum Frequency
4.5 Linux Distribution Selection
5 Other Software
5.1 MythTV
5.1.1 Multiple Machines
5.1.2 Frontend Playback Profiles
5.1.3 Recording Settings
5.1.4 Mythfilldatabase Scheduling
5.2 MySQL Database Tweaks
5.3 XORG CPU Hogging
5.4 Lightweight Window Managers
File Systems
Local File Systems
Put the database on a separate spindle
MythTV reads and writes large video files and it reads and writes small bits of metadata from a database. The small bits of metadata are accessed frequently enough that the seeks can more than halve the overall performance of access to the large video files MythTV also uses. Past the experimentation point it makes a lot of sense to allocate one small disk to the OS and the database and use a separate drive or drives for everything else. The database is generally under /var unless you have moved it.

General Tips For Any File System
Combat Fragmentation
Fragmentation happens when the data placement of files is not contiguous on disk, causing time-consuming head seeks when reading or writing a file.

MythTV recordings on disk can become quite fragmented, due to several factors, such as the fact that MythTV writes large files over a very long period of time, the fact that recording files may have drastically different sizes, and the fact that many MythTV systems have multiple capture cards--allowing for recording multiple shows at once. Note, also, that any time MythTV is recording multiple shows to a single filesystem (even if in different directories and/or in different Storage Groups), the recordings will necessarily be fragmented.

Configuring multiple local filesystems within MythTV's Storage Groups will allow MythTV to write recordings to separate filesystems, thereby minimizing fragmentation. Therefore, the best approach to combat fragmentation is to ensure each computer running mythbackend has at least as many local (and available) filesystems as capture cards. If using a combination of local and network-mounted filesystems, you may need to adjust the Storage Groups Weighting to cause MythTV to write to network-mounted filesystems (though doing so may negatively impact performance, meaning the use of a sufficient number of local filesystems or the use of only network-mounted filesystems is preferred). The availability of a filesystem is somewhat dependent on that filesystem having space available for writing (i.e. having 2 filesystems for 2 capture cards with one filesystem completely full and the other only half full will not help prevent fragmentation, though if both are full, autoexpiration should allow either to be used).

Fragmentation can be measured by the "filefrag" command on most any filesystem.

Disabling File Access Time Logging
Most filesystems log the access times of files. Generally this file metadata shouldn't be necessary, however, if for some strange reason you experience problems, then don't apply this tweak.

To disable the logging of file access times, add the "noatime" and "nodiratime" options to your /etc/fstab:

# 1.5 TB RAID 5 array. Large file optimization: 64m of prealloc
# NO logging of access times: improves performance
# NO block devices or suid progs allowed: improves security
/dev/md0    /terabyte   xfs defaults,noatime,nodiratime,nosuid,nodev,allocsize=64m 0   0
If you get something like the following, the mount option is not supported for your filesystem:

mount: wrong fs type, bad option, bad superblock on /dev/md0,
      missing codepage or helper program, or other error
      In some cases useful info is found in syslog - try
      dmesg | tail  or so

# dmesg | tail would return something like:
YOUR_FILESYSTEM_TYPE: unknown mount option [noatime].
Using "relatime" Mount Option
You may also wish to look into the "relatime" mount option to improve performance, but still have file access times updated. This is an alternative to using noatime and nodiratime. For more information on this (and related discussion), see: Linux: Replacing atime With relatime

XFS-Specific Tips
Combat Fragmentation
Under XFS, an additional command can be used to measure filesystem fragmentation: "xfs_bmap".

The xfs filesystem has a mount option which can help combat this fragmentation: allocsize

 allocsize=size
       Sets the buffered I/O end-of-file preallocation size when
       doing delayed allocation writeout (default size is 64KiB).
       Valid values for this option are page size (typically 4KiB)
       through to 1GiB, inclusive, in power-of-2 increments.
This can be added to /etc/fstab, for example:

   /dev/sd1  /video     xfs     defaults,allocsize=64m 0 0
This essentially causes xfs to speculatively preallocate 64m of space at a time for a file when writing, and can greatly reduce fragmentation. MythTV syncs the file it is writing to disk regularly to prevent the filesystem for freezing up for long periods of time writing large chunks of data that MythTV is generating and so preventing smooth simultaneous playback of the same or different file from that filesystem.

For files which are already heavily fragmented, the xfs_fsr command (from the xfsdump package) can be used to defragment individual files, or an entire filesystem.

Run the following command to determine how fragmented your filesystem is:

  xfs_db -c frag -r /dev/sda1
xfs_fsr with no parameters will run for two hours. The -t parameter specifies how long it runs in seconds. It keeps track of where it got up to and can be run repeatedly. It can be added to our crontab to periodically defragment your disk. Add the following to /etc/crontab:

  30 1 * * * root /usr/sbin/xfs_fsr -t 21600 >/dev/null 2>&1
to run it every night at 1:30 for 6 hours.

Don't forget to see the complete XFS_Filesystem wiki page that includes general info about XFS, defragmenting, disk checking and maintenance, etc.

Optimizing XFS on RAID Arrays
Some more RAID specific tweaks for XFS were found in this helpful article: Optimizing XFS on RAID Arrays. This section is a slightly reformatted version of that article. Please note the author of that article incorrectly used the term "block size" in some places when he really meant "stripe size" or "chunk size".

block size refers to the filesystem's unit of data transfer. This is set at format time for the filesystem. The default value is 4096 bytes (4 KiB), the minimum is 512, and the maximum is 65536 (64 KiB). XFS on Linux currently only supports pagesize or smaller blocks.
chunk size or stripe size refers to the RAID array's unit of data transfer. This is set during RAID array creation time for the array (in software raid, the --chunk=X option to mdadm). For me, mkfs.xfs complained when using chunk size=256KB, and block size=4096 bytes and specifying a sunit & swidth calculated using the block size. The values it mentions are correct if calculated using the chunk size. Therefore, this section assumes sunit & swidth calculated using chunk size.
If you are having trouble, try my script.

XFS has builtin optimizations for reading data from RAID arrays. These options can be specified at mkfs time or at mount time (you can even set them while the system is running using the mount -o remount command) and can affect the performance of your system.

There are two parameters for tweaking how XFS handles your RAID arrays (there are actually four, but you only need to use these two): sunit and swidth. sunit is the stripe unit and swidth is the stripe width. The stripe unit sits on a single disk while the stripe width spans the entire array; in this way the sunit is similar to the stripe size of your array.

Before you begin, you’ll need to know:

What type of RAID array you’re using
The number of disks in the array
The stripe size (aka the chunk size in software RAID).
For RAID{1,0,10} arrays, the number of “disks” is equal to the number of spindles.

For RAID{5,6} arrays, the number of disks is equal to N-1 for RAID5 and N-2 for RAID6, where N is the number of spindles.

If you guessed that the sunit is equal to your stripe size, you’re almost correct. The sunit is measured in 512-byte block units (from the mount man page), so for a 64kB stripe size your sunit=128, for 256kB use sunit=512.

As mentioned before, the swidth spans the entire array, but is also measured in 512-byte blocks, so you’ll want to multiply the number of disks calculated above by the sunit for your stripe size.

Note: If you have not formatted your xfs partition yet, you may set a blocksize in mkfs.xfs using the -b size=X option. The default is usually 4096 bytes (4 KiB) on most systems. (Remember blocksize is limited to your system's memory pagesize. blocksize <= pagesize). If already created, use the following command, and look for bsize=X in output:

# replace /dev/md0 with your device's name
xfs_info /dev/md0
Examples
Calculate the correct values for your system using these examples as a guideline.

A 4-disk RAID0 array with a 64kB stripe size will have a sunit of 128 and a swidth of 4*128=512. Your mkfs.xfs and mount commands would thus be:

#Note that you should only need to use one of these.  You may also add the sunit and swidth options to /etc/fstab to make the second one permanent.
mkfs.xfs -d sunit=128,swidth=512 /dev/whatever
mount -o remount,sunit=128,swidth=512
An 8-disk RAID6 array with a 256kB stripe size will have a sunit of 512 and a swidth of (8-2)*512=3072. Your commands would thus be:

mkfs.xfs -d sunit=512,swidth=3072 /dev/whatever
mount -o remount,sunit=512,swidth=3072
Further Information
If you are having trouble figuring this out (as I did at first), here is a useful bash script to help you. It only requires that you have the "bc" bash calculator program installed. To get it in Ubuntu, use "sudo apt-get install bc".

Put the following in a text editor, and chmod +x it, tweak values to match your system and run.

#!/bin/bash
BLOCKSIZE=4096 # Make sure this is in bytes
CHUNKSIZE=256  # Make sure this is in KiB
NUMSPINDLES=8
RAID_TYPE=6
RAID_DEVICE_NAME="/dev/md0" # Specify device name for your RAID device
FSLABEL="mythtv" # specify filesystem label for generating mkfs line here

case "$RAID_TYPE" in
0)
    RAID_DISKS=${NUMSPINDLES};
    ;;
1)
    RAID_DISKS=${NUMSPINDLES};
    ;;
10)
    RAID_DISKS=${NUMSPINDLES};
    ;;
5)
    RAID_DISKS=`echo "${NUMSPINDLES} - 1" | bc`;
    ;;
6)
    RAID_DISKS=`echo "${NUMSPINDLES} - 2" | bc`;
    ;;
*)
    echo "Please specify RAID_TYPE as one of: 0, 1, 10, 5, or 6."
    exit
    ;;
esac

SUNIT=`echo "${CHUNKSIZE} * 1024 / 512" | bc`
SWIDTH=`echo "$RAID_DISKS * ${SUNIT}" | bc`

echo "System blocksize=${BLOCKSIZE}"
echo "Chunk Size=${CHUNKSIZE} KiB"
echo "NumSpindles=${NUMSPINDLES}"
echo "RAID Type=${RAID_TYPE}"
echo "RAID Disks (usable for data)=${RAID_DISKS}"
echo "Calculated values:"
echo "Stripe Unit=${SUNIT}"
echo -e "Stripe Width=${SWIDTH}\n"
echo "mkfs line:"
echo -e "mkfs.xfs -b size=${BLOCKSIZE} -d sunit=${SUNIT},swidth=${SWIDTH} -L ${FSLABEL} ${RAID_DEVICE_NAME}\n"
echo "mount line:"
echo -e "mount -o remount,sunit=${SUNIT},swidth=${SWIDTH}\n"
echo "Add these options to your /etc/fstab to make permanent:"
echo "sunit=${SUNIT},swidth=${SWIDTH}"
Please refer to the following references for more details and other useful tweaks to improve the performance of your XFS filesystem:

Filesystem Performance Tweaking with XFS
Optimizing XFS on RAID Arrays
Network File Systems
Disable NFS file attribute caching
if you are using SMB (not CIFS), you can try the ttl option using "-o ttl=100" which should set your timeout lower than the default. The default is supposed to be 1000ms which equals 1 second, but one user has reported that setting ttl=100 corrected the issue for him, so SMB users can give it a try.

NFS servers
Ensure that your NFS server is running in 'async' mode (configured in /etc/exports). The default for many NFS servers is 'async', but recent versions of debian now default to 'sync', which can result in very low throughput and the dreaded "TFW, Error: Write() -- IOBOUND" errors. Example of setting async in /etc/exports:

/mnt/store      192.168.1.3/32(rw,async,udp)
There are a few other NFS mount options that can help, such as "intr", "nfsvers=3", "actimeo=0" , "noatime" and "tcp"/"udp". You can read the man pages for a more detailed description, but suggestions are below.

nfsvers=3 - This tells the client to use NFS v3, which is better than NFS v2. Of course, the server has to also support it.

actimeo=0 - disable this attribute caching to allow the frontend to see updates from the backend quicker. The problem has been seen where LiveTV fails to transition from one program to another. The cache file attribute prevents the frontend from opening the new file promptly. This also causes more load on the server if that is a issue.

tcp - This tells NFS to use TCP instead of UDP. It has been reported to improve performance for some, but has also caused repeated 3-5 second filesystem freeze-ups for others. If you have a network with only 1000-mbit clients or suffer performance problems with udp, you can try this. Poor performance with tcp may be improved by setting rsize and wsize to appropriate values (usuall 32KB).

udp - This tell NFS to use UDP instead of TCP. This is the traditional mechanism for NFS to utilize. For networks containing wifi or 100-mbit clients this is probably the best option. If you get video stuttering with either tcp or udp, try the other one.

rsize=32768,wsize=32768 - These tell your nfs client to use a particular block size, 32KB in this case. Modern NFS clients auto-negotiate a block size so this isn't really necessary for udp where a block size of 32KB is generally auto-negotiated. However with tcp the auto-negotiated block size can be too large resulting in very high latency. On Linux, check the output of /proc/mounts and if rsize or wsize depart very far from 32KB, you probably do want to set this.

intr - Makes I/O to a NFS mounted filesystem interuptable if the server is down. If not given the I/O becomes a uninteruptable sleep which causes the process to be impossible to kill until the server comes up again. This is a no-op on newer Linux kernels and you must use async instead.

async - Like intr this allows you to kill a process that has a file open on an unresponsive NFS server. On Linux this should always be used for A/V only volumes, unless you are using a kernel that still supports intr. Otherwise, you can end up with a permanently hung backend or frontend if a remote volume goes down for some reason.

soft - If the NFS server becomes unavailable the NFS client will generate "soft" errors instead of hanging. Some software will handle this well, other much less well. In the later case file corruption will result. For a file system solely used for A/V data this is safe and can avoid the a frontend or backend entering uninterruptible sleep.

Example /etc/fstab entry:

 server:/mythtv/rec0 /mythtv/rec0 nfs async,nfsvers=3,actimeo=0,tcp,rsize=32768,wsize=32768
 server:/mythtv/rec1 /mythtv/rec1 nfs async,nfsvers=3,actimeo=0,udp
RemoteFS
This is a fuse based file system that may be well suited to providing network access at remote frontends typically required. I have utilised this on my own setup and have found it to be faster / more responsive than the previous NFS setup I was using (regardless of options used above). NFS provides lots of sophisticated features for handling large numbers of users accessing via different types of network links and therefore latencies etc. these features are unlikely to be of any benefit for typically LAN connected systems especially as they are often just ro. The author appears to have done quite a lot of performance testing which you can see here https://sourceforge.net/apps/mediawiki/remotefs/index.php?title=Development:Performance_Tests Certainly worthy of testing / review IMHO.

Devices
Capture Cards
For backend machines, or machines that are a combination frontend/backend, the type of capture card used will impact performance. With a typical analog capture card, such as the popular bttv cards, the CPU must encode the raw video to MPEG-4 or RTjpeg on the fly. When watching live TV on a combination frontend/backend machine, the machine has to both encode AND decode the video stream simultaneously.

Luckily there are two options:

Hardware MPEG-2 Capture Cards, such as the popular Hauppauge PVR-150 and PVR-500.
Digital tuners, such as the pcHDTV HD-5500, which work with both OTA 8VSB signals as well as QAM for digital cable systems.
With cards of this type, the machine's CPU doesn't have to encode the incoming video. Instead, it simply receives the MPEG-2 stream from the card and dumps it to disk. This makes the recording process a simple operation, with relatively low resource usage.

Video Cards & Hardware Accelerated Video
Several options are available for accelerating video output:

VDPAU
VDPAU is currently NVIDIA-only for the time being, but provides for GPU-accelerated decode of MPEG-1, MPEG-2, H.264, and VC-1 bitstreams, as well as post-processing of decoded video including temporal and spatial deinterlacing, inverse telecine, and noise reduction.

VAAPI
CrystalHD
CPU / Processor
Clock Speed Throttling
There are several conditions in which your computer's CPU may be scaled down from its maximum clock speed:

A laptop or notebook has scaled down the CPU automatically due to being unplugged from an AC power source and running on the battery
The system has detected an unsafe thermal condition, and has scaled back the clock speed to avoid damage
The CPU speed has been configured incorrectly in the BIOS
The CPU speed has been manually configured to a lower speed at runtime
You can check your CPU's current operating frequency by running the command:

cat /proc/cpuinfo
If your system is slowing down because it is at its thermal limits, the only real option is to beef up your cooling capacity. This could be in the form of a larger heatsink, a larger fan, or even liquid cooling. A CPU that is incorrectly configured in the BIOS should be easy to check and easy to fix, but take care that you don't unintentionally overclock it in the process. Changing a manual control or overriding an automatic speed control will likely be distribution-dependent, or subject to your choice of adjustment tools.

Networks
Wireless Networks
While it is possible to run MythTV over a wireless network, you may find that you have better performance when using a wired connection. With a wireless connection, your bandwidth & latency are dependent on your distance from the access point, interference from other devices, the number of wireless users on the network, and the capabilities of your equipment at both ends. If you find that you have trouble with skips and/or dropouts when watching content on a wireless front end, it would be good to test the same setup with a wired connection to determine if the network is the problem.

Ethernet Full-Duplex Mode
Make sure that your ethernet adapters are running in full duplex mode. Check your current configuration with this command:

ethtool eth0
Typically both sides will be configured for autonegotiation by default and you will get the best possible connection automatically but there are conditions--typically involving old or buggy hardware--when this may not happen. The following can be used to disable autonegotiation and force a 100base-T network adapter into full duplex mode, when autonegotiation is failing.

ethtool -s eth0 speed 100 duplex full autoneg off
This problem can exhibit itself with "IOBOUND" errors in your logs.

Note: To use full-duplex mode, your network card must be connected to a switch (not a hub) and the switch must be configured to allow full-duplex operation (almost always the default) on the ports that are being used. By definition, a network switch supports full duplex operation and a network hub (sometimes referred to as a repeater) does not. If you are connecting to a hub, full-duplex operation will not be possible. Most switches support using 100base-T (Fast Ethernet) as well as 10base-T, while most hubs will only use 10base-T, and while a few 100base-T hubs (and 10base-T switches) do exist, they are quite rare. Gigabit switches can reliably be expected to handle both fast ethernet and normal ethernet connections in addition to the gigabit ethernet speeds.

Do not disable autonegotiation if things are currently working correctly. This will only create new problems, not prevent future ones. Forced connections can't advertise what they are capable of so the autonegotiating side must assume half-duplex. You will actually be creating a problem if the now forced connection was already full-duplex. It should be noted that most consumer-level switches and home routers do not support manual port configuration and this will result in them selecting a half-duplex connection if the remote end is no longer participating in connection negotiation. Nearly all of the time, using autonegotiation on all of the equipment will give you the best possible results. If you encounter problems with autonegotiation you can opt to manually configure settings for that device but it is highly recommended that you manually configure every piece of equipment on that segment as well.

Operating System
Kernel Configuration
If you're compiling your own kernel, you might want to try out the following options:

Processor Family
Ensure that the "Processor Family" (in "Processor Type and Features") is configured correctly.

IDE Controller
Ensure that the correct IDE controller is set (in "Device Drivers->ATA/ATAPI support->PCI IDE chipset support"). There is a generic IDE controller driver in the kernel that will handle many different chipsets, but it's performance is sub-par.

Preemptible Kernel
Kernel preemption allows high priority threads to interrupt even kernel operations -- this ensures the lowest possible latency when responding to important events. (Note: apparently some IVTV drivers show stability problems with a preemptible kernel.)

Timer Frequency
Increasing the scheduler's timer frequency to 1000Hz can reduce latency between multiple threads of execution (at a small cost to overall performance), e.g. when recording/playing multiple video streams. Generally you will want to pick 300Hz which is neatly divisible by both 50Hz (PAL) and 60Hz (NTSC) because of the frame rates involved in displaying your media.

On some machines you may hear an annoying high-pitched "whistle": reduce the frequency to 250Hz or lower to avoid this.

Realtime Threads
The mythfrontend & mythtv threads can be configured to run with "realtime" priorities - if the frontend is configured this way, and if sufficient privileges are available to the user running mythfrontend.

The HOWTO has an excellent section on how to set your system up to enable this (look for "Enabling real-time scheduling of the display thread.") You will also need to select "Enable Realtime Priority Threads" in the General Playback frontend setup dialogue.

Realtime threads can help smooth out video and audio, because the system scheduler gives very high priority to mythtv. For more information on how this works, see the Real-Time chapter in Robert Love's great Linux Kernel Development book.

PCI Latency
Incorrect or less-than-optimal settings of PCI Latency can cause performance-related problems. See the page PCI Latency

RTC Maximum Frequency
See Adjusting_the_RTC_Interrupt_Frequency

Linux Distribution Selection
At a more fundamental level, your choice of a Linux distribution can have a large impact on the overall performance of your Myth machine. Most "modern" distributions (Fedora, Ubuntu, etc) come with default installations intended to give the best initial user experience by providing support for scores of devices & programs, with automation wherever possible. The downside to this, is that these default installations have large kernels and large numbers of background processes running to support this usage.

While any distribution can be whittled down to meet a more focused need, it takes an effort to do so. An alternative approach, is to select a distribution such as Gentoo that provides you with a blank slate by default. This allows you to add only the components you need, ensuring a clean system with minimal effort.

Other Software
MythTV
Multiple Machines
One great feature of the MythTV architecture is that the recording and playback functions are split between two applications - the backend and the frontend. While they can both be run on the same machine, one of the easiest performance boosts is to simply split these tasks between two machines.

If desired, it is even possible to set up an additional backend machine to assist with recording and/or commercial flagging & transcoding tasks. This sort of arrangement may be beneficial if the backend machines are low-power (unable to keep up with the transcoding jobs), and is a good way to ensure that post-processing operations do not interfere with active recordings.

Frontend Playback Profiles
The choice of an appropriate playback profile can make a huge difference in the perceived performance of your MythTV frontend. The playback profile decides which video decoder will be used, how the on-screen display is rendered, and which video filters (deinterlacing, etc) are used. The playback profile also dictates how hardware acceleration is used, which is especially important on low-end PCs or machines processing HD content.

Recording Settings
Part of "optimizing" is determining what you actually need, not just making something as good as it can get.

For example, if you only watch recordings on your iPod and nothing else, you might as well configure your tuners to record TV at 320x240 resolution. Doing this will allow faster processing (commercial removal, etc), and the reduced file sizes will let you store more videos and copy them to/from your devices faster. Likewise, if you intend to burn a significant number of your recordings to DVD, you could save your backend a lot of work by saving your recordings directly to a DVD-compliant resolution, and in DVD-compliant MPEG-2 if your capture card supports it. (More information is available in the MythArchive page.)

Even if you watch your recordings from a frontend on your TV, you may still find it worthwhile to play with the recording settings. You may find that a lower audio bitrate eliminates hiss in the audio track, or that a lower resolution with an equivalent bitrate produces fewer objectionable video artifacts. Or, if your frontend isn't very powerful, or your network is congested, a lower bitrate may help make smooth video possible where it otherwise would not have been.

Mythfilldatabase Scheduling
By default, Mythfilldatabase runs automatically every 24 hours to keep your listings up to date. Mythfilldatabase is known for being able to saturate I/O systems (See Troubleshooting:Mythfilldatabase_IO_bottleneck), which can cause problems on heavily used or low-power backends. If you have recording or playback issues during the default script timeslot (2AM-5AM), you can manually adjust the script's schedule via the frontend setup menu to better suit your particular usage. Running the database and MySQL on a different machine is another way to alleviate these issues, to ensure consistent performance.

MySQL Database Tweaks
Taken from this thread in mythtv-users.

Add the following to the [mysqld] section of /etc/my.cnf to see improvements in database speed for MythTV as well as MythWeb. Check your default values using 'mysql> show global variables;'

key_buffer = 48M
max_allowed_packet = 8M
table_cache = 128 # this setting is deprecated in mysql 5.6.23 and will prevent mysql from starting
sort_buffer_size = 48M
net_buffer_length = 1M
thread_cache_size = 4
query_cache_type = 1
query_cache_size = 4M
There are also example my.cnf files included with your MySQL installation that have suggested values based on the amount of memory your system has. Information about them can be found in the MySQL documentation.

There is a great Perl script available at mysqltuner.pl. It will look through many of the MySQL server settings and report on variables that need to be changed. Hints on usage [1]

XORG CPU Hogging
Under some circumstances, X can use huge amounts of CPU. This may be fixed in some cases by increasing its priority above the base value of 0 (i.e. to a negative value). e.g. renice -2 [pid for X]. If you must renice a process, do so in small steps. Raising applications above the priority of mechanisms like kjournald or ksoftirqd can have adverse side-effects.

A second way of lowering Xorg CPU usage (especially when decoding is accelerated with XvMC or VDPAU) with nVidia cards, is to add

Option      "UseEvents"   "True"
to the Device section of your Xorg.conf. (warning: although this works well for watching hd content, it's considered unstable for 3D software like gaming, etc... )

Lightweight Window Managers
While KDE & Gnome provide for a nice user experience, they also bring along a lot of baggage which is unnecessary for a dedicated Myth machine. Switching to a lightweight window manager such as WindowMaker,Fluxbox, or Ratpoison will reduce startup times and give you more available system resources at runtime.



---
https://freeswitch.org/confluence/display/FREESWITCH/Performance+Testing+and+Configurations

Performance Testing and Configurations
Skip to end of metadata
Created by John Boteler, last modified on 2015.03.23 Go to start of metadata
About
Discussion of testing performance of FreeSWITCH™ with links to test scenario open source projects.
 
 Click here to expand Table of Contents
 
Measures of Performance
When people say performance it can mean a wide variety of things. In reality performance typically comes down to two bottle necks which are SIP, and RTP. These typically translate into calls per second and concurrent calls respectively. Additionally, high volume systems might experience bottlenecks with database servers running out of connections or even bandwidth when looking up account or configuration data.
Calls per Second (CPS)
Since calls per second is simply a measure of how many calls are being setup and torn down per second the limiting factor is the ability to process the SIP messages. Depending on the type of traffic you have this may or may not be a factor. There are a variety of components that can contribute to this bottleneck, FreeSWITCH and it's libraries being only some of them.
Concurrent Calls
Using modern hardware concurrent calls is less a limit of SIP but rather the RTP media streaming. This can further be broken down to available bandwidth and the packets per second. The theoretical limit on concurrent calls through a gigabit Ethernet port would be around 10,500 calls without RTCP, assuming G.711 codec and the link-level overheads. Theory is great and all, but in reality the kernel networking layer will be your limiting factor due to the packets per second of the RTP media stream.
Configurations
Recommended Configurations
A 64-bit CPU running a 64-bit operating system and a 64-bit version of FreeSWITCH is recommended. A bare metal system provides consistent, predictable performance and most importantly for real–time applications like this, a reliable kernel clock for RTP packet timing. With a virtual machine it is difficult to determine where any problems might originate and improper propagation of the hardware clock through the VM host to the guest operating system is not always available so the RTP tests will be rendered meaningless.
Debian linux is the recommended OS, since that's the OS used by the core developers and therefore the best tested. It will work on some other operating systems though.
Recommended ULIMIT settings
The following are recommended ulimit settings for FreeSWITCH when you want maximum performance.
ulimit -c unlimited # The maximum size of core files created.
ulimit -d unlimited # The maximum size of a process's data segment.
ulimit -f unlimited # The maximum size of files created by the shell (default option)
ulimit -i unlimited # The maximum number of pending signals
ulimit -n 999999    # The maximum number of open file descriptors.
ulimit -q unlimited # The maximum POSIX message queue size
ulimit -u unlimited # The maximum number of processes available to a single user.
ulimit -v unlimited # The maximum amount of virtual memory available to the process.
ulimit -x unlimited # ???
ulimit -s 240         # The maximum stack size
ulimit -l unlimited # The maximum size that may be locked into memory.
ulimit -a           # All current limits are reported.
 
Recommended SIP settings
Turn off every module you don't need that is not also needed by FreeSWITCH
Turn presence off in the profiles
libsofia only handles 1 thread per profile, so if that is your bottle neck use more profiles
mod_cdr_csv is slower than mod_xml_cdr
Reports of running more than a single instance of FreeSWITCH has helped.
Disable console logging when not needed - loglevel 0
Ethernet Tuning in linux

Beware buffer bloat
Prior to the bufferbloat guys coming in and talking to us there was a note in here that one should "set the buffers to maximum." That advice is WRONG on so many levels. To make a long story short, when you're doing real-time media like VoIP you absolutely do not want large buffers. On an unsaturated network link you won't notice anything, but when you have a saturated network the larger buffers will cause your RTP packets to be buffered instead of discarded.
 
So, what should your rx/tx queuelens be? Only you can know for sure, but it's good to experiment. Normally in linux it defaults to 1000. IF you are using a good traffic shaping qdisc (pfifo_fast or SFB or others) AND prioritizing udp/rtp traffic you can leave it alone, but it still is a good idea to lower it significantly for VoIP applications, depending on your workload and connectivity.
Don't use the default pfifo qdisc, regardless. It outputs packets in strict fifo order.
To see your current settings use ethtool:
ethtool settings
[root@server:~]# ethtool -g eth0
Ring parameters for eth0:
Pre-set maxima:
RX:             256
RX Mini:        0
RX Jumbo:       0
TX:             256
Current hardware settings:
RX:             256
RX Mini:        0
RX Jumbo:       0
TX:             128
 
 
These were the defaults on my Lenny install. If you needed to change it you can do this:
ethtool suggested changes
[root@server:~]# ethtool -G eth0 rx 128
[root@server:~]# ethtool -g eth0
Ring parameters for eth0:
Pre-set maxima:
RX:             256
RX Mini:        0
RX Jumbo:       0
TX:             256
Current hardware settings:
RX:             128
RX Mini:        0
RX Jumbo:       0
TX:             128
 
There is no one correct answer to what you should set the ring buffers to. It all depends on your traffic. Dave Taht from the Bufferbloat project reports that, based on his observations and experiences and papers such as http://www.cs.clemson.edu/~jmarty/papers/bittorrentBroadnets.pdf , that at present in home systems it is better to have no more than 32 unmanaged TX buffers on a 100Mbit network. It appears on my Lenny they are 32/64:
One man's buffer settings
[root@server:~]# ethtool -G eth0 rx 32 tx 32
[root@server:~]# ethtool -g eth0
Ring parameters for eth0:
Pre-set maximums:
RX:             256
RX Mini:        0
RX Jumbo:       0
TX:             256
Current hardware settings:
RX:             32
RX Mini:        0
RX Jumbo:       0
TX:             64
You'll note you can't with this driver reduce the TX buffer to a more optimum level!! This means that you will incur nearly a 10ms delay in the driver alone (at maximum packet size and load) on packets if you are on a 100Mbit network.
(similarly, a large TXQUEUELEN translates to lots of delay too)
On a gigibit network interface, the default TX queues and TXQUEUELEN are closer to usable, but still too large.
Having larger RX buffers is OK, to some extent. You need to be able to absorb bursts without packet loss. Tuning and observation of actual packet loss on the receive channel is a good idea.
And lastly, the optimum for TX is much lower on a 3Mbit uplink than a 100Mbit uplink. The debloat-testing kernel contains some Ethernet and wireless drivers that allow reducing TX to 1.
TCP/IP Tuning
For a server that is used primarily for VoIP, TCP Cubic (the default in Linux) can stress the network subsystem too much. Using TCP Vegas (which ramps up based on measured latency) is used by several FreeSWITCH users in production, as a "kinder, gentler" TCP for command and control functions.
To enable Vegas rather than Cubic you can, at boot:
modprobe tcp_vegas
echo vegas > /proc/sys/net/ipv4/tcp_congestion_control
--- Some interesting comments about tcp_vegas at http://tomatousb.org/forum/t-267882/
FreeSWITCH's core.db I/O Bottleneck
On a normal configuration, core.db is written to disk almost every second, generating hundreds of block-writes per second. To avoid this problem, turn /usr/local/freeswitch/db into an in-memory filesystem. If you use SSDs, it is CRITICAL that you move core.db to a RAM disk to prolong the life of the SSD.
On current FreeSWITCH versions you should use the documented "core-db-name" parameter in switch.conf.xml (simply restart FreeSwitch to apply the changes):
   <param name="core-db-name" value="/dev/shm/core.db" />
Otherwise you may create a dedicated in-memory filesystem, for example by adding the following to the end of /etc/fstab
fstab Example
#
# Example of /etc/fstab entry (using default size)
#
tmpfs /usr/local/freeswitch/db tmpfs defaults 0 0
#
# To specify a size for the filesystem use the appropriate mount(1) option:
#
# tmpfs /usr/local/freeswitch/db tmpfs defaults,size=4g 0 0
#
To use the new filesystem run the following commands (or the equivalent commands for your OS):
mount /usr/local/freeswitch/db
/etc/init.d/freeswitch restart
An alternative is to move the core DB into an ODBC database, which will move this processing to a DBMS which is capable of handling large numbers of requests far better and can even move this processing onto another server. Consider using freeswitch.dbh to take advantage of pooling.
Stress Testing
KNOW
IF YOU DO NOT UNDERSTAND HOW TO STRESS TEST PROPERLY THEN YOUR RESULTS WILL BE WORTHLESS.
Using SIPp is part dark art, part voodoo, part Santeria. YOU HAVE BEEN WARNED
When using SIPp's uas and uac to test FreeSWITCH, you need to make sure there is media back and forth. If you just send media from one sipp to another without echoing the RTP back (-rtp_echo), FS will timeout due to MEDIA_TIMEOUT. This is to avoid incorrect billing when one side has no media for more than certain period of time.
See Also
FreeSWITCH performance test on PC Engines APU — Stanislav Sinyagin tests FreeSWITCH™ transcoding performance with only one test machine
SSD Tuning for Linux — special considerations for systems using Solid State Drives for storage
SIPp — Open source test toll and traffic generator for SIP
SIPpy Cup — Ben Langfeld contributes this scenario generator for SIPp to simplify the creation of test profiles and especially compatible media
check_voip_call — Henry Huang contributes this project to work with Nagios
http://www.bandcalc.com/ — Bandwidth calculator for different codecs and use cases
http://www.cs.clemson.edu/%7Ejmarty/papers/bittorrentBroadnets.pdf — Paper on buffer sizing based on bittorrent usage



---
http://ubuntuforums.org/archive/index.php/t-1939900.html

[SOLVED] Moving Files to memory


Ceiber Boy
March 12th, 2012, 10:22 PM
is it possible to 'store' files in memory? Hear me out!

I have a shed of memory and a slow HDD, so when transcoding large files, instead of trying to read and write to the HDD at the same time would it be quicker to move all the data to memory? I could then use my quad core CPU flat out. Avoiding the bottleneck of my HDD.

i am expecting one of three answers:
1, no, stop being silly.
2, yes but the specific program your using would have to be writtern to do so.
3, yes, this is how

Thanks.:p
Dave_L
March 12th, 2012, 10:34 PM
You can easily create a RAM disk.

Example of creating a 512mb RAM disk with mount point /tmp/ram:


mkdir -p /tmp/ram
sudo mount -t tmpfs -o size=512M tmpfs /tmp/ram/
Ceiber Boy
March 13th, 2012, 11:24 AM
You can easily create a RAM disk.

Example of creating a 512mb RAM disk with mount point /tmp/ram:


mkdir -p /tmp/ram
sudo mount -t tmpfs -o size=512M tmpfs /tmp/ram/


Brilliant. Thanks you Dave
Bucky Ball
March 13th, 2012, 11:32 AM
Ceiber Boy and others please take note; from the heading of this page:


The Community Chat area is for lighthearted and enjoyable discussions, like you might find around a water cooler at work.

This forum is not intended for problem-solving. Please use appropriate forums in future. Thanks. ;)
