filename: tmpfs_moving-transcode-video-to-tmpfs_20151222.txt

https://lime-technology.com/forum/index.php?topic=37553.0

Topic: Plex: Guide to Moving Transcoding to RAM  (Read 9836 times)
Offline jonp
Administrator
Hero Member
*****
Posts: 4435
Lime Technology

Plex: Guide to Moving Transcoding to RAM
« on: January 07, 2015, 10:16:28 AM »
Hey everyone, just thought I'd put this up here after reading a syslog by another forum member and realizing a repeating pattern I've seen here where folks decide to let Plex create temporary files for transcoding on an array or cache device instead of in RAM.

Why should I move transcoding into RAM?  What do I gain?
In short, transcoding is both CPU and IO intensive.  Many write operations occur to the storage medium used for transcoding, and when using an SSD specifically, this can cause unnecessary wear and tear that would lead to SSD burnouts happening more quickly than is necessary.  By moving transcoding to RAM, you alleviate the burden from your non-volatile storage devices.  RAM isn't subject to "burn out" from usage like an SSD would be, and transcoding doesn't need nearly as much space in memory to perform as some would think.

How much RAM do I need for this?
A single stream of video content transcoded to 12mbps on my test system took up 430MB on the root ram filesystem.  The quality of the source content shouldn't matter, only the bitrate to which you are transcoding.  In addition, there are other settings you can tweak to transcoding that would impact this number including how many second of transcoding should occur in advance of being played.  Bottom line:  If you have 4GB or less of total RAM on your system, you may have to tweak settings based on how many different streams you intend on transcoding simultaneously.  If you have 8GB or more, you are probably in the safe zone, but obviously the more RAM you use in general, the less space will be available for transcoding.

How do I do this
There are two tweaks to be made in order to move your transcoding into RAM.  One is to the Docker Container you are running and the other is a setting from within the Plex web client itself.

Step 1:  Changing your Plex Container Properties
From within the webGui, click on "Docker" and click on the name of the PlexMediaServer container.  From here, add a new volume mapping:

/transcode to /tmp

Click "Apply" and the container will be started with the new mapping.

Step 2:  Changing the Plex Media Server to use the new transcode directory
Connect to the Plex web interface from a browser (e.g. http://tower:32400/web).  From there, click the wrench in the top right corner of the interface to get to settings.  Now click the "Server" tab at the top of this page.  On the left, you should see a setting called "Transcoder."  Clicking on that and then clicking the "Show Advanced" button will reveal the magical setting that let's you redirect the transcoding directory.  Type "/transcode" in there and click apply and you're all set.  You can tweak some of the other settings if desired to see if that improves your media streaming experience.

Thanks for reading and enjoy!
 Logged
Twitter: limetechnology
Wiki Documentation

Want to work with me one-on-one?  Schedule a services session today!
 Offline StevenD
Hero Member
*****
Posts: 1112

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #1 on: January 07, 2015, 10:42:23 AM »
Thanks. I will implement this when I get home  today.
 Logged



My 48TB Rig
 Offline clowrym
Full Member
***
 
Posts: 157

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #2 on: January 07, 2015, 11:35:38 AM »
I'll set this on my PLugin as well.....
 Logged
24 Bay Rack Mount AIC Server Case from TAMS
Supermicro - X7DB8-X
Unraid Pro
Currently 2TB parity 13TB Storage, SSD 240GB Cache 
16GB Ram
Powerconnect 6024 24Port 1Gb Switch
Second 10Bay Arca 3100r Testing Server
 Offline jumperalex
Hero Member
*****
Posts: 1749

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #3 on: January 07, 2015, 12:28:07 PM »
hmmm an interesting think piece. I can certainly see the benefits in some cases, but I would disagree that SSD wear is of any real concern. By your own stats you're only looking at writing 430mb to the SSD. I don't know how long that video is, so I started from your 12mbps value (a reasonable, but even high one) and turned that into 129,600 MB/day or 127GB/day if you were to run a single stream for 24 hours non-stop.

Per this article http://techreport.com/review/27062/the-ssd-endurance-experiment-only-two-remain-after-1-5pb it is reasonable to expect ~ 1PB of writes before seeing the start of errors (some drives a bit less, some drives MUCC more). But based on 1PB = 104,8576GB that means we can push a 127GB stream, non-stop, for 8256 days, or 22.6 years.

I mean sure, there are other write operations going on our SSD's but I think it is fair to say even if we doubled, or tripled our daily SSD write throughput we're still safe from hitting the endurance limits.

My real concern with transcoding to RAM is that pushing three or four simultaneous transcoded streams might stretch the memory limits of some people, though it is unlikely for most.

I will add that even in the Plex forums there are discussions about dealing with /tmp (iirc?) writing to ram instead of disc and being a problem. This is just the first link I found in a google search of "plex /tmp ram" https://forums.plex.tv/index.php/topic/119669-issues-with-tmp-on-ram/
 Logged
UnRAID 6.1.6 | FLASH: PNY Classic Attache 8GB | CASE: NZXT Elite 210 | MOBO: Asus M5A97 R2.0 2501 | MEMORY: Kingston 8GB 12800 ECC UDIMM | CPU: AMD FX-8320| Video: ATI Rage XL 8MB PCI | SATA Add-on: HighPoint Rocket 620 2-port SATAIII | PARITY: Hitachi Green 2TB | DATA: Hitachi Green 2TB x2, Seagate Green 2TB x1, WDC Black 640GB | CACHE/Docker: Crucial MX100 512GB | Dockers: Plex, Handbrake

Mount an SSD with Trim [DEPRECATED]
 Offline jonp
Administrator
Hero Member
*****
Posts: 4435
Lime Technology

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #4 on: January 07, 2015, 02:11:12 PM »
Quote from: jumperalex on January 07, 2015, 12:28:07 PM
hmmm an interesting think piece. I can certainly see the benefits in some cases, but I would disagree that SSD wear is of any real concern. By your own stats you're only looking at writing 430mb to the SSD. I don't know how long that video is, so I started from your 12mbps value (a reasonable, but even high one) and turned that into 129,600 MB/day or 127GB/day if you were to run a single stream for 24 hours non-stop.

Per this article http://techreport.com/review/27062/the-ssd-endurance-experiment-only-two-remain-after-1-5pb it is reasonable to expect ~ 1PB of writes before seeing the start of errors (some drives a bit less, some drives MUCC more). But based on 1PB = 104,8576GB that means we can push a 127GB stream, non-stop, for 8256 days, or 22.6 years.

I mean sure, there are other write operations going on our SSD's but I think it is fair to say even if we doubled, or tripled our daily SSD write throughput we're still safe from hitting the endurance limits.

My real concern with transcoding to RAM is that pushing three or four simultaneous transcoded streams might stretch the memory limits of some people, though it is unlikely for most.

I will add that even in the Plex forums there are discussions about dealing with /tmp (iirc?) writing to ram instead of disc and being a problem. This is just the first link I found in a google search of "plex /tmp ram" https://forums.plex.tv/index.php/topic/119669-issues-with-tmp-on-ram/

Good write up.  I'll concede that this may or may not be a big deal, but no one can dispute the fact that cells on flash memory do suffer from endurance wear and will eventually burn out whereas RAM does not.

As far as people having issues, I haven't researched anything on Plex forums or otherwise regarding that.  All I can say is that I personally have yet to have an issue from doing it this way and I've been doing this pretty much since Docker was implemented in beta 6.  If someone has undesirable effects on their system, of course they should change it back.

I just wanted to put this out there because I do it this way, don't have issues, and can see the logic in doing this especially for those that don't have an SSD in their system.

Edit:

Regarding RAM consumption for multiple streams, I think you're just restating what I mentioned in the OP.  If you don't have enough RAM, don't do this, or try tweaking settings.  I don't know how many folks out there ever have 4 concurrent streams in their home.  Guess it depends on the size of your family.  For my household of 3, it's pretty much no more than 1-2 streams at a time.  I also have 16-32GB of RAM in every system I own, so I'm "RAM rich" and don't worry about the # of streams.
« Last Edit: January 07, 2015, 02:12:59 PM by jonp »
 Logged
Twitter: limetechnology
Wiki Documentation

Want to work with me one-on-one?  Schedule a services session today!
 Offline jonp
Administrator
Hero Member
*****
Posts: 4435
Lime Technology

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #5 on: January 07, 2015, 02:16:12 PM »
Oh, and you should probably read through that link you posted.  I don't think that was a statement against using /tmp in general, but just one guys issue.  Also, the recommendation in there about "making it as large as your largest piece of media" is just nonsense.  The transcoder doesn't need that much space as it's default setting is to only transcode 60 second of content at a time.
 Logged
Twitter: limetechnology
Wiki Documentation

Want to work with me one-on-one?  Schedule a services session today!
 Offline StevenD
Hero Member
*****
Posts: 1112

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #6 on: January 07, 2015, 02:20:30 PM »
I got three transcoded streams running successfully.  Of course, I do have 16GB allocated to unRAID right now.
 Logged



My 48TB Rig
 Offline archedraft
Community Developer
Hero Member
*****
 
Posts: 1604

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #7 on: January 07, 2015, 02:25:20 PM »
I suppose this still applies if I have Plex running on an Ubuntu VM?
 Logged
| unRAID: 6.1.6 Plus :: MOBO: ASRock H77 Pro4-M :: CPU: Intel i7-3770s 3.10GHz :: RAM: G.SKILL Ripjaws X Series 16GB :: PSU: CORSAIR TX650 V2 650W |
| Parity: 4TB WD Green :: Cache: 750GB WD Black :: Data [7TB]: 4TB WD Green + 3TB WD Red |
| VMdisk: 250GB Samsung SSD 840 EVO [KVM] :: Windows 10 (Nvidia GeForce GTX 750 Ti & USB) :: Ubuntu Server 14.04 |
| VMdisk-2: 2TB Toshiba DT01ACA200 [KVM] :: OS X El Capitan :: OS X Yosemite (Radeon HD 6450 & USB) :: Windows 7 (Nvidia GeForce GTX 750 Ti & USB) |
 Offline jumperalex
Hero Member
*****
Posts: 1749

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #8 on: January 07, 2015, 02:57:52 PM »
Quote from: jonp on January 07, 2015, 02:16:12 PM
Oh, and you should probably read through that link you posted.  I don't think that was a statement against using /tmp in general, but just one guys issue.  Also, the recommendation in there about "making it as large as your largest piece of media" is just nonsense.  The transcoder doesn't need that much space as it's default setting is to only transcode 60 second of content at a time.

Yeah that was just one link from that search query. Others, like yours, had success though they generally admitted to not finding much benefit (usually related to stream initiation speed which was their goal). 

Its not a horrible idea, and you are right flash does eventually wear out. I just don't know that it is enough to matter or to drive change absent other benefits. Change for change sake and all that [shrug]. 

For sure, I'm glad to see you posting the method, regardless of use case, because it means Limetech is "on it" with regards to plex :)

archdraft: if you look at the link / search term I posted you'll see that it does indeed apply as an alternative method for general linux install.  You'll also see discussions about using ramfs vs tmpfs (iirc) and the possibility of rollover into swap if you run out of ram etc etc. That means you need to have swap of course; I do not but I also am not planning to change transcoding to ram right now as can probably tell.

StevenD: can you characterize the memory usage with those three streams? Keep in mind you need to let them run to completion to get the full story. Plex does not discard .ts files until the stream is stopped. So usage will grow and grow until that point.
 Logged
UnRAID 6.1.6 | FLASH: PNY Classic Attache 8GB | CASE: NZXT Elite 210 | MOBO: Asus M5A97 R2.0 2501 | MEMORY: Kingston 8GB 12800 ECC UDIMM | CPU: AMD FX-8320| Video: ATI Rage XL 8MB PCI | SATA Add-on: HighPoint Rocket 620 2-port SATAIII | PARITY: Hitachi Green 2TB | DATA: Hitachi Green 2TB x2, Seagate Green 2TB x1, WDC Black 640GB | CACHE/Docker: Crucial MX100 512GB | Dockers: Plex, Handbrake

Mount an SSD with Trim [DEPRECATED]
 Offline Kryspy
Hero Member
*****
 
Posts: 508
Assume I know Next to Nothing

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #9 on: January 07, 2015, 03:02:47 PM »
Been running fine for me since you suggested it a while back jonp :)  16GB of DDR3 1600 here.

Kryspy
 Logged
Case: Lian-Li PC Q-25
Motherboard: ASUS H87I-PLUS
CPU: Intel Core i5-4670 CPU @ 3.40GHz
Ram: 16 GB DDR3 RAM
Hard Drives: 3TB WD Red parity 2TB WD Red 1TB WD Green 1.5 WD Green 500 GB WD Green cache drive.
unRAID Server PLUS: 6.1.6
Media Players: Apple TV 4, Roku Streaming Stick
 Offline Pducharme
Community Developer
Sr. Member
*****
Posts: 480

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #10 on: January 07, 2015, 07:17:53 PM »
Implemented :)

Thanks for the tip.  I have 32GB ECC, so i guess i'm all good :)
 Logged
unRAID Server Pro :  V6 (6.1.4) Pro
10 x WD Red 3TB, 1 x WD Green 3TB (All in array (XFS) | 1 x WD Red 3TB (Parity)  |  1 x Samsung 840 Evo 512GB SSD (Cache Drive, BTRFS) | SanDisk Cruzer Fit 16GB USB 2.0 (Flash)

CASE: Lian-Li PC-A77F & 4 x iStarUSA BPN-DE350SS-RED 5-in-3 Module | MB: Supermicro  S10SLL-F-O | Memory: Samsung ECC (32GB) | CPU: Intel Xeon E3-1240v3 3.40GHz
PCI-e card:  2 x LSI MegaRAID 9240-8i Flashed | PSU: SeaSonic X Series X-850 GOLD| UPS: CyberPower 1350VA Pure Sine LCD UPS.
 Offline jonp
Administrator
Hero Member
*****
Posts: 4435
Lime Technology

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #11 on: January 07, 2015, 08:27:00 PM »
Quote from: archedraft on January 07, 2015, 02:25:20 PM
I suppose this still applies if I have Plex running on an Ubuntu VM?
Yes, but in that instance, you have a little more work to do.

option a:  use virtfs and mount /tmp to a path somewhere in the VM.  Then use that as your transcode directory following the guide I outline in step 2 in the original post.  Not sure how well this method would work exactly.

Option b: manually create a virtual disk under /tmp and mount that to the VM, then specify a path to that mount for transcoding.  Downside with this is you'd have to recreate the vdisk on every reboot (or script it in through the go file).

Option C:  get on the docker bangwagon and ditch that old VM to the curb!
 Logged
Twitter: limetechnology
Wiki Documentation

Want to work with me one-on-one?  Schedule a services session today!
 Offline ClunkClunk
Full Member
***
Posts: 231

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #12 on: January 07, 2015, 08:59:55 PM »
Quote from: jumperalex on January 07, 2015, 12:28:07 PM
I mean sure, there are other write operations going on our SSD's but I think it is fair to say even if we doubled, or tripled our daily SSD write throughput we're still safe from hitting the endurance limits.

Thank you. There's so much FUD about SSD wear that I see all sorts of goofy workarounds in lots of places. Your write up was quite succinct.
 Logged
29TB unRAID - 3.0GHz Core 2 Quad - Gigabyte GA-EP45-UD3P - 4GB RAM
 Offline extremeaudio
Sr. Member
****
Posts: 264

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #13 on: January 08, 2015, 02:26:12 AM »
I have 8GB of RAM. I would never do more than 4 simultaneous streams. Am I safe enough to just map the transcode directory to RAM and get going? I had instances with very old unRAID where my server has crashed many times and this was attributed by some members to suspected unusually high RAM usage by Plex, hence the skepticism.
 Logged
unRAID 6.0 beta6: Intel Quad Core Processor, Intel DG41 MoBo, 8GB RAM. Drives: 4 x WD 3TB for Data, 1x WD 3TB Parity. 500GB Cache Drive
 Offline jonp
Administrator
Hero Member
*****
Posts: 4435
Lime Technology

Re: Plex: Guide to Moving Transcoding to RAM
« Reply #14 on: January 08, 2015, 04:51:21 AM »
Quote from: extremeaudio on January 08, 2015, 02:26:12 AM
I have 8GB of RAM. I would never do more than 4 simultaneous streams. Am I safe enough to just map the transcode directory to RAM and get going? I had instances with very old unRAID where my server has crashed many times and this was attributed by some members to suspected unusually high RAM usage by Plex, hence the skepticism.
I would think so but what other apps are you running and are you running VMs at all?


---
https://forums.plex.tv/discussion/80100/plex-transcoding-on-ramdisk

Plex Transcoding on RAMDisk
 praeixpraeix Posts: 5 September 2013 in General Discussions (Public)
Hey all,
 
I just recently stumbled on an article that made me feel like I was back in 1994 again and it was about RAMDisk's and how much faster they are than SSD's.  I decided to try it out and indeed the RAMDisk I made benchmarked at over 5 GB/s read and 6 GB/s write speeds (my SSD's get 250+ MB/s).  It got me thinking, I have this home media server with an abundance of extra RAM (32 GB total) and I'd like to find a way to utilize that.  Then I was wondering what exactly I'd do with a RAMDisk and I was considering if this would be a viable transcoding drive for Plex.  I've Googled it, but haven't found much information especially as it pertains to Plex.
 
Currently, I don't have many issues with Plex and network speed isn't one of them, so I'm not looking to "speed" things up necessarily.  What I'm more interested in is whether or not this could solve the issue that no matter what I do, I can't watch videos streamed by Plex and continue downloading and processing data in the background.  I'm almost positive that the reason the video freezes on me when this happens is because I'm running SATA drives and if I had SAS drives that would allow simultaneous read/write, I don't think this would be an issue.  But, instead of purchasing a SAS controller card at a few hundred bucks and then spending hundreds more on SAS drives, what if I just used all this extra RAM I have and make a RAMDisk?
 
I'd be very interested in hearing if anybody is currently using this, how well it works, etc.  I'd also like to know how transcoding with Plex is done, more specifically, does it do a frame at a time and then push it to the device?  How big would the RAMDisk need to be for it to be efficient?
 
I appreciate any input in this matter!
 
Tagged: transcoding
 SmegheidSmegheid Posts: 5 September 2013
It depends what transcoding you're wanting to speed up. Many transcoding operations are very CPU-limited, meaning the CPU is the bottleneck, not any other part of the system. Granted, this is more true for video than it is for music, but for the most part you could probably speed up your storage system and find it wouldn't matter one bit.
It also might not make any sense to speed up the transcoding, if Plex's on the fly transcoding we're talking about. For example, if your server can transcode video at 2x realtime (say), it can already keep up with the demands of any single media player. Being able to transcode a single stream at 4x real time makes no difference whatsoever.
Sequential transfer rates sound really impressive for RAM disks, but again they're pointless for this sort of application. The bitrate of a raw CD stream is 1440 kbits/sec, or 180KB/sec. Any compressed 2-channel, 16-bit 44kHz audio will by definition be less than that. Your SSD can stream that data at least a million times faster than that; what's the point of a RAM disk at that point?
Bluray's maximum bitrate is something in the region of 60Mbit/sec, or call it 7.5MB/sec. That's a paltry data rate for a modern computer to keep up with, and again by definition any compressed media will have a lower rate. A cheap software RAID array of mechanical disks can easily outdo that by a factor of 10 or more and your SSD is 33 times faster. For transcoding, there's no point in being able to shovel the data at the CPU any faster as it simply can't keep up.
RAM disks are a pain in the bum, to be honest. They excel as temporary scratch space for working on data with next to no access time hit, but they're hopeless otherwise. As soon as you shut the machine down, the RAM disk's contents are lost, and have to be repopulated the next time the system starts up.
IMHO, if you wanted to make use of the extra RAM on your home server, a better approach might be to get into virtualisation.
I have a Vsphere whitebox at home ("whitebox" is the term given to a server that's built from comparatively cheap commodity hardware rather than anything that's expensive and on VMware's hardware compatibility lists). It's quad-core Ivy bridge (i5-3470) system with 16GB of RAM, a boot/datastore SSD and a handful of 3TB drives; it runs the free version of Vsphere 5.1 (register with VMware and they give you a key to turn the trial version into something permanent), but it could quite easily be built on top of Microsoft's Hyper-V or something open-source like Xen. I chose Vsphere as that's what I wanted to learn at the time.
On there I run an instance of FreeNAS (with the onboard SATA controller handed over to that VM), several instances of Ubuntu 12.04 for various purposes (including one that runs Plex server), an instance of WinXP for a couple of bits and bobs I'm playing with that are Windows-only and a small cluster of virtual machines for work.
Doing things this way means I've been free to pick and choose between functions. I like ZFS as a filesystem, so something BSD-based made a lot of sense and FreeNAS was simple to get up and running. However, a lot of other software runs on Linux; if I'd dedicated the entire machine to FreeNAS, I would have been out of luck in running any of that.
Another nice side-effect is the segregation between functions. For example, nothing I do to the VM running Plex can affect my FreeNAS install; I can't bork my data just because I much up something in Ubuntu. Similarly, I can feel free to fire up new VMs for work or whatever that I can trash if I make a complete mess of them, and it affects nothing else.
The hardware cost no more than a pre-built Atom NAS box, consumes about the same amount of idle power (Ivy Bridge idles at single-digit Watts; it's damn impressive) and offers far more flexibility in what I can do with the system. Instead of hoping everything plays nicely with one another under a single OS, I have over a dozen servers up and running that all do their own thing. The main cost over a NAS is my time; I had to spend the time to set everything up and so forth.
The reason RAM is important is that every instance of a VM incurs some overhead. I'm running many instances of different (and sometimes the same) operating system, which isn't free. However, RAM is cheap, and ~100MB or so per instance of (say) Ubuntu server is something I'd planned for by building with 16GB.
The problem with virtualisation is it really has to be planned from the beginning. It's hard to take a machine that already has data on it and turn it into a VM factory (where does the data go in the meantime?). It's also hard to take just any old machine and turn it into a Vsphere system; there are certain hardware features that are very desirable for running Vsphere, and not all processors, chipsets and motherboards support them.
Like I said, it cost me some time to set up, but it's opened up a world of possibilities that I wouldn't otherwise have.
 VulcanworldsVulcanworlds Posts: 3Members, Plex Pass Plex Pass September 2013 edited September 2013
So I read your topic and decided to try it out. Just set the transcoder temporary directory to the mounted tmpfs and it's working smoothly.
If  you're not familiar with ramfs / tmpfs here's a good read:
http://www.thegeekstuff.com/2008/11/overview-of-ramfs-and-tmpfs-on-linux/#more-251
I'd recommend a tmpfs.
The transcoder, it seems, stores many .ts files in different directories for each show being watched. I believe it cleans up after itself after the video finishes. I of course made my initial tmpfs 1024m so it's taking the better part of a movie to see what happens when it reaches the limit. 
I read tmpfs will pour over into swap once it's full, I really hope plex will just cleanup watched ts files out as the tmpfs fills up.
@Smegheid - your post was an interesting read, the reason I'd continue to use a ram disk for transcoding is to remove unnecessary read and writes on my SSD.
EDIT:
well finally played to the 1GB mark, plex clears the tmp dir and my 1GB tmpfs is empty again, the transcoder kicks off and hangs as the player was still buffering & w/ cpu utilization but nothing written to the tmp dir.
Some purge logic needs to be introduced to manage the transcoder's temp directory if we want to have a limited directory size.
TBH it would be nice to not have to write the transcoded data back to disk if we don't have to. For multi-user setups that come under concurrent load this could prove to be quite useful.
 praeixpraeix Posts: 5 September 2013
Thanks for the responses and very solid info!
@Smegheid - I really don't have any need to speed things up per se.  I guess what I'm trying to say is my primary issue isn't speed at all, but instead it's doing too much disk access at one time or at least I think that's the problem .  Here's my setup:  i7-3770k overclocked to 4.2Ghz, MSI Z77A-GD65 mobo, 32 GB of RAM, PowerColor 7870 Myst Edition, Corsair SATA III 128 GB SSD's (for the OS which includes my installed programs) and my video drive is a 3 TB USB 3.0 external Seagate 7200 rpm drive.  When I'm downloading something and I'm watching a video at the same time, it's fine until the download gets processed and is being extracted.  When that happens, my video that I'm transcoding freezes and won't recover unless I restart the video from scratch after the entire process is over.  So what I was wondering is if I use a RAMdisk as the transcoding destination, would this eliminate the freezing?  Currently my transcoding drive is setup to transcode to the external 3 TB drive (which again is what I'm playing the video from as well as where the downloaded data is being written) although I've also had it transcoding on the SSD and there was no difference on the freezing.  I suppose this could also be an issue with the processor, but that just seems so unlikely considering I've ran this thing through seemingly much higher loads in benchmarking and it hasn't hiccuped or done anything weird.  It just seems like a disk access issue and so I was wondering if a RAMdisk would offload that stress on these drives.  I do have a bunch of other hard drives, none RAIDed, just random old drives that I've thrown in here and I haven't tried putting the transcoder on any of those either to see if it would help.  I suppose I could do that to test it as well.  I just figured that since I overdid it on the RAM with 32 GB and I've maybe used at most, 10 GB of that at any given time that I could utilize that extra RAM with the RAMdisk and save some money instead of buying commercial-grade SAS equipment.  I definitely am interested in transcoding to multiple devices simultaneously at some point in the future as well.
@Vulcanworlds - I never really thought about it from the perspective of conserving read/writes on the SSD to prolong longevity, but that's a good point.   I have a trial edition of Dataram's RAMdisk and it's a great program, but only allows for a max of 4 GB to a single disk.  I was wondering whether or not if you're transcoding a 13 GB MKV file if that would mean that I'd at least need a 13 GB RAMdisk to effectively transcode or if I'd need more RAM allocated for that.  I'm totally willing to purchase the software because it would only be $20 and I like the fact that you can have Windows mount the drive on startup, yet you can still throw away the data after each reboot and not commit it to the saved image file.  When you say some "purge logic needs to be introduced" I believe you're saying that it doesn't automatically purge the transcode directory, correct?  I noticed a while back when the transcoding was still setup on my SSD that WinDirStat showed a huge amount of data being eaten up by the transcoding directory, so I suppose that would be the case.  With this program, it'll theoretically throw away that data every reboot, but that doesn't help if the thing never reboots, which is the goal...  Maybe there's some Windows scripting that could be done on this to automate clearing the directory out?
Again, thanks for the helpful info!  Whenever I get a plan figured out and have more time, I'd be happy to post my findings on here as well.
 jqvillanovajqvillanova Posts: 81Members, Plex Pass Plex Pass September 2013
I've done it in Archlinux and it doesn't really is worthy because the bottleneck would not be your HD , I think SATA III is 3 gbps IIRC so split it in a half, so 1,5 per up and down . Still enough. As plex uses ffmpeg for all things, and it relies on your CPU speed , that is the bottleneck, do a test, encode a file manually with ffmpeg with x264 and mp3 or AAC and that is exactly what plex does to transcode. What I'm frying to argue is that a bluray movie is 40 Mbps < 1,5 gbps so the drive wouldn't be a bottleneck issue AFAIK. 

If you want to preserve read/write cycles in SSD to prevent isolation then is a good choice but you need at least 16-32 Gb of ram, because tmpfs uses only half of your ram at most, so if plex is transcoding and tmpfs wants out of space then your transcoding session aka your movie and your joy will end suddenly.

Cheers.
 praeixpraeix Posts: 5 September 2013
@jvillanova - I wouldn't even call this a bottleneck issue really.  I have no speed concerns for wanting to do this.  It's a matter of figuring out if my HDD/SSD are under too much stress from read/writes and whether or not a RAMdisk will solve that and preserving read/write cycles would be an added benefit.  I understand that the CPU is what does the heavy lifting on the transcoding process   Also SATA III is 6 Gbps theoretical.  On my SSD I see real-world speeds around 250 Mb/s (2 Gbps) sequential and 150 Mb/s (1.2 Gbps) random.
Maybe I should just forget about it and build a 21 Tb SuperNAS like Linus did... 
 VulcanworldsVulcanworlds Posts: 3Members, Plex Pass Plex Pass September 2013
praeix wrote on September 10 2013, 4:20 AM: »
Thanks for the responses and very solid info!
@Smegheid - I really don't have any need to speed things up per se.  I guess what I'm trying to say is my primary issue isn't speed at all, but instead it's doing too much disk access at one time or at least I think that's the problem .  Here's my setup:  i7-3770k overclocked to 4.2Ghz, MSI Z77A-GD65 mobo, 32 GB of RAM, PowerColor 7870 Myst Edition, Corsair SATA III 128 GB SSD's (for the OS which includes my installed programs) and my video drive is a 3 TB USB 3.0 external Seagate 7200 rpm drive.  When I'm downloading something and I'm watching a video at the same time, it's fine until the download gets processed and is being extracted.  When that happens, my video that I'm transcoding freezes and won't recover unless I restart the video from scratch after the entire process is over.  So what I was wondering is if I use a RAMdisk as the transcoding destination, would this eliminate the freezing?  Currently my transcoding drive is setup to transcode to the external 3 TB drive (which again is what I'm playing the video from as well as where the downloaded data is being written) although I've also had it transcoding on the SSD and there was no difference on the freezing.  I suppose this could also be an issue with the processor, but that just seems so unlikely considering I've ran this thing through seemingly much higher loads in benchmarking and it hasn't hiccuped or done anything weird.  It just seems like a disk access issue and so I was wondering if a RAMdisk would offload that stress on these drives.  I do have a bunch of other hard drives, none RAIDed, just random old drives that I've thrown in here and I haven't tried putting the transcoder on any of those either to see if it would help.  I suppose I could do that to test it as well.  I just figured that since I overdid it on the RAM with 32 GB and I've maybe used at most, 10 GB of that at any given time that I could utilize that extra RAM with the RAMdisk and save some money instead of buying commercial-grade SAS equipment.  I definitely am interested in transcoding to multiple devices simultaneously at some point in the future as well.
@Vulcanworlds - I never really thought about it from the perspective of conserving read/writes on the SSD to prolong longevity, but that's a good point.   I have a trial edition of Dataram's RAMdisk and it's a great program, but only allows for a max of 4 GB to a single disk.  I was wondering whether or not if you're transcoding a 13 GB MKV file if that would mean that I'd at least need a 13 GB RAMdisk to effectively transcode or if I'd need more RAM allocated for that.  I'm totally willing to purchase the software because it would only be $20 and I like the fact that you can have Windows mount the drive on startup, yet you can still throw away the data after each reboot and not commit it to the saved image file.  When you say some "purge logic needs to be introduced" I believe you're saying that it doesn't automatically purge the transcode directory, correct?  I noticed a while back when the transcoding was still setup on my SSD that WinDirStat showed a huge amount of data being eaten up by the transcoding directory, so I suppose that would be the case.  With this program, it'll theoretically throw away that data every reboot, but that doesn't help if the thing never reboots, which is the goal...  Maybe there's some Windows scripting that could be done on this to automate clearing the directory out?
Again, thanks for the helpful info!  Whenever I get a plan figured out and have more time, I'd be happy to post my findings on here as well.
I'd think your bottleneck is the USB 3.0 drive, put one of your spare hdd's in the box and set that as the transcoding directory. With it being the only I/O for that disk, you should be better off than having the transcoding write to the external hdd. Or, use a spare hdd as the download destination or extract destination. It'd be better to have 1 disk be read from while writing to another rather than your current setup which is attempting to read 2 different files (the archive and the file to transcode) while attempting to write 2 different files (the transcoded data & the extracted data) all over the same USB port. Sounds like a good recipe for some I/O blockage.
It sounds like you're running windows, have you watched your drive I/O at all? Task Manager > Performance Tab > Resource Monitor
In my experience, my 3770k @ 4.5 seems to handle transcoding 2 different 1080p movies simultaneously to the same 1 TB wd black sata III drive haven't really tried anything past that, though cpu utilization wise it seems like maybe 3 is all I'd manage.
Technically you're correct about the ramdisk size. I'm thinking plex doesn't clear the transcoding directory until the file finishes playing or (a guess) you navigate back to the library. So, if you had enough space in your ramdisk for the entire transcoded file it should work without freezing & crashing PMS. If you want, try it out with a 20gb ramdisk and leave the other 12gb for windows. As long as you've got a single user or multiple users aren't all watching movies you may be okay.
The purge logic I mentioned is something that the plex developers should / would need to implement such that the transcoded directory only holds on to the previous few minutes you just watched rather than from the beginning of the file.
 praeixpraeix Posts: 5 September 2013 edited September 2013
@Vulcanworlds - I offloaded the transcoding directory for Plex to a spare drive that I just got.  It's a 2 TB drive with a bunch of reallocated sectors, so if it fails, no big loss.  I'll have to see how this goes, but I'm a bit skeptical of whether or not this will help.  The logistics of how data flows in from my downloads and then becomes available in the Plex library might be impossible to solve.  I don't see how I can get the data downloaded, uncompressed, moved to the library, but yet play from that same library simultaneously without encountering issues.
For instance, say you're using Sickbeard/Couchpotato/Sabnzbd and you have them all installed on your local disk C:.  By default they download to a directory on C:, but you can of course change that download destination to a different drive like V:\.  Now on V:\ you want Plex to read that as your library so you set it to the directory there.  Now say you have another drive that you set as your transcoding drive T:\.  Now you have something downloading and you watch something at the same time so it's downloading to V:\ and you're watching a transcoded file, it's still accessing the V:\ drive for your video and then trasncoding it to T:\ right?  Eventually the download gets done and it needs to extract and process, so it's doubling the reads and writing to V: as well, correct?  Even if you left Sab to download to your C:\, you'd still need to eventually move it at some point to the V:\ drive to make it available to the Plex library, which would still be causing stress on the disk if you're trying to watch something at the same time.  After the download completes, Sab needs to process the file and extract it, potentially repair it with parity, etc. or am I missing something?
It just seems like there's no way to get around the disk that houses the library on it from being accessed by multiple sources both reading and writing simultaneously, which is why I was considering going with SAS drives for that, but of course those are so crazy expensive.  I thought potentially using the RAMdisk would at least eliminate the stress caused to the disk by transcoding, but I think the bigger issue is simultaneous read/writes...
Ugh my head is spinning...
EDIT:  What about RAID'ing the drive that the Plex library resides on?  If I put those in a RAID that would increase the bandwidth and cut the read/writes in half correct?
 thejinx0rthejinx0r Posts: 193Members, Plex Pass Plex Pass September 2013
praeix wrote on September 15 2013, 9:31 PM: »
@Vulcanworlds - I offloaded the transcoding directory for Plex to a spare drive that I just got.  It's a 2 TB drive with a bunch of reallocated sectors, so if it fails, no big loss.  I'll have to see how this goes, but I'm a bit skeptical of whether or not this will help.  The logistics of how data flows in from my downloads and then becomes available in the Plex library might be impossible to solve.  I don't see how I can get the data downloaded, uncompressed, moved to the library, but yet play from that same library simultaneously without encountering issues.
For instance, say you're using Sickbeard/Couchpotato/Sabnzbd and you have them all installed on your local disk C:.  By default they download to a directory on C:, but you can of course change that download destination to a different drive like V:\.  Now on V:\ you want Plex to read that as your library so you set it to the directory there.  Now say you have another drive that you set as your transcoding drive T:\.  Now you have something downloading and you watch something at the same time so it's downloading to V:\ and you're watching a transcoded file, it's still accessing the V:\ drive for your video and then trasncoding it to T:\ right?  Eventually the download gets done and it needs to extract and process, so it's doubling the reads and writing to V: as well, correct?  Even if you left Sab to download to your C:\, you'd still need to eventually move it at some point to the V:\ drive to make it available to the Plex library, which would still be causing stress on the disk if you're trying to watch something at the same time.  After the download completes, Sab needs to process the file and extract it, potentially repair it with parity, etc. or am I missing something?
It just seems like there's no way to get around the disk that houses the library on it from being accessed by multiple sources both reading and writing simultaneously, which is why I was considering going with SAS drives for that, but of course those are so crazy expensive.  I thought potentially using the RAMdisk would at least eliminate the stress caused to the disk by transcoding, but I think the bigger issue is simultaneous read/writes...
Ugh my head is spinning...
EDIT:  What about RAID'ing the drive that the Plex library resides on?  If I put those in a RAID that would increase the bandwidth and cut the read/writes in half correct?
A couple of things:
1) Raid'ing your drives won't necessarily increase the available bandwidth to your media. It really depends on how it's setup.
2) Unless I miss understood, it seems like your media and your download path are on your external hard drive. Is that correct?
If so, you mentioned your external media drive is USB3, but is it plugged in to a USB3?
If it is on an external hard drive, that could be your bottle neck/ From my experience with external drives (which is not much), transfers over USB usually deteriorate if multiple read/write are occuring on the same drive. 
What I would suggest is that you download your media to your SSD and then manually transfer them to your external later, which is a hassle, but could be done automatically with a script.
 pztpzt Posts: 1Members, Plex Pass September 2013
Vulcanworlds wrote on September 9 2013, 3:21 AM: »
Some purge logic needs to be introduced to manage the transcoder's temp directory if we want to have a limited directory size.
TBH it would be nice to not have to write the transcoded data back to disk if we don't have to. For multi-user setups that come under concurrent load this could prove to be quite useful.
Running Plex Media Server from a machine with fairly intensive operations on the filesystem I really like running transcoder reads/writes on a tmpfs. I've been doing this for a little while now. It all runs smoothly, but I do need to restart playback if the tmpfs fills up. So that's for anything where the quality after transcoding is still fairly high and the file is lengthy. (say, a 1080p 3 hour movie gets transcoded to a high bitrate 720p)
I see no reason why Plex would hang on to older parts of the transcoding session. It doesn't seem to influence seeking much. Can't really think of another reason they would be kept in the first place. Would love to hear more reasons if there are any, though.
So yes, I think it would be great if space would get purged/recycled. Alternatively it would also be great to simply have a configurable space limit per transcoding session. (Or a collective limit for all sessions, but that seems more error-prone.)
 praeixpraeix Posts: 5 September 2013
@thejixn0r - if it's setup in a RAID-0, then it's just striped across all drives, right?  That should theoretically increase the bandwidth unless I'm mistaken.  Guess it really doesn't matter since my largest media storage drives are external and I'd need to rip them out of the enclosures.  Also, yes it's on a 3 TB Seagate 7200 RPM USB 3.0 drive.  The drive typically sees sequential read/writes around 145 MB/s when transferring to/from my SSD.  Not sure about how multiple connections degrade it, but it definitely is degraded.  As soon as something decompresses in SAB, whatever I'm watching, whether transcoding or not freezes up.  I've even tried just watching something in VLC and that freezes as soon as something is unpacking in Sab.  After the unpacking is over, the video is still frozen and I have to exit out of VLC and restart the video, but then it's fine until it happens again.
@pzt - it seems as if Plex doesn't remove transcoding data from the transcode directory and if that fills up, then I can see how it would cause problems.  It would need more intensive purge logic than just a flush though because if you only had the last few minutes as Vulcanworlds suggested, then wouldn't that eliminate the ability to effectively search/fast-forward/rewind?  Granted, that feature doesn't seem to work too keen for me right now anyways, especially if I'm streaming to a 360, but just throwing out questions.
I haven't had the free time lately that I wish I'd have to really dig into this stuff, test, and post results.  I'm also out of money to buy any other equipment to benchmark against.
 SmegheidSmegheid Posts: 5 September 2013
Vulcanworlds, that's an interesting idea. I hadn't considered that plex would generate intermediate files in the process of transcoding; I figured it would just be a plain old stream that didn't actually land anywhere.
In that case, tmpfs is perfect. It's pretty much designed for this sort of thing, for data that is short-lived and not permanent.
One word of warning to anyone else reading that has a passing familiarity with Unix/Linux is that /tmp is no longer a RAM disk on a bunch of Linux distributions. Ubuntu (and a bunch of its derivatives) now leave /tmp on the root filesystem, which  can be a surprise. Even the server version of Ubuntu (certainly 12.04) does this these days.
 MirabisMirabis Posts: 67Members, Plex Pass Plex Pass May 24
I've been trying to cleanup the transcode dir with a cron task:
 
find /ramdisk/plex-transcode-* -type f \( -iname 'media-0*.ts' ! -iname "*.tmp" \) -mmin 0.33 -exec rm  {} \;
find /ramdisk/plex-transcode-* -type f \( -iname 'chunk-0*' ! -iname "*.tmp" \) -mmin 0.33 -exec rm {} \;
 
Which seems to delete the old files, but  somehow it's not a 100% stable fix



---
https://www.mythtv.org/wiki/Optimizing_Performance

Optimizing Performance
This HOWTO aims to collect the multitude of tips regarding optimizing performance of your system for use with MythTV.

Contents [hide] 
1 File Systems
1.1 Local File Systems
1.1.1 Put the database on a separate spindle
1.1.2 General Tips For Any File System
1.1.2.1 Combat Fragmentation
1.1.2.2 Disabling File Access Time Logging
1.1.2.3 Using "relatime" Mount Option
1.1.3 XFS-Specific Tips
1.1.3.1 Combat Fragmentation
1.1.3.2 Optimizing XFS on RAID Arrays
1.1.3.2.1 Examples
1.1.3.3 Further Information
1.2 Network File Systems
1.2.1 Disable NFS file attribute caching
1.2.2 NFS servers
1.3 RemoteFS
2 Devices
2.1 Capture Cards
2.2 Video Cards & Hardware Accelerated Video
2.2.1 VDPAU
2.2.2 VAAPI
2.2.3 CrystalHD
2.3 CPU / Processor
2.3.1 Clock Speed Throttling
3 Networks
3.1 Wireless Networks
3.2 Ethernet Full-Duplex Mode
4 Operating System
4.1 Kernel Configuration
4.1.1 Processor Family
4.1.2 IDE Controller
4.1.3 Preemptible Kernel
4.1.4 Timer Frequency
4.2 Realtime Threads
4.3 PCI Latency
4.4 RTC Maximum Frequency
4.5 Linux Distribution Selection
5 Other Software
5.1 MythTV
5.1.1 Multiple Machines
5.1.2 Frontend Playback Profiles
5.1.3 Recording Settings
5.1.4 Mythfilldatabase Scheduling
5.2 MySQL Database Tweaks
5.3 XORG CPU Hogging
5.4 Lightweight Window Managers
File Systems
Local File Systems
Put the database on a separate spindle
MythTV reads and writes large video files and it reads and writes small bits of metadata from a database. The small bits of metadata are accessed frequently enough that the seeks can more than halve the overall performance of access to the large video files MythTV also uses. Past the experimentation point it makes a lot of sense to allocate one small disk to the OS and the database and use a separate drive or drives for everything else. The database is generally under /var unless you have moved it.

General Tips For Any File System
Combat Fragmentation
Fragmentation happens when the data placement of files is not contiguous on disk, causing time-consuming head seeks when reading or writing a file.

MythTV recordings on disk can become quite fragmented, due to several factors, such as the fact that MythTV writes large files over a very long period of time, the fact that recording files may have drastically different sizes, and the fact that many MythTV systems have multiple capture cards--allowing for recording multiple shows at once. Note, also, that any time MythTV is recording multiple shows to a single filesystem (even if in different directories and/or in different Storage Groups), the recordings will necessarily be fragmented.

Configuring multiple local filesystems within MythTV's Storage Groups will allow MythTV to write recordings to separate filesystems, thereby minimizing fragmentation. Therefore, the best approach to combat fragmentation is to ensure each computer running mythbackend has at least as many local (and available) filesystems as capture cards. If using a combination of local and network-mounted filesystems, you may need to adjust the Storage Groups Weighting to cause MythTV to write to network-mounted filesystems (though doing so may negatively impact performance, meaning the use of a sufficient number of local filesystems or the use of only network-mounted filesystems is preferred). The availability of a filesystem is somewhat dependent on that filesystem having space available for writing (i.e. having 2 filesystems for 2 capture cards with one filesystem completely full and the other only half full will not help prevent fragmentation, though if both are full, autoexpiration should allow either to be used).

Fragmentation can be measured by the "filefrag" command on most any filesystem.

Disabling File Access Time Logging
Most filesystems log the access times of files. Generally this file metadata shouldn't be necessary, however, if for some strange reason you experience problems, then don't apply this tweak.

To disable the logging of file access times, add the "noatime" and "nodiratime" options to your /etc/fstab:

# 1.5 TB RAID 5 array. Large file optimization: 64m of prealloc
# NO logging of access times: improves performance
# NO block devices or suid progs allowed: improves security
/dev/md0    /terabyte   xfs defaults,noatime,nodiratime,nosuid,nodev,allocsize=64m 0   0
If you get something like the following, the mount option is not supported for your filesystem:

mount: wrong fs type, bad option, bad superblock on /dev/md0,
      missing codepage or helper program, or other error
      In some cases useful info is found in syslog - try
      dmesg | tail  or so

# dmesg | tail would return something like:
YOUR_FILESYSTEM_TYPE: unknown mount option [noatime].
Using "relatime" Mount Option
You may also wish to look into the "relatime" mount option to improve performance, but still have file access times updated. This is an alternative to using noatime and nodiratime. For more information on this (and related discussion), see: Linux: Replacing atime With relatime

XFS-Specific Tips
Combat Fragmentation
Under XFS, an additional command can be used to measure filesystem fragmentation: "xfs_bmap".

The xfs filesystem has a mount option which can help combat this fragmentation: allocsize

 allocsize=size
       Sets the buffered I/O end-of-file preallocation size when
       doing delayed allocation writeout (default size is 64KiB).
       Valid values for this option are page size (typically 4KiB)
       through to 1GiB, inclusive, in power-of-2 increments.
This can be added to /etc/fstab, for example:

   /dev/sd1  /video     xfs     defaults,allocsize=64m 0 0
This essentially causes xfs to speculatively preallocate 64m of space at a time for a file when writing, and can greatly reduce fragmentation. MythTV syncs the file it is writing to disk regularly to prevent the filesystem for freezing up for long periods of time writing large chunks of data that MythTV is generating and so preventing smooth simultaneous playback of the same or different file from that filesystem.

For files which are already heavily fragmented, the xfs_fsr command (from the xfsdump package) can be used to defragment individual files, or an entire filesystem.

Run the following command to determine how fragmented your filesystem is:

  xfs_db -c frag -r /dev/sda1
xfs_fsr with no parameters will run for two hours. The -t parameter specifies how long it runs in seconds. It keeps track of where it got up to and can be run repeatedly. It can be added to our crontab to periodically defragment your disk. Add the following to /etc/crontab:

  30 1 * * * root /usr/sbin/xfs_fsr -t 21600 >/dev/null 2>&1
to run it every night at 1:30 for 6 hours.

Don't forget to see the complete XFS_Filesystem wiki page that includes general info about XFS, defragmenting, disk checking and maintenance, etc.

Optimizing XFS on RAID Arrays
Some more RAID specific tweaks for XFS were found in this helpful article: Optimizing XFS on RAID Arrays. This section is a slightly reformatted version of that article. Please note the author of that article incorrectly used the term "block size" in some places when he really meant "stripe size" or "chunk size".

block size refers to the filesystem's unit of data transfer. This is set at format time for the filesystem. The default value is 4096 bytes (4 KiB), the minimum is 512, and the maximum is 65536 (64 KiB). XFS on Linux currently only supports pagesize or smaller blocks.
chunk size or stripe size refers to the RAID array's unit of data transfer. This is set during RAID array creation time for the array (in software raid, the --chunk=X option to mdadm). For me, mkfs.xfs complained when using chunk size=256KB, and block size=4096 bytes and specifying a sunit & swidth calculated using the block size. The values it mentions are correct if calculated using the chunk size. Therefore, this section assumes sunit & swidth calculated using chunk size.
If you are having trouble, try my script.

XFS has builtin optimizations for reading data from RAID arrays. These options can be specified at mkfs time or at mount time (you can even set them while the system is running using the mount -o remount command) and can affect the performance of your system.

There are two parameters for tweaking how XFS handles your RAID arrays (there are actually four, but you only need to use these two): sunit and swidth. sunit is the stripe unit and swidth is the stripe width. The stripe unit sits on a single disk while the stripe width spans the entire array; in this way the sunit is similar to the stripe size of your array.

Before you begin, you’ll need to know:

What type of RAID array you’re using
The number of disks in the array
The stripe size (aka the chunk size in software RAID).
For RAID{1,0,10} arrays, the number of “disks” is equal to the number of spindles.

For RAID{5,6} arrays, the number of disks is equal to N-1 for RAID5 and N-2 for RAID6, where N is the number of spindles.

If you guessed that the sunit is equal to your stripe size, you’re almost correct. The sunit is measured in 512-byte block units (from the mount man page), so for a 64kB stripe size your sunit=128, for 256kB use sunit=512.

As mentioned before, the swidth spans the entire array, but is also measured in 512-byte blocks, so you’ll want to multiply the number of disks calculated above by the sunit for your stripe size.

Note: If you have not formatted your xfs partition yet, you may set a blocksize in mkfs.xfs using the -b size=X option. The default is usually 4096 bytes (4 KiB) on most systems. (Remember blocksize is limited to your system's memory pagesize. blocksize <= pagesize). If already created, use the following command, and look for bsize=X in output:

# replace /dev/md0 with your device's name
xfs_info /dev/md0
Examples
Calculate the correct values for your system using these examples as a guideline.

A 4-disk RAID0 array with a 64kB stripe size will have a sunit of 128 and a swidth of 4*128=512. Your mkfs.xfs and mount commands would thus be:

#Note that you should only need to use one of these.  You may also add the sunit and swidth options to /etc/fstab to make the second one permanent.
mkfs.xfs -d sunit=128,swidth=512 /dev/whatever
mount -o remount,sunit=128,swidth=512
An 8-disk RAID6 array with a 256kB stripe size will have a sunit of 512 and a swidth of (8-2)*512=3072. Your commands would thus be:

mkfs.xfs -d sunit=512,swidth=3072 /dev/whatever
mount -o remount,sunit=512,swidth=3072
Further Information
If you are having trouble figuring this out (as I did at first), here is a useful bash script to help you. It only requires that you have the "bc" bash calculator program installed. To get it in Ubuntu, use "sudo apt-get install bc".

Put the following in a text editor, and chmod +x it, tweak values to match your system and run.

#!/bin/bash
BLOCKSIZE=4096 # Make sure this is in bytes
CHUNKSIZE=256  # Make sure this is in KiB
NUMSPINDLES=8
RAID_TYPE=6
RAID_DEVICE_NAME="/dev/md0" # Specify device name for your RAID device
FSLABEL="mythtv" # specify filesystem label for generating mkfs line here

case "$RAID_TYPE" in
0)
    RAID_DISKS=${NUMSPINDLES};
    ;;
1)
    RAID_DISKS=${NUMSPINDLES};
    ;;
10)
    RAID_DISKS=${NUMSPINDLES};
    ;;
5)
    RAID_DISKS=`echo "${NUMSPINDLES} - 1" | bc`;
    ;;
6)
    RAID_DISKS=`echo "${NUMSPINDLES} - 2" | bc`;
    ;;
*)
    echo "Please specify RAID_TYPE as one of: 0, 1, 10, 5, or 6."
    exit
    ;;
esac

SUNIT=`echo "${CHUNKSIZE} * 1024 / 512" | bc`
SWIDTH=`echo "$RAID_DISKS * ${SUNIT}" | bc`

echo "System blocksize=${BLOCKSIZE}"
echo "Chunk Size=${CHUNKSIZE} KiB"
echo "NumSpindles=${NUMSPINDLES}"
echo "RAID Type=${RAID_TYPE}"
echo "RAID Disks (usable for data)=${RAID_DISKS}"
echo "Calculated values:"
echo "Stripe Unit=${SUNIT}"
echo -e "Stripe Width=${SWIDTH}\n"
echo "mkfs line:"
echo -e "mkfs.xfs -b size=${BLOCKSIZE} -d sunit=${SUNIT},swidth=${SWIDTH} -L ${FSLABEL} ${RAID_DEVICE_NAME}\n"
echo "mount line:"
echo -e "mount -o remount,sunit=${SUNIT},swidth=${SWIDTH}\n"
echo "Add these options to your /etc/fstab to make permanent:"
echo "sunit=${SUNIT},swidth=${SWIDTH}"
Please refer to the following references for more details and other useful tweaks to improve the performance of your XFS filesystem:

Filesystem Performance Tweaking with XFS
Optimizing XFS on RAID Arrays
Network File Systems
Disable NFS file attribute caching
if you are using SMB (not CIFS), you can try the ttl option using "-o ttl=100" which should set your timeout lower than the default. The default is supposed to be 1000ms which equals 1 second, but one user has reported that setting ttl=100 corrected the issue for him, so SMB users can give it a try.

NFS servers
Ensure that your NFS server is running in 'async' mode (configured in /etc/exports). The default for many NFS servers is 'async', but recent versions of debian now default to 'sync', which can result in very low throughput and the dreaded "TFW, Error: Write() -- IOBOUND" errors. Example of setting async in /etc/exports:

/mnt/store      192.168.1.3/32(rw,async,udp)
There are a few other NFS mount options that can help, such as "intr", "nfsvers=3", "actimeo=0" , "noatime" and "tcp"/"udp". You can read the man pages for a more detailed description, but suggestions are below.

nfsvers=3 - This tells the client to use NFS v3, which is better than NFS v2. Of course, the server has to also support it.

actimeo=0 - disable this attribute caching to allow the frontend to see updates from the backend quicker. The problem has been seen where LiveTV fails to transition from one program to another. The cache file attribute prevents the frontend from opening the new file promptly. This also causes more load on the server if that is a issue.

tcp - This tells NFS to use TCP instead of UDP. It has been reported to improve performance for some, but has also caused repeated 3-5 second filesystem freeze-ups for others. If you have a network with only 1000-mbit clients or suffer performance problems with udp, you can try this. Poor performance with tcp may be improved by setting rsize and wsize to appropriate values (usuall 32KB).

udp - This tell NFS to use UDP instead of TCP. This is the traditional mechanism for NFS to utilize. For networks containing wifi or 100-mbit clients this is probably the best option. If you get video stuttering with either tcp or udp, try the other one.

rsize=32768,wsize=32768 - These tell your nfs client to use a particular block size, 32KB in this case. Modern NFS clients auto-negotiate a block size so this isn't really necessary for udp where a block size of 32KB is generally auto-negotiated. However with tcp the auto-negotiated block size can be too large resulting in very high latency. On Linux, check the output of /proc/mounts and if rsize or wsize depart very far from 32KB, you probably do want to set this.

intr - Makes I/O to a NFS mounted filesystem interuptable if the server is down. If not given the I/O becomes a uninteruptable sleep which causes the process to be impossible to kill until the server comes up again. This is a no-op on newer Linux kernels and you must use async instead.

async - Like intr this allows you to kill a process that has a file open on an unresponsive NFS server. On Linux this should always be used for A/V only volumes, unless you are using a kernel that still supports intr. Otherwise, you can end up with a permanently hung backend or frontend if a remote volume goes down for some reason.

soft - If the NFS server becomes unavailable the NFS client will generate "soft" errors instead of hanging. Some software will handle this well, other much less well. In the later case file corruption will result. For a file system solely used for A/V data this is safe and can avoid the a frontend or backend entering uninterruptible sleep.

Example /etc/fstab entry:

 server:/mythtv/rec0 /mythtv/rec0 nfs async,nfsvers=3,actimeo=0,tcp,rsize=32768,wsize=32768
 server:/mythtv/rec1 /mythtv/rec1 nfs async,nfsvers=3,actimeo=0,udp
RemoteFS
This is a fuse based file system that may be well suited to providing network access at remote frontends typically required. I have utilised this on my own setup and have found it to be faster / more responsive than the previous NFS setup I was using (regardless of options used above). NFS provides lots of sophisticated features for handling large numbers of users accessing via different types of network links and therefore latencies etc. these features are unlikely to be of any benefit for typically LAN connected systems especially as they are often just ro. The author appears to have done quite a lot of performance testing which you can see here https://sourceforge.net/apps/mediawiki/remotefs/index.php?title=Development:Performance_Tests Certainly worthy of testing / review IMHO.

Devices
Capture Cards
For backend machines, or machines that are a combination frontend/backend, the type of capture card used will impact performance. With a typical analog capture card, such as the popular bttv cards, the CPU must encode the raw video to MPEG-4 or RTjpeg on the fly. When watching live TV on a combination frontend/backend machine, the machine has to both encode AND decode the video stream simultaneously.

Luckily there are two options:

Hardware MPEG-2 Capture Cards, such as the popular Hauppauge PVR-150 and PVR-500.
Digital tuners, such as the pcHDTV HD-5500, which work with both OTA 8VSB signals as well as QAM for digital cable systems.
With cards of this type, the machine's CPU doesn't have to encode the incoming video. Instead, it simply receives the MPEG-2 stream from the card and dumps it to disk. This makes the recording process a simple operation, with relatively low resource usage.

Video Cards & Hardware Accelerated Video
Several options are available for accelerating video output:

VDPAU
VDPAU is currently NVIDIA-only for the time being, but provides for GPU-accelerated decode of MPEG-1, MPEG-2, H.264, and VC-1 bitstreams, as well as post-processing of decoded video including temporal and spatial deinterlacing, inverse telecine, and noise reduction.

VAAPI
CrystalHD
CPU / Processor
Clock Speed Throttling
There are several conditions in which your computer's CPU may be scaled down from its maximum clock speed:

A laptop or notebook has scaled down the CPU automatically due to being unplugged from an AC power source and running on the battery
The system has detected an unsafe thermal condition, and has scaled back the clock speed to avoid damage
The CPU speed has been configured incorrectly in the BIOS
The CPU speed has been manually configured to a lower speed at runtime
You can check your CPU's current operating frequency by running the command:

cat /proc/cpuinfo
If your system is slowing down because it is at its thermal limits, the only real option is to beef up your cooling capacity. This could be in the form of a larger heatsink, a larger fan, or even liquid cooling. A CPU that is incorrectly configured in the BIOS should be easy to check and easy to fix, but take care that you don't unintentionally overclock it in the process. Changing a manual control or overriding an automatic speed control will likely be distribution-dependent, or subject to your choice of adjustment tools.

Networks
Wireless Networks
While it is possible to run MythTV over a wireless network, you may find that you have better performance when using a wired connection. With a wireless connection, your bandwidth & latency are dependent on your distance from the access point, interference from other devices, the number of wireless users on the network, and the capabilities of your equipment at both ends. If you find that you have trouble with skips and/or dropouts when watching content on a wireless front end, it would be good to test the same setup with a wired connection to determine if the network is the problem.

Ethernet Full-Duplex Mode
Make sure that your ethernet adapters are running in full duplex mode. Check your current configuration with this command:

ethtool eth0
Typically both sides will be configured for autonegotiation by default and you will get the best possible connection automatically but there are conditions--typically involving old or buggy hardware--when this may not happen. The following can be used to disable autonegotiation and force a 100base-T network adapter into full duplex mode, when autonegotiation is failing.

ethtool -s eth0 speed 100 duplex full autoneg off
This problem can exhibit itself with "IOBOUND" errors in your logs.

Note: To use full-duplex mode, your network card must be connected to a switch (not a hub) and the switch must be configured to allow full-duplex operation (almost always the default) on the ports that are being used. By definition, a network switch supports full duplex operation and a network hub (sometimes referred to as a repeater) does not. If you are connecting to a hub, full-duplex operation will not be possible. Most switches support using 100base-T (Fast Ethernet) as well as 10base-T, while most hubs will only use 10base-T, and while a few 100base-T hubs (and 10base-T switches) do exist, they are quite rare. Gigabit switches can reliably be expected to handle both fast ethernet and normal ethernet connections in addition to the gigabit ethernet speeds.

Do not disable autonegotiation if things are currently working correctly. This will only create new problems, not prevent future ones. Forced connections can't advertise what they are capable of so the autonegotiating side must assume half-duplex. You will actually be creating a problem if the now forced connection was already full-duplex. It should be noted that most consumer-level switches and home routers do not support manual port configuration and this will result in them selecting a half-duplex connection if the remote end is no longer participating in connection negotiation. Nearly all of the time, using autonegotiation on all of the equipment will give you the best possible results. If you encounter problems with autonegotiation you can opt to manually configure settings for that device but it is highly recommended that you manually configure every piece of equipment on that segment as well.

Operating System
Kernel Configuration
If you're compiling your own kernel, you might want to try out the following options:

Processor Family
Ensure that the "Processor Family" (in "Processor Type and Features") is configured correctly.

IDE Controller
Ensure that the correct IDE controller is set (in "Device Drivers->ATA/ATAPI support->PCI IDE chipset support"). There is a generic IDE controller driver in the kernel that will handle many different chipsets, but it's performance is sub-par.

Preemptible Kernel
Kernel preemption allows high priority threads to interrupt even kernel operations -- this ensures the lowest possible latency when responding to important events. (Note: apparently some IVTV drivers show stability problems with a preemptible kernel.)

Timer Frequency
Increasing the scheduler's timer frequency to 1000Hz can reduce latency between multiple threads of execution (at a small cost to overall performance), e.g. when recording/playing multiple video streams. Generally you will want to pick 300Hz which is neatly divisible by both 50Hz (PAL) and 60Hz (NTSC) because of the frame rates involved in displaying your media.

On some machines you may hear an annoying high-pitched "whistle": reduce the frequency to 250Hz or lower to avoid this.

Realtime Threads
The mythfrontend & mythtv threads can be configured to run with "realtime" priorities - if the frontend is configured this way, and if sufficient privileges are available to the user running mythfrontend.

The HOWTO has an excellent section on how to set your system up to enable this (look for "Enabling real-time scheduling of the display thread.") You will also need to select "Enable Realtime Priority Threads" in the General Playback frontend setup dialogue.

Realtime threads can help smooth out video and audio, because the system scheduler gives very high priority to mythtv. For more information on how this works, see the Real-Time chapter in Robert Love's great Linux Kernel Development book.

PCI Latency
Incorrect or less-than-optimal settings of PCI Latency can cause performance-related problems. See the page PCI Latency

RTC Maximum Frequency
See Adjusting_the_RTC_Interrupt_Frequency

Linux Distribution Selection
At a more fundamental level, your choice of a Linux distribution can have a large impact on the overall performance of your Myth machine. Most "modern" distributions (Fedora, Ubuntu, etc) come with default installations intended to give the best initial user experience by providing support for scores of devices & programs, with automation wherever possible. The downside to this, is that these default installations have large kernels and large numbers of background processes running to support this usage.

While any distribution can be whittled down to meet a more focused need, it takes an effort to do so. An alternative approach, is to select a distribution such as Gentoo that provides you with a blank slate by default. This allows you to add only the components you need, ensuring a clean system with minimal effort.

Other Software
MythTV
Multiple Machines
One great feature of the MythTV architecture is that the recording and playback functions are split between two applications - the backend and the frontend. While they can both be run on the same machine, one of the easiest performance boosts is to simply split these tasks between two machines.

If desired, it is even possible to set up an additional backend machine to assist with recording and/or commercial flagging & transcoding tasks. This sort of arrangement may be beneficial if the backend machines are low-power (unable to keep up with the transcoding jobs), and is a good way to ensure that post-processing operations do not interfere with active recordings.

Frontend Playback Profiles
The choice of an appropriate playback profile can make a huge difference in the perceived performance of your MythTV frontend. The playback profile decides which video decoder will be used, how the on-screen display is rendered, and which video filters (deinterlacing, etc) are used. The playback profile also dictates how hardware acceleration is used, which is especially important on low-end PCs or machines processing HD content.

Recording Settings
Part of "optimizing" is determining what you actually need, not just making something as good as it can get.

For example, if you only watch recordings on your iPod and nothing else, you might as well configure your tuners to record TV at 320x240 resolution. Doing this will allow faster processing (commercial removal, etc), and the reduced file sizes will let you store more videos and copy them to/from your devices faster. Likewise, if you intend to burn a significant number of your recordings to DVD, you could save your backend a lot of work by saving your recordings directly to a DVD-compliant resolution, and in DVD-compliant MPEG-2 if your capture card supports it. (More information is available in the MythArchive page.)

Even if you watch your recordings from a frontend on your TV, you may still find it worthwhile to play with the recording settings. You may find that a lower audio bitrate eliminates hiss in the audio track, or that a lower resolution with an equivalent bitrate produces fewer objectionable video artifacts. Or, if your frontend isn't very powerful, or your network is congested, a lower bitrate may help make smooth video possible where it otherwise would not have been.

Mythfilldatabase Scheduling
By default, Mythfilldatabase runs automatically every 24 hours to keep your listings up to date. Mythfilldatabase is known for being able to saturate I/O systems (See Troubleshooting:Mythfilldatabase_IO_bottleneck), which can cause problems on heavily used or low-power backends. If you have recording or playback issues during the default script timeslot (2AM-5AM), you can manually adjust the script's schedule via the frontend setup menu to better suit your particular usage. Running the database and MySQL on a different machine is another way to alleviate these issues, to ensure consistent performance.

MySQL Database Tweaks
Taken from this thread in mythtv-users.

Add the following to the [mysqld] section of /etc/my.cnf to see improvements in database speed for MythTV as well as MythWeb. Check your default values using 'mysql> show global variables;'

key_buffer = 48M
max_allowed_packet = 8M
table_cache = 128 # this setting is deprecated in mysql 5.6.23 and will prevent mysql from starting
sort_buffer_size = 48M
net_buffer_length = 1M
thread_cache_size = 4
query_cache_type = 1
query_cache_size = 4M
There are also example my.cnf files included with your MySQL installation that have suggested values based on the amount of memory your system has. Information about them can be found in the MySQL documentation.

There is a great Perl script available at mysqltuner.pl. It will look through many of the MySQL server settings and report on variables that need to be changed. Hints on usage [1]

XORG CPU Hogging
Under some circumstances, X can use huge amounts of CPU. This may be fixed in some cases by increasing its priority above the base value of 0 (i.e. to a negative value). e.g. renice -2 [pid for X]. If you must renice a process, do so in small steps. Raising applications above the priority of mechanisms like kjournald or ksoftirqd can have adverse side-effects.

A second way of lowering Xorg CPU usage (especially when decoding is accelerated with XvMC or VDPAU) with nVidia cards, is to add

Option      "UseEvents"   "True"
to the Device section of your Xorg.conf. (warning: although this works well for watching hd content, it's considered unstable for 3D software like gaming, etc... )

Lightweight Window Managers
While KDE & Gnome provide for a nice user experience, they also bring along a lot of baggage which is unnecessary for a dedicated Myth machine. Switching to a lightweight window manager such as WindowMaker,Fluxbox, or Ratpoison will reduce startup times and give you more available system resources at runtime.



---
https://freeswitch.org/confluence/display/FREESWITCH/Performance+Testing+and+Configurations

Performance Testing and Configurations
Skip to end of metadata
Created by John Boteler, last modified on 2015.03.23 Go to start of metadata
About
Discussion of testing performance of FreeSWITCH™ with links to test scenario open source projects.
 
 Click here to expand Table of Contents
 
Measures of Performance
When people say performance it can mean a wide variety of things. In reality performance typically comes down to two bottle necks which are SIP, and RTP. These typically translate into calls per second and concurrent calls respectively. Additionally, high volume systems might experience bottlenecks with database servers running out of connections or even bandwidth when looking up account or configuration data.
Calls per Second (CPS)
Since calls per second is simply a measure of how many calls are being setup and torn down per second the limiting factor is the ability to process the SIP messages. Depending on the type of traffic you have this may or may not be a factor. There are a variety of components that can contribute to this bottleneck, FreeSWITCH and it's libraries being only some of them.
Concurrent Calls
Using modern hardware concurrent calls is less a limit of SIP but rather the RTP media streaming. This can further be broken down to available bandwidth and the packets per second. The theoretical limit on concurrent calls through a gigabit Ethernet port would be around 10,500 calls without RTCP, assuming G.711 codec and the link-level overheads. Theory is great and all, but in reality the kernel networking layer will be your limiting factor due to the packets per second of the RTP media stream.
Configurations
Recommended Configurations
A 64-bit CPU running a 64-bit operating system and a 64-bit version of FreeSWITCH is recommended. A bare metal system provides consistent, predictable performance and most importantly for real–time applications like this, a reliable kernel clock for RTP packet timing. With a virtual machine it is difficult to determine where any problems might originate and improper propagation of the hardware clock through the VM host to the guest operating system is not always available so the RTP tests will be rendered meaningless.
Debian linux is the recommended OS, since that's the OS used by the core developers and therefore the best tested. It will work on some other operating systems though.
Recommended ULIMIT settings
The following are recommended ulimit settings for FreeSWITCH when you want maximum performance.
ulimit -c unlimited # The maximum size of core files created.
ulimit -d unlimited # The maximum size of a process's data segment.
ulimit -f unlimited # The maximum size of files created by the shell (default option)
ulimit -i unlimited # The maximum number of pending signals
ulimit -n 999999    # The maximum number of open file descriptors.
ulimit -q unlimited # The maximum POSIX message queue size
ulimit -u unlimited # The maximum number of processes available to a single user.
ulimit -v unlimited # The maximum amount of virtual memory available to the process.
ulimit -x unlimited # ???
ulimit -s 240         # The maximum stack size
ulimit -l unlimited # The maximum size that may be locked into memory.
ulimit -a           # All current limits are reported.
 
Recommended SIP settings
Turn off every module you don't need that is not also needed by FreeSWITCH
Turn presence off in the profiles
libsofia only handles 1 thread per profile, so if that is your bottle neck use more profiles
mod_cdr_csv is slower than mod_xml_cdr
Reports of running more than a single instance of FreeSWITCH has helped.
Disable console logging when not needed - loglevel 0
Ethernet Tuning in linux

Beware buffer bloat
Prior to the bufferbloat guys coming in and talking to us there was a note in here that one should "set the buffers to maximum." That advice is WRONG on so many levels. To make a long story short, when you're doing real-time media like VoIP you absolutely do not want large buffers. On an unsaturated network link you won't notice anything, but when you have a saturated network the larger buffers will cause your RTP packets to be buffered instead of discarded.
 
So, what should your rx/tx queuelens be? Only you can know for sure, but it's good to experiment. Normally in linux it defaults to 1000. IF you are using a good traffic shaping qdisc (pfifo_fast or SFB or others) AND prioritizing udp/rtp traffic you can leave it alone, but it still is a good idea to lower it significantly for VoIP applications, depending on your workload and connectivity.
Don't use the default pfifo qdisc, regardless. It outputs packets in strict fifo order.
To see your current settings use ethtool:
ethtool settings
[root@server:~]# ethtool -g eth0
Ring parameters for eth0:
Pre-set maxima:
RX:             256
RX Mini:        0
RX Jumbo:       0
TX:             256
Current hardware settings:
RX:             256
RX Mini:        0
RX Jumbo:       0
TX:             128
 
 
These were the defaults on my Lenny install. If you needed to change it you can do this:
ethtool suggested changes
[root@server:~]# ethtool -G eth0 rx 128
[root@server:~]# ethtool -g eth0
Ring parameters for eth0:
Pre-set maxima:
RX:             256
RX Mini:        0
RX Jumbo:       0
TX:             256
Current hardware settings:
RX:             128
RX Mini:        0
RX Jumbo:       0
TX:             128
 
There is no one correct answer to what you should set the ring buffers to. It all depends on your traffic. Dave Taht from the Bufferbloat project reports that, based on his observations and experiences and papers such as http://www.cs.clemson.edu/~jmarty/papers/bittorrentBroadnets.pdf , that at present in home systems it is better to have no more than 32 unmanaged TX buffers on a 100Mbit network. It appears on my Lenny they are 32/64:
One man's buffer settings
[root@server:~]# ethtool -G eth0 rx 32 tx 32
[root@server:~]# ethtool -g eth0
Ring parameters for eth0:
Pre-set maximums:
RX:             256
RX Mini:        0
RX Jumbo:       0
TX:             256
Current hardware settings:
RX:             32
RX Mini:        0
RX Jumbo:       0
TX:             64
You'll note you can't with this driver reduce the TX buffer to a more optimum level!! This means that you will incur nearly a 10ms delay in the driver alone (at maximum packet size and load) on packets if you are on a 100Mbit network.
(similarly, a large TXQUEUELEN translates to lots of delay too)
On a gigibit network interface, the default TX queues and TXQUEUELEN are closer to usable, but still too large.
Having larger RX buffers is OK, to some extent. You need to be able to absorb bursts without packet loss. Tuning and observation of actual packet loss on the receive channel is a good idea.
And lastly, the optimum for TX is much lower on a 3Mbit uplink than a 100Mbit uplink. The debloat-testing kernel contains some Ethernet and wireless drivers that allow reducing TX to 1.
TCP/IP Tuning
For a server that is used primarily for VoIP, TCP Cubic (the default in Linux) can stress the network subsystem too much. Using TCP Vegas (which ramps up based on measured latency) is used by several FreeSWITCH users in production, as a "kinder, gentler" TCP for command and control functions.
To enable Vegas rather than Cubic you can, at boot:
modprobe tcp_vegas
echo vegas > /proc/sys/net/ipv4/tcp_congestion_control
--- Some interesting comments about tcp_vegas at http://tomatousb.org/forum/t-267882/
FreeSWITCH's core.db I/O Bottleneck
On a normal configuration, core.db is written to disk almost every second, generating hundreds of block-writes per second. To avoid this problem, turn /usr/local/freeswitch/db into an in-memory filesystem. If you use SSDs, it is CRITICAL that you move core.db to a RAM disk to prolong the life of the SSD.
On current FreeSWITCH versions you should use the documented "core-db-name" parameter in switch.conf.xml (simply restart FreeSwitch to apply the changes):
   <param name="core-db-name" value="/dev/shm/core.db" />
Otherwise you may create a dedicated in-memory filesystem, for example by adding the following to the end of /etc/fstab
fstab Example
#
# Example of /etc/fstab entry (using default size)
#
tmpfs /usr/local/freeswitch/db tmpfs defaults 0 0
#
# To specify a size for the filesystem use the appropriate mount(1) option:
#
# tmpfs /usr/local/freeswitch/db tmpfs defaults,size=4g 0 0
#
To use the new filesystem run the following commands (or the equivalent commands for your OS):
mount /usr/local/freeswitch/db
/etc/init.d/freeswitch restart
An alternative is to move the core DB into an ODBC database, which will move this processing to a DBMS which is capable of handling large numbers of requests far better and can even move this processing onto another server. Consider using freeswitch.dbh to take advantage of pooling.
Stress Testing
KNOW
IF YOU DO NOT UNDERSTAND HOW TO STRESS TEST PROPERLY THEN YOUR RESULTS WILL BE WORTHLESS.
Using SIPp is part dark art, part voodoo, part Santeria. YOU HAVE BEEN WARNED
When using SIPp's uas and uac to test FreeSWITCH, you need to make sure there is media back and forth. If you just send media from one sipp to another without echoing the RTP back (-rtp_echo), FS will timeout due to MEDIA_TIMEOUT. This is to avoid incorrect billing when one side has no media for more than certain period of time.
See Also
FreeSWITCH performance test on PC Engines APU — Stanislav Sinyagin tests FreeSWITCH™ transcoding performance with only one test machine
SSD Tuning for Linux — special considerations for systems using Solid State Drives for storage
SIPp — Open source test toll and traffic generator for SIP
SIPpy Cup — Ben Langfeld contributes this scenario generator for SIPp to simplify the creation of test profiles and especially compatible media
check_voip_call — Henry Huang contributes this project to work with Nagios
http://www.bandcalc.com/ — Bandwidth calculator for different codecs and use cases
http://www.cs.clemson.edu/%7Ejmarty/papers/bittorrentBroadnets.pdf — Paper on buffer sizing based on bittorrent usage



---
http://ubuntuforums.org/archive/index.php/t-1939900.html

[SOLVED] Moving Files to memory


Ceiber Boy
March 12th, 2012, 10:22 PM
is it possible to 'store' files in memory? Hear me out!

I have a shed of memory and a slow HDD, so when transcoding large files, instead of trying to read and write to the HDD at the same time would it be quicker to move all the data to memory? I could then use my quad core CPU flat out. Avoiding the bottleneck of my HDD.

i am expecting one of three answers:
1, no, stop being silly.
2, yes but the specific program your using would have to be writtern to do so.
3, yes, this is how

Thanks.:p
Dave_L
March 12th, 2012, 10:34 PM
You can easily create a RAM disk.

Example of creating a 512mb RAM disk with mount point /tmp/ram:


mkdir -p /tmp/ram
sudo mount -t tmpfs -o size=512M tmpfs /tmp/ram/
Ceiber Boy
March 13th, 2012, 11:24 AM
You can easily create a RAM disk.

Example of creating a 512mb RAM disk with mount point /tmp/ram:


mkdir -p /tmp/ram
sudo mount -t tmpfs -o size=512M tmpfs /tmp/ram/


Brilliant. Thanks you Dave
Bucky Ball
March 13th, 2012, 11:32 AM
Ceiber Boy and others please take note; from the heading of this page:


The Community Chat area is for lighthearted and enjoyable discussions, like you might find around a water cooler at work.

This forum is not intended for problem-solving. Please use appropriate forums in future. Thanks. ;)
