http://unix.stackexchange.com/questions/16809/how-to-wget-youtube-horrible-url
filename: wget-or-curl-Youtube-videos.txt

How-to-wget-or-curl-Youtube-videos.txt

How to wget Youtube horrible URL
up vote 5 down vote favorite

I'm going to wget youtube search page where the URL is horrible. This is example when searching
searchkeyword sorted by upload date:
$> wget http://www.youtube.com/results?search_type=videos&search_query=searchkeyword&search_sort=video_date_uploaded&\
suggested_categories=26%2C27%2C22%2C28%2C24

and this one is page 2 of the search result:
$> wget http://www.youtube.com/results?search_type=videos&search_query=searchkeyword&search_sort=video_date_uploaded\
&suggested_categories=26%2C27%2C22%2C28%2C24&page=2

The wget give me misleading page for both pages.

***
Wrapping the URL in single quotation marks should do the trick. Also, you will want to give a name to the
downloaded file as well (otherwise it will have the weird name that wget guessed from the URL). So:
$> wget 'http://www.youtube.com/results?search_type=videos&search_query=searchkeyword&search_sort=video_date_uploaded\
&suggested_categories=26%2C27%2C22%2C28%2C24' -O nice_name.html

Explanation as per laebshade's comment (thanks for reminding me): The many funny characters in the URL will
be interpreted by the shell as special commands or other control sequences.
From the top of my head, ~ ! # ^ & * will all mean something else to the shell (depends on what shell you
are using). You can use \ to escape them one by one, or quote the whole thing.

***
It would be good to explain the reason for using quotes. The reason it's not giving you the right URL is
because the URL contains an '&' (ampersand) character, which your shell interprets as a fork() to background;
in other words, your shell sends the first part of the command $> wget http://youtube.com/results?search_type=videos&
to the background, then treats the next part of the "command" as a new one, until it reaches another &, and
so on and so forth.

***
Don't use wget for downloading individual pages. It is better suited for things like recursive downloads of
whole sites. Try curl instead.

Also, make sure you quote your arguments on the command line so that you don't run into shell globing
problems with characters like ?.

$> curl 'http://www.youtube.com/results?search_type=videos&search_query=searchkeyword&\
search_sort=video_date_uploaded&suggested_categories=26%2C27%2C22%2C28%2C24&page=2' > results.html

***
Instead of > results.html you can also use curl http:://url.com/randompage -o results.html

***
Use youtube-dl for downloading Youtube videos. It's a epic python script. It req's Python. If you have Ubuntu,
install with this command: sudo apt-get install youtube-dl otherwise you can directly download it from the
website: http://rg3.github.com/youtube-dl/

---
http://linux.byexamples.com/archives/302/how-to-wget-flv-from-youtube/

how to wget flv from youtube
August 1st, 2007

To strip flv from youtube links, you have to complete two steps. Firstly extract the encrypted string from a
youtube link, next append to get_video.php and download through wget.

The youtube links, that people distribute around will looks something like below
http://www.youtube.com/watch?v=B6fnR--IDKc

Step one, download it and extract the encrypted strings using grep.
$> wget -O test http://www.youtube.com/watch?v=B6fnR--IDKc

-O is to force the output filename as specified. The case above, output filename will be 'test'.

To extract the encrypted string, use grep
$> grep "player2.swf" test

Updates! Due to the changes of youtube, the grep shown above was no longer working, grep 'fullscreen' instead,
read the comment for more details.


The encrypted string will be in between player2.swf? and ??

hl=en&video_id=B6fnR--IDKc&l=34&t=OEgsToPDskIlg4F-gKjbwl_pqIP6IPeI&soff=1&sk=V7GrH18hYLfT0sz5dm7GXgC

Step two, append the string to http://www.youtube.com/get_video.php? and start download.
$> wget -O test.flv "http://www.youtube.com/get_video.php?hl=en&video_id=B6fnR--IDKc&l=34&t=OEgsToPDskIlg4F-\
gKjbwl_pqIP6IPeI&soff=1&sk=V7GrH18hYLfT0sz5dm7GXgC"

Too much manual works?
You can write a bash script to ease your download, turn all manual steps into one simple step. Let say the
script is know as dltube.sh, you just need to execute this:
$> dltube.sh http://www.youtube.com/watch?v=B6fnR--IDKc

You will get the output filename as the_title_of_the_youtube.flv.

There are some hints , in case you are interested on creating this bash script.
1. The title of the flv, you can grep it from ‘test’ (first output file from youtube link shows above).
$> grep "id=\"video_title\"" test

The output will be
<h1 id="video_title">xvidcap screen capture on mplayer</h1>

2. You can extract the title from the grep string using cut.
$> grep "id=\"video_title\"" test | cut -d">" -f2 | cut -d"<" -f1

Check out command cut for usage examples.

wget, a powerful downloader
If you think wget is only a command line downloader, you are wrong. wget is capable of doing a lots of
download. Bellow ...

Performs a batch download using wget
If I want to download all files in a directory of a website, i have to perform a long wget command, they is
what I do at...

wget files based on the links in a file.
Let say I store all my links into a file call url2dl.txt and separate each link by newline. I can do this
wget -i url2d...


---

