filename: jv_java-module-system-tutorial-multif_20200813.txt
https://blog.codefx.org/java/java-module-system-tutorial/

Code-First Java Module System Tutorial

   The Java Platform Module System (JPMS) brings modularization to Java and the JVM and it changes how
   we program in the large. To get the most out of it, we need to know it well, and the first step is to
   learn the basics. In this tutorial I'll first show you a simple Hello World example and then we'll
   take an existing demo application and modularize it with Java 9. We will create module declarations (
   module-info.java) and use the module path to compile, package, and run the application - code first,
   explanations second, so you can cut to the chase.

   I use two projects in this tutorial and both can be found on GitHub: The first is a very simple
   Hello World example, the other the ServiceMonitor, which is the same one I use in my book on
   the module system. Check them out if you want to take a closer look. All commands like javac, jar,
   and java refer to the Java 9 variants.

Hello, Modular World
   Let's start with the simplest possible application, one that prints Hello, modular World! Here's the
   class:

package org.codefx.demo.jpms;

public class HelloModularWorld {

    public static void main(String[] args) {
        System.out.println("Hello, modular World!");
    }

}

   To become a module, it needs a module-info.java in the project's root source directory:

module org.codefx.demo.jpms_hello_world {
    // this module only needs types from the base module 'java.base';
    // because every Java module needs 'java.base', it is not necessary
    // to explicitly require it - I do it nonetheless for demo purposes
    requires java.base;
    // this export makes little sense for the application,
    // but once again, I do this for demo purposes
    exports org.codefx.demo.jpms;
}

   With the common src/main/java directory structure, the program's directory layout looks as follows:

  JPMS-Hello-World/
  +-- src/
      +-- main/
          +-- java/
              +-- org.codefx.demo.jpms/
              |   +-- HelloModularWorld.java
              +-- module-info.java

   These are the commands to compile, package and launch it:

$> javac
    -d target/classes
    ${source-files}
$> jar --create
    --file target/jpms-hello-world.jar
    --main-class org.codefx.demo.jpms.HelloModularWorld
    -C target/classes .
$> java
    --module-path target/jpms-hello-world.jar
    --module org.codefx.demo.jpms_hello_world

   Very similar to what we would have done for a non-modular application, except we're now using
   something called a "module path" and can define the project's main class (without a manifest). Let's
   see how that works.

Modules
     Modules are like JARs with additional characteristics

   The basic building block of the JPMS are modules (surprise!). Like JARs, they are a container for
   types and resources; but unlike JARs, they have additional characteristics - these are the most
   fundamental ones:
     * a name, preferably one that is globally unique
     * declarations of dependencies on other modules
     * a clearly defined API that consists of exported packages

   The JDK was split into about a hundred so-called platform modules. You can list them with java
   --list-modules and look at an individual module with java --describe-module ${module}. Go ahead, give
   it a try with java.sql or java.logging:

$> java --describe-module java.sql

> java.sql@9
> exports java.sql
> exports javax.sql
> exports javax.transaction.xa
> requires java.logging transitive
> requires java.base mandated
> requires java.xml transitive
> uses java.sql.Driver

   A module's properties are defined in a module declaration, a file module-info.java in the project's
   root, which looks as follows:

module ${module-name} {
   requires ${module-name};
   exports ${package-name};
}

   It gets compiled into a module-info.class, called module descriptor, and ends up in the JAR's root.
   This descriptor is the only difference between a plain JAR and a modular JAR.

   Let's go through the three module properties one by one: name, dependencies, exports.

Name
   The most basic property that JARs are missing is a name that compiler and JVM can use to identify it
   with. It is hence the most prominent characteristic of a module. We will have the possibility and
   even the obligation to give every module we create a name.

     The best name for a module is the reverse-domain naming scheme that is already commonly used for
     packages

   Naming a module will often be pretty natural as most tools we use on a daily basis, be it IDEs, build
   tools, or even issue trackers and version control systems, already have us name our projects. But
   while it makes sense to take that name as a springboard on the search for a module name, it is
   important to choose wisely!

   The module system leans heavily on a module's name. Conflicting or evolving names in particular cause
   trouble, so it is important that the name is:
     * globally unique
     * stable

   The best way to achieve that is the reverse-domain naming scheme that is already commonly used for
   packages:

module org.codefx.demo.jpms {

}

Dependencies And Readability
     All dependencies have to be made explicit with requires directives

   Another thing we missed in JARs was the ability to declare dependencies, but with the module system,
   these times are over: Dependencies have to be made explicit - all of them, on JDK modules as well as
   on third-party libraries or frameworks.

   Dependencies are declared with requires directives, which consist of the keyword itself followed by a
   module name. When scanning modules, the JPMS builds a readability graph, where modules are nodes and
   requires directives get turned into so-called readability edges - if module org.codefx.demo.jpms
   requires module java.base, then at runtime org.codefx.demo.jpms reads java.base.

   The module system will throw an error if it cannot find a required module with the right name, which
   means compiling as well as launching an application will fail if modules are missing. This achieves
   reliable configuration one of the goals of the module system, but can be prohibitively strict -
   check my post on optional dependencies to see a more lenient alternative.

   All types the Hello World example needs can be found in the JDK module java.base, the so-called base
   module. Because it contains essential types like Object, all Java code needs it and so it doesn't
   have to be required explicitly. Still, I do it in this case to show you a requires directive:

module org.codefx.demo.jpms {
   requires java.base;
}

Exports And Accessibility
     A module's API is defined by its exports directives

   A module lists the packages it exports. For code in one module (say org.codefx.demo.jpms) to access
   types in another (say String in java.base), the following accessibility rules must be fulfilled:
     * the accessed type ( String) must be public
     * the package containing the type ( java.lang) must be exported by its module (java.base)
     * the accessing module (org.codefx.demo.jpms) must read the accessed one (java.base), which is
       typically achieved by requiring it

     Reflection lost its superpowers

   If any of these rules are violated at compile or run time, the module systems throws an error. This
   means that public is no longer really public. A public type in a non-exported package is as
   inaccessible to the outside world as a non-public type in an exported package. Also note that
   reflection lost its superpowers. It is bound by the exact same accessibility rules unless command
   line flags are used.

   Since our example has no meaningful API, no outside code needs to access it and so we don't actually
   have to export anything. Once again I'll do it nonetheless for demonstration purposes:

module org.codefx.demo.jpms_hello_world {
   requires java.base;
   exports org.codefx.demo.jpms;
}

Module Path
   We now know how we can define modules and their essential properties. What's still a little unclear
   is how exactly we tell the compiler and runtime about them. The answer is a new concept that
   parallels the class path:

   The module path is a list whose elements are artifacts or directories that contain artifacts.
   Depending on the operating system, module path elements are either separated by : (Unix-based) or ;
   (Windows). It is used by the module system to locate required modules that are not found among the
   platform modules. Both javac and java as well as other module-related commands can process it - the
   command line options are --module-path and -p.

   All artifacts on the module path are turned into modules. This is even true for plain JARs, which get
   turned into automatic modules.

Compiling, Packaging, Running
   Compiling works much like without the module system:

$> javac
   -d target/classes
   ${source-files}

   (You of course have to replace ${source-files} with an actual enumeration of the involved files, but
   that crowds the examples, so I don't do it here.)

   The module system kicks in as soon as a module-info.java is among the source files. All non-JDK
   dependencies the module under compilation requires need to be on the module path. For the Hello World
   example, there are no such dependencies.

   Packaging with jar is unchanged as well. The only difference is that we no longer need a manifest to
   declare an application's entry point - we can use --main-class for that:

$> jar --create
   --file target/jpms-hello-world.jar
   --main-class org.codefx.demo.jpms.HelloModularWorld
   -C target/classes .

   Finally, launching looks a little different. We use the module path instead of the class path to tell
   the JPMS where to find modules. All we need to do beyond that is to name the main module with
   --module:

$> java
   --module-path target/jpms-hello-world.jar
   --module org.codefx.demo.jpms_hello_world

   And that's it! We've created a very simple, but nonetheless modular Hello-World application and
   successfully build and launched it. Now it's time to turn to a slightly less trivial example to see
   mechanisms like dependencies and exports in action.

The ServiceMonitor
   Let's imagine a network of services that cooperate to delight our users; maybe a social network or a
   video platform. We want to monitor those services to determine how healthy the system is and spot
   problems when they occur (instead of when customers report them). This is where the example
   application, the ServiceMonitor comes in: It monitors these services (another big surprise).

   As luck would have it, the services already collect the data we want, so all the ServiceMonitor needs
   to do is query them periodically. Unfortunately not all services expose the same REST API - two
   generations are in use, Alpha and Beta. That's why ServiceObserver is an interface with two
   implementations.

   Once we have the diagnostic data, in the form of a DiagnosticDataPoint, they can be fed to a
   Statistician, which aggregates them to Statistics. These, in turn, are stored in a
   StatisticsRepository as well as made available via REST by MonitorServer. The Monitor class ties
   everything together.

   All in all, we end up with these types:
     * DiagnosticDataPoint: service data for a time interval
     * ServiceObserver: interface for service observation that returns DiagnosticDataPoint
     * AlphaServiceObserver and BetaServiceObserver: each observes a variant of services
     * Statistician: computes Statistics from DiagnosticDataPoint
     * Statistics: holds the computed statistics
     * StatisticsRepository: stores and retrieve Statistics
     * MonitorServer: answers REST calls for the statistics
     * Monitor: ties everything together

   ServiceMonitor's classes

   The application depends on the Spark micro web framework and we reference it by the module name
   spark.core. It can be found in the libs directory together with its transitive dependencies.

   With what we learned so far, we already know how to organize the application as a single module.
   First, we create the module declaration module-info.java in the project's root:

module monitor {
   requires spark.core;
}

   Note that we should choose a module name like org.codefx.demo.monitor, but that would crowd the
   examples, so I'll stick to the shorter monitor. As explained, it requires spark.core and because the
   application has no meaningful API, it exports no packages.

   We can then compile, package, and run it as follows:

$> javac
   --module-path libs
   -d classes/monitor
   ${source-files}
$> jar --create
   --file mods/monitor.jar
   --main-class monitor.Main
   -C classes/monitor .
$> java
   --module-path mods
   --module monitor

   As you can see, we no longer use Maven's target directory and instead create classes in classes and
   modules in mods. This makes the examples easier to parse. Note that unlike earlier, we already have
   to use the module path during compilation because this application has non-JDK dependencies.

   And with that we've created a single-module ServiceMonitor!

Splitting Into Modules
   Now that we got one module going, it's time to really start using the module system and split the
   ServiceMonitor up. For an application of this size it is of course ludicrous to turn it into several
   modules, but it's a demo, so here we go.

   The most common way to modularize applications is a separation by concerns. ServiceMonitor has the
   following, with the related types in parenthesis:
     * collecting data from services ( ServiceObserver, DiagnosticDataPoint)
     * aggregating data into statistics ( Statistician, Statistics)
     * persisting statistics ( StatisticsRepository)
     * exposing statistics via a REST API ( MonitorServer)

   But not only the domain logic generates requirements. There are also technical ones:
     * data collection must be hidden behind an API
     * Alpha and Beta services each require a separate implementation of that API ( AlphaServiceObserver
       and BetaServiceObserver)
     * orchestration of all concerns ( Monitor)

   This results in the following modules with the mentioned publicly visible types:
     * monitor.observer ( ServiceObserver, DiagnosticDataPoint)
     * monitor.observer.alpha ( AlphaServiceObserver)
     * monitor.observer.beta ( BetaServiceObserver)
     * monitor.statistics ( Statistician, Statistics)
     * monitor.persistence ( StatisticsRepository)
     * monitor.rest ( MonitorServer)
     * monitor ( Monitor)

   Superimposing these modules over the class diagram, it is easy to see the module dependencies emerge:
   Monitor's modules

Reorganizing Source Code
   A real-life project consists of myriad files of many different types. Obviously, source files are the
   most important ones but nonetheless only one kind of many - others are test sources, resources, build
   scripts or project descriptions, documentation, source control information, and many others. Any
   project has to choose a directory structure to organize those files and it is important to make sure
   it does not clash with the module system's characteristics.

   If you have been following the module system's development under Project Jigsaw and studied the
   official quick start guide or some early tutorials, you might have noticed that they use a
   particular directory structure, where there's a src directory with a subdirectory for each project.
   That way ServiceMonitor would look as follows:

ServiceMonitor
+ classes
+ mods
- src
   + monitor
   - monitor.observer
      - monitor
         - observer
            DiagnosticDataPoint.java
            ServiceObserver.java
      module-info.java
   + monitor.observer.alpha
   + monitor.observer.beta
   + monitor.persistence
   + monitor.rest
   + monitor.statistics
- test-src
   + monitor
   + monitor.observer
   + monitor.observer.alpha
   + monitor.observer.beta
   + monitor.persistence
   + monitor.rest
   + monitor.statistics

   This results in a hierarchy concern/module and I don't like it. Most projects that consist of several
   sub-projects (what we now call modules) prefer separate root directories, where each contains a
   single module's sources, tests, resources, and everything else mentioned earlier. They use a
   hierarchy module/concern and this is what established project structures provide.

   The default directory structure, implicitly understood by tools like Maven and Gradle, implement that
   hierarchy. First and foremost, they give each module its own directory tree. In that tree the src
   directory contains production code and resources (in main/java and main/resources, respectively) as
   well as test code and resources (in test/java and test/resources, respectively):

ServiceMonitor
+ monitor
- monitor.observer
   - src
      - main
         - java
            - monitor
               - observer
                  DiagnosticDataPoint.java
                  ServiceObserver.java
            module-info.java
         + resources
      + test
         + java
         + resources
   + target
+ monitor.observer.alpha
+ monitor.observer.beta
+ monitor.persistence
+ monitor.rest
+ monitor.statistics

   I will organize the ServiceMonitor almost like that, with the only difference that I will create the
   bytecode in a directory classes and JARS in a directory mods, which are both right below
   ServiceMonitor, because that makes the scripts shorter and more readable.

   Let's now see what those declarations infos have to contain and how we can compile and run the
   application.

Declaring Modules
   We've already covered how modules are declared using module-info.java, so there's no need to go into
   details. Once you've figured out how modules need to depend on one another (your build tool should
   know that; otherwise ask JDeps), you can put in requires directives and the necessary exports
   emerge naturally from imports across module boundaries.

module monitor.observer {
   exports monitor.observer;
}

module monitor.observer.alpha {
   requires monitor.observer;
   exports monitor.observer.alpha;
}

module monitor.observer.beta {
   requires monitor.observer;
   exports monitor.observer.beta;
}

module monitor.statistics {
   requires monitor.observer;
   exports monitor.statistics;
}

module monitor.persistence {
   requires monitor.statistics;
   exports monitor.persistence;
}

module monitor.rest {
   requires spark.core;
   requires monitor.statistics;
   exports monitor.rest;
}

module monitor {
   requires monitor.observer;
   requires monitor.observer.alpha;
   requires monitor.observer.beta;
   requires monitor.statistics;
   requires monitor.persistence;
   requires monitor.rest;
}

   By the way, you can use JDeps to create an initial set of module declarations. Whether created
   automatically or manually, in a real-life project you should verify whether your dependencies and
   APIs are as you want them to be. It is likely that over time, some quick fixes introduced
   relationships that you'd rather get rid of. Do that now or create some backlog issues.

Compiling, Packaging, And Running
   Very similar to before when it was only a single module, but more often:

$> javac
   -d classes/monitor.observer
   ${source-files}
$> jar --create
   --file mods/monitor.observer.jar
   -C classes/monitor.observer .

# monitor.observer.alpha depends on monitor.observer,
# so we place 'mods', which contains monitor.observer.jar,
# on the module path
$> javac
   --module-path mods
   -d classes/monitor.observer.alpha
   ${source-files}
$> jar --create
   --file mods/monitor.observer.alpha.jar
   -C classes/monitor.observer.alpha .

# more of the same ... until we come to monitor,
# which once again defines a main class
$> javac
   --module-path mods
   -d classes/monitor
   ${source-files}
$> jar --create
   --file mods/monitor.jar
   --main-class monitor.Main
   -C classes/monitor .

   Congratulations, you've got the basics covered! You now know how to organize, declare, compile,
   package, and launch modules and understand what role the module path, the readability graph, and
   modular JARs play.

On The Horizon
   If you weren't so damn curious this post could be over now, but instead I'm going to show you a few
   of the more advanced features, so you know what to read about next.

Implied Readability
   The ServiceMonitor module monitor.observer.alpha describes itself as follows:

module monitor.observer.alpha {
   requires monitor.observer;
   exports monitor.observer.alpha;
}

   Instead it should actually do this:

module monitor.observer.alpha {
   requires transitive monitor.observer;
   exports monitor.observer.alpha;
}

   Spot the transitive in there? It makes sure that any module reading monitor.observer.alpha also reads
   monitor.observer. Why would you do that? Here's a method from alpha's public API:

public static Optional<ServiceObserver> createIfAlphaService(String service) {
   // ...
}

   It returns an Optional<ServiceObserver>, but ServiceObserver comes from the monitor.observer module -
   that means every module that wants to call alpha's createIfAlphaService needs to read
   monitor.observer as well or such code won't compile. That's pretty inconvenient, so modules like
   alpha that use another module's type in their own public API should generally require that module
   with the transitive modifier.

   There are more uses for implied readability.

Optional Dependencies

   This is quite straight-forward: If you want to compile against a module's types, but don't want to
   force its presence at runtime you can mark your dependency as being optional with the static
   modifier:

module monitor {
   requires monitor.observer;
   requires static monitor.observer.alpha;
   requires static monitor.observer.beta;
   requires monitor.statistics;
   requires monitor.persistence;
   requires static monitor.rest;
}

   In this case monitor seems to be ok with the alpha and beta observer implementations possibly being
   absent and it looks like the REST endpoint is optional, too.

   There are a few things to consider when coding against optional dependencies.

Qualified Exports
   Regular exports have you make the decision whether a package's public types are accessible only
   within the same module or to all modules. Sometimes you need something in between, though. If you're
   shipping a bunch of modules, you might end up in the situation, where you'd like to share code
   between those modules but not outside of it. Qualified exports to the rescue!

module monitor.util {
   exports monitor.util to monitor, monitor.statistics;
}

   This way only monitor and monitor.statistics can access the monitor.util package.

Open Packages And Modules
   I said earlier that reflection's superpowers were revoked - it now has to play by the same rules as
   regular access. Reflection still has a special place in Java's ecosystem, though, as it enables
   frameworks like Hibernate, Spring and so many others.

   The bridge between those two poles are open packages and modules:

module monitor.persistence {
   opens monitor.persistence.dtos;
}

// or even

open module monitor.persistence.dtos { }

   An open package is inaccessible at compile time (so you can't write code against its types), but
   accessible at run time (so reflection works). More than just being accessible, it allows reflective
   access to non-public types and members (this is called deem reflection). Open packages can be
   qualified just like exports and open modules simply open all their packages.

Services
   Instead of having the main module monitor depend on monitor.observer.alpha and monitor.observer.beta,
   so it can create instances of AlphaServiceObserver and BetaServiceObserver, it could let the module
   system make that connection:

module monitor {
   requires monitor.observer;
   // monitor wants to use a service
   uses monitor.observer.ServiceObserverFactory;
   requires monitor.statistics;
   requires monitor.persistence;
   requires monitor.rest;
}

module monitor.observer.alpha {
   requires monitor.observer;
   // alpha provides a service implementation
   provides monitor.observer.ServiceObserverFactory
       with monitor.observer.alpha.AlphaServiceObserverFactory;
}

module monitor.observer.beta {
   requires monitor.observer;
   // beta provides a service implementation
   provides monitor.observer.ServiceObserverFactory
       with monitor.observer.beta.BetaServiceObserverFactory;
}

   This way, monitor can do the following to get an instance of each provided observer factory:

List<ServiceObserverFactory> observerFactories = ServiceLoader
   .load(ServiceObserverFactory.class).stream()
   .map(Provider::get)
   .collect(toList());

   It uses the ServiceLoader API, which exists since Java 6, to inform the module system that it
   needs all implementations of ServiceObserverFactory. The JPMS will then track down all modules in the
   readability graph that provide that service, create an instance of each and return them.

   There are two particularly interesting consequences:
     * the module consuming the service does not have to require the modules providing it
     * the application can be configured by selecting which modules are placed on the module path

   Services are a wonderful way to decouple modules and its awesome that the module system gives this
   mostly ignored concept a second life and puts it into a prominent place.

Reflection
   Ok, we're really done now and you've learned a lot. Quick recap:
     * a module is a run-time concept created from a modular JAR
     * a modular JAR is like any old plain JAR, except that it contains a module descriptor
       module-info.class, which is compiled from a module declaration module-info.java
     * the module declaration gives a module its name, defines its dependencies (with requires, requires
       static, and requires transitive) and API (with exports and exports to), enables reflective access
       (with open and opens to) and declares use or provision of services
     * modules are placed on the module path where the JPMS finds them during module resolution, which
       is the phase that processes descriptors and results in a readability graph

   If you want to learn more about the module system, read the posts I linked above, check the JPMS
   tag, or get my book The Java Module System (Manning). Also, be aware that migrating to Java 9 can
   be challenging - check my migration guide for details.


---
https://blog.codefx.org/java/module-system-optional-dependencies/

Optional Dependencies with 'requires static'

   The Java Platform Module System (JPMS) has a strong opinion on dependencies: By default, they
   need to be required (to be accessible) and then they need to be present both at compile and at run
   time. This does not work with optional dependencies, though, where code is written against artifacts
   that are not necessarily present at run time. Fortunately, the JPMS has a requires static clause that
   can be used in these exact situations.

   I will show you a couple of examples in which the default behavior's strictness leads to problems and
   then introduce the module system's solution to optional dependencies: requires static. Coding against
   them is not trivial, though, so we will have a close look at that as well.

Overview
   If you don't know much about the module system yet, you should read this tutorial, so you've for
   the basics covered. Some examples build on the optional-dependencies branch of a small demo
   application, called the ServiceMonitor.

The Conundrum Of Unrequired Dependencies
   To nail down where exactly the strictness of regular requires clauses leads to problems, I want to
   start with two examples. While similar in some aspects there are differences that become important
   later when we discuss how we code against potentially missing dependencies.

The Utility Library
   Let's start with an imaginary library we're maintaining, uber.lib, that integrates with a handful of
   other libraries. Its API offers functionality that builds on them and thus exposes their types. We'll
   play this through with the example of com.google.guava, which in our hypothetical scenario was
   already turned into a Java module that uber.lib wants to code against.

   As maintainers of uber.lib we assume that nobody who is not already using Guava is ever going to call
   the Guava portion of our library. This makes sense in certain cases: Why would you call a method in
   uber.lib that creates a nice report for a com.google.common.graph.Graph instance if you don't have
   such a graph?

   For uber.lib that means that it can function perfectly without com.google.guava: If Guava makes it
   into the module graph, clients might call into that portion of the uber.lib API. If it doesn't,
   they won't and the library will be fine as well. We can say that uber.lib never needs the dependency
   for its own sake.

     With regular dependencies optional relationships can not be implemented.

   With regular requires clauses, such an optional relationship can not be implemented, though.
   According to the rules for readability and accessibility, uber.lib has to require
   com.google.guava to compile against its types but this forces all clients to always have Guava on the
   module path when launching their application.

   If uber.lib integrates with a handful of libraries, it would make clients depend on all of them even
   though they might never use more than one.
   That's not a nice move from us.

The Fancy Statistics Library
   The second example comes from the demo application, which contains a module monitor.statistics.
   Let's assume there was some advanced statistics library containing a module stats.fancy that
   monitor.statistics wants to use but which could not be present on the module path for each deployment
   of the application. (The reason for that is irrelevant but let's go with a license that prevents the
   fancy code from being used "for evil" but, evil masterminds that we are, we occasionally want to do
   just that.)

   We would like to write code in monitor.statistics that uses types from the fancy module but for that
   to work we need to depend on it with a requires clause. If we do that, though, the module system
   would not let the application launch if stats.fancy is not present.

   The conundrum of optional dependencies

   Deadlock. Again.

Optional Dependencies With 'requires static'
   When a module needs to be compiled against types from another module but does not want to depend on
   it at run time, it can use a requires static clause. If foo requires static bar, the module system
   behaves different at compile and run time:
     * At compile time, bar must be present or there will be an error. During compilation bar is
       readable by foo.
     * At run time, bar might be absent and that will cause neither error nor warning. If it is present,
       it is readable by foo.

   We can immediately put this into action and create an optional dependency from monitor.statistics to
   stats.fancy:

module monitor.statistics {
   requires monitor.observer;
   requires static stats.fancy;
   exports monitor.statistics;
}

   If stats.fancy is missing during compilation, we get an error when the module declaration is
   compiled:

monitor.statistics/src/main/java/module-info.java:3:
   error: module not found: stats.fancy
       requires static stats.fancy;
                            ^
1 error

   At launch time, though, the module system does not care whether stats.fancy is present or not.

   Similarly, the module descriptor for uber.lib declares all dependencies as optional:

module uber.lib {
   requires static com.google.guava;
   requires static org.apache.commons.lang;
   requires static org.apache.commons.io;
   requires static io.javaslang;
   requires static com.aol.cyclops;
}

   Now that we know how to declare optional dependencies, two questions remain to be answered:
     * Under what circumstances will it be present?
     * How can we code against an an optional dependency?

   We will answer both questions next.

Resolution Of Optional Dependencies
   Module resolution is the process that, given an initial module and a universe of observable
   modules, builds a module graph by resolving requires clauses. When a module is being resolved, all
   modules it requires must be found in the universe of observable modules. If they are, they are added
   to the module graph; otherwise an error occurs. It is important to note that modules that did not
   make it into the module graph during resolution are not available later during compilation or
   execution, either.

   At compile time, module resolution handles optional dependencies just like regular dependencies. At
   run time, though, requires static clauses are mostly ignored. When the module system encounters one
   it does not try to fulfill it, meaning it does not even check whether the named module is present in
   the universe of observable modules.

     A module that is only an optional dependency will not be available at run time.

   As a consequence even if a module is present on the module path (or in the JDK for that matter), it
   will not be added to the module graph just because of an optional dependency. It will only make it
   into the graph if it is also a regular dependency of some other module that is being resolved or
   because it was added explicitly with the command line flag --add-modules.

   Maybe you stumbled across the phrase that optional dependencies "are mostly ignored". Why mostly?
   Well, one thing the module system does is if an optional dependency makes it into a graph, a
   readability edge is added. This ensures that if the optional module is present, its types can be
   accessed straight away.

Coding Against Optional Dependencies

   Optional dependencies require a little more thought when writing code against them because this is
   what happens when monitor.statistics uses types in stats.fancy but the module isn't present at run
   time:

Exception in thread "main" java.lang.NoClassDefFoundError:
   stats/fancy/FancyStats
       at monitor.statistics/monitor.statistics.Statistician
           .<init>(Statistician.java:15)
       at monitor/monitor.Main.createMonitor(Main.java:42)
       at monitor/monitor.Main.main(Main.java:22)
Caused by: java.lang.ClassNotFoundException: stats.fancy.FancyStats
       ¨... many more

   Oops. We usually don't want our code to do that.

   Generally speaking, when the code that is currently being executed references a type, the Java
   Virtual Machine checks whether it is already loaded. If not, it tells the class loader to do that and
   if that fails, the result is a NoClassDefFoundError, which usually crashes the application or at
   least fails out of the chunk of logic that was being executed.

     With optional dependencies we opt out of the checks that make the module system safe.

   This is something JAR hell was famous for and that the module system wants to overcome by
   checking declared dependencies when launching an application. But with requires static we opt out of
   that check, which means we can end up with a NoClassDefFoundError after all. What can we do against
   that?

Established Dependency
   Before looking into solutions, though, we need to see whether we really have a problem. In the case
   of uber.lib we expect to only use types from an optional dependency if the code calling into the
   library already uses them, meaning class loading already succeeded.

   In other words, when uber.lib gets called all required dependencies must be present or the call would
   not have been possible. So we don't have a problem after all and don't need to do anything.

   An optional dependency might always be present when the calling code gets executed

Internal Dependency
   The general case is different, though. It might very well be the module with the optional dependency
   that first tries to load classes from it, so the risk of a NoClassDefFoundError is very real.

   An optional dependency might only be used internally

   One solution for this is to make sure that all possible calls into the module with the optional
   dependency have to go through a checkpoint before accessing the dependency. That checkpoint has to
   evaluate whether the dependency is present and send all code that arrives at it down a different
   execution path if it isn't.

   It might be necessary to check presence of an optional dependency

   The module system offers a way to check whether a module is present. I explained in my newsletter
   how to get there and why I use the new stack-walking API, so here you'll just have to trust me
   when I say that this is the way to go:

import static java.lang.StackWalker.Option.RETAIN_CLASS_REFERENCE;
import java.lang.StackWalker.StackFrame;

public class ModuleUtils {

   public static boolean isModulePresent(String moduleName) {
       return StackWalker
               .getInstance(RETAIN_CLASS_REFERENCE)
               .walk(frames -> frames
                       .map(StackFrame::getDeclaringClass)
                       .filter(declaringClass ->
                               declaringClass != ModuleUtils.class)
                       .findFirst()
                       .orElse((Class) ModuleUtils.class))
               .getModule()
               .getLayer()
               .findModule(moduleName)
               .isPresent();
       // chain all the methods!
   }

}

   (In a real application it might make sense to cache the value as to not always repeat the same
   check.)

   Calling this method with an argument like "stats.fancy" will return whether that module is present.
   If called with the name of a regular dependency (simple requires clause), the result will always be
   true because otherwise the module system would not have let the application launch. If called with
   the name of an optional dependency ( requires static clause), the result will either be true or
   false.

   If an optional dependency is present, the module system established readability and so it is safe to
   go down an execution path that uses types from the module. If it is absent, choosing such a path
   would lead to a NoClassDefFoundError, so a different one has to be found.

Summary
   Sometimes you want to write code against a dependency that might not always be present at run time.
   To make the dependency's types available at compile time but not enforce its presence at launch time,
   the module system offers the requires static clause. Note, though, that a module does not get picked
   up during resolution if it is only referenced this way and that special care needs to be taken to
   make sure code does not crash if the optional dependency is absent at run time.

   To learn more about the module system check out the JPMS tag or get my book The Java 9 Module
   System (with Manning). If you're interested in the historical perspective, check the Project
   Jigsaw tag.


---
https://blog.codefx.org/java/java-9-migration-guide/

Java 9 Migration Guide: The Seven Most Common Challenges

   I'm sure you've heard that updating to Java 9 is no walk in the park, maybe even that it's an
   incompatible update and that a migration makes no sense for large code bases. After doing exactly
   that, migrating an old and fairly large code base, I can tell you that it's not that bad. It's more
   work than bumping to Java 8, true, but it's time well spent. More than anything else, the migration
   uncovered some small and a few not so small problems that needed fixing regardless of the migration
   itself and we took the opportunity to do just that.

   I collected a few surprising details over at java9.wtf but condensed the seven largest issues
   into this Java 9 migration guide. It's as much a post as it is a resource to come back to, so put it
   on speed dial and search it when you have a concrete problem. Also note that while you need to know a
   bit about the module system (here's a tutorial), this is not about modularizing your
   application - it is only about getting it to compile and run on Java 9.

Overview
   This is a list of the seven most likely problems to trip you up during a migration to Java 9:

   Each section explains the problem, the most common symptoms that help you identify it and a set of
   possible fixes (usually command line options for java or javac). A future post will tie
   individual fixes into a larger migration strategy and make some recommendations based on my
   experiences.

Illegal Access To Internal APIs
   One of the module system's biggest selling points is strong encapsulation. It makes sure non-public
   classes as well as classes from non-exported packages are inaccessible from outside the module. First
   and foremost, this of course applies to the platform modules shipped with the JDK, where only java.*
   and javax.* packages are fully supported. Most com.sun.* and sun.* packages, on the other hand, are
   internal and hence inaccessible by default.

   While the Java 9 compiler behaves exactly as you would expect and prevents illegal access, the same
   is not true for the run time. To offer a modicum of backwards compatibility it eases migration and
   improves the chances of applications built on Java 8 to run on Java 9 by granting access to internal
   classes. If reflection is used for the access, a warning is emitted.

Symptoms

   During compilation against Java 9 you see compile errors similar to the following:

error: package com.sun.java.swing.plaf.nimbus is not visible
import com.sun.java.swing.plaf.nimbus.NimbusLookAndFeel;
                             ^
   (package com.sun.java.swing.plaf.nimbus is declared
   in module java.desktop, which does not export it)
1 error

   Warnings emitted for reflection look as follows:

Static access to [Nimbus Look and Feel]
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by j9ms.internal.Nimbus
   (file:...) to constructor NimbusLookAndFeel()
WARNING: Please consider reporting this
   to the maintainers of j9ms.internal.Nimbus
WARNING: Use --illegal-access=warn to enable warnings
   of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Reflective access to [Nimbus Look and Feel]

Fixes
   The most obvious and sustainable fix for dependencies on internal APIs is to get rid of them. Replace
   them with maintained APIs and you paid back some high-risk technical debt.

   If that can't be done for whatever reason, the next best thing is to acknowledge the dependencies and
   inform the module system that you need to access it. To that end you can use two command line
   options:
     * The option --add-exports $module/$package=$readingmodule can be used to export $package of
       $module to $readingmodule. Code in $readingmodule can hence access all public types in $package
       but other modules can not. When setting $readingmodule to ALL-UNNAMED, all code from the class
       path can access that package. During a migration to Java 9, you will always use that placeholder.
       The option is available for the java and javac commands.
     * This covers access to public members of public types but reflection can do more than that: With
       the generous use of setAccessible(true) it allows interaction with non-public classes, fields,
       constructors, and methods (sometimes called deep reflection), which even in exported packages are
       still encapsulated. The java option --add-opens uses the same syntax as --add-exports and opens
       the package to deep reflection, meaning all of its types and their members are accessible
       regardless of their visibility modifiers.

   You obviously need --add-exports to appease the compiler but gathering --add-exports and --add-opens
   for the run time has advantages as well:
    1. the run time's permissive behavior will change in future Java releases, so you have to do that
       work at some point anyway
    2. --add-opens makes the warnings for illegal reflective access go away
    3. as I will show in a minute, you can make sure no new dependencies crop up by making the run time
       actually enforce strong encapsulation

Going Further
   Compiling against Java 9 helps hunting down dependencies on internal APIs in the project's code base.
   But the libraries and frameworks your project uses are just as likely to make trouble.

   JDeps is the perfect tool to find compile dependencies on JDK-internal APIs in your project and your
   dependencies. If you're not familiar with it, I've written a tutorial that gets you started.
   Here's how to use it for the task at hand:

$> jdeps --jdk-internals -R --class-path 'libs/*' $project

   Here, libs is a folder containing all of your dependencies and $project your project's JAR. Analyzing
   the output is beyond this article's scope but it's not that hard - you'll manage.

   Finding reflective access is a little tougher. The run time's default behavior is to warn you once
   for the first illegal access to a package, which is insufficient. Fortunately, there's the
   --illegal-access=$value option, where $value can be:
     * permit: Access to all JDK-internal APIs is permitted to code on the class path. For reflective
       access, a single warning is issued for the first access to each package. (Default in Java 9, but
       will be removed in a future release.)
     * warn: Behaves like permit but a warning is issued for each reflective access.
     * debug: Behaves like warn but a stack trace is included in each warning.
     * deny: The option for those who believe in strong encapsulation:
       All illegal access is forbidden by default.

   Particularly deny is very helpful to hunt down reflective access. It is also a great default value to
   set once you've collected all required --add-exports and --add-opens options. This way, no new
   dependencies can crop up without you noticing it.

Dependencies On Java EE Modules
   There's a lot of code in Java SE that's actually Java EE related. It ended up in these six modules:
     * java.activation with javax.activation package
     * java.corba with javax.activity, javax.rmi, javax.rmi.CORBA, and org.omg.* packages
     * java.transaction with javax.transaction package
     * java.xml.bind with all javax.xml.bind.* packages
     * java.xml.ws with javax.jws, javax.jws.soap, javax.xml.soap, and all javax.xml.ws.* packages
     * java.xml.ws.annotation with javax.annotation package

   For various compatibility reasons (one of them being split packages, which we will look at next),
   code on the class path does not see these modules by default, which leads to compile or run time
   errors.

Symptoms

   Here's a compile error for a class using JAXBException from the java.xml.bind module:

error: package javax.xml.bind is not visible
import javax.xml.bind.JAXBException;
               ^
   (package javax.xml.bind is declared in module java.xml.bind,
       which is not in the module graph)
1 error

   If you get it past the compiler but forget to massage the run time, you'll get a
   NoClassDefFoundError:

Exception in thread "main" java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException
   at monitor.Main.main(Main.java:27)
Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException
   at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582)
   at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:185)
   at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:496)
   ... 1 more

Fixes on Java 9 and 10
   Once you modularized your code, you can declare a regular dependency in the module's declaration.
   Until then, --add-modules $module comes to your rescue, which makes sure $module is available and can
   be added to both java and javac. If you add java.se.ee, you'll have access to all Java EE modules.

Fixes on Java 11 and later
   Java 11 removes the Java EE modules, so from then on you will need third-party implementations
   instead. This StackOverflow answer contains a list of alternatives. Note that using third-party
   dependencies already works from Java 9 on, so you don't have to use --add-modules as a stopgap.

Split Packages
   This one is a little tricky... To enforce consistency a module is not allowed to read the same package
   from two different modules. The actual implementation is stricter, though, and no two modules are
   allowed to even contain the same package (exported or not). The module system operates under that
   assumption and whenever a class needs to be loaded, it looks up which module contains that package
   and goes looking for the class in there (which should boost class loading performance).

   To safeguard the assumption the module system checks that no two named modules split a package and
   barfs if it finds any that do. During migration you're not quite in that situation, though. Your code
   comes from the class path, which puts it into the so-called unnamed module. To maximize compatibility
   it is not scrutinized and no module-related checks are applied to it.

   Now, in the case of split packages, this means a split between a named module (e.g. in the JDK) and
   the unnamed module is not discovered. Which may sound very fortunate, is the opposite if you mix in
   the class loading behavior: If a package is split between a module and the class path, for classes
   from that package class loading will always and only look into the module. This means classes in the
   class path portion of the package are effectively invisible.

Symptoms
   The symptom is that a class from the class path can not be loaded even though it's definitely there,
   leading to compile errors like this:

error: cannot find symbol
   symbol:   class Nonnull
   location: package javax.annotation

   Or, at run time, to NoClassDefFoundErrors like above.

   One example where this can occur is with the various JSR-305 implementations. A project using, for
   example, the annotations javax.annotation.Generated (from java.xml.ws.annotation) and
   java.annotation.Nonnull (from com.google.code.findbugs:jsr305) will have trouble compiling. It is
   either missing the Java EE annotations or, when the module is added like described above, will
   encounter a split package and not see the JSR 305 module.

Fixes
   The migration path will be different, depending on the artifact that splits the JDK package. In some
   cases it might be more than just some classes that go into a random JDK package but a replacement for
   an entire JDK module, for example because it overrides an endorsed standard. In that case, you
   are looking for the --upgrade-module-path $dir option - modules found in $dir are used to replace
   upgradeable modules in the run time.

   If you indeed just have a couple of classes that split a package, the long-term solution is to remove
   the split. In case that is not possible in the short-term, you can patch the named module with the
   content from the class path. The option --patch-module $module=$artifact will merge all classes from
   $artifact into $module, putting all portions of the split package into the same module, thus mending
   split.

   In the case of java.xml.ws.annotation and @Nonnull that would be
   --patch-module java.xml.ws.annotation=path/to/jsr305-3.0.2.jar. For this particular problem, there
   are other viable solutions, though, which I explore in a separate post.

   There are a few things to look out for, though. First of all, the patched module must actually make
   it into the module graph, for which it might be necessary to use --add-modules. Then, it must have
   access to all the dependencies that it needs to run successfully. Since named modules can not access
   code from the class path, this might make it necessary to start creating some automatic modules,
   which goes beyond the scope of this post.

Going Further
   Finding split package by try and error is pretty unnerving. Fortunately JDeps reports them, so if
   you analyze your project and its dependencies, the first lines of output will report split packages.
   You can use the same command as above:

$> jdeps --jdk-internals -R --class-path '$libs/*' project.jar

Casting To URL Class Loader
   The class loading strategy that I just described is implemented in a new type and in Java 9 the
   application class loader is of that type. That means it is not a URLClassLoader, anymore, so the
   occasional (URLClassLoader) getClass().getClassLoader() or (URLClassLoader)
   ClassLoader.getSystemClassLoader() sequences will no longer execute. This is another typical example
   where Java 9 is backwards compatible in the strict sense (because that it's a URLCassLoader was never
   specified) but which can nonetheless cause migration challenges.

Symptoms
   This one is very obvious. You'll get a ClassCastException complaining that the new AppClassLoader is
   no URLClassLoader:

Exception in thread "main" java.lang.ClassCastException:
   java.base/jdk.internal.loader.ClassLoaders$AppClassLoader
   cannot be cast to java.base/java.net.URLClassLoader
       at monitor.Main.logClassPathContent(Main.java:46)
       at monitor.Main.main(Main.java:28)

Fixes
   The class loader was probably cast to access methods specific to URLClassLoader. If so, your chances
   to do a migration with only small changes are slim. The only supported (and hence accessible) super
   types of the new AppClassLoader are SecureClassLoader and ClassLoader and only few methods were
   added here in 9. Still, have a look, they might do what you're looking for.

   If you've used the URLClassLoader to dynamically load user provided code (for example as part of a
   plugin infrastructure) by appending to the class path, then you have to find a new way to do that as
   it can not be done with Java 9. You should instead consider creating a new class loader for that.
   This has the added advantage that you'll be able to get rid of the new classes as they are not loaded
   into the application class loader. If you're compiling against Java 9, you should read up on
   layers - they give you a clean abstraction for loading an entirely new module graph.

Rummaging Around In Runtime Images
   With the JDK being modularized the layout of the run time image fundamentally changed. Files like
   rt.jar, tools.jar, and dt.jar are gone; the JDK classes are now bundled into jmod files (one per
   module), a purposely unspecified file format that allows future optimizations without regards to
   backwards compatibility. Furthermore the distinction between JRE and JDK is gone.

   All of this has been unspecified but that doesn't mean that there's no code out there depending on
   these details. Particularly tools like IDEs (although these have mostly been updated already) will
   have compatibility problems with these changes and will stop working in unpredictable ways unless
   they're updated.

   As a consequence of these changes, the URL you get for system resources, e.g. from
   ClasLoader::getSystemResource, changed. It used to be of the following form:
   jar:file:$javahome/lib/rt.jar!$path, where $path is something like java/lang/String.class. It now
   looks like jrt:/$module/$path. Of course all APIs that create or consume such URLs were updated but
   non-JDK code handcrafting these URLs will have to be updated for Java 9.

   Furthermore, the Class::getResource* and ClassLoader::getResource* methods no longer read
   JDK-internal resources. Instead use Module::getResourceAsStream to access module-internal resources
   or create a JRT file system as follows:

FileSystem fs = FileSystems.getFileSystem(URI.create("jrt:/"));
fs.getPath("java.base", "java/lang/String.class"));

Boot Class Path
   I'm in murky waters here because I never used the -Xbootclasspath option, which is mostly removed.
   Apparently its features are replaced by various new command line options (paraphrasing from JEP
   220 here):
     * the javac option --system can be used to specify an alternate source of system modules
     * the javac option --release can be used to specify an alternate platform version
     * the java option --patch-module option, mentioned above, can be used to inject content into
       modules in the initial module graph

New Version Strings
   After more than 20 years, Java has finally and officially accepted that it's no longer on version
   1.x. Hooray! So from Java 9 on, the system property java.version and its siblings no longer start
   with 1.x but with x, i.e. 9 in Java 9.

Symptoms
   There are no clear-cut symptoms - pretty much everything could go wrong if some utility function
   determines the wrong version. It's not too hard to find, though. A full text search for the following
   strings should lead to all version-string-specific code: java.version, java.runtime.version,
   java.vm.version, java.specification.version, java.vm.specification.version.

Fixes
   If you are willing to raise your project's requirements to Java 9, you can eschew the whole system
   property prodding and parsing and instead use the new Runtime.Version type, which makes all of
   this much easier. If you want to stay compatible to pre Java 9, you could still use the new API by
   creating a multi-release JAR. If that's also out of the question, it looks like you actually have
   to write some code (uch!) and branch based on the major version.

Summary
   Now you know how to use internal APIs ( --add-export and --add-opens), how to make sure Java EE
   modules are present ( --add-modules), and how to deal with split packages ( --patch-module). These
   are the most likely problems you'll encounter during a migration. Less common and also less easy to
   fix without access to the problematic code are casts to URLClassLoader, problems due to the new
   runtime image layout and resource URLs, the removed -Xbootclasspath, and new version strings.

   Knowing how to fix these will give you very good chances to overcome all your migration challenges
   and make your application compile and run on Java 9. If not, take a look at JEP 261's Risks and
   Assumptions sections, which lists a few other potential pitfalls.

   If you're a little overwhelmed by all this, wait for my next post, which gives some advice on how
   to string these individual fixes into a comprehensive migration strategy, for example by including
   build tools and continuous integration. Or get my book, where I explain all of this and more.


---
https://github.com/openjfx/samples/tree/master/CommandLine/Modular/CLI

README.md

samples
   JavaFX 13 samples to run with different options and build tools.

   Download JDK 11 or later for your operating system. Make sure JAVA_HOME is properly set to the
   JDK installation directory.

   Download JavaFX SDK for your operating system and unzip to a desired location.

   Download JavaFX jmods for your operating system and unzip to a desired location.

Modular - CLI
   hellofx sample modular project to run on command line, without build tools

Linux / Mac
   If you run on Linux or Mac, follow these steps:
cd CommandLine/Modular/CLI/hellofx
export PATH_TO_FX=path/to/javafx-sdk-13/lib
export PATH_TO_FX_MODS=path/to/javafx-jmods-13
javac --module-path $PATH_TO_FX -d mods/hellofx $(find src -name "*.java")

   To run the project:
java --module-path $PATH_TO_FX:mods -m hellofx/hellofx.HelloFX

   To create and run a custom JRE:
$JAVA_HOME/bin/jlink --module-path $PATH_TO_FX_MODS:mods --add-modules hellofx --output hellofx
hellofx/bin/java -m hellofx/hellofx.HelloFX

Windows
   If you run on Windows, follow these steps:
cd CommandLine\Modular\CLI\hellofx
set PATH_TO_FX="path\to\javafx-sdk-13\lib"
set PATH_TO_FX_MODS="path\to\javafx-jmods-13"
dir /s /b src\*.java > sources.txt & javac --module-path %PATH_TO_FX% -d mods/hellofx @sources.txt & del sourc
es.txt

   To run the project:
java --module-path "%PATH_TO_FX%;mods" -m hellofx/hellofx.HelloFX

   To create and run a custom JRE:
jlink --module-path "%PATH_TO_FX_MODS%;mods" --add-modules hellofx --output hellofx
hellofx\bin\java -m hellofx/hellofx.HelloFX


---
