filename: running-proxmox-inside-a-docker-container-20250916.txt
https://www.xda-developers.com/i-tried-running-proxmox-inside-a-docker-container/

I tried running Proxmox inside a Docker container
Sep 8, 2025

   Unlike other virtualization platforms, Proxmox is extremely versatile - to the point where you could
   mold your PVE setup to your liking. With all the community scripts floating around, it's possible to
   fine-tune every aspect of your Proxmox workstation - be it the LXCs and VMs or even the host PVE
   instance. Plus, since it's fairly lightweight, you could install Proxmox on practically any setup
   and still expect reliable performance on container-heavy workloads.

   All these aspects considered, I came up with a terrible shower thought: what if I tried to run
   Proxmox inside a container? I've previously configured a Proxmox VM running in VirtualBox using
   nested virtualization, but deploying it as a containerized environment seemed even more ridiculous.
   Turns out, Proxmox works surprisingly well inside a container, and while it's not the ideal way to
   set up a PVE node, it's still a decent workaround if you wish to tinker with the virtualization
   platform without relegating a PC to it.

Setting up Dockermox

It's fairly simple if you're used to Docker containers
   The Dockermox repository by genius developer rtedpro-cpu lies at the heart of this project. It's
   something I ran into while doomscrolling on GitHub, and it makes running the Proxmox container a
   breeze. First, I ran the ls /dev/fuse command to confirm whether the /dev/fuse module is operational
   on my system, as it's required by Dockermox. Luckily, the terminal showed /dev/fuse, and it was time
   to deploy the Proxmox container.

   Since I'd already set up Docker on my PC, I used the sudo docker run -itd --name proxmoxve --hostname
   pve -p 8006:8006 --privileged rtedpro/proxmox:8.4.9 command to spin up the Dockermox instance. Unlike
   a typical Proxmox installation, where you'll have to create a user account inside the wizard, the PVE
   container already ships with pre-configured user credentials. So, I logged into the web UI by typing
   localhost:8006 into my browser and entering root as the Username and Password.

   While the overall setup was complete, I still had to set up the Network stack. Dockermox doesn't
   include a network bridge by default, and I had to configure it manually. First, I ran the following
   command to add a custom network to my Docker setup.

$> sudo docker network create \
--driver bridge \
--subnet=192.168.1.0/24 \
eth2

   After that, I ran sudo docker network connect eth2 proxmoxve to pair the newly-created network with
   the containerized PVE node. Then, I headed back to the Proxmox web UI, created a Linux bridge, and
   hit the Apply Configuration button to make the changes permanent. With that, it was time to test
   whether the Proxmox container lived up to a standard bare-metal PVE node.

You'll have to configure the network settings manually
   Since I was curious about how the Proxmox workstation would handle VMs, I decided to spin a couple of
   virtual machines. I started my tests with Fedora, and after going through the usual VM creation
   wizard, I used the QEMU console to access it. Although a bare-metal setup is more responsive, the
   containerized Proxmox setup wasn't bad by any means. In fact, I ran three separate VMs without
   running into any major performance issues or crashes.

   The only caveat (albeit a minor one) is that I had to assign a static IP to each virtual machine. I
   could technically configure a DHCP server to simplify things a bit, but the lack of a dedicated
   network stack made the container-based Proxmox setup less beginner-friendly.

LXCs require some modifications. But they work just as smoothly
   While the virtual machines worked without a lot of hassle, LXCs required some config file tweaks
   before I could run them. First, I had to disable the ConditionVirtualization parameter by heading to
   the /lib/systemd/system/lxcfs.service directory and commenting ConditionVirtualization. I also ran
   the systemctl daemon-reload and systemctl restart lxcfs commands to reload the lxcfs service with
   ConditionVirtualization disabled.

   Afterwards, I ran nano /var/lib/lxc/container_ID/config (where container_ID refers to the identifier
   used by the LXC) and added lxc.apparmor.profile=unconfined to disable AppArmor, which is known for
   causing issues with LXCs in this setup. The only drawback is that I'd have to run lxc-start -n
   container_ID every time I needed to boot an LXC. But otherwise, the performance was surprisingly
   decent.

How feasible is a container-based Proxmox setup?
   Multiple old laptops, with a MacBook displaying the Proxmox UI resting on top of them

   All things considered, it's a solid way to tinker with Proxmox. Sure, I'd always recommend going the
   bare-metal route whenever you can, especially since PVE requires minimal system resources. But if
   you're not willing to set aside an entire PC just for Proxmox or lack the sheer specs to run PVE
   inside a virtual machine (it does cause a lot of processing overhead), deploying the virtualization
   platform as a container can help you get accustomed to it. Heck, if you're patient enough, you can
   even build a decent self-hosting workstation out of your containerized Proxmox node.


---

