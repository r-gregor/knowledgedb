filename: bash_trap-parallel-watch-wait_roundup-multif_20150324.txt
http://stackoverflow.com/questions/5033354/run-script-before-bash-exits

[26]Run script before Bash exits

   up vote 6 down vote [27]favorite
   2

   I'd like to run a script every time I close a Bash session.

   I use XFCE and Terminal 0.4.5 (Xfce Terminal Emulator), I would like to run a script every
   time I close a tab in Terminal including the last one (when I close Terminal).

   Something like .bashrc but running at the end of every session.

   .bash_logout doesn't work
   [28]bash [29]xfce
   [30]share|[31]improve this question
   asked Feb 17 '11 at 19:04
   [32]F.C.
   3,60331843
   add a comment |

3 Answers 3

   [33]active [34]oldest [35]votes
   up vote 7 down vote accepted

   You use trap (see man bash):
trap /u1/myuser/on_exit_script.sh EXIT

   The command can be added to your .profile/.login

   This works whether you exit the shell normally (e.g. via exit command) or simply kill the
   terminal window/tab, since the shell gets the EXIT signal either way - I just tested by
   exiting my putty window.
   [36]share|[37]improve this answer
   answered Feb 17 '11 at 19:21
   [38]DVK
   68k13126224
   add a comment |
   up vote 0 down vote

   If you close your session with "exit", might be able to something like alias
   endbash="./runscript;exit" and just exit by entering endbash. I'm not entirely sure this
   works, as I'm running windows at the moment.

   edit: DVK has a better answer.
   [39]share|[40]improve this answer
      [41]edited Feb 17 '11 at 19:26
   answered Feb 17 '11 at 19:15
   [42]Gail Terman
   391112
   add a comment |
   up vote 0 down vote

   Write you script in "~/.bash_logout". It executed by bash(1) when login shell exits.
   [43]share|[44]improve this answer
   answered Nov 8 '13 at 17:08
   [45]jellyfish
   9826

   Note that this only works if bash is executed as a login shell, e.g. if you login via SSH
   or check "Run command as a login shell" in gnome-terminal's Profile Preferences. –



---
http://www.skorks.com/2010/05/executing-multiple-commands-a-bash-productivity-tip/

Executing Multiple Commands – A Bash Productivity Tip

   May 2, 2010 By [10]Alan Skorkin [11]15 Comments

   Multiple I love [12]shell productivity hacks. I believe that if you work in a Linux
   environment, you owe it to yourself to become as good as you can be at working with your
   shell of choice (e.g. bash). You see, most people who have worked with Linux for any length
   of time have some level of skill with the shell, that level is usually mediocre at best.
   But, you do meet the occasional person who wows you with what they can accomplish and I
   will tell you right now that their skill does not come from superior knowledge (or at least
   not fully from superior knowledge). It is all about maximizing the effect of the knowledge
   you do have, finding little tricks and hacks that will save you, potentially less than a
   second, every time you perform a particular action. Thing is, some actions you might do
   hundreds of times per day, so those seconds really start to add up, especially once you
   have accumulated dozens of these hacks. Anyway, here is one such tip.

Running Multiple Commands

   Whenever you work with the shell you almost always need to run several commands in a row. I
   am not talking about piping commands to each other, but just running several in sequence.
   Surprisingly, people usually tend to wait for each command to execute before running the
   next one. Why not execute all the commands at once? Bash has some decent support for this,
   and if you train yourself to do it this way, not only will it potentially save you some
   time, but it will also force you to think further ahead; all of which will make you that
   little bit more productive. Let's have a look.

   Firstly we need to create some commands to make it easier to see what is happening. The
   first one will be called cmd1, and all it will do is sleep for 2 seconds then output
   something to the screen.
#!/bin/bash
sleep 2
echo "cmd1"

   The second command will be called long-cmd and it will sleep for 30 seconds before
   outputting something to the screen.
#!/bin/bash
sleep 30
echo "long-cmd"

   The third command will be called cmd-fail and it will exit with a non-zero exit status to
   simulate a failing command.
#!/bin/bash
echo "Failing..."
exit 2

   We're now ready to run some commands in sequence by kicking them off at the same time. All
   you need to do is separate the commands with semicolons and they will execute one after
   another.
alan@alan-ubuntu-vm:~/tmp$ ./cmd1 ; ./cmd1 ; date
cmd1
cmd1
Sat May  1 23:29:49 EST 2010

   As you can see, the two cmd1 commands executed first and then the date command ran – as
   expected. The only problem here is this, if a preceding command fails, the subsequent ones
   will still run:
alan@alan-ubuntu-vm:~/tmp$ ./cmd1 ; ./cmd-fail ; date
cmd1
Failing...
Sat May  1 23:32:33 EST 2010

   This may be the behaviour we desire, in which case all is well, but what if we do care
   about the success of the preceding commands. In that case, separate the commands with
   double ampersand instead of semicolon.
alan@alan-ubuntu-vm:~/tmp$ ./cmd1 && ./cmd-fail && date
cmd1
Failing...

   As soon as one of the commands fails, no subsequent command will be executed – handy.

   Here is another thought, if you don't need to run multiple commands in sequence, but simply
   want to kick off multiple commands, why not execute all of them at once by putting them all
   in the background. All you need to do is separate each command with a single ampersand:
alan@alan-ubuntu-vm:~/tmp$ ./long-cmd & ./long-cmd & ./long-cmd &
[1] 2643
[2] 2644
[3] 2645
alan@alan-ubuntu-vm:~/tmp$

   All three of our long commands have kicked off in the background. This works because as
   soon as we background the first command by putting an ampersand after it, the shell gives
   us another prompt and is ready to accept more input, so we can keep going and execute as
   many commands as we want on the same line, by backgrounding them all. It is a slightly
   quicker way (than waiting for each command to finish, or backgrounding each one separately)
   to execute multiple commands if they don't rely on each other. If the commands produce
   output, all the output will still go to the screen, so the output from all the commands can
   potentially get mixed up, but you can, of course, still [13]redirect the output of each
   command somewhere else. You can also easily check on the status of each of the backgrounded
   commands:
alan@alan-ubuntu-vm:~/tmp$ jobs
[1]   Running                 ./long-cmd &
[2]-  Running                 ./long-cmd &
[3]+  Running                 ./long-cmd &

   The number in square brackets is the id of the job, and you can use it to perform actions
   on the running job, such as bringing it to the foreground, or killing it:
alan@alan-ubuntu-vm:~/tmp$ ./long-cmd & ./long-cmd & ./long-cmd &
[1] 2679
[2] 2680
[3] 2681
alan@alan-ubuntu-vm:~/tmp$ kill %2
alan@alan-ubuntu-vm:~/tmp$ jobs
[1]   Running                 ./long-cmd &
[2]-  Terminated              ./long-cmd
[3]+  Running                 ./long-cmd &
alan@alan-ubuntu-vm:~/tmp$  fg %1
./long-cmd

   If you foreground a job by using fg, and want to put it back in the background after, all
   you need to do is press Ctrl-Z and then background the job again by using its id.
alan@alan-ubuntu-vm:~/tmp$ fg %1
./long-cmd
^Z
[1]+  Stopped                 ./long-cmd
alan@alan-ubuntu-vm:~/tmp$ bg %1
[1]+ ./long-cmd &
alan@alan-ubuntu-vm:~/tmp$ jobs
[1]   Running                 ./long-cmd &
[2]-  Running                 ./long-cmd &
[3]+  Running                 ./long-cmd &
alan@alan-ubuntu-vm:~/tmp$

   There you go, not revolutionary, but some handy tips to keep in mind to shave off a few
   seconds here and there. And believe you me, those seconds really do start to add up after a
   while.



---
http://www.cyberciti.biz/tips/nohup-execute-commands-after-you-exit-from-a-shell-prompt.html

nohup Execute Commands After You Exit From a Shell Prompt

   by [12]nixCraft on January 4, 2006 · [13]37 comments· LAST UPDATED February 2, 2014

   in [14]CentOS, [15]Debian Linux, [16]FreeBSD
   [17][terminal.png]

   Most of the time you login into remote server via ssh. If you start a shell script or
   command and you exit (abort remote connection), the process / command will get killed.
   Sometime job or command takes a long time. If you are not sure when the job will finish,
   then it is better to leave job running in background. But, if you log out of the system,
   the job will be stopped and terminated by your shell. What do you do to keep job running in
   the background when process gets SIGHUP?

Say hello to nohup command

   The answer is simple, use nohup command line-utility which allows to run command/process or
   shell script that can continue running in the background after you log out from a shell:

nohup command syntax:

   The syntax is as follows
   nohup command-name &
   OR
   nohup /path/to/command-name arg1 arg2 &

   Where,
     * command-name : is name of shell script or command name. You can pass argument to
       command or a shell script.
     * & : nohup does not automatically put the command it runs in the background; you must do
       that explicitly, by ending the command line with an & symbol.

   Use [18]jobs -l command to list all jobs:
   # jobs -l

nohup command examples

   First, login to remote server using ssh command:
   $ ssh user@remote.server.com
   OR
   $ ssh vivek@server1.cyberciti.biz

   I am going to execute a shell script called pullftp.sh:
   # nohup pullftp.sh &

   Type exit or press CTRL + D exit from remote server:
   # exit

   In this example, I am going to find all programs and scripts with setuid bit set on, enter:
   # nohup find / -xdev -type f -perm +u=s -print > out.txt &

   Type exit or press CTRL + D exit from remote server.
   # exit

   Please note that nohup does not change the scheduling priority of COMMAND; use nice command
   for that purpose. The syntax is:
   # nohup nice -n -5 ls / > out.txt &
   As you can see nohup keep processes running after you exit from a shell. Read man page of
   [19]nohup(1) and [20]nice(1) for more information. Please note that nohup is almost
   available on Solaris/BSD/Linux/UNIX variants.

Other options (suggested by readers)

   You can use at command to queue a job for later execution. For example, you can run
   pullftp.sh script to queue (one minute) later execution:
   $ echo "pullftp.sh" | at now + 1 minute
   You can also use screen command for same. The [21]disown shell internal command for same
   purpose. Here is how you can try it out:
   $ pullftp.sh &
   $ disown -h
   $ exit

   From the bash [22]bash(1) man page:

     By default, removes each JOBSPEC argument from the table of active jobs. If the -h
     option is given, the job is not removed from the table, but is marked so that SIGHUP is
     not sent to the job if the shell receives a SIGHUP. The -a option, when JOBSPEC is not
     supplied, means to remove all jobs from the job table; the -r option means to remove
     only running jobs.



---
http://unix.stackexchange.com/questions/10646/repeat-a-unix-command-every-x-seconds-forever

[25]Repeat a Unix command every x seconds forever

   up vote 175 down vote [26]favorite
   48

   There's a builtin Unix command repeat whose first argument is the number of times to repeat
   a command, where the command (with any arguments) is specified by the remaining arguments
   to repeat. For example,
% repeat 100 echo "I will not automate this punishment."

   will echo the given string 100 times and then stop.

   I'd like a similar command, let's call it forever, that works similarly except the first
   argument is the number of seconds to pause between repeats, and it repeats forever. For
   example,
% forever 5 echo "This will get echoed every 5 seconds forever and ever."

   I thought I'd ask if such a thing exists before I write it. I know it's like a 2-line Perl
   or Python script but maybe there's a more standard way to do this. If not, feel free to
   post a solution in your favorite scripting language, [27]rosetta stone style.

   PS: Maybe a better way to do this would be to generalize repeat to take both the number of
   times to repeat (with -1 meaning infinity) and the number of seconds to sleep between
   repeats. The above examples would then become:
% repeat 100 0 echo "I will not automate this punishment."
% repeat -1 5 echo "This will get echoed every 5 seconds forever."

***
   This question came from our site for professional and enthusiast programmers.
   1
   Why not: repeat [-t time] [-n number] command [args...]? –  [36]Jonathan Leffler Feb 17 '09
   at 0:22
   11
   Cool. Unfortunately both on my Ubuntu and my AIX repeat is command not found. Can you do a
   type repeat and let me know where's coming from? –  Davide Nov 22 '09 at 23:47
   3
   I'm also on Ubuntu and don't have repeat installed. Any information on how to install this
   would be helpful. –  [37]Rory Dec 21 '09 at 13:02
   6
   repeat is a builtin command in csh and tcsh. –  [38]Keith Thompson Jan 12 '12 at 9:25
   2
   @KeithThompson, csh, tcsh and zsh. Though it's more a keyword than a builtin –
   [39]Stéphane Chazelas Oct 21 '12 at 9:54
    |  [40]show 2 more comments

23 Answers 23

   [41]active [42]oldest [43]votes
   up vote 241 down vote

   Try the watch command.
Usage: watch [-dhntv] [--differences[=cumulative]] [--help] [--interval=<n>]
             [--no-title] [--version] <command>`

   So that:
watch -n1  command

   will run the command every second, forever.

   On Mac OS X, you can get watch from [44]Mac Ports, or you can get it via Homebrew:
brew install watch

   [45]share|[46]improve this answer
   [47]edited Oct 17 '13 at 16:45
   [48]Gilles
   253k32389731
   answered Feb 17 '09 at 0:22
   [49]Gregor Brandt
   2,307133

   Aha, thanks! I don't have that on my system (Mac OS X) -- do you have a link? –  dreeves
   Feb 17 '09 at 0:24
   2
   you can use MacPorts to install it. sudo port install watch –  pope Feb 17 '09 at 0:27

   fink has a package... –  [50]dmckee Feb 17 '09 at 0:27
   7
   Also available in homebrew (brew install watch). –  [51]Lyle May 12 '11 at 21:45
   9
   +1 For a new powerful tool. I used to while true; do X; sleep Y; done - This is way better.
   –  [52]Adam Matan May 16 '11 at 11:14
    |  [53]show 3 more comments
   up vote 87 down vote

   Bash

   while + sleep:
while true
do
    echo "Hi"
    sleep 1
done

   [54]share|[55]improve this answer
   [56]edited Jan 16 '12 at 19:32
   [57]Kevin
   14.6k42866
   answered Feb 17 '09 at 0:13
   pope

   Cool, thanks; which shell is that? But note that I'm looking for something you execute as a
   single command-line command. –  dreeves Feb 17 '09 at 0:19
   1
   I believe it works as written in a generic Bourne shell, as well. –  [58]dmckee Feb 17 '09
   at 0:24
   3
   @dreeves, using ";" as separator, commands can be execeted in a single line. Look at
   Toony's answer as a example –  [59]bbaja42 Jan 12 '12 at 9:00
   28
   sleep 1 always returns true so you can use it as the while condition. Not the most readable
   thing in the world but absolutely legit: while sleep 1; do echo "Hi"; done –  [60]David
   Costa Jan 12 '12 at 10:46
   add a comment |
   up vote 25 down vote

   One problem that all the answers posted so far have is that the time the command is
   executed can drift. For example, if you do a sleep 10 between commands, and the command
   takes 2 seconds to run, then it's going to run every 12 seconds; if it takes a variable
   amount of time to run, then over the long term the time when it runs can be unpredictable.

   This might be just what you want; if so, use one of the other solutions, or use this one
   but simplify the sleep call.

   For one-minute resolution, cron jobs will run at the specified time, regardless of how long
   each command takes. (In fact a new cron job will launch even if the previous one is still
   running.)

   Here's a simple Perl script that sleeps until the next interval, so for example with an
   interval of 10 seconds the command might run at 12:34:00, 12:34:10, 12:34:20, etc., even if
   the command itself takes several seconds. If the command runs more than interval seconds,
   the next interval will be skipped (unlike cron). The interval is computed relative to the
   epoch, so an interval of 86400 seconds (1 day) will run at midnight UTC.
#!/usr/bin/perl

use strict;
use warnings;

if (scalar @ARGV < 2) {
    die "Usage: $0 seconds command [args...]\n";
}

$| = 1;  # Ensure output appears

my($interval, @command) = @ARGV;

# print ">>> interval=$interval command=(@command)\n";

while (1) {
    print "sleep ", $interval - time % $interval, "\n";
    sleep $interval - time % $interval;
    system @command; # TODO: Handle errors (how?)
}

   [61]share|[62]improve this answer
   [63]edited Jan 17 '12 at 9:57
   [64]manatwork
   12.4k14364
   answered Jan 12 '12 at 11:29
   [65]Keith Thompson
   5,31021626

   See also [66]this other question –  [67]Stéphane Chazelas Oct 21 '12 at 9:52
   add a comment |
   up vote 20 down vote

   This is just a shorter version of other while+sleep answers, if you are running this kind
   of tasks often as your daily routine, using this saves you from unnecessary key presses,
   and if your command line starts to get longer understanding this one is a bit easier. But
   this one starts with sleeping first.

   This is generally useful if you need to follow something has one-line output like machine
   load:
while sleep 1; do uptime; done

   [68]share|[69]improve this answer
   answered Jan 19 '13 at 15:53
   [70]Bekir Dogan
   30122

   Yes. This should be documented next to UUOC. –  [71]Johan Mar 15 '13 at 9:05
   4
   And if you want it to operate like "watch" you can do while clear; do date; command;sleep
   5; done –  [72]Johan Mar 15 '13 at 9:12
   add a comment |
   up vote 16 down vote

   In bash:
bash -c 'while [ 0 ]; do echo "I will not automate this punishment in absurdum."; done'

   (echo could be replaced by any command...

   Or in perl:
perl -e 'for (;1;) {print "I will not automate this punishment in absurdum.\n"}'

   Where print "I will not automate this punishment in absurdum.\n" could be replaced with
   "any" command surrounded with backticks (`).

   And for a pause, add a sleep statement inside the for loop:
bash -c 'while [ 0 ]; do echo "I will not automate this punishment in absurdum."; sleep 1; done'

   and
perl -e 'for (;1;) {print "I will not automate this punishment in absurdum.\n"; sleep 1}'

   [73]share|[74]improve this answer
   [75]edited Jan 16 '12 at 19:35
   [76]Kevin
   14.6k42866
   answered Feb 17 '09 at 0:36
   Tooony
   16
   while [ 0 ] is a bad way to write it. It means 0 is a string with length > 0. while [ 1 ]
   or while [ jeff ] would do the same thing. Better to write while true. –  [77]Mikel Apr 5
   '11 at 10:17
   5
   @Mikel: Or while : –  [78]Keith Thompson Jan 12 '12 at 9:27
   add a comment |
   up vote 10 down vote

   I like the scripts, but an alternate approach would be to use cron...at least for n = m*60

   ::blushes:: Hangs head in shame. Thanks Jonathan.
   [79]share|[80]improve this answer
   answered Feb 17 '09 at 0:18
   [81]dmckee
   1,030513
   2
   cron doesn't work at second granularities - minutes are as small as it works with. –
   [82]Jonathan Leffler Feb 17 '09 at 0:21

   Aye, there's that... –  [83]dmckee Feb 17 '09 at 0:22
   1
   With the single-line shell answers, the disadvantage is if they die, it just stops forever.
   With cron, it starts a new one each time. –  [84]kurtm Oct 17 '13 at 17:32
   add a comment |
   up vote 6 down vote

   I think all the answer here so far are either too convoluted, or instead answer a different
   question:
     * "How to run a program repeatedly so that there are X seconds delay between when the
       program finished, and the next starts".

   The real question was:
     * "How to run a program every X seconds"

   These are two very different things when the command takes time to finish.

   Take for instance the script foo.sh (pretend this is a program that takes a few seconds to
   complete).
#!/bin/bash
# foo.sh
echo `date +"%H:%M:%S"` >> output.txt;
sleep 2.5;
# ---

   You wish to run this every second, and most would suggest watch -n1 ./foo.sh, or while
   sleep 1; do ./foo.sh; done. However, this gives the output:
15:21:20
15:21:23
15:21:27
15:21:30
15:21:34

   Which is not exactly being run every second. Even with the -p flag, as the man watch page
   suggests might solve this, the result is the same.

   An easy way to accomplish the desired task, which some touch on, is to run the command in
   the background. In other words:
while sleep 1; do (./foo.sh &) ; done

   And that is all there is to it.

   You could run it every 500 ms with sleep 0.5, or what have you.
   [85]share|[86]improve this answer
       [87]edited Jun 4 '14 at 11:36
   answered Jan 29 '14 at 14:42
   [88]swalog
   16123
   add a comment |
   up vote 4 down vote

Recent bash >= 4.2 under recent Linux kernel, based answer.

   In order to limit execution time, there is no forks! Only built-in are used.

   Under recent linux kernels, there is a procfile /proc/timer_list containing time
   information in nanoseconds.

Depending on granularity and duration of submited command...

   If you wanna run a command exactly once by second, your command have to end in less than a
   second! And from there, you have to sleep only the rest of current seconds...

   If delay is more important and your command don't require significant time, you could:
command=(echo 'Hello world.')
delay=10
while :;do
    printf -v now "%(%s)T" -1
    read -t $(( delay-(now%delay) )) foo
    ${command[@]}
  done.

   But if you're goal is to obtain finer granularity, you have to

Use nanoseconds information to wait until begin of a second...

   For this, I wrote a little bash function:
# bash source file for nano wait-until-next-second

mapfile  </proc/timer_list _timer_list
for ((_i=0;_i<${#_timer_list[@]};_i++));do
    ((_c+=${#_timer_list[_i]}))
    [[ ${_timer_list[_i]} =~ ^now ]] && TIMER_LIST_READ=$_c
    [[ ${_timer_list[_i]} =~ offset:.*[1-9] ]] && \
        TIMER_LIST_OFFSET=${_timer_list[_i]//[a-z.: ]} && \
        break
done
unset _i _timer_list _c
readonly TIMER_LIST_OFFSET TIMER_LIST_READ
waitNextSecondHires() {
    local nsnow nsslp
    read -N$TIMER_LIST_READ nsnow </proc/timer_list
    nsnow=${nsnow%% nsecs*}
    nsnow=$((${nsnow##* }+TIMER_LIST_OFFSET))
    nsslp=$((2000000000-10#${nsnow:${#nsnow}-9}))
    read -t .${nsslp:1} foo
}

   After sourcing them, you could:
command=(echo 'Hello world.')
while :;do
    waitNextSecondHires
    ${command[@]}
  done.

   run ${command[@]} directly on command line, than compare to
command=(eval "echo 'Hello world.';sleep .3")
while :;do
    waitNextSecondHires
    ${command[@]}
  done.

   this must give exactly same result.
   [89]share|[90]improve this answer
   answered Jan 29 '14 at 17:31
   [91]F. Hauri
   1,382513

   read -t is a built-in, while sleep is not. This could have border effect (ie hitting
   [key:return] interrupt quietly the sleep), but this could be treated by testing $foo –
   [92]F. Hauri Jan 29 '14 at 17:34

   Very impressive answer –  [93]gena2x Jun 4 '14 at 13:37
   add a comment |
   up vote 3 down vote

   Care to try this out (Bash)?
forever ()   {
    TIMES=shift;
    SLEEP=shift;
    if [ "$TIMES" = "-1" ]; then
        while true;
        do
            $@
            sleep $SLEEP
        done
    else
        repeat "$TIMES" $@
    fi; }

   [94]share|[95]improve this answer
   [96]edited Jan 17 '12 at 9:54
   [97]manatwork
   12.4k14364
   answered Feb 17 '09 at 0:39
   Gregg Lind

   I'm not to hot at shell, so improvements welcome. And I don't seem to have a "repeat"
   command in my distro. –  Gregg Lind Feb 17 '09 at 0:40
   2
   repeat is specific to csh and tcsh. –  [98]Keith Thompson Jan 12 '12 at 9:42
   add a comment |
   up vote 2 down vote

   As mentioned by gbrandt, if the watch command is available, definitely use it. Some Unix
   systems, however, don't have it installed by default (at least they don't where I work).

   Here's another solution with slightly different syntax and output (works in BASH and SH):
while [ 1 ] ; do
    <cmd>
    sleep <x>
    echo ">>>>>>>>>>>>>" `date` ">>>>>>>>>>>>>>"
done

   Edit: I removed some "." in the last echo statement...holdover from my Perl days ;)
   [99]share|[100]improve this answer
   [101]edited Jan 16 '12 at 19:36
   [102]Kevin
   14.6k42866
   answered Feb 17 '09 at 0:39
   bedwyr
   3
   while [ 1 ] only works because 1 is treated as a string, just like while [ -n 1 ]. while [
   0 ] or while [ jeff ] would do the same thing. while true makes much more sense. –
   [103]Mikel Apr 5 '11 at 10:19
   add a comment |
   up vote 2 down vote

Perl

#!/usr/bin/env perl
# First argument is number of seconds to sleep between repeats, remaining
# arguments give the command to repeat forever.

$sleep = shift;
$cmd = join(' ', @ARGV);

while(1) {
  system($cmd);
  sleep($sleep);
}

   [104]share|[105]improve this answer
   [106]edited Jan 17 '12 at 9:55
   [107]manatwork
   12.4k14364
   answered Feb 17 '09 at 0:17
   [108]dreeves
   1,004267
   add a comment |
   up vote 2 down vote

   If your intention is not to display a message to your screen, and if you could afford to
   repeat the job in terms of minutes, [109]crontab, perhaps, would be your best tool. For
   example, if you wish to execute your command every minute, you would write something like
   this in your crontab file:
* * * * * my_precious_command

   Please check out the tutorial for further example. Also, you can set the timings easily
   using [110]Crontab Code Generator.
   [111]share|[112]improve this answer
   answered Feb 29 '12 at 9:54
   [113]Barun
   1,347721
   add a comment |
   up vote 1 down vote

   You could source this recursive function:
#!/bin/bash
ininterval () {
    delay=$1
    shift
    $*
    sleep $delay
    ininterval $delay $*
}

   or add an:
ininterval $*

   and call the script.
   [114]share|[115]improve this answer
   [116]edited Jan 16 '12 at 19:37
   [117]Kevin
   14.6k42866
   answered Apr 5 '11 at 12:10
   [118]user unknown
   4,1271235
   add a comment |
   up vote 1 down vote

   ... I wonder how much complicated solutions can be created when solving this problem.

   It can be so easy...
open /etc/crontab

   put there 1 next line to the end of file like:
*/NumberOfSeconds * * * * user /path/to/file.sh

   If you want to run something every 1 second, just put there:
*/60 * * * * root /path/to/file.sh

where that file.sh could be chmod 750 /path/to/file.sh

   and inside of that file.sh should be:
#!/bin/bash
#What does it do
#What is it runned by

your code or commands

   and thats all!

   ENJOY!
   [119]share|[120]improve this answer
   answered Dec 28 '12 at 14:28
   [121]MIrra
   4811413
   2
   This is not correct. */60 is going to run every 60 minutes, and */5, for example, every 5
   minutes. –  [122]mika Oct 2 '13 at 17:47
   add a comment |
   up vote 1 down vote

   Bash and some of its kindred shells have the convenient (( ... )) notation wherein
   arithmetic expressions can be evaluated.

   So as an answer to your third challenge, where both the repeat count and delay between each
   repeat should be configurable, here's one way to do it:
repet=10
delay=1

i=0
while (( i++ < repet )); do
  echo Repetition $i
  sleep $delay
done

   This answer also suffers from the timing drift covered in [123]Keith's answer.
   [124]share|[125]improve this answer
   answered Apr 26 '13 at 10:55
   [126]Thor
   6,3291343
   add a comment |
   up vote 1 down vote

   You could run a script from init (adding a line to /etc/inittab). This script must run your
   command, sleep for the time you want to wait until run the script again, and exit it. Init
   will start your script again after exit.
   [127]share|[128]improve this answer
   answered Nov 11 '13 at 19:01
   [129]user28364
   111
   add a comment |
   up vote 1 down vote

   What if we had both ?

   Here's the idea : with only "interval", it repeats forever. With "interval" and "times", it
   repeats this number of times, separated by "interval".

   The usage :
$ loop [interval [times]] command

   So, the algorithm will be :
     * List item
     * if $1 contains only digits, it is the interval (default 2)
     * if $2 contains only digits, it is the number of times (default infinite)
     * while loop with these parameters
          + sleep "interval"
          + if a number of times has been given, decrement a var until it is reached

   Therefore :
loop() {
    local i=2 t=1 cond

    [ -z ${1//[0-9]/} ] && i=$1 && shift
    [ -z ${1//[0-9]/} ] && t=$1 && shift && cond=1
    while [ $t -gt 0 ]; do
        sleep $i
        [ $cond ] && : $[--t]
        $@
    done
}

   [130]share|[131]improve this answer
       [132]edited Nov 11 '13 at 22:33
   answered Nov 11 '13 at 22:27
   [133]Baronsed
   112

   Note: it does not work with floats, despite sleep does accept them. –  [134]Baronsed Nov 11
   '13 at 22:37
   add a comment |
   up vote 1 down vote
#! /bin/sh

# Run all programs in a directory in parallel
# Usage: run-parallel directory delay
# Copyright 2013 by Marc Perkel
# docs at http://wiki.junkemailfilter.com/index.php/How_to_run_a_Linux_script_every_few_seconds_unde
r_cron"
# Free to use with attribution

if [ $# -eq 0 ]
then
   echo
   echo "run-parallel by Marc Perkel"
   echo
   echo "This program is used to run all programs in a directory in parallel"
   echo "or to rerun them every X seconds for one minute."
   echo "Think of this program as cron with seconds resolution."
   echo
   echo "Usage: run-parallel [directory] [delay]"
   echo
   echo "Examples:"
   echo "   run-parallel /etc/cron.20sec 20"
   echo "   run-parallel 20"
   echo "   # Runs all executable files in /etc/cron.20sec every 20 seconds or 3 times a minute."
   echo
   echo "If delay parameter is missing it runs everything once and exits."
   echo "If only delay is passed then the directory /etc/cron.[delay]sec is assumed."
   echo
   echo 'if "cronsec" is passed then it runs all of these delays 2 3 4 5 6 10 12 15 20 30'
   echo "resulting in 30 20 15 12 10 6 5 4 3 2 executions per minute."
   echo
   exit
fi

# If "cronsec" is passed as a parameter then run all the delays in parallel

if [ $1 = cronsec ]
then
   $0 2 &
   $0 3 &
   $0 4 &
   $0 5 &
   $0 6 &
   $0 10 &
   $0 12 &
   $0 15 &
   $0 20 &
   $0 30 &
   exit
fi

# Set the directory to first prameter and delay to second parameter

dir=$1
delay=$2

# If only parameter is 2,3,4,5,6,10,12,15,20,30 then automatically calculate
# the standard directory name /etc/cron.[delay]sec

if [[ "$1" =~ ^(2|3|4|5|6|10|12|15|20|30)$ ]]
then
   dir="/etc/cron.$1sec"
   delay=$1
fi

# Exit if directory doesn't exist or has no files

if [ ! "$(ls -A $dir/)" ]
then
   exit
fi

# Sleep if both $delay and $counter are set

if [ ! -z $delay ] && [ ! -z $counter ]
then
   sleep $delay
fi

# Set counter to 0 if not set

if [ -z $counter ]
then
   counter=0
fi

# Run all the programs in the directory in parallel
# Use of timeout ensures that the processes are killed if they run too long

for program in $dir/* ; do
   if [ -x $program ]
   then
      if [ "0$delay" -gt 1 ]
      then
         timeout $delay $program &> /dev/null &
      else
         $program &> /dev/null &
      fi
   fi
done

# If delay not set then we're done

if [ -z $delay ]
then
   exit
fi

# Add delay to counter

counter=$(( $counter + $delay ))

# If minute is not up - call self recursively

if [ $counter -lt 60 ]
then
   . $0 $dir $delay &
fi

# Otherwise we're done

   [135]share|[136]improve this answer
   answered Jan 9 '14 at 14:32
   [137]user56318
   111
   add a comment |
   up vote 1 down vote

   Easy way to repeat a job from crontab with an interval less than one minute (20 seconds
   example) :

   crontab: * * * * * script.sh

   script.sh:
#!/bin/bash
>>type your commands here.

sleep 20
>>retype your commands here.

sleep 20
>>retype your commands here.

   [138]share|[139]improve this answer
   [140]edited Mar 28 '14 at 22:15
   [141]goldilocks
   36.8k651108
   answered Mar 28 '14 at 21:26
   [142]Charles nakhel
   799
   add a comment |
   up vote 1 down vote

   I wound up up creating a variant on swalog's answer. With his you had to wait X seconds for
   the first iteration, I'm also running my in the foreground so..
./foo.sh;while sleep 1; do (./foo.sh) ; done

   [143]share|[144]improve this answer
   answered Dec 13 '14 at 7:44
   [145]Duane Lortie
   111
   add a comment |
   up vote 0 down vote

   This solution works in MacOSX 10.7. It works wonderfully.
bash -c 'while [ 0 ]; do \
      echo "I will not automate this punishment in absurdum."; done'

   In my case
bash -c 'while [ 0 ]; do ls; done'

   or
bash -c 'while [ 0 ]; do mv "Desktop/* Documents/Cleanup"; done'

   to clean up my desktop constantly.
   [146]share|[147]improve this answer
   [148]edited Jan 12 '12 at 6:19
   [149]Mat
   22.7k36999
   answered Jan 11 '12 at 15:27
   [150]Dan Ruiz
   91

   Did you mean to have a sleep command there? –  [151]Keith Thompson Jan 12 '12 at 9:41
   3
   while [ 0 ] is an odd way to write an infinite loop; it works because the test command
   (also known as [) treats a non-empty string as true. while : ; do ... ; done is more
   idiomatic, or if you prefer you can use while true ; do ... ; done. –  [152]Keith Thompson
   Jan 17 '12 at 8:11
   add a comment |
   up vote 0 down vote

   To repeatedly run a command in a console window I usually run something like this:

   while true; do (run command here); done

   This works for multiple commands as well, for example, to display a continually updating
   clock in a console window:

   while true; do clear; date; sleep 1; done
   [153]share|[154]improve this answer
   answered Dec 24 '12 at 12:09
   [155]Thomas Bratt
   1337

   Why the downvote? You can see this is a working solution by copy and pasting the example
   into a terminal window. –  [156]Thomas Bratt Jan 27 '13 at 14:20

   I think you got down voted because it is a little bit dangerous and consumes the CPU. –
   [157]ojblass Jul 6 '13 at 20:22
   add a comment |
   up vote -2 down vote

   Quick, dirty and probably dangerous to boot, but if you're adventurous and know what you're
   doing, put this into repeat.sh and chmod 755 it,
while true
do
    eval $1
    sleep $2
done

   Invoke it with ./repeat.sh <command> <interval>

   My spidey sense says this is probably an evil way of doing this, is my spidey sense right?
   [158]share|[159]improve this answer
   [160]edited Feb 29 '12 at 4:27
   [161]Kevin
   14.6k42866
   answered Feb 28 '12 at 18:27
   [162]mallyone
   11
   7
   Eval!? What for? sleep $1; shift; "$@" or similar would be much better. –  



---
http://superuser.com/questions/175799/does-bash-have-a-hook-that-is-run-before-executing-a-command

[25]Does bash have a hook that is run before executing a command?

   up vote 57 down vote [26]favorite
   35

   In bash, can I arrange for a function to be executed just before running a command?

   There is $PROMPT_COMMAND, which is executed before showing a prompt, i.e., just after
   running a command.

   Bash's $PROMPT_COMMAND is analogous to zsh's precmd function; so what I'm looking for is a
   bash equivalent to zsh's preexec.

   Example applications: set your terminal title to the command being executed; automatically
   add time before every command.
   [27]command-line [28]bash [29]hook
   [30]share|[31]improve this question
        [32]edited Aug 14 '10 at 11:17
   community wiki
   [33]2 revs
   [34]Gilles
   add a comment |

5 Answers 5

   [35]active [36]oldest [37]votes
   up vote 48 down vote accepted

   Not natively, but it can be hacked up using the DEBUG trap. [38]This code sets up preexec
   and precmd functions similar to zsh. The command line is passed as a single argument to
   preexec.

   Here is a simplified version of the code to set up a precmd function that is executed
   before running each command.
preexec () { :; }
preexec_invoke_exec () {
    [ -n "$COMP_LINE" ] && return  # do nothing if completing
    [ "$BASH_COMMAND" = "$PROMPT_COMMAND" ] && return # don't cause a preexec for $PROMPT_COMMAND
    local this_command=`HISTTIMEFORMAT= history 1 | sed -e "s/^[ ]*[0-9]*[ ]*//"`;
    preexec "$this_command"
}
trap 'preexec_invoke_exec' DEBUG

   This trick is due to [39]Glyph Lefkowitz; thanks to [40]bcat for locating the original
   author.

   Edit. An updated version of Glyph's hack can be found here:
   [41]https://github.com/rcaloras/bash-preexec
   [42]share|[43]improve this answer
           [44]edited Jan 3 at 11:01
   community wiki
   [45]5 revs, 2 users 86%
   [46]Gilles
   8
   FYI: This code is by Glyph Lefkowitz and the original source can be found here:
   [47]glyf.livejournal.com/63106.html –  [48]bcat Jan 2 '11 at 6:31

   Pinging @bcat again. He's active on SO, maybe he sees this. –  [49]Daniel Beck♦ Jan 26 '11
   at 15:19
   1
   Thanks for mentioning my little hack here. I'm glad that this is helping people close the
   gap between bash and zsh :). –  [50]Glyph Jan 16 '13 at 23:30

   what does "$this_command do?" –  [51]user35142 Jun 30 '13 at 2:11

   [ "$BASH_COMMAND" = "$PROMPT_COMMAND" ] && return –  [52]user35142 Jun 30 '13 at 2:23
    |  [53]show 2 more comments
   up vote 12 down vote

   You can use the trap command (from help trap):
If a SIGNAL_SPEC is DEBUG, ARG is executed before every simple command.

   For example, to change the terminal title dynamically you may use:
trap 'echo -e "\e]0;$BASH_COMMAND\007"' DEBUG

   From [54]this source.
   [55]share|[56]improve this answer
    answered [57]Aug 26 '10 at 22:14
   community wiki
   [58]cYrus
   1
   Interesting ... on my old Ubuntu server, help trap says "If a SIGNAL_SPEC is DEBUG, ARG is
   executed after every simple command" [emphasis mine]. –  [59]LarsH Apr 30 '13 at 21:26

   I used a combination of this answer with some of the special stuff in the accepted answer:
   trap '[ -n "$COMP_LINE" ] && [ "$BASH_COMMAND" != "$PROMPT_COMMAND" ] && date "+%X";echo -e
   "\e]0;$BASH_COMMAND\007"' DEBUG. This puts the command into the title and also prints the
   current time right before every command, but doesn't do so when executing $PROMPT_COMMAND.
   –  [60]CoreDumpError Sep 16 '14 at 22:50

   @CoreDumpError, since you've refactored the code you should negate all the conditions: the
   first one hence becomes: [ -z "$COMP_LINE" ]. –  [61]cYrus Sep 17 '14 at 8:58

   @cYrus Thanks! I don't know nearly enough bash programming to have noticed that problem. –
   [62]CoreDumpError Sep 18 '14 at 16:46

   @LarsH: Which version do you have? I have BASH_VERSION="4.3.11(1)-release" and it says "ARG
   is executed before every simple command." –  [63]musiphil Nov 20 '14 at 18:21
    |  [64]show 1 more comment
   up vote 2 down vote

   I wrote a method to log all 'bash' commands/builtins into a text-file or a 'syslog' server
   without using a patch or a special executable tool.

   It is very easy to deploy, as it is a simple shellscript that need to be called once at the
   initialization of the 'bash'.

   See the method [65]here.
   [66]share|[67]improve this answer
      [68]edited Jul 11 '12 at 22:24
   community wiki
   [69]2 revs, 2 users 74%
   [70]Francois Scheurer
   add a comment |
   up vote 1 down vote

   I recently had to solve this exact problem for a side project of mine. I made a fairly
   robust and resilient solution that emulates zsh's preexec and precmd functionality for
   bash.

   [71]https://github.com/rcaloras/bash-preexec

   It was originally based off Glyph Lefkowitz's solution, but I've improved on it and brought
   it up to date. Happy to help or add a feature if needed.
   [72]share|[73]improve this answer
         answered [74]Jan 3 at 10:53
   community wiki
   [75]RCCola
   add a comment |
   up vote 0 down vote

   Thank you for the hints! I ended up using this:
#created by francois scheurer

#sourced by '~/.bashrc', which is the last runned startup script for bash invocation
#for login interactive, login non-interactive and non-login interactive shells.
#note that a user can easily avoid calling this file by using options like '--norc';
#he also can unset or overwrite variables like 'PROMPT_COMMAND'.
#therefore it is useful for audit but not for security.

#prompt & color
#http://www.pixelbeat.org/docs/terminal_colours/#256
#http://www.frexx.de/xterm-256-notes/
_backnone="\e[00m"
_backblack="\e[40m"
_backblue="\e[44m"
_frontred_b="\e[01;31m"
_frontgreen_b="\e[01;32m"
_frontgrey_b="\e[01;37m"
_frontgrey="\e[00;37m"
_frontblue_b="\e[01;34m"
PS1="\[${_backblue}${_frontgreen_b}\]\u@\h:\[${_backblack}${_frontblue_b}\]\w\\$\[${_backnone}${_fro
ntgreen_b}\] "

#'history' options
declare -rx HISTFILE="$HOME/.bash_history"
chattr +a "$HISTFILE" # set append-only
declare -rx HISTSIZE=500000 #nbr of cmds in memory
declare -rx HISTFILESIZE=500000 #nbr of cmds on file
declare -rx HISTCONTROL="" #does not ignore spaces or duplicates
declare -rx HISTIGNORE="" #does not ignore patterns
declare -rx HISTCMD #history line number
history -r #to reload history from file if a prior HISTSIZE has truncated it
if groups | grep -q root; then declare -x TMOUT=3600; fi #timeout for root's sessions

#enable forward search (ctrl-s)
#http://ruslanspivak.com/2010/11/25/bash-history-incremental-search-forward/
stty -ixon

#history substitution ask for a confirmation
shopt -s histverify

#add timestamps in history - obsoleted with logger/syslog
#http://www.thegeekstuff.com/2008/08/15-examples-to-master-linux-command-line-history/#more-130
#declare -rx HISTTIMEFORMAT='%F %T '

#bash audit & traceabilty
#
#
declare -rx AUDIT_LOGINUSER="$(who -mu | awk '{print $1}')"
declare -rx AUDIT_LOGINPID="$(who -mu | awk '{print $6}')"
declare -rx AUDIT_USER="$USER" #defined by pam during su/sudo
declare -rx AUDIT_PID="$$"
declare -rx AUDIT_TTY="$(who -mu | awk '{print $2}')"
declare -rx AUDIT_SSH="$([ -n "$SSH_CONNECTION" ] && echo "$SSH_CONNECTION" | awk '{print $1":"$2"->
"$3":"$4}')"
declare -rx AUDIT_STR="[audit $AUDIT_LOGINUSER/$AUDIT_LOGINPID as $AUDIT_USER/$AUDIT_PID on $AUDIT_T
TY/$AUDIT_SSH]"
declare -rx AUDIT_SYSLOG="1" #to use a local syslogd
#
#PROMPT_COMMAND solution is working but the syslog message are sent *after* the command execution,
#this causes 'su' or 'sudo' commands to appear only after logouts, and 'cd' commands to display wron
g working directory
#http://jablonskis.org/2011/howto-log-bash-history-to-syslog/
#declare -rx PROMPT_COMMAND='history -a >(tee -a ~/.bash_history | logger -p user.info -t "$AUDIT_ST
R $PWD")' #avoid subshells here or duplicate execution will occurs!
#
#another solution is to use 'trap' DEBUG, which is executed *before* the command.
#http://superuser.com/questions/175799/does-bash-have-a-hook-that-is-run-before-executing-a-command
#http://www.davidpashley.com/articles/xterm-titles-with-bash.html
#set -o functrace; trap 'echo -ne "===$BASH_COMMAND===${_backvoid}${_frontgrey}\n"' DEBUG
set +o functrace #disable trap DEBUG inherited in functions, command substitutions or subshells, nor
mally the default setting already
#enable extended pattern matching operators
shopt -s extglob
#function audit_DEBUG() {
#  echo -ne "${_backnone}${_frontgrey}"
#  (history -a >(logger -p user.info -t "$AUDIT_STR $PWD" < <(tee -a ~/.bash_history))) && sync && h
istory -c && history -r
#  #http://stackoverflow.com/questions/103944/real-time-history-export-amongst-bash-terminal-windows
#  #'history -c && history -r' force a refresh of the history because 'history -a' was called within
 a subshell and therefore
#  #the new history commands that are appent to file will keep their "new" status outside of the sub
shell, causing their logging
#  #to re-occur on every function call...
#  #note that without the subshell, piped bash commands would hang... (it seems that the trap + proc
ess substitution interfer with stdin redirection)
#  #and with the subshell
#}
##enable trap DEBUG inherited for all subsequent functions; required to audit commands beginning wit
h the char '(' for a subshell
#set -o functrace #=> problem: completion in commands avoid logging them
function audit_DEBUG() {
    #simplier and quicker version! avoid 'sync' and 'history -r' that are time consuming!
    if [ "$BASH_COMMAND" != "$PROMPT_COMMAND" ] #avoid logging unexecuted commands after Ctrl-C or E
mpty+Enter
    then
        echo -ne "${_backnone}${_frontgrey}"
        local AUDIT_CMD="$(history 1)" #current history command
        #remove in last history cmd its line number (if any) and send to syslog
        if [ -n "$AUDIT_SYSLOG" ]
        then
            if ! logger -p user.info -t "$AUDIT_STR $PWD" "${AUDIT_CMD##*( )?(+([0-9])[^0-9])*( )}"
            then
                echo error "$AUDIT_STR $PWD" "${AUDIT_CMD##*( )?(+([0-9])[^0-9])*( )}"
            fi
        else
            echo $( date +%F_%H:%M:%S ) "$AUDIT_STR $PWD" "${AUDIT_CMD##*( )?(+([0-9])[^0-9])*( )}"
>>/var/log/userlog.info
        fi
    fi
    #echo "===cmd:$BASH_COMMAND/subshell:$BASH_SUBSHELL/fc:$(fc -l -1)/history:$(history 1)/histline
:${AUDIT_CMD%%+([^ 0-9])*}===" #for debugging
}
function audit_EXIT() {
    local AUDIT_STATUS="$?"
    if [ -n "$AUDIT_SYSLOG" ]
    then
        logger -p user.info -t "$AUDIT_STR" "#=== bash session ended. ==="
    else
        echo $( date +%F_%H:%M:%S ) "$AUDIT_STR" "#=== bash session ended. ===" >>/var/log/userlog.i
nfo
    fi
    exit "$AUDIT_STATUS"
}
#make audit trap functions readonly; disable trap DEBUG inherited (normally the default setting alre
ady)
declare -fr +t audit_DEBUG
declare -fr +t audit_EXIT
if [ -n "$AUDIT_SYSLOG" ]
then
    logger -p user.info -t "$AUDIT_STR" "#=== New bash session started. ===" #audit the session open
ning
else
    echo $( date +%F_%H:%M:%S ) "$AUDIT_STR" "#=== New bash session started. ===" >>/var/log/userlog
.info
fi
#when a bash command is executed it launches first the audit_DEBUG(),
#then the trap DEBUG is disabled to avoid a useless rerun of audit_DEBUG() during the execution of p
ipes-commands;
#at the end, when the prompt is displayed, re-enable the trap DEBUG
declare -rx PROMPT_COMMAND="trap 'audit_DEBUG; trap DEBUG' DEBUG"
declare -rx BASH_COMMAND #current command executed by user or a trap
declare -rx SHELLOPT #shell options, like functrace
trap audit_EXIT EXIT #audit the session closing

   Enjoy!



---
http://en.wikipedia.org/wiki/Watch_(Unix)

watch (Unix)

   From Wikipedia, the free encyclopedia
   Jump to: [7]navigation, [8]search

   watch is a GNU command-line tool that runs the specified [9]command repeatedly and displays
   the output on [10]stdout so you can watch it change over time. By default, the command is
   run every two seconds, although this is adjustable with the -n secs argument. Since the
   command is passed to sh -c, you may need to encase it in quotes for it to run correctly.

Contents

     * [11]1 Syntax
     * [12]2 Example
     * [13]3 Arguments
     * [14]4 See also
     * [15]5 External links

Syntax[[16]edit]

watch [options] command [command options]

Example[[17]edit]

watch "[18]ps -e | grep php"

   This will generate a list of [19]processes every two seconds, filter for all lines that
   contain the word "php", and display the results on the screen. The output might look
   something like this:
Every 2s: ps -e | grep php                             Tue Jan 30 14:56:33 2007

reconst  30028  0.0  0.0  7044 2596 ?        S    Jan23   0:00 vim -r core/html_api.php
cinonet  28009  0.0  0.2 20708 11064 ?       SN   Jan25   0:30 php5.cgi
donoiz   23810  0.0  0.2 22740 10996 ?       SN   Jan27   0:30 php.cgi 43/pdf

   The watch command is useful for viewing changes over time, like repeatedly running the
   [20]ls -l command to watch a file's size change, or running ps as in the above example to
   monitor certain processes continuously.

Arguments[[21]edit]

     * -d – Highlights differences between iterations
     * -h – Displays a help message, then exits
     * -n secs – Specifies the interval between executions of the command in seconds
     * -t – Tells watch not to display the header
     * -v – Prints version information, then exits



---
https://pebblesinthesand.wordpress.com/2008/05/22/a-srcipt-for-running-processes-in-parallel-in-bash/

A script for running processes in parallel in Bash

   May 22, 2008 — kawakamasu

   In Bash you can start new processes (theads) on the background simply by running a command
   with ampersand &. The wait command can be used to wait until all background processes have
   finished (to wait for a certain process do wait PID where PID is a process ID). So here’s a
   simple pseudocode for parallel processing:
for ARG in  $*; do
    command $ARG &
    NPROC=$(($NPROC+1))
    if [ "$NPROC" -ge 4 ]; then
        wait
        NPROC=0
    fi
done

   I.e. you run 4 processes at a time and wait until all of them have finished before
   executing the next four. This is a sufficient solution if all of the processes take equally
   long to finish. However this is suboptimal if running time of the processes vary a lot.

   A better solution is to track the process IDs and poll if all of them are still running. In
   Bash $! returns the ID of last initiated background process. If a process is running, the
   corresponding PID is found in directory /proc/.

   Based on the ideas given in a Ubuntu forum [30]thread and a [31]template on command line
   parsing, I wrote a simple script “parallel” that allows you to run virtually any simple
   command concurrently.

   Assume that you have a program proc and you want to run something like proc *.jpg using
   three concurrent processes. Then simply do
   parallel -j 3 proc *.jpg
   The script takes care of dividing the task. Obviously -j 3 stands for three simultaneous
   jobs.
   If you need command line options, use quotes to separate the command from the variable
   arguments, e.g.
   parallel -j 3 "proc -r -A=40" *.jpg
   Furthermore, -r allows even more sophisticated commands by replacing asterisks in the
   command string by the argument:
   parallel -j 6 -r "convert -scale 50% * small/small_*" *.jpg
   I.e. this executes convert -scale 50% file1.jpg small/small_file1.jpg for all the jpg
   files. This is a real-life example for scaling down images by 50% (requires imagemagick).

   Finally, here’s the script. It can be easily manipulated to handle different jobs, too.
   Just write your command between #DEFINE COMMAND and #DEFINE COMMAND END.
#!/bin/bash
NUM=0
QUEUE=""
MAX_NPROC=2 # default
REPLACE_CMD=0 # no replacement by default
USAGE="A simple wrapper for running processes in parallel.
Usage: `basename $0` [-h] [-r] [-j nb_jobs] command arg_list
        -h              Shows this help
        -r              Replace asterix * in the command string with argument
        -j nb_jobs      Set number of simultanious jobs [2]
 Examples:
        `basename $0` somecommand arg1 arg2 arg3
        `basename $0` -j 3 \"somecommand -r -p\" arg1 arg2 arg3
        `basename $0` -j 6 -r \"convert -scale 50% * small/small_*\" *.jpg"

function queue {
        QUEUE="$QUEUE $1"
        NUM=$(($NUM+1))
}

function regeneratequeue {
        OLDREQUEUE=$QUEUE
        QUEUE=""
        NUM=0
        for PID in $OLDREQUEUE
        do
                if [ -d /proc/$PID  ] ; then
                        QUEUE="$QUEUE $PID"
                        NUM=$(($NUM+1))
                fi
        done
}

function checkqueue {
        OLDCHQUEUE=$QUEUE
        for PID in $OLDCHQUEUE
        do
                if [ ! -d /proc/$PID ] ; then
                        regeneratequeue # at least one PID has finished
                        break
                fi
        done
}

# parse command line
if [ $# -eq 0 ]; then #  must be at least one arg
        echo "$USAGE" >&2
        exit 1
fi

while getopts j:rh OPT; do # "j:" waits for an argument "h" doesnt
    case $OPT in
        h)      echo "$USAGE"
                exit 0 ;;
        j)      MAX_NPROC=$OPTARG ;;
        r)      REPLACE_CMD=1 ;;
        \?)     # getopts issues an error message
                echo "$USAGE" >&2
                exit 1 ;;
    esac
done

# Main program
echo Using $MAX_NPROC parallel threads
shift `expr $OPTIND - 1` # shift input args, ignore processed args
COMMAND=$1
shift

for INS in $* # for the rest of the arguments
do
        # DEFINE COMMAND
        if [ $REPLACE_CMD -eq 1 ]; then
                CMD=${COMMAND//"*"/$INS}
        else
                CMD="$COMMAND $INS" #append args
        fi
        echo "Running $CMD"

        $CMD &
        # DEFINE COMMAND END

        PID=$!
        queue $PID

        while [ $NUM -ge $MAX_NPROC ]; do
                checkqueue
                sleep 0.4
        done
done
wait # wait for all processes to finish before exit



---
http://serverfault.com/questions/368054/run-an-interactive-bash-subshell-with-initial-commands-without-returning-to-the

[25]Run an interactive bash subshell with initial commands without returning to the (“super”)
shell immediately

   up vote 12 down vote [26]favorite
   3

   I want to run a bash subshell, (1) run a few commands, (2) and then remain in that subshell
   to do as I please. I can do each of these individually:
    1. Run command using -c flag:
$> bash -c "ls; pwd; <other commands...>"

       however, it immediately returns to the "super" shell after the commands are executed. I
       can also just run an interactive subshell:
    2. Start new bash process:
$> bash

       and it won't exit the subshell until I say so explicitly... but I can't run any initial
       commands. The closest solution I've found is:
$> bash -c "ls; pwd; <other commands>; exec bash"

       which works, but not the way I wanted to, as it runs the given commands in one
       subshell, and then opens a separate one for interaction.

   I want to do this on a single line. Once I exit the subshell, I should return back to the
   regular "super"shell without incident. There must be a way~~

   NB: What I am not asking...
    1. not asking where to get a hold of the bash man page
    2. not asking how to read initializing commands from a file... I know how to do this, it's
       not the solution I'm looking for
    3. not interested in using tmux or gnu screen
    4. not interested in giving context to this. I.e., the question is meant to be general,
       and not for any specific purpose
    5. if possible, I want to avoid using workarounds that sort of accomplish what I want, but
       in a "dirty" way. I just want to do this on a single line. In particular, I don't want
       to do something like xterm -e 'ls'

   [27]bash
   [28]share|[29]improve this question
   [30]edited Sep 10 '13 at 23:50
   user70463
   asked Mar 9 '12 at 15:12
   [31]SABBATINI Luca
   6113

   I can imagine an Expect solution, but it's hardly the one-liner you want. In what way is
   the exec bash solution unsuitable for you? –  [32]glenn jackman Mar 9 '12 at 16:21

   @glennjackman sorry, I'm not familiar with the jargon. What is an "Expect solution"? Also,
   the exec bash solution involves two separate subshells. I want one continuous subshell. –
   [33]SABBATINI Luca Mar 9 '12 at 19:18

   The beauty of exec is that it replaces the first subshell with the second, so you're only
   left 1 shell below the parent. If your initialization commands set environment variables,
   they will exist in the exec'ed shell. –  [34]glenn jackman Mar 9 '12 at 19:45
   add a comment |

3 Answers 3

   [35]active [36]oldest [37]votes
   up vote 9 down vote

   This can be easily done with [38]temporary named pipes:
bash --init-file <(echo "ls; pwd")

   Credit for this answer goes to the comment from [39]Lie Ryan. I found this really useful,
   and it's less noticeable in the comments, so I thought it should be its own answer.
   [40]share|[41]improve this answer
   answered Apr 2 '14 at 20:17
   [42]Jonathan Potter
   19111
   add a comment |
   up vote 3 down vote

   You can do this in a roundabout way with a temp file, although it will take two lines:
echo "ls; pwd" > initfile
bash --init-file initfile

   [43]share|[44]improve this answer
   answered Mar 9 '12 at 16:21
   [45]Eduardo Ivanec
   7,7881427

   Now to wrap that into a function! –  [46]cjc Mar 9 '12 at 16:56

   For a nice effect you can make the temp file remove itself by including rm $BASH_SOURCE in
   it. –  [47]Eduardo Ivanec Mar 9 '12 at 17:05

   Eduardo, thank you. That's a nice solution, but... are you saying that this can't be done
   without having to fiddle with file I/O. There's obvious reasons why I would prefer to keep
   this as a self contained command, because the moment files come into the mix I'll have to
   start worrying about how to make random temp-files and then, as you mentioned, deleting
   them. It just requires so much more effort this way if I want to be rigorous. Hence the
   desire for a more minimalistic, elegant solution. –  [48]SABBATINI Luca Mar 9 '12 at 17:20

   Use the mktemp utility to create a unique temp file. –  [49]cjc Mar 9 '12 at 17:39
   4
   This is an old question, but Bash can create temporary named pipes by using the following
   syntax: bash --init-file <(echo "ls; pwd"). –  [50]Lie Ryan Feb 13 '14 at 14:59
    |  [51]show 4 more comments
   up vote 0 down vote

   The "Expect solution" I was referring to is programming a bash shell with the [52]Expect
   programming language:
#!/usr/bin/env expect
set init_commands [lindex $argv 0]
set bash_prompt {\$ $}              ;# adjust to suit your own prompt
spawn bash
expect -re $bash_prompt {send -- "$init_commands\r"}
interact
puts "exiting subshell"

   You'd run that like: ./subshell.exp "ls; pwd"
   
