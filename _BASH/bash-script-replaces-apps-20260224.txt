filename: bash-script-replaces-apps-20260224.txt
https://www.howtogeek.com/this-bash-script-replaced-apps-i-use-everyday/

This Bash script replaced 3 apps I use everyday
Feb 16, 2026

   My system never breaks, but it constantly decays. Caches pile up, duplicates multiply, and filenames
   slowly turn into nonsense. Keeping things clean became a job of its own.

   For a long time, I relied on a small collection of tools to keep this under control. They all worked
   well, but they shared the same flaw. They were reactive, manual, and required me to remember to run
   them. Eventually, I realized they were all enforcing simple rules I already understood. So I stopped
   opening apps and wrote a single Bash script to apply those rules automatically.

System cleanup
   I used to rely on BleachBit to clean up my system: browser caches, temporary files, old logs, and
   the general junk that grows over time.
   Getting the BleachBit package from the website.

   BleachBit does its job well, but the workflow always felt slightly off. I had to remember to open it,
   scan, review the list, and click through a GUI for something that was fundamentally predictable. My
   browser cache lives in the same directory every time. Temporary files always sit in /tmp. Old logs
   always grow in /var/log. None of this is dynamic or surprising.

   In practice, I wasn't "discovering" anything. I was just repeatedly applying the same rules:
   delete known cache directories, clear temporary files, empty the trash, and remove log
   files older than a certain age. That's exactly the kind of work a shell script is good at.

   My cleanup function is essentially a list of safe, explicit actions. Just a small set of directories
   I actually want cleaned, and a time threshold for anything that should expire.

# Clear common cache directories
rm -rf ~/.cache/*
rm -rf ~/.mozilla/firefox/*/cache2/*
rm -rf ~/.config/google-chrome/Default/Cache/*

# Clear temporary files
rm -rf /tmp/*

# Remove old logs (older than 7 days)
find /var/log -type f -mtime +7 -exec rm -f {} \;

   This looks aggressive, but it's deliberately boring. Every path is something I've manually cleaned
   before. There's no recursion over my home directory, no pattern-based deletion, and no attempt to be
   clever. When I'm unsure about a rule, I start with a dry run using find ... -print instead of -exec
   rm.

   The result is that I no longer "clean my system". It just happens. What used to be a periodic
   BleachBit session is now a single function I can run on demand or schedule with cron and forget
   entirely.

Finding duplicates
   For finding duplicate files, I used Czkawka. It's a powerful tool that can scan entire
   directories, compare files by size or hash, and show you exactly where you're wasting disk space.
   An instance of running Czkawka on Linux.

   The issue wasn't functionality; it was friction. Running Czkawka meant launching another app,
   choosing scan modes, waiting for results, and then manually reviewing lists every time. I only ever
   ran it when disk space became a problem, even though duplicates were constantly gathering in the
   background.

   Under the hood, duplicate detection is not magic. It's a simple pipeline: group files by size,
   compute hashes for candidates, and treat identical hashes as duplicates. Once you understand
   that, the problem stops being "which app should I use?" and becomes "why am I not automating this?"

   My script follows that exact logic. It first groups files by size to avoid unnecessary work, then
   computes a hash for each group, and finally reports files with identical hashes. By default, it runs
   in dry mode and only prints results.

# Find files larger than 1MB and group by size
find ~/Documents -type f -size +1M -exec ls -l {} \; \
	| awk '{print $5, $9}' \
	| sort -n > sizes.txt

# Compute hashes for potential duplicates
awk '{print $2}' sizes.txt | xargs sha256sum > hashes.txt

# Show duplicate hashes
sort hashes.txt | uniq -d -w64

   Just like with cleanup, the script never deletes anything by default. It only reports duplicates. If
   I want to remove them, I do it explicitly and usually keep the newest copy. Automation is useful, but
   blind deletion is how you lose data.

   This completely replaced my need for Czkawka. I no longer scan for duplicates as a special task. I
   just run one command, get a list, and deal with it immediately.
   Tux, the Linux mascot, wearing sunglasses and peeking from behind a large terminal window on a bright
   blue background.

Bulk renaming
   For bulk renaming, I relied on GPRename. It's the kind of tool you don't think much about, but
   end up using all the time, especially when dealing with photos, downloads, or files coming from other
   people.

   The problem wasn't that GPRename was bad. It was that I was always doing the same things: lowercasing
   filenames, replacing spaces with dashes, removing weird characters, or adding a date prefix. Every
   session was a small variation of the previous one, but still required opening a GUI and clicking
   through options.

   At some point, it clicked that I wasn't really deciding anything. I had a personal naming convention,
   and I was just enforcing it manually over and over again.

   Once you see renaming as a set of rules, Bash becomes an obvious fit. You already have loops,
   regex, and string manipulation. And unlike a GUI, the rules live in plain text.

for f in *; do
	new=$(echo "$f" \
		| tr '[:upper:]' '[:lower:]' \
		| tr ' ' '-' \
		| sed 's/[^a-z0-9._-]//g')

	echo "$f -> $new"
	mv "$f" "$new"
done

   As with everything else, I usually start with echo instead of mv to preview the result.
   Renaming fiiles is fast, but also irreversible if you're not careful. This completely removed
   GPRename from my workflow. Instead of opening a renaming tool, I just apply my naming rules directly.
   Hand holding a hard drive with cloud backup icons showing error alerts and an active backup progress
   bar.

The payoff
   The practical result is that I no longer open any of those three apps for three separate activities.
   I just run one script, or let it run in the background.

   What used to be maintenance tasks are now just side effects of using my system. Disk space stays
   predictable. My home directory doesn't silently decay. File names follow the same conventions without
   me thinking about them.

   More importantly, the mental effort disappeared. There are no dashboards to check, no buttons to
   click, and no decisions to make. The rules are written once, in plain text, and applied consistently.
   Konsole Terminal open on the Kubuntu Focus Ir14 Linux laptop.


---

