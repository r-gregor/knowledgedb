filename: bash-clh-podcast-to-audiobook-20260113.txt
https://kevinboone.me/clh_podcast_to_audiobook.html

Command-line hacking: Downloading a podcast to create an audiobook

   This is another article in my occasional series on doing useful and (I hope) somewhat unusual things
   under Linux, using scripting and command-line utilities. Today's example is, I concede, likely to be
   useful only to a small number of people -- perhaps only me. Nevertheless, it does illustrate some
   useful features of Bash scripting, including XML parsing and array handling. As always, I'm only
   going to outline the code here -- for the complete code, please refer to the Download section at the
   end.

The application
   I listen to a lot of podcasts, particularly when I'm working alone in the woods I manage. There's no
   cellular coverage in these woods, so I can't rely on being able to stream content on demand, which I
   suspect is the way most people listen to this kind of material. Some proprietary smartphone podcast
   apps can cache some programs locally, but this requires a measure of forward planning. In any case,
   I'm trying to reduce my reliance on smartphones, and I'd rather just turn the podcast into a bunch of
   MP3 files, and play them on an offline audio player.

   So this application downloads an entire podcast series (perhaps restricted to a date range), and
   saves each program in the series as an audio file. Each saved file will have a name that starts with
   the broadcast date, so programs can be played in the appropriate order.

   Effectively, I'm converting a podcast stream into a local audiobook, with each individual program one
   'chapter' of the book. Apps like Smart Audiobook Player handle this kind of arrangement particularly
   well, but I can even copy the downloaded files onto my OpenSwim headset, to listen while swimming.

   Of course, to keep up to date with new programs, I'll need to be able to run the script periodically,
   and avoid downloading anything I already have. I sometimes apply a bitrate reduction to the
   downloaded stream, because saving several years' worth of podcast programs can otherwise take a lot
   of storage. I find that voice-only podcasts sound fine at 64 kbits/second, single channel, although I
   wouldn't apply such a transformation to music.

Constraints
   First, and most importantly, my script works only with podcasts that are distributed using an RSS
   feed, obtainable without authentication using an HTTP request. In my experience, nearly all
   interesting podcasts are delivered this way, but it might require some detective work to find the
   source of the RSS file. Some podcast hosts make the RSS source really obvious, others do not. If
   you're used to listening to podcasts using Spotify, for example, you probably won't find the RSS feed
   there -- you'll need to hunt for it by title using a web search. Some podcast hosts will reveal an
   RSS feed only to paid subscribers. Moreover, some hosts have regional restrictions, or block VPNs and
   proxies. There isn't much a script can do about any of this: it assumes that the RSS, and the audio
   streams themselves, are freely accessible, and that the user can find the source, and provide its URL
   on the command line.

   Second, my script assumes that all audio streams are in MP3 format. There's nothing in the logic
   itself that creates this restriction, but I'm using id3v2 to write tags in the downloaded files. This
   utility only supports ID3 tags, which are typically found in MP3 files. It wouldn't be difficult to
   extend the script to handle other file types, but all the podcasts I listen to are in MP3 format, so
   I don't really have a way to test such an extension.

   Third, in general I want to be able to select a specific program from a podcast series to play; but
   at the end of that program, I usually want to play the next program in date order. This means that
   the files the script downloads must be sortable by date within an audio player, and this in turn
   means putting the date in the filename (and probably in the 'title' tag as well).

RSS podcast format
   The first thing the script does is download the podcast's RSS feed, which we'll pass on the command
   line. The RSS that defines a podcast is an XML file with the following basic structure.
<rss version="2.0">
<channel>
	<title>Title of the podcast</title>
	...
	<item>
		<title>Title of this item</title>
		<description>Text description of this item</description>
		<enclosure url="audio_stream_url.mp3" type="audio/mpeg" />
		<pubDate>Publication date of the item in RFC2822 format</pubDate>
	</item>
	<item>
	...
	</item>
	...
</channel>

   There's a header that provides a description of the podcast, then a number of <item> elements, one
   per program. For our purposes, the information we need to extract for each program are the title, the
   URL, and the publication date. For more robust downloading, we might also extract the program length
   which, in theory, all podcast hosts should provide. With that information, we could check that the
   downloaded stream matched the program description, which would be useful for error detection. So far,
   however, I haven't used the length information.

   There's a lot more information in the RSS file that I haven't shown, and the challenge is to find a
   robust method to separate the necessary information from the unnecessary, bearing in mind that
   different RSS files will have different layouts. Some, for example, have no line breaks at all, so we
   can't just use a simple `grep` for a pattern: we'll have to parse the XML properly.

How the script works
   The first job is to download the RSS, using wget (curl also works fine). Then we'll need to parse the
   XML, splitting out the information we need for each program.

   My go-to tool here -- the one I nearly always use for parsing XML in a script -- is xsltproc. This
   utility applies an XSLT stylesheet transformation to an XML input, producing plain text or XML as the
   output.

   There isn't space here to explain XSLT in detail, but I think the XSLST snippet below is reasonably
   self-explanatory.
<xsl:stylesheet version="1.0" ...>
<xsl:output method="text"/>
<xsl:template match="/rss/channel">
<xsl:for-each select="item">
	<xslrvalue-of select="pubDate"/>$DELIMITER<xsl:value-of select="title"/>$DELIMITER<xsl:value-of select="enclosure/@url"/>$DELIMITER
</xsl:for-each>
</xsl:template>
</xsl:stylesheet>

   The <item> elements we need are all grouped in the RSS within a <channel> element, which is a
   sub-element of <rss>. So xsltproc applies the for-each template to every item, in the order the items
   appear in the XML. This template extracts the three values we need, and writes each to a temporary
   file, each followed by a separator, one line per item.

   The purpose of this XSLT transformation is to write a temporary file that is easy for a script to
   loop over line-by-line -- something that isn't practicable with XML.

   The separator can be any text that we can be reasonably sure won't appear in the data itself. At
   present, I'm using @@@, but this is trivially easy to change in the script, as it's a global
   variable.

   The output of the stylesheet transformation looks like this:
date1@@@title1@@@URL1@@@
date2@@@title2@@@UR2L@@@
...

   The script reads the transformed RSS file line-by-line, which is straightforward enough:
while read -r line; do
  ...
done < $transformed_file

   There are, of course, many other ways of reading a file using a Bash script; I'm not claiming my
   approach is the best -- it's just the one I'm familiar with.

   For each line, we split out the date, title, and URL into shell variables. Bash does have some
   built-in line tokenizing support, but I've not found it very useful with multi-character separators.
   The separators must be multi-character in this application, because there's really no single
   character that we can be sure won't appear in the data. Of course, we could use single-character
   separators with some method of escaping them when they aren't actually separators; but I don't think
   that's any more convenient.

   My approach to splitting out the individual data elements is to read each up to the delimiter into an
   array element, and then assign the array elements to named variables. The code for doing this is
   somewhat opaque, and I'd be interested to know if there is a more readable method.

 s=$line
 array=();
while [[ $s ]]; do
	array+=( "${s%%"$DELIMITER"*}" );
	s=${s#*"$DELIMITER"};
done;

 date=${array[0]}
 title=${array[1]}
 url=${array[2]}

   What the code in the while loop does is to match the input line up to the delimiter sequence, adding
   the matched text to the end of the array. Then the matching section is removed from the line, and we
   loop until the line is empty. Note that this approach requires that the delimiter is really a
   terminator -- there must be one at the end of each token, including at the end of the line. That's
   why the XSLT transformation writes a delimiter at the end of the line, not just between the tokens.
   Of course, we could just add the delimiter to the line before splitting it, if there were only
   delimiters between the tokens.

   It should go without saying that the technique I'm using here only works if the file we're parsing
   has an exact number of tokens on each line, and they're always in the same order. Since the script
   writes that file itself, that's easy to arrange. However, using this parsing technique on input we
   don't control would require better error-checking.

   For each program in the RSS we now have the date, title, and URL. We need to form a filename, which I
   base on the date and the title: we need the date to ensure that the programs get played in the right
   order, and the title to make it easier to see what each program is about.

   Because the audio player will typically play tracks in alphanumeric order of filename, we can't use
   the raw date from the RSS file as part of the filename. This is because RSS stipulates dates in
   RFC2822 format, that is:
Sunday, 26 Jan 2025 12:35:00 BST

   If we used this date in the filename, we'd end up with all 'Friday' files being played first, because
   these would be earliest in alphanumerical order. Instead, we need to write a date in the filename
   that sorts correctly in date order. That is, we need the year first, then the month, then the day,
   then the time. We perhaps don't need the time at all, but it's conceivable that some podcasts will
   publish multiple programs in the same day.

   Fortunately, the Linux date utility will read RFC2822 dates, and we can output the date in any format
   we like, thus:
sortable_date=`date -d "$date" +%Y-%m-%d_%H_%M`

   The format we use for the date isn't important, so long as it's sortable; it's helpful if it's
   human-readable as well, otherwise we could just use the Unix epoch date, which is a simple integer.

   We'll form the filename from the date and the program title, but we need to be careful to remove or
   convert the characters that aren't legal in filenames. For example:

sanitized_title="${title//:/_}"             # Remove :
sanitized_title="${sanitized_title//\?/_}"  # Remove ?
sanitized_title="${sanitized_title//\*/@}"  # Remove *
sanitized_title="${sanitized_title//\"/\'}" # Remove "

sortable_title="${sortable_date} ${title}"
sortable_sanitized_title="${sortable_date} ${sanitized_title}"
output_file="${output_dir}/${sortable_sanitized_title}.${extension}"

   So at this point we have a stream URL, and a filename under which to store it, that will sort
   properly in the audio player. We now need to download the file; I've found wget works better than
   curl here, for reasons that are not clear to me.
wget -O "${output_file}" "${url}"

   We only want to download the file if it doesn't already exist. We can also restrict the downloads to
   a particular date range, if we don't want to download ten year's worth of programs. I won't describe
   these tests here, because they're trivial, but they're in the full source code.

   For the finishing touches, we can tag the MP3 files as we download them. It's almost imperative to
   write the 'title' tag, because audio players sometimes sort by title in preference to filename. We
   probably want to write the 'album' tag as well, because audio players typically group tracks by
   album, rather than assuming that all tracks in a particular directory go together.
id3v2 -t "${sortable_title}" -A "${album}" ... "${output_file}"

   And, essentially, that's it. The full script has some additional features; it can, for example, apply
   a quality reduction to the downloaded file, so the podcast uses less storage. There's also a fair bit
   of error checking that I haven't shown here.

Further work
   One useful addition would be to check that the downloaded stream results in a file of the correct
   size. If it does not, then we should not store a file at all, so we can try to download it again
   later. The reason this check might be necessary is that sometimes there can be an outage in the
   podcast host, that results in the delivery of an error message, rather than an MP3 file. Or,
   occasionally, the download will be incomplete. The script won't handle these situations very well
   because, once there is a file in the output directory with the right name, it won't be overwritten,
   even if its contents are nonsense.

   Making this improvement would require parsing the 'length' fields from the RSS feed, and adding them
   to the temporary file generated by the XSLT transformation. The rest of the script would then read
   this value when parsing the temporary file and, after downloading the stream, compare the resulting
   file's length with the length in the RSS.

   It would also be useful to be able to handle streams other than MP3 but, so far, I haven't used any,
   so I haven't been motivated to write the code. You'd need a tagger for each supported type, such as
   Atomic Parsley for MP4-type files.

Download
   The full source for rsspodfetch is available [**1][https://github.com/kevinboone/rsspodfetch]from my GitHub
   repository.


---
[**1]
https://github.com/kevinboone/rsspodfetch/blob/main/rsspodfetch.sh

#!/bin/bash

## rsspodfetch.sh
##
## A script to keep local copies of podcast audio files in sync with the source;
##   that is, to convert podcast streams to local audiobooks.
##
## Prerequisites: wget, xsltproc, id3v2, ffmpeg (if enabling quality reduction)
##
## Usage:
## rsspodfetch.sh {rss_feed_url} {output_dir} {album} {artist} {genre} {max_days_old}
##   feed_url     : the HTTP(S) URL from which to fetch the podcast RSS feed
##   output_dir   : the directory into which downloaded audio files will be saved
##   album        : the album tag to write into downloaded files
##   artist       : the artist tag to write into downloaded files
##   genre        : the genre tag to write into downloaded files
##   max_days_old : the oldest file to download (based on publication date)
##
## All arguments except max_days_old are mandatory. max_days_old defaults to 365
##
## Note: at present, this utility only works with MP3 streams, and the stream URL must
##   have a name that ends in '.mp3'. This is because `id3v2` only works with
##   ID3 tags, as found in MP3.
##
## Copyright (c)2026 Kevin Boone, GPLv3.0

## Set REDUCE_QUALITY to 'yes' if you want to reduce bitrate, etc, using ffmpeg.
## Of course, ffmpeg must be available. You'll need to tune the arguments to
##   ffmpeg, for the quality you require (deflt. 64kbits, 22kHz, mono). See
##   line ~200.

#REDUCE_QUALITY=yes

## Specify a delimiter for the fields in the intermediate TSV file. It can be
##   any string that is not going to appear anywhere in the podcast URL or title
DELIMITER=@@@

## Assign the command-line arguments to variables with more useful names
feed_url=$1
output_dir=$2
album=$3
artist=$4
genre=$5
max_days_old=$6

## Various temporary files that we will generate during the downloading
##   and parsing of the RSS file
rss_file=/tmp/$$.rss
xslt_file=/tmp/$$.xslt
tsv_file=/tmp/$$.tsv

## Let's check all the pre-requisites are in place before doing much else
if ! which xsltproc > /dev/null ; then
	echo Usage: $0 requires xsltproc, which seems not to be available
	exit
fi

if ! which wget > /dev/null ; then
	echo Usage: $0 requires wget, which seems not to be available
	exit
fi

if ! which id3v2 > /dev/null ; then
	echo Usage: $0 requires id3v2, which seems not to be available
	exit
fi

## Let's make sure the command line specified the output directory, and
##   that it is writeable
if [ "$output_dir" == "" ] ; then
	echo Usage: $0 {feed_url} {output_directory}
	exit
fi

if [ ! -w "$output_dir" ]; then
	echo $output_dir is not a writable directory
	exit
fi

if [ "$max_days_old" == "" ] ; then
	echo Maximum age not set: defaulting to 365 days
	max_days_old=365
fi

## Get the RSS file, and abort if we can't
echo Fetching RSS file from $feed_url
wget -O $rss_file $feed_url
if [[ $? -ne 0 ]] ; then
	echo Can\'t download RSS from $feed_url
	exit
fi

## This is the only bit of this utility that is remotely clever. We'll use
##  xsltproc to parse the RSS XML file into something that can be scanned and
##  parsed further by a simple shell script. We'll write the XSLT transformation
##  from this file into a file in /tmp, making this script self-contained.
## The XSLT extracts the parts of the <item> definition that we really need --
##  the publication date, title, and stream URL. These will be written, one
##  item to a line, with the fields separated by $DELIMITER
cat > $xslt_file << EOF
<xsl:stylesheet version="1.0"
	 xmlns:xsl="http://www.w3.org/1999/XSL/Transform">
<xsl:output method="text"/>
<xsl:template match="/rss/channel">
<xsl:for-each select="item">
	<xsl:value-of select="pubDate"/>$DELIMITER<xsl:value-of select="title"/>$DELIMITER<xsl:value-of select="enclosure/@url"/>$DELIMITER
</xsl:for-each>
</xsl:template>
</xsl:stylesheet>
EOF

## The output from the XSLT transformation is to the file $tsv_file
echo Parsing RSS file
xsltproc $xslt_file $rss_file > $tsv_file

## Stop if the XLST transformation failed
if [[ $? -ne 0 ]] ; then
	echo Can\'t parse RSS file from $feed_url
	exit
fi

## Now we'll read the generated $tsv_file line-by-line, downloading from the
##  stream URL when necessary
while read -r line; do
	# Only process not-blank lines
	if [ ! "$line" == "" ] ; then
		s=$line
		# Split the line into an array, with each token terminated by $DELIMITER
		# The syntax of this parameter substitution is pretty ugly, as you see
		array=();
		while [[ $s ]]; do
			array+=( "${s%%"$DELIMITER"*}" );
			s=${s#*"$DELIMITER"};
		done;

		# Having split the line into an array, extract the relevant bits into
		#   variables with more useful names
		date=${array[0]}
		title=${array[1]}
		url=${array[2]}
		# Extract the file extension from the URL, but bear in mind we only
		#   support .mp3 at present
		extension="${url##*.}"
		# The URL may contain material after the extension, which we would have
		#   to remove, if were were handling anything but MP3 streams...
		extension="mp3" # ...but we're not, at present

		# TODO: warn/fail if this isn't an MP3 stream

		# Make a sanitized title with no :, ?, *, or " characters, to use in the output
		#   filename; some filesystems choke on these characters.
		# TODO: we might need to remove other characters as well, for maximum
		#   compatibility
		sanitized_title="${title//:/_}"
		sanitized_title="${sanitized_title//\?/_}"
		sanitized_title="${sanitized_title//\*/@}"
		sanitized_title="${sanitized_title//\"/\'}"

		# Convert the RFC2822 date-time from the RSS to a format that
		#   will sort alphanumerically into date-time order
		# Note that I'm using the British date format YY-MM-DD, not the US
		#   YY-DD-MM, because the US version won't sort properly
		sortable_date=`date -d "$date" +%Y-%m-%d_%H_%M`

		# From the date and the RSS title, form a title tag and an output filename.
		# Both have the same form, but the filename uses the sanitized version
		#   of the filename, avoiding illegal characters. There should not no need
		#   to worry about specific characters in the title tag.
		sortable_title="${sortable_date} ${title}"
		sortable_sanitized_title="${sortable_date} ${sanitized_title}"
		output_file="${output_dir}/${sortable_sanitized_title}.${extension}"

		# Work out how old the stream is, from the publication date
		epoch_date_now=`date +%s`
		epoch_date_pub=`date -d "$date" +%s`
		days_old=$(( ($epoch_date_now - $epoch_date_pub) / 3600 / 24 ))

		# Print what we've worked out, for debugging purposes
		echo -e
		echo -e "date=$date"
		echo -e "sortable_date=$sortable_date"
		echo -e "title=$title"
		echo -e "sortable_title=$sortable_title"
		echo -e "sortable_sanitized_title=$sortable_sanitized_title"
		echo -e "url=$url"
		echo -e "extension=$extension"
		echo -e "output file=$output_file"

		echo -e "days old=${days_old}"

		# Now download the file if it is not already present in the output
		#   directory, and if it is not too old
		if [ -e "$output_file" ] ; then
			echo File ${output_file} exists -- not downloading
		elif [[ $days_old -lt $max_days_old ]] ; then
			echo Download ${url} to ${output_file}
			wget -O "${output_file}" "${url}"
			id3v2 -t "${sortable_title}" -A "${album}" -g "${genre}" -a "${artist}" \
				      --TCOM "${artist}" "${output_file}"

			# Adjust quality using ffmpeg, if specified. Note that ffmpeg respects
			#   existing tags, so we can do this after tagging with id3v2. Note also
			#   that we need -nostdin here, else ffmpeg fights with the script for
			#   the console
			if [[ "$REDUCE_QUALITY" == "yes" ]] ; then
				echo Adjusting bitrate -- please be patient
				temp_audio_file="/tmp/$$.mp3"
				ffmpeg -nostdin -i "${output_file}" -ar 22050 \
				      -ac 1 -c:a libmp3lame -q:a 4 "${temp_audio_file}"
				mv "${temp_audio_file}" "${output_file}"
				rm "${temp_audio_file}"
			fi
		else
			echo Not downloading ${url} -- too old
		fi

	fi
done < $tsv_file

# If we get here, we probably did something successfully. Tidy
#  up temporary files
Vrm -f "$tsv_file" "$xslt_file" "$rss_file"

---
README.md

rsspodfetch.sh is a simple Bash script for Linux, which maintains a local copy of an audio podcast that is
defined by an RSS file or stream. That is, it converts a podcast stream into a locally-stored audiobook, with
one 'chapter' per podcast program. The script works by parsing the RSS file, and downloading the audio stream
for each program to completion, saving the resulting audio files to a specified directory. It shouldn't
	attempt to download a file that already exists in the output directory. The utility can therefore be run
	as often as required, and will ensure that the local cache is up to date.

I listen to podcasts offline on my Android phone, using Smart Audiobook Player. Any audio player should be
fine for this purpose, but rsspodfetch.sh specifically writes files that are palatable to Smart Audiobook
Player, Listen, and similar apps. In particular:

It sets the same 'album' tag identically in each file it writes in a particular run, so that files in the same
podcast get grouped together as part of the same 'book'.  It stores each file with a name that contains the
program's publication date, in a sortable format. It also writes the 'title' tag in a similar way. Smart
Audiobook Player assumes that tracks should be played in alphanumeric order of track title or, if there is no
title, in order of filename. This is common behaviour for local audio file players, however.

Why?
I listen to podcasts when I'm working in my woods, where I have no Internet access. Some proprietary podcast
players can maintain a local cache if you ask them to, but using such a feature requires a measure of forward
planning. In addition, an audiobook player like Smart or Listen has a much nicer, simpler user interface than
most podcast players. Podcast players don't always play programs in date order or, worse, play them in reverse
date order, newest first. Audiobook players don't seem to have this problem, so long as files are properly
named.

So, every so often, I just run this script to update all the podcasts I listen to, and then copy the entire
set of directories to my Android phone using a file manager. The file manager is smart enough not to copy
files that are already on the phone, so this is generally a quick operation.

CAUTION
Downloading an offline copy of a podcast that has been running for years will use a lot of storage -- many
gigabytes. I have a terabyte SD card in my smartphone, and offline copies of podcasts take up a fair
proportion of that storage. rsspodfetch.sh can optionally reduce the audio quality of streams as it downloads
them, and this can save a lot of storage.

By default, rsspodfetch.sh downloads items whose publication date is within the last year, but this can be
changed on the command line if necessary.

Prerequisites
rsspodfetch.sh requires wget, id3v2, and xsltproc. The script checks that these utilities are available on the
$PATH, and will quit if they are not.

You'll also need ffmpeg if you enable quality reduction of the saved files.

Usage
rsspodfetch.sh {feed_url} {output_dir} {album} {artist} {genre} {max_days_old}

feed_url     : the HTTP(S) URL from which to fetch the podcast RSS feed
output_dir   : the directory into which downloaded audio files will be saved
album        : the album tag to write into downloaded files
artist       : the artist tag to write into downloaded files
genre        : the genre tag to write into downloaded files
max_days_old : the oldest file to download (based on publication date)

All arguments except max_days_old are mandatory. max_days_old defaults to 365.

Set the environment variable REDUCE_QUALITY=yes if you want to reduce the audio quality after downloading.
You'll probably need to edit the ffmpeg line to set the bitrate, sample rate, etc., that you like.

How it works
The RSS file that defines an audio podcast has the format outlined below. There is a header containing
information about the podcast as a whole, and then a series of entries, one for each specific program. Within
the you'll see the program title, the publication date, and the stream URL.

<rss version="2.0">
<channel>
	<title>Title of the podcast</title>
	<item>
		<title>Title of this item</title>
		<description>Text description of this item</description>
		<enclosure url="audio_stream_url.mp3" type="audio/mpeg" />
		<pubDate>Publication date of the item in RFC2822 format</pubDate>
	</item>
	<item>
	...
	</item>
	...
</channel>
</rss>

rsspodfetch.sh retrieves and reads the RSS file, and ensures that there is a local copy of the stream defined
in each entry. It saves the local copy in a specified directory, using a name based on the program title and
the publication date. Since a local audio file player will probably sort by filename, if we want to play
programs from oldest to newest, we must ensure that the filename begins with the date, and that the date is in
a format that can be sorted by the player. In addition, we probably need to set the title tag in the saved
file so that it, too, begins with the date. This is because some audio players will prefer the title tag to
the filename, when it comes to sorting. Because players like Smart Audiobook Player and Listen group files by
the 'album' tag, we must also ensure that each file in a specific podcast has the same album tag. Sometimes
the podcast producer will set meaningful title and album tags, but we can't rely on this -- we need to set
tags for all files.

As each in the RSS feed has a unique combination of title and publication date, we can combine these to form
the filename of the local file, and also to ensure that we don't try to download streams that are already in
the output directory.

rsspodfetch.sh uses xsltproc to parse the XML of the RSS file. It does this by extracting each program's
entry, and writing the relevant parts (title, date, and URL) to an intermediate text file, one line at a time.
On each line the variables are separated by tokens (default to "@@@"). The script then reads the text file
line by line, downloading each item from its URL.

Optionally the script will call ffmpeg to reduce the quality of the audio file. Some podcast streams are
delivered at absurdly high bitrates or sample rates for human speech. Reducing the quality can radically
reduce storage requirements, while still storing a file that is listenable.

Notes
At present, rsspodfetch supports only MP3 streams. This is because it uses id3v2 to apply tags to the files it
downloads. MP3 format is almost ubiquitous in the audio podcast world, but not completely. To handle other
format's we'd need to update the utility to detect the file type, and then invoke a tagger appropriate to that
type, rather than id3v2. For MP4/M4A/M4B files, for example, we could use AtomicParsley.

rsspodfetch.sh removes ? and : characters from the names of files it stores, replacing them with _. It also
replaces double-quotes with single-quotes. This is because some filesystems choke on those characters. The
tags stored in MP3 files -- also based on the title -- may still contain these characters: there's no problem
using them in tags. It's plausible that some filesystems will reject other characters that are sometimes found
in the titles of podcast programs, and which I haven't anticipated.

This fussiness about legal characters probably only comes to a head when copying the downloaded files to a
reader device. Most Linux filesystems are pretty generous in the characters they accept, but players may use
FAT or exFAT filesystems, which are not. It's best to test with a small set of files (e.g., by specifying a
small date range) before downloading a ten-year podcast, only to find that half the files can't be copied.

It would be possible to use the podcast title, as found in the RSS file, as the 'album' tag, and avoid the
need to specific it on the command line. However, I have conventions about how I tag my files, and I suspect
that others who store audio files for offline use probably do, too. So rsspodfetch.sh requires that tag
information be given on the command line. It sets the 'composer' tag to be the same as the 'artist'. This
would be easy to change, if necessary.

Please be aware that some podcast hosting servers are fussy, and might limit the number of downloads in a
particular time. Some block access via VPNs, for some reason. I don't know why, but I've found the wget is
more palatable to podcast servers than curl is, which is why I've used wget in this utility.

rsspodfetch.sh only checks whether a file exists before it downloads; it doesn't check anything else about the
file. If a download fails, for any reason, and leaves an incomplete file, it won't get overwritten on the next
update. Similarly, if the server delivers something that isn't an audio file (because of some problem), that
also won't get overwritten with an audio file later. It would be nice to check at least whether the stored
file was the same size as the file on the server, but podcast hosts don't always provide file length
information, so the utility would have to download the file completely to find out. In the event of a failure,
some manual clean-up of the local directory might sometimes be necessary.

Not all podcasts have an RSS feed at all. Some podcast hosts do provide RSS, but only to users who pay a fee.
It should go without saying that rsspodfetch.sh requires access to an RSS feed, that can be obtained by a wget
operation, not just a web browser.


---

