filename: sw-design-the-simplest-thing-that-could-possibly-work-20250901.txt
https://www.seangoedecke.com/the-simplest-thing-that-could-possibly-work/

Do the simplest thing that could possibly work

   When designing software systems, do the simplest thing that could possibly work.

   It's surprising how far you can take this piece of advice. I genuinely think you can do this all the
   time. You can follow this approach for fixing bugs, for maintaining existing systems, and for
   architecting new ones.

   A lot of engineers design by trying to think of the "ideal" system: something well-factored,
   near-infinitely scalable, elegantly distributed, and so on. I think this is entirely the wrong way to
   go about software design. Instead, spend that time understanding the current system deeply, then do
   the simplest thing that could possibly work.

Simple can be underwhelming
   [**1][https://www.seangoedecke.com/good-system-design]System design requires competence with a lot of
   different tools: app servers, proxies, databases, caches, queues, and so on. As they gain familiarity with
   these tools, junior engineers naturally want to use them. It's fun to construct systems out of many
   different components! And it feels very satisfying to draw boxes and arrows on a whiteboard - like you're
   doing real engineering.

   However, as with many skills, real mastery often involves learning when to do less, not more. The
   fight between an ambitious novice and an old master is a well-worn cliche in martial arts movies: the
   novice is a blur of motion, flipping and spinning. The master is mostly still. But somehow the
   novice's attacks never seem to quite connect, and the master's eventual attack is decisive.

   In software, this means that [**2][https://www.seangoedecke.com/great-software-design]great software design
   looks underwhelming. It doesn't look like anything much is happening at all. You can tell you're in the
   presence of great software design because you start having thoughts like "oh, I didn't realise the problem
   was that easy" or "oh nice, you don't actually have to do anything difficult".

   [7]Unicorn is great software design, because it delivers all the most important guarantees in a web
   server (request isolation, horizontal scaling, crash recovery) by leaning on Unix primitives.
   The industry-standard Rails REST API is great software design, because it gives you exactly what you
   need for a CRUD app in the most boring way possible. I don't think any of these are impressive
   software. But they're impressive feats of design, because they do the simplest thing that could
   possibly work.

   You should do that too! Suppose you've got a Golang application that you want to add some kind of
   rate limiting to. What's the simplest thing that could possibly work? Your first idea might be to add
   some kind of persistent storage (say, Redis) to track per-user request counts with a leaky-bucket
   algorithm. That would work! But do you need a whole new piece of infrastructure? What if instead you
   kept those per-user request counts in-memory? Sure, you'd lose some rate limiting data when the
   application is restarted, but does that matter? Actually, are you sure your edge proxy doesn't
   support rate limiting already? Could you just write a couple of lines in a config file instead of
   implementing the feature at all?

   Maybe your edge proxy doesn't support rate limiting. Maybe you can't track it in-memory because you
   have too many server instances running in parallel, so the tightest rate limit you could enforce that
   way is too wide. Maybe it's a dealbreaker if you ever lose rate limiting data, because people are
   hammering your service that hard. In that case, the simplest thing that could possibly work is adding
   persistent storage, so you should go and do that. But if you could do one of the easier approaches,
   wouldn't you want to?

   You really can build a whole application from scratch this way: start with the absolute simplest
   thing, and then only extend it when you have new requirements that force you to. It sounds silly, but
   it works. Think of it as taking [10]YAGNI as the ultimate design principle: above
   single-responsibility, above choosing the best tool for the job, and above "good design".

What's wrong with doing the simplest thing?
   Of course, there are three big problems with always doing the simplest thing that could possibly
   work. The first is that, by not anticipating future requirements, you end up with an inflexible
   system or a [11]big ball of mud. The second is that it's not clear what "simplest" means, so at worst
   I'm saying "to design well, always do good design". The third is that you ought to be building
   systems that can scale, not systems that just work right now. Let's take those objections in order.

Big balls of mud
   To some engineers, "do the simplest thing that could possibly work" sounds like I'm telling them to
   stop doing engineering. If the simplest thing is usually a quick kludge, does that mean this advice
   will inevitably lead to a complete mess? We've all seen codebases with hacks stacked on top of hacks,
   and they definitely don't look like good design.

   But are hacks simple? I actually don't think so. The problem with a hack or a kludge is precisely
   that it isn't simple: that it adds complexity to the codebase by introducing another thing you have
   to always remember. Hacks are just easier to think of. Figuring out the proper fix is hard because it
   requires having to understand the entire codebase (or large sections of it). In fact, the proper fix
   is almost always much simpler than the hack.

   It is not easy to do the simplest thing that could possibly work. When you're looking at a problem,
   the first few solutions that come to mind are unlikely to be the simplest ones. Figuring out the
   simplest solution requires considering many different approaches. In other words, it requires doing
   engineering.

What is simplicity?
   Engineers disagree a lot about what constitutes simple code. If "simplest" already means "with good
   design", is it just a tautology to say "you should do the simplest thing that could possibly work?"
   In other words, is Unicorn really simpler than [12]Puma? Is adding in-memory rate limiting
   really simpler than using Redis? Here's a rough, intuitive definition of simplicity:
    1. Simple systems have fewer "moving pieces": fewer things you have to think about when you're
       working with them
    2. Simple systems are less internally-connected. They are composed from components with clear,
       straightforward interfaces

   Unix processes are simpler than threads (and thus Unicorn is simpler than Puma) because processes are
   less connected: they do not share memory. This makes a lot of sense to me! But I don't think it gives
   you the tools to figure out what's simpler in every case.

   What about in-memory rate limiting vs Redis? On the one hand, in-memory is simpler because you don't
   have to think about all the things involved in standing up a separate service with persistent memory.
   On the other hand, Redis is simpler because the rate limiting guarantees it offers are more
   straightforward - you don't have to worry about the case where one server instance thinks a user is
   rate limited and another one doesn't.

   When I'm not sure what "seems" simpler to me, I like to use this tiebreaker: simple systems are
   stable. If you're comparing two states of a software system, and one will require more ongoing work
   if no requirements change, the other one is simpler. Redis must be deployed and maintained, it can
   have its own incidents, it requires its own monitoring, it requires a separate deployment in any new
   environments the service finds itself in, and so on. Thus in-memory rate limiting is simpler than
   Redis.

Why wouldn't you want to be scalable?
   A certain type of engineer is now screaming to themselves "but in-memory rate limiting won't scale!"
   Doing the simplest thing that could possibly work will emphatically not deliver the most web-scale
   system. It will deliver a system that works well at the current scale. Is this irresponsible
   engineering?

   No. In my view, the cardinal sin of big tech SaaS engineering is an obsession with scale. I've seen
   so much unavoidable pain caused by over-engineering systems to prepare for several orders of
   magnitude more than the current scale.

   The main reason to not try this is that it doesn't work. In my experience, for any non-trivial
   codebase, you can't anticipate how it will behave at several orders of magnitude more traffic,
   because you don't know ahead of time where all the bottlenecks are going to be. At most you can try
   to make sure you're ready for 2x or 5x the current traffic, and then stand by to deal with problems
   as they come in.

   The other reason not to try this is that it makes your codebase inflexible. It's fun to decouple your
   service into two pieces so they can be scaled independently (I have seen this happen maybe ten times,
   and I have seen them actually be usefully scaled independently maybe once). But that makes certain
   features very hard to implement, because they now require coordination over the wire. In the worst
   case, they require transactions over the wire, which is a genuinely hard engineering problem. Most of
   the time you just don't have to do any of this!

Final thoughts
   The longer I spend working in tech, the less optimistic I become about our collective ability to
   predict where a system is going. It's hard enough to get your head around where a system currently
   is. And in fact, that's the main practical difficulty in doing good design: getting an accurate
   big-picture understanding of the system. Most design is done without that understanding, and most
   design is thus pretty bad.

   There are, broadly speaking, two ways to develop software. The first is to predict what your
   requirements might look like six months or a year from now, and then design the best system for that
   purpose. The second is to design the best system for what your requirements actually look like right
   now: in other words, to do the simplest thing that could possibly work.


---
[**1]
https://www.seangoedecke.com/good-system-design/

Everything I know about good system design

   I see a lot of bad system design advice. One classic is the LinkedIn-optimized "bet you never heard
   of queues" style of post, presumably aimed at people who are new to the industry. Another is the
   Twitter-optimized "you're a terrible engineer if you ever store booleans in a database" clever
   trick. Even good system design advice can be kind of bad. I love Designing Data-Intensive
   Applications, but I don't think it's particularly useful for most system design problems engineers
   will run into.

   What is system design? In my view, if software design is how you assemble lines of code, system
   design is how you assemble services. The primitives of software design are variables, functions,
   classes, and so on. The primitives of system design are app servers, databases, caches, queues, event
   buses, proxies, and so on.

   This post is my attempt to write down, in broad strokes, everything I know about good system design.
   A lot of the concrete judgment calls do come down to experience, which I can't convey in this post.
   But I'm trying to write down what I can.

Recognizing good design
   What does good system design look like? I've written before that it looks underwhelming. In
   practice, it looks like nothing going wrong for a long time. You can tell that you're in the presence
   of good design if you have thoughts like "huh, this ended up being easier than I expected", or "I
   never have to think about this part of the system, it's fine". Paradoxically, good design is
   self-effacing: bad design is often more impressive than good. I'm always suspicious of
   impressive-looking systems. If a system has distributed-consensus mechanisms, many different forms of
   event-driven communication, CQRS, and other clever tricks, I wonder if there's some fundamental bad
   decision that's being compensated for (or if the system is just straightforwardly over-designed).

   I'm often alone on this. Engineers look at complex systems with many interesting parts and think
   "wow, a lot of system design is happening here!" In fact, a complex system usually reflects an
   absence of good design. I say "usually" because sometimes you do need complex systems. I've worked on
   many systems that earned their complexity. However, a complex system that works always evolves from a
   simple system that works. Beginning from scratch with a complex system is a really bad idea.

State and statelessness
   The hard part about software design is state. If you're storing any kind of information for any
   amount of time, you have a lot of tricky decisions to make about how you save, store and serve it. If
   you're not storing information, your app is "stateless". As a non-trivial example, GitHub has an
   internal API that takes a PDF file and returns a HTML rendering of it. That's a real stateless
   service. Anything that writes to a database is stateful.

   You should try and minimize the amount of stateful components in any system. (In a sense this is
   trivially true, because you should try to minimize the amount of all components in a system, but
   stateful components are particularly dangerous.) The reason you should do this is that stateful
   components can get into a bad state. Our stateless PDF-rendering service will safely run forever, as
   long as you're doing broadly sensible things: e.g. running it in a restartable container so that if
   anything goes wrong it can be automatically killed and restored to working order. A stateful service
   can't be automatically repaired like this. If your database gets a bad entry in it (for instance, an
   entry with a format that triggers a crash in your application), you have to manually go in and fix it
   up. If your database runs out of room, you have to figure out some way to prune unneeded data or
   expand it.

   What this means in practice is having one service that knows about the state - i.e. it talks to a
   database - and other services that do stateless things. Avoid having five different services all
   write to the same table. Instead, have four of them send API requests (or emit events) to the first
   service, and keep the writing logic in that one service. If you can, it's worth doing this for the
   read logic as well, although I'm less absolutist about this. It's sometimes better for services to do
   a quick read of the user_sessions table than to make a 2x slower HTTP request to an internal sessions
   service.

Databases
   Since managing state is the most important part of system design, the most important component is
   usually where that state lives: the database. I've spent most of my time working with SQL databases
   (MySQL and PostgreSQL), so that's what I'm going to talk about.

Schemas and indexes
   If you need to store something in a database, the first thing to do is define a table with the schema
   you need. Schema design should be flexible, because once you have thousands or millions of records,
   it can be an enormous pain to change the schema. However, if you make it too flexible (e.g. by
   sticking everything in a "value" JSON column, or using "keys" and "values" tables to track arbitrary
   data) you load a ton of complexity into the application code (and likely buy some very awkward
   performance constraints). Drawing the line here is a judgment call and depends on specifics, but in
   general I aim to have my tables be human-readable: you should be able to go through the database
   schema and get a rough idea of what the application is storing and why.

   If you expect your table to ever be more than a few rows, you should put indexes on it. Try to make
   your indexes match the most common queries you're sending (e.g. if you query by email and type,
   create an index with those two fields). Indexes work like nested dictionaries, so make sure to put
   the highest-cardinality fields first (otherwise each index lookup will have to scan all users of type
   to find the one with the right email). Don't index on every single thing you can think of, since each
   index adds write overhead.

Bottlenecks
   Accessing the database is often the bottleneck in high-traffic applications. This is true even when
   the compute side of things is relatively inefficient (e.g. Ruby on Rails running on a preforking
   server like Unicorn). That's because complex applications need to make a lot of database calls -
   hundreds and hundreds for every single request, often sequentially (because you don't know if you
   need to check whether a user is part of an organization until after you've confirmed they're not
   abusive, and so on). How can you avoid getting bottlenecked?

   When querying the database, query the database. It's almost always more efficient to get the database
   to do the work than to do it yourself. For instance, if you need data from multiple tables, JOIN them
   instead of making separate queries and stitching them together in-memory. Particularly if you're
   using an ORM, beware accidentally making queries in an inner loop. That's an easy way to turn a
   select id, name from table to a select id from table and a hundred select name from table where id =
   ?.

   Every so often you do want to break queries apart. It doesn't happen often, but I've run into queries
   that were ugly enough that it was easier on the database to split them up than to try to run them as
   a single query. I'm sure it's always possible to construct indexes and hints such that the database
   can do it better, but the occasional tactical query-split is a tool worth having in your toolbox.

   Send as many read queries as you can to database replicas. A typical database setup will have one
   write node and a bunch of read-replicas. The more you can avoid reading from the write node, the
   better - that write node is already busy enough doing all the writes. The exception is when you
   really, really can't tolerate any replication lag (since read-replicas are always running at least a
   handful of ms behind the write node). But in most cases replication lag can be worked around with
   simple tricks: for instance, when you update a record but need to use it right after, you can fill in
   the updated details in-memory instead of immediately re-reading after a write.

   Beware spikes of queries (particularly write queries, and particularly transactions). Once a database
   gets overloaded, it gets slow, which makes it more overloaded. Transactions and writes are good at
   overloading databases, because they require a lot of database work for each query. If you're
   designing a service that might generate massive query spikes (e.g. some kind of bulk-import API),
   consider throttling your queries.

Slow operations, fast operations
   A service has to do some things fast. If a user is interacting with something (say, an API or a web
   page), they should see a response within a few hundred ms. But a service has to do other things
   that are slow. Some operations just take a long time (converting a very large PDF to HTML, for
   instance). The general pattern for this is splitting out the minimum amount of work needed to do
   something useful for the user and doing the rest of the work in the background. In the PDF-to-HTML
   example, you might render the first page to HTML immediately and queue up the rest in a background
   job.

   What's a background job? It's worth answering this in detail, because "background jobs" are a core
   system design primitive. Every tech company will have some kind of system for running background
   jobs. There will be two main components: a collection of queues, e.g. in Redis, and a job runner
   service that will pick up items from the queues and execute them. You enqueue a background job by
   putting an item like {job_name, params} on the queue. It's also possible to schedule background jobs
   to run at a set time (which is useful for periodic cleanups or summary rollups). Background jobs
   should be your first choice for slow operations, because they're typically such a well-trodden path.

   Sometimes you want to roll your own queue system. For instance, if you want to enqueue a job to run
   in a month, you probably shouldn't put an item on the Redis queue. Redis persistence is typically not
   guaranteed over that period of time (and even if it is, you likely want to be able to query for those
   far-future enqueued jobs in a way that would be tricky with the Redis job queue). In this case, I
   typically create a database table for the pending operation with columns for each param plus a
   scheduled_at column. I then use a daily job to check for these items with scheduled_at <= today, and
   either delete them or mark them as complete once the job has finished.

Caching
   Sometimes an operation is slow because it needs to do an expensive (i.e. slow) task that's the same
   between users. For instance, if you're calculating how much to charge a user in a billing service,
   you might need to do an API call to look up the current prices. If you're charging users per-use
   (like OpenAI does per-token), that could (a) be unacceptably slow and (b) cause a lot of traffic for
   whatever service is serving the prices. The classic solution here is caching: only looking up the
   prices every five minutes, and storing the value in the meantime. It's easiest to cache in-memory,
   but using some fast external key-value store like Redis or Memcached is also popular (since it means
   you can share one cache across a bunch of app servers).

   The typical pattern is that junior engineers learn about caching and want to cache everything, while
   senior engineers want to cache as little as possible. Why is that? It comes down to the first point I
   made about the danger of statefulness. A cache is a source of state. It can get weird data in it, or
   get out-of-sync with the actual truth, or cause mysterious bugs by serving stale data, and so on. You
   should never cache something without first making a serious effort to speed it up. For instance, it's
   silly to cache an expensive SQL query that isn't covered by a database index. You should just add the
   database index!

   I use caching a lot. One useful caching trick to have in the toolbox is using a scheduled job and a
   document storage like S3 or Azure Blob Storage as a large-scale persistent cache. If you need to
   cache the result of a really expensive operation (say, a weekly usage report for a large customer),
   you might not be able to fit the result in Redis or Memcached. Instead, stick a timestamped blob of
   the results in your document storage and serve the file directly from there. Like the database-backed
   long-term queue I mentioned above, this is an example of using the caching idea without using a
   specific cache technology.

Events
   As well as some kind of caching infrastructure and background job system, tech companies will
   typically have an event hub. The most common implementation of this is Kafka. An event hub is just a
   queue - like the one for background jobs - but instead of putting "run this job with these params" on
   the queue, you put "this thing happened" on the queue. One classic example is firing off a "new
   account created" event for each new account, and then having multiple services consume that event and
   take some action: a "send a welcome email" service, a "scan for abuse" service, a "set up per-account
   infrastructure" service, and so on.

   You shouldn't overuse events. Much of the time it's better to just have one service make an API
   request to another service: all the logs are in the same place, it's easier to reason about, and you
   can immediately see what the other service responded with. Events are good for when the code sending
   the event doesn't necessarily care what the consumers do with the event, or when the events are
   high-volume and not particularly time-sensitive (e.g. abuse scanning on each new Twitter post).

Pushing and pulling
   When you need data to flow from one place to a lot of other places, there are two options. The
   simplest is to pull. This is how most websites work: you have a server that owns some data, and when
   a user wants it they make a request (via their browser) to the server to pull that data down to them.
   The problem here is that users might do a lot of pulling down the same data - e.g. refreshing their
   email inbox to see if they have any new emails, which will pull down and reload the entire web
   application instead of just the data about the emails.

   The alternative is to push. Instead of allowing users to ask for the data, you allow them to register
   as clients, and then when the data changes, the server pushes the data down to each client. This is
   how GMail works: you don't have to refresh the page to get new emails, because they'll just appear
   when they arrive.

   If we're talking about background services instead of users with web browsers, it's easy to see why
   pushing can be a good idea. Even in a very large system, you might only have a hundred or so services
   that need the same data. For data that doesn't change much, it's much easier to make a hundred HTTP
   requests (or RPC, or whatever) whenever the data changes than to serve up the same data a thousand
   times a second.

   Suppose you did need to serve up-to-date data to a million clients (like GMail, does). Should those
   clients be pushing or pulling? It depends. Either way, you won't be able to run it all from a single
   server, so you'll need to farm it out to other components of the system. If you're pushing, that will
   likely mean sticking each push on an event queue and having a horde of event processors each pulling
   from the queue and sending out your pushes. If you're pulling, that will mean standing up a bunch
   (say, a hundred) of fast read-replica cache servers that will sit in front of your main
   application and handle all the read traffic.

Hot paths
   When you're designing a system, there are lots of different ways users can interact with it or data
   can flow through it. It can get a bit overwhelming. The trick is to mainly focus on the "hot paths":
   the part of the system that is most critically important, and the part of the system that is going to
   handle the most data. For instance, in a metered billing system, those pieces might be the part that
   decides whether or not a customer gets charged, and the part that needs to hook into all user actions
   on the platform to identify how much to charge.

   Hot paths are important because they have fewer possible solutions than other design areas. There are
   a thousand ways you can build a billing settings page and they'll all mainly work. But there might be
   only a handful of ways that you can sensibly consume the firehose of user actions. Hot paths also go
   wrong more spectacularly. You have to really screw up a settings page to take down the entire
   product, but any code you write that's triggered on all user actions can easily cause huge problems.

Logging and metrics
   How do you know if you've got problems? One thing I've learned from my most paranoid colleagues is to
   log aggressively during unhappy paths. If you're writing a function that checks a bunch of conditions
   to see if a user-facing endpoint should respond 422, you should log out the condition that was hit.
   If you're writing billing code, you should log every decision made (e.g. "we're not billing for this
   event because of X"). Many engineers don't do this because it adds a bunch of logging boilerplate and
   makes it hard to write beautifully elegant code, but you should do it anyway. You'll be happy you did
   when an important customer is complaining that they're getting a 422 - even if that customer did
   something wrong, you still need to figure out what they did wrong for them.

   You should also have basic observability into the operational parts of the system. That means
   CPU/memory on the hosts or containers, queue sizes, average time per-request or per-job, and so on.
   For user-facing metrics like time per-request, you also need to watch the p95 and p99 (i.e. how slow
   your slowest requests are). Even one or two very slow requests are scary, because they're
   disproportionately from your largest and most important users. If you're just looking at averages,
   it's easy to miss the fact that some users are finding your service unusable.

Killswitches, retries, and failing gracefully
   I wrote a [https://www.seangoedecke.com/killswitches]whole post about killswitches that I won't repeat
   here, but the gist is that you should think carefully about what happens when the system fails badly.

   Retries are not a magic bullet. You need to make sure you're not putting extra load on other services
   by blindly retrying failed requests. If you can, put high-volume API calls inside a "circuit
   breaker": if you get too many 5xx responses in a row, stop sending requests for a while to let the
   service recover. You also need to make sure you're not retrying write events that may or may not have
   succeeded (for instance, if you send a "bill this user" request and get back a 5xx, you don't know if
   the user has been billed or not). The classic solution to this is to use an "idempotency key", which
   is a special UUID in the request that the other service uses to avoid re-running old requests: every
   time they do something, they save the idempotency key, and if they get another request with the same
   key, they silently ignore it.

   It's also important to decide what happens when part of your system fails. For instance, say you have
   some rate limiting code that checks a Redis bucket to see if a user has made too many requests in the
   current window. What happens when that Redis bucket is unavailable? You have two options: fail open
   and let the request through, or fail closed and block the request with a 429.

   Whether you should fail open or closed depends on the specific feature. In my view, a rate limiting
   system should almost always fail open. That means that a problem with the rate limiting code isn't
   necessarily a big user-facing incident. However, auth should (obviously) always fail closed: it's
   better to deny a user access to their own data than to give a user access to some other user's data.
   There are a lot of cases where it's not clear what the right behavior is. It's often a difficult
   tradeoff.

Final thoughts
   There are some topics I'm deliberately not covering here. For instance, whether or when to split your
   monolith out into different services, when to use containers or VMs, tracing, good API design. Partly
   this is because I don't think it matters that much (in my experience, monoliths are fine), or because
   I think it's too obvious to talk about (you should use tracing), or because I just don't have the
   time (API design is complicated).

   The main point I'm trying to make is what I said at the start of this post: good system design is not
   about clever tricks, it's about knowing how to use boring, well-tested components in the right place.
   I'm not a plumber, but I imagine good plumbing is similar: if you're doing something too exciting,
   you're probably going to end up with crap all over yourself.

   Especially at large tech companies, where these components already exist off the shelf (i.e. your
   company already has some kind of event bus, caching service, etc), good system design is going to
   look like nothing. There are very, very few areas where you want to do the kind of system design you
   could talk about at a conference. They do exist! I have seen hand-rolled data structures make
   features possible that wouldn't have been possible otherwise. But I've only seen that happen once or
   twice in ten years. I see boring system design every single day.


---
[**2]
https://www.seangoedecke.com/great-software-design/

Great software design looks underwhelming

   Years ago I spent a lot of time reviewing coding challenges. The challenge itself was very
   straightforward - building a CLI tool that hit an API and allowed the user to page through and
   inspect the data. We allowed any language, so I saw all kinds of approaches. At one point I came
   across a challenge I thought was literally perfect. It was a single Python file (maybe thirty lines
   of code in total), written in a very workmanlike style: the simplest, most straightforward way to
   meet the challenge requirements.

   When I sent it to another reviewer, suggesting that we use this as a reference point for what a 10/10
   looked like, I was genuinely shocked to hear from them that they wouldn't have passed that challenge
   through to an interview. According to them, it didn't demonstrate enough understanding of
   sophisticated language features. It was too simple.

   Years later, I'm even more convinced that I was right and that reviewer was wrong. Great software
   design is supposed to be too simple. I think now I can finally begin to articulate why.

Eliminating risk
   Every software system has a lot of things that can go wrong. Sometimes these are called "failure
   modes" of the system. Here's a sample:
     * SSL certificates expire and aren't renewed
     * Database fills up and becomes too slow or out of memory
     * User data gets overwritten or corrupted
     * Users see a broken UI experience
     * Core user flows (e.g. saving records) fail to work

   There are two ways of designing around a potential failure mode. The first is to be reactive: adding
   rescue clauses around risky blocks of code, making sure failed API requests are retried, setting up
   graceful degradation so errors don't blow up the whole experience, adding logging and metrics so bugs
   can be easily identified, and so on. This is worth doing. In fact, I believe this kind of (frankly
   paranoid) attitude is the mark of an experienced software engineer. But working like this is not a
   mark of good design. It's often a signal that you're papering over flaws in a bad design.

   The second way to handle potential failure modes is to design them out of existence. What does that
   mean in practice?

Protecting the hot paths
   Sometimes it means moving components out of the hot path. I once worked on a catalog endpoint that
   (due to other design choices) was extremely inefficient, in the order of ~200ms per record. This
   exposed us to a few nasty failure modes: resource starvation for the rest of the app, proxy timeouts
   on index requests, and users just giving up after waiting ten seconds for a response. We ended up
   moving the endpoint construction code into a cron job, sticking the results in blob storage, and
   having the catalog endpoint serve the blob. We still had the nasty 200ms-per-record code, but it was
   now under our control: it couldn't be triggered by user actions, and if it failed the worst-case
   scenario is we'd just serve a stale blob.

Removing components
   Sometimes it means using fewer components altogether. Another service I worked on was a documentation
   CRM that had a really bespoke system for pulling various bits of docs out of different repositories
   and stitching them together into database entries (sometimes pulling docs directly out of code
   comments). This was originally a good decision - at the time, it was hard to get teams to write any
   kind of docs at all, so the system had to be maximally flexible. But as the company grew, it was very
   much showing its age. The sync job stored some state in the database and some state on disk, and
   often triggered strange git errors when the state on disk got out of sync or the underlying host ran
   out of memory. We ended up removing the database entirely, shifting all the docs into a central
   repository, and reworking the documentation page as a normal static site. All kinds of possible
   runtime and operational bugs were removed, just like that.

Centralizing state
   Sometimes it means normalizing your state. One of the worst kinds of failure mode are bugs that leave
   your state (e.g. your database rows) in an inconsistent or corrupted state: one table says one thing,
   but another table says differently. This is bad because fixing the bug is only the start of the work.
   You have to go in and repair all the damaged records, which can involve some detective work to figure
   out what the right value ought to be (or in the worst case, guessing). Designing so that the crucial
   parts of your state have a single source of truth is often worth taking on a lot of other pain.

Using robust systems
   Sometimes it means relying on battle-tested systems. My favourite example for this is the Ruby
   webserver Unicorn. It's the most straightforward, unsophisticated way you could possibly build a
   webserver on top of Linux. First, you take a server process that listens on a socket and handles one
   request at a time. Handling one request at a time won't scale: incoming requests will queue up on the
   socket faster than the server can clear them. So what do you do? You fork that server process a
   bunch. Because of the way fork works, each child process is already listening on the original socket,
   so standard Linux socket logic handles spreading requests evenly between your server processes. If
   anything goes wrong, you can kill the child process and instantly fork off another one.

   Some people think it's a bit silly to like Unicorn so much because it's obviously less scalable than
   a threaded server. But I love it for two reasons. First, because it hands off so much work to the
   process and socket Linux primitives. That's smart because they're ultra-reliable. Second, because
   it's really, really hard for a Unicorn worker to do anything nasty to another Unicorn worker. Process
   isolation is a lot more reliable than thread isolation. That's why Unicorn is the chosen webserver
   for most big Rails companies: Shopify, GitHub, Zendesk, and so on. Great software design doesn't mean
   that your software is ultra-performant. It means that it's a good fit for the task.

Summary
   Great software design looks simple because it eliminates as many failure modes as possible during the
   design stage. The best way to eliminate a failure mode is to not do something exciting (or if you
   can, not do anything at all).

   Not all failure modes are created equal. You want to try hardest to eliminate the really scary ones
   (like data inconsistency), even if it means making slightly clunky choices elsewhere.

   These are all relatively boring, unsexy ideas. But great software design is boring and unsexy. It's
   easy to get excited about big ideas like CQRS or microservices or service meshes. Great software
   design doesn't look like big exciting ideas. Most of the time it doesn't look like anything at all.


---

