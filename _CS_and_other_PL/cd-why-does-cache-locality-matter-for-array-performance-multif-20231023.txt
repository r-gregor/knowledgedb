filename: cd_why-does-cache-locality-matter-for-array-performance_multif_20231023.txt
https://stackoverflow.com/questions/12065774/why-does-cache-locality-matter-for-array-performance

Why does cache locality matter for array performance?

   In the following [http://www.geeksforgeeks.org/archives/10245]blog there is a statement about the
   advantage of arrays over linked lists:
     Arrays have better cache locality that can make a pretty big difference in performance.

   What does that mean? I don't understand how cache locality can provide a huge performance benefit.

***
       If you understand how cache works, then you'll also understand 1) "Locality of Reference" is
       a Good Thing, and 2) accessing data from arrays is usually more likely to have good "locality"
       than accessing the same data from a list.
     * 1
       One thing worth noting is that while this is true, a singly-linked list combined with a
       contiguous allocator can be an enormous asset, mainly because transferring elements from one
       container to another just involves pointer logic. If you look at the memory layout of those,
       however, it's contiguous and looks like an array with just links to the next element in the
       array, and so it's still cache-friendly (at least until the list is all reorganized).

***
   See my answer [https://stackoverflow.com/a/7812002/950912]about spatial and temporal locality.

   In particular, arrays are contiguous memory blocks, so large chunks of them will be loaded into the
   cache upon first access. This makes it comparatively quick to access future elements of the array.
   Linked lists on the other hand aren't necessarily in contiguous blocks of memory, and could lead to
   more cache misses, which increases the time it takes to access them.

   Consider the following possible memory layouts for an array data and linked list l_data of large
   structs
Address      Contents       | Address      Contents
ffff 0000    data[0]        | ffff 1000    l_data
ffff 0040    data[1]        |   ....
ffff 0080    data[2]        | ffff 3460    l_data->next
ffff 00c0    data[3]        |   ....
ffff 0100    data[4]        | ffff 8dc0    l_data->next->next
                            | ffff 8e00    l_data->next->next->next
                            |   ....
                            | ffff 8f00    l_data->next->next->next->next

   If we wanted to loop through this array, the first access to ffff 0000 would require us to go to
   memory to retrieve (a very slow operation in CPU cycles). However, after the first access the rest of
   the array would be in the cache, and subsequent accesses would be much quicker. With the linked list,
   the first access to ffff 1000 would also require us to go to memory. Unfortunately, the processor
   will cache the memory directly surrounding this location, say all the way up to ffff 2000. As you can
   see, this doesn't actually capture any of the other elements of the list, which means that when we go
   to access l_data->next, we will again have to go to memory.

***
   Typically, when using an array you access items that are near each other. This is especially true
   when accessing an array sequentially.

   When you access memory, a chunks of it are cached at various levels. Cache locality refers to the
   likelihood of successive operations being in the cache and thus being faster. In an array, you
   maximize the chances of sequential element access being in the cache.

   With lists, by counter-example, there's no guarantee that items which appear sequentially in the list
   are actually arranged near eachother in memory. This means fewer cache hits, and degraded


---
https://cs.stackexchange.com/questions/71985/row-major-vs-column-major-order-2d-arrays-access-in-programming-languages

Row Major Vs Column Major Order: 2D arrays access in programming languages

...

Both Row major and column major order takes same amount of time. Since both have same number of operations.
i.e. Lets take A[m][n]. Then the corresponding row major and column major formulas are :

    row major = [base address + (i*n + j) * size of the data_type].
    Column major = [base address + (j*m + i) * size of the data_type].

From the above formulas we can define that both have 4 operations to perform. So no factor affects the time
of accessing the data.

***
    Your answer is correct if memory access takes O(1) irrespective of the underlying structure used for
	data storage but that's not the case always when you are working at low level. Do remember storing data at
	continuous memory will always provide must faster lookup than when the data is stored at different memory
	locations.


---
https://www.geeksforgeeks.org/performance-analysis-of-row-major-and-column-major-order-of-storing-arrays-in-c/

Performance analysis of Row major and Column major order of iterating arrays in C

   In computing, row-major order and column-major order are methods for storing multidimensional arrays
   in linear storage such as random access memory.
   The two mentioned ways differ from each other with respect to the order in which elements are stored
   contiguously in the memory. The elements in row-major order are arranged consecutively along the row
   and that in the column-major order are arranged consecutively along the column. While the terms
   allude to the rows and columns of a two-dimensional array, i.e. a matrix, the orders can be
   generalized to arrays of any dimension by noting that the terms row-major and column-major are
   equivalent to lexicographic and lexicographic orders, respectively.
   In C (and many other languages like C++, Java etc), 2-D arrays are stored in row-major order.(though
   Pascal and Fortran follows column major order)

   This program DOES NOT illustrates that row major order storing of arrays in C is more efficient than
   column-major order.

   It only shows that in C language, 2-D arrays are stored in row major order and thus iterating its
   elements in a row major order is more efficient.

   In languages like Pascal and Fortran, iterating by column major order will be more efficient because
   2-D arrays are stored in column major order there.

   The reason for this is explained properly here
   https://cs.stackexchange.com/questions/71985/row-major-vs-column-major-order-2d-arrays-access-in-prog
   ramming-languages.


#include <stdio.h>
#include <time.h>
int m[999][999];
//Taking both dimensions same so that while running the loops,
//number of operations (comparisons, iterations, initializations)
//are exactly the same. Refer this for more
// https://www.geeksforgeeks.org/a-nested-loop-puzzle/

void main() {
	int i, j;
	clock_t start, stop;
	double d = 0.0;

	start = clock();
	for (i = 0; i < 999; i++)
		for (j = 0; j < 999; j++)
			m[i][j] = m[i][j] + (m[i][j] * m[i][j]);

	stop = clock();
	d = (double)(stop - start) / CLOCKS_PER_SEC;
	printf("The run-time of row major order is %lf\n", d);

	start = clock();
	for (j = 0; j < 999; j++)
		for (i = 0; i < 999; i++)
			m[i][j] = m[i][j] + (m[i][j] * m[i][j]);

	stop = clock();
	d = (double)(stop - start) / CLOCKS_PER_SEC;
	printf("The run-time of column major order is %lf", d);
}


   Output:
The run-time of row major order is 0.067300
The run-time of column major order is 0.136622


   Time Complexity: O(999*999), as we are using nested loops to traverse 999*999 times.

   Auxiliary Space: O(999*999), as we are  using extra space for matrix.


---

